{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "european-northern",
   "metadata": {},
   "source": [
    "# Set Client Margin "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "gross-category",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "\n",
    "#For UPS client freight & accessorial discounts, write \"YES\"/\"NO\" for rounding/not rounding to 2 decimal places.\n",
    "Round_Discount = \"NO\" \n",
    "\n",
    "BarrettDisc_Fsc_FedEx = 0.35 # change Fsc discount to 0 for customer quote\n",
    "BarrettDisc_Fsc_UPS = 0.2  # change Fsc discount to 0 for customer quote\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "indian-nothing",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set UPS margin for the client\n",
    "Margin_on_Min_GRND_UPS = 0.0\n",
    "Margin_on_Min_AIR_UPS = 0.0\n",
    "\n",
    "Margin_on_UPS_Parcel_Freight_GRND = 0.0\n",
    "Margin_on_UPS_Parcel_Freight_AIR = 0.0\n",
    "\n",
    "Margin_on_Value_Add_GRND_UPS = 0.0\n",
    "Margin_on_Value_Add_AIR_UPS = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "45aebb31-552b-4a79-9037-8eea17828d8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set FedEx margin for the client\n",
    "Margin_on_Min_GRND_FedEx = 0.0\n",
    "Margin_on_Min_AIR_FedEx = 0.0\n",
    "\n",
    "Margin_on_FedEx_Parcel_Freight_GRND = 0.0\n",
    "Margin_on_FedEx_Parcel_Freight_AIR = 0.0\n",
    "\n",
    "Margin_on_Value_Add_GRND_FedEx = 0.0\n",
    "Margin_on_Value_Add_AIR_FedEx = 0.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "994da850-d443-4ccd-960c-f111a2897580",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set DHL, USPS, and MI margin for the client\n",
    "Margin_on_DHL = 0.0\n",
    "Margin_on_USPS = 0.0\n",
    "Margin_on_MI = 0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "separate-species",
   "metadata": {},
   "source": [
    "# Carrier Dim Factor 2023"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "optimum-century",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dim factor 2023\n",
    "dim_factor_ups = 280\n",
    "dim_factor_ups_srpt = 139\n",
    "dim_factor_dhl = 166\n",
    "dim_factor_usps = 166\n",
    "dim_factor_mi = 166\n",
    "dim_factor_fedex = 300 # 280 for customer, 300 for Barrett\n",
    "dim_factor_fedex_spst = 999 # 300 if cubic inch >= 3456, no quote when billed weight > 9 lbs, rate sheet has up to 70lbs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbbe6deb",
   "metadata": {},
   "source": [
    "# Set the Year based on PLD \n",
    "This quote can only apply to 1 year at one time. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "cecb090f",
   "metadata": {},
   "outputs": [],
   "source": [
    "nqd_dhl_date_1 = '2024-01-01'\n",
    "nqd_dhl_date_2 = '2024-09-30'\n",
    "nqd_dhl_date_3 = '2024-10-01'\n",
    "nqd_dhl_date_4 = '2024-12-31'\n",
    "nqd_dhl_date_1 = pd.to_datetime(nqd_dhl_date_1)\n",
    "nqd_dhl_date_2 = pd.to_datetime(nqd_dhl_date_2)\n",
    "nqd_dhl_date_3 = pd.to_datetime(nqd_dhl_date_3)\n",
    "nqd_dhl_date_4 = pd.to_datetime(nqd_dhl_date_4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vocational-unemployment",
   "metadata": {},
   "source": [
    "# ======Model Structure======"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "collect-nickname",
   "metadata": {},
   "source": [
    "Source tables:\n",
    "1. carrierRateCard_2023: published freight rate card (insert newer version once a year)\n",
    "\n",
    "   1-1. UPSAccessorial_2023: UPS accessorial rate card. (insert newer version once a year)\n",
    "\n",
    "\n",
    "2. areaSurchargeZipsUs_2023: a table to assign the type of DAS Category (insert newer version once a year)\n",
    "\n",
    "3. FSC: fuel surcharge rate (insert a new number once a week before automation)\n",
    "\n",
    "4. zip_Outbound: after decided the origin facility, get zone from destination zip \n",
    "\n",
    "5. ZiptoState: get state from destination zip to decide the origin facility\n",
    "    \n",
    "6. PLD: for rerate\n",
    "\n",
    "Structure:\n",
    "\n",
    "1 - a. Create Freight Rate Card (tbl_PublishedRates)  \n",
    "1 - b. Create Freight Rate Card (tbl_BarrettRates)  \n",
    "1 - c. Create Freight Rate Card (tbl_clientRates)\n",
    "\n",
    "2 - a. Create UPS accessorial rates  \n",
    "2 - b. Prepare all other rates for rerate\n",
    "\n",
    "3 - a. Prepare PLD for rerate: Load, Clean, and Format PLD  \n",
    "3 - b. Rerate Begin  \n",
    "3 - c. Output and analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sitting-testament",
   "metadata": {},
   "source": [
    "# Step 1 - a & 1 - b\n",
    "*rate card: Add UPS published rates, DHL Barrett rates, and USPS Barrett rates. Needs to renew once a year. Can add more carriers later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "powerful-niger",
   "metadata": {},
   "outputs": [],
   "source": [
    "# in the back up material section at the end\n",
    "tbl_BarrettRates = pd.read_csv('C:\\\\Users\\\\SLiu\\\\BD\\\\model\\\\RateQuote\\\\tbl_BarrettRates.csv')\n",
    "tbl_publishedRates = pd.read_csv('C:\\\\Users\\\\SLiu\\\\BD\\\\model\\\\RateQuote\\\\tbl_publishedRates.csv')\n",
    "tbl_BarrettRates['ZONE'] = tbl_BarrettRates['ZONE'].astype(str)\n",
    "tbl_BarrettRates['VERSION'] = tbl_BarrettRates['VERSION'].astype(str)\n",
    "tbl_publishedRates['ZONE'] = tbl_publishedRates['ZONE'].astype(str)\n",
    "tbl_publishedRates['VERSION'] = tbl_publishedRates['VERSION'].astype(str)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "psychological-destination",
   "metadata": {},
   "source": [
    "# Step 1 - c. Create Freight Rate Card (tbl_clientRates)\n",
    "*Add ups, dhl, and usps clientRates in tbl_publishedRates. Read the margins located at the top of this model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "photographic-people",
   "metadata": {},
   "outputs": [],
   "source": [
    "# UPSAccessorial_2023 has 2 tabs, UPS_Rates and UPS_Accessorial. UPS_Rates is used to create Barrett Rates\n",
    "xls = pd.ExcelFile('Accessorial_2024.xlsx')\n",
    "UPS_Rates = pd.read_excel(xls, 'UPS_Rates')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "upper-trick",
   "metadata": {},
   "outputs": [],
   "source": [
    "#add 2 columns to this table with min and max weight\n",
    "def f(col):\n",
    "    if col['WEIGHT_BREAK'] == '1-5lb':\n",
    "        return 5\n",
    "    elif col['WEIGHT_BREAK'] == '6-10lb':\n",
    "        return 10\n",
    "    elif col['WEIGHT_BREAK'] == '11-20lb':\n",
    "        return 20\n",
    "    elif col['WEIGHT_BREAK'] == '21-30lb':\n",
    "        return 30\n",
    "    elif (col['WEIGHT_BREAK'] == '31+lb'):\n",
    "        return 999\n",
    "    elif (col['SERVICE_LEVEL'] == 'SurePost - Less than 1lb') & (col['WEIGHT_BREAK'] == '1-16oz'):\n",
    "        return 16\n",
    "    elif (col['SERVICE_LEVEL'] == 'SurePost - Greater than 1lb') & (col['WEIGHT_BREAK'] == '1-3lb'):\n",
    "        return 3\n",
    "    elif (col['SERVICE_LEVEL'] == 'SurePost - Greater than 1lb') & (col['WEIGHT_BREAK'] == '4-6lb'):\n",
    "        return 6\n",
    "    elif (col['SERVICE_LEVEL'] == 'SurePost - Greater than 1lb') & (col['WEIGHT_BREAK'] == '7-8lb'):\n",
    "        return 8\n",
    "    elif (col['SERVICE_LEVEL'] == 'SurePost - Greater than 1lb') & (col['WEIGHT_BREAK'] == '9lb'):\n",
    "        return 9  \n",
    "\n",
    "def g(col):\n",
    "    if col['WEIGHT_BREAK'] == '1-5lb':\n",
    "        return 1\n",
    "    elif col['WEIGHT_BREAK'] == '6-10lb':\n",
    "        return 6\n",
    "    elif col['WEIGHT_BREAK'] == '11-20lb':\n",
    "        return 11\n",
    "    elif col['WEIGHT_BREAK'] == '21-30lb':\n",
    "        return 21\n",
    "    elif col['WEIGHT_BREAK'] == '31+lb':\n",
    "        return 31\n",
    "    elif (col['SERVICE_LEVEL'] == 'SurePost - Less than 1lb') & (col['WEIGHT_BREAK'] == '1-16oz'):\n",
    "        return 1\n",
    "    elif (col['SERVICE_LEVEL'] == 'SurePost - Greater than 1lb') & (col['WEIGHT_BREAK'] == '1-3lb'):\n",
    "        return 1\n",
    "    elif (col['SERVICE_LEVEL'] == 'SurePost - Greater than 1lb') & (col['WEIGHT_BREAK'] == '4-6lb'):\n",
    "        return 4\n",
    "    elif (col['SERVICE_LEVEL'] == 'SurePost - Greater than 1lb') & (col['WEIGHT_BREAK'] == '7-8lb'):\n",
    "        return 7\n",
    "    elif (col['SERVICE_LEVEL'] == 'SurePost - Greater than 1lb') & (col['WEIGHT_BREAK'] == '9lb'):\n",
    "        return 9 \n",
    "\n",
    "UPS_Rates['WEIGHT_LOWER'] = UPS_Rates.apply(g, axis=1)\n",
    "UPS_Rates['WEIGHT_UPPER'] = UPS_Rates.apply(f, axis=1)\n",
    "\n",
    "\n",
    "# add client margin mins column\n",
    "def h(col):   \n",
    "    if col['SERVICE_LEVEL'] == 'Next Day Air Early AM':\n",
    "        return col['MIN_WITH_REBATE']\n",
    "    \n",
    "    elif (col['SERVICE_LEVEL'] == 'Next Day Air') | (col['SERVICE_LEVEL'] == 'Next Day Air Saver') | (col['SERVICE_LEVEL'] == '2nd A.M. Day Air') | (col['SERVICE_LEVEL'] == '2nd Day Air') | (col['SERVICE_LEVEL'] == '3 Day Select'): \n",
    "        try: \n",
    "            return min(col['MIN_WITH_REBATE'] + 4, col['MIN_WITH_REBATE'] / (1-Margin_on_Min_AIR_UPS))\n",
    "        except ValueError:\n",
    "            return 0\n",
    "\n",
    "    elif (col['SERVICE_LEVEL'] == 'Ground Commercial') | (col['SERVICE_LEVEL'] == 'Ground Residential'): \n",
    "        try: \n",
    "            return min(10.7, col['MIN_WITH_REBATE'] / (1-Margin_on_Min_GRND_UPS))\n",
    "        except ValueError:\n",
    "            return 0\n",
    "\n",
    "    elif (col['SERVICE_LEVEL'] == 'SurePost - Greater than 1lb'): \n",
    "        try: \n",
    "            return min(9.1, col['MIN_WITH_REBATE'] / (1-Margin_on_Min_GRND_UPS))\n",
    "        except ValueError:\n",
    "            return 0\n",
    "        \n",
    "    elif (col['SERVICE_LEVEL'] == 'SurePost - Less than 1lb'): \n",
    "        try: \n",
    "            return min(8.86, col['MIN_WITH_REBATE'] / (1-Margin_on_Min_GRND_UPS))\n",
    "        except ValueError:\n",
    "            return 0\n",
    "  \n",
    "    elif col['SERVICE_LEVEL'] == 'Ground CWT': \n",
    "        try: \n",
    "            return min(80, col['MIN_WITH_REBATE'] / (1-Margin_on_Min_GRND_UPS))\n",
    "        except ValueError:\n",
    "            return 0\n",
    "  \n",
    "    elif (col['SERVICE_LEVEL'] == 'Standard to Canada') | (col['SERVICE_LEVEL'] == 'Standard to Mexico'):\n",
    "        try: \n",
    "            return min(col['MIN_WITH_REBATE'] + 4, col['MIN_WITH_REBATE'] / (1-Margin_on_Min_GRND_UPS))\n",
    "        except ValueError:\n",
    "            return 0\n",
    "\n",
    "UPS_Rates['CLIENT_MARGIN_MINS'] = UPS_Rates.apply(h, axis=1)\n",
    "\n",
    "# add ADD_MARGIN_(COL_M/DECIMAL) column\n",
    "UPS_Rates['DISCOUNT_RECIPROCAL_(BDC_PAY_ON_THE_DOLLAR)'] = 1 - UPS_Rates['NET_CONTRACT_DISC']\n",
    "UPS_Rates.loc[UPS_Rates['SERVICE_LEVEL'] == 'Next Day Air Early AM', 'DISCOUNT_RECIPROCAL_(BDC_PAY_ON_THE_DOLLAR)'] = None\n",
    "\n",
    "def i(col):   \n",
    "    if (col['SERVICE_LEVEL'] == 'SurePost - Greater than 1lb') | (col['WEIGHT_BREAK'] == '9lb'):\n",
    "        return col['DISCOUNT_RECIPROCAL_(BDC_PAY_ON_THE_DOLLAR)'] / (1-Margin_on_UPS_Parcel_Freight_AIR)\n",
    "    elif col['SERVICE_LEVEL'] == 'Next Day Air Early AM':\n",
    "        return\n",
    "    elif (col['SERVICE_LEVEL'] == 'Ground Commercial') | (col['SERVICE_LEVEL'] == 'Ground Residential') | (col['SERVICE_LEVEL'] == 'SurePost - Less than 1lb') | (col['SERVICE_LEVEL'] == 'SurePost - Greater than 1lb'): \n",
    "        return col['DISCOUNT_RECIPROCAL_(BDC_PAY_ON_THE_DOLLAR)'] / (1-Margin_on_UPS_Parcel_Freight_GRND)\n",
    "    else:\n",
    "        return col['DISCOUNT_RECIPROCAL_(BDC_PAY_ON_THE_DOLLAR)'] / (1-Margin_on_UPS_Parcel_Freight_AIR)\n",
    "\n",
    "UPS_Rates['ADD_MARGIN_(COL_M/DECIMAL)'] = UPS_Rates.apply(i, axis=1) \n",
    "\n",
    "# add DISCOUNT_TO_CLIENT(1-COLN) column \n",
    "def j(col):   \n",
    "    if (col['SERVICE_LEVEL'] == 'Standard to Canada') | (col['SERVICE_LEVEL'] == 'Standard to Mexico'):\n",
    "        return max(0, 1 - col['ADD_MARGIN_(COL_M/DECIMAL)'])\n",
    "    elif col['SERVICE_LEVEL'] == 'Next Day Air Early AM':\n",
    "        return\n",
    "    else:\n",
    "        return 1 - col['ADD_MARGIN_(COL_M/DECIMAL)']\n",
    "\n",
    "UPS_Rates['DISCOUNT_TO_CLIENT(1-COLN)'] = UPS_Rates.apply(j, axis=1)\n",
    "\n",
    "\n",
    "# This is the left side of \"Rates\" tab from \"Customer Rate Contract Cards 2023\" doc\n",
    "# UPS_Rates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "adef52a4-6cb3-4194-b85d-f20ffdb0331b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# UPSAccessorial_2023 has 2 tabs, UPS_Rates and UPS_Accessorial. UPS_Rates is used to create Barrett Rates\n",
    "xls = pd.ExcelFile('Accessorial_2024.xlsx')\n",
    "FedEx_Rates = pd.read_excel(xls, 'FedEx_Rates')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "dbc47bf0-35e9-4db6-b70a-1401c9f7f8f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the value of the left side of \"Rates\" tab from \"Customer Rate Contract Cards 2023\" doc\n",
    "#add 2 columns to this table with min and max weight\n",
    "def f(col):\n",
    "    if col['WEIGHT_BREAK'] == '1-5lb':\n",
    "        return 5\n",
    "    elif col['WEIGHT_BREAK'] == '6-10lb':\n",
    "        return 10\n",
    "    elif col['WEIGHT_BREAK'] == '11-20lb':\n",
    "        return 20\n",
    "    elif col['WEIGHT_BREAK'] == '21-30lb':\n",
    "        return 30\n",
    "    elif (col['WEIGHT_BREAK'] == '31+lb'):\n",
    "        return 999\n",
    "    elif (col['SERVICE_LEVEL'] == 'FedEx SmartPost - Less than 1lb') & (col['WEIGHT_BREAK'] == '1-3oz'):\n",
    "        return 3\n",
    "    elif (col['SERVICE_LEVEL'] == 'FedEx SmartPost - Less than 1lb') & (col['WEIGHT_BREAK'] == '4-16oz'):\n",
    "        return 16\n",
    "    elif (col['SERVICE_LEVEL'] == 'FedEx SmartPost - Greater than 1lb') & (col['WEIGHT_BREAK'] == '1-2lbs'):\n",
    "        return 2\n",
    "    elif (col['SERVICE_LEVEL'] == 'FedEx SmartPost - Greater than 1lb') & (col['WEIGHT_BREAK'] == '3 lbs'):\n",
    "        return 3\n",
    "    elif (col['SERVICE_LEVEL'] == 'FedEx SmartPost - Greater than 1lb') & (col['WEIGHT_BREAK'] == '4-70lbs'):\n",
    "        return 70\n",
    "    elif (col['SERVICE_LEVEL'] == 'FedEx SmartPost - Greater than 1lb') & (col['WEIGHT_BREAK'] == '70+lbs'):\n",
    "        return 999  \n",
    "\n",
    "def g(col):\n",
    "    if col['WEIGHT_BREAK'] == '1-5lb':\n",
    "        return 1\n",
    "    elif col['WEIGHT_BREAK'] == '6-10lb':\n",
    "        return 6\n",
    "    elif col['WEIGHT_BREAK'] == '11-20lb':\n",
    "        return 11\n",
    "    elif col['WEIGHT_BREAK'] == '21-30lb':\n",
    "        return 21\n",
    "    elif col['WEIGHT_BREAK'] == '31+lb':\n",
    "        return 31\n",
    "    elif (col['SERVICE_LEVEL'] == 'FedEx SmartPost - Less than 1lb') & (col['WEIGHT_BREAK'] == '1-3oz'):\n",
    "        return 1\n",
    "    elif (col['SERVICE_LEVEL'] == 'FedEx SmartPost - Less than 1lb') & (col['WEIGHT_BREAK'] == '4-16oz'):\n",
    "        return 4\n",
    "    elif (col['SERVICE_LEVEL'] == 'FedEx SmartPost - Greater than 1lb') & (col['WEIGHT_BREAK'] == '1-2lbs'):\n",
    "        return 1\n",
    "    elif (col['SERVICE_LEVEL'] == 'FedEx SmartPost - Greater than 1lb') & (col['WEIGHT_BREAK'] == '3 lbs'):\n",
    "        return 3\n",
    "    elif (col['SERVICE_LEVEL'] == 'FedEx SmartPost - Greater than 1lb') & (col['WEIGHT_BREAK'] == '4-70lbs'):\n",
    "        return 4\n",
    "    elif (col['SERVICE_LEVEL'] == 'FedEx SmartPost - Greater than 1lb') & (col['WEIGHT_BREAK'] == '70+lbs'):\n",
    "        return 70\n",
    "\n",
    "FedEx_Rates['WEIGHT_LOWER'] = FedEx_Rates.apply(g, axis=1)\n",
    "FedEx_Rates['WEIGHT_UPPER'] = FedEx_Rates.apply(f, axis=1)\n",
    "\n",
    "\n",
    "# add client margin mins column\n",
    "def h(col):   \n",
    "    if col['SERVICE_LEVEL'] == 'FedEx First Overnight':\n",
    "        return col['MIN_WITH_REBATE']\n",
    "    \n",
    "    elif (col['SERVICE_LEVEL'] == 'FedEx Priority Overnight') | (col['SERVICE_LEVEL'] == 'FedEx Standard Overnight') | (col['SERVICE_LEVEL'] == 'FedEx 2nd Day AM') | (col['SERVICE_LEVEL'] == 'FedEx 2Day') | (col['SERVICE_LEVEL'] == 'FedEx Express Saver'): \n",
    "        try: \n",
    "            return min(col['MIN_WITH_REBATE'] + 4, col['MIN_WITH_REBATE'] / (1-Margin_on_Min_AIR_FedEx))\n",
    "        except ValueError:\n",
    "            return 0\n",
    "\n",
    "    elif (col['SERVICE_LEVEL'] == 'FedEx Ground') | (col['SERVICE_LEVEL'] == 'FedEx Home Delivery'): \n",
    "        try: \n",
    "            return min(10.7, col['MIN_WITH_REBATE'] / (1-Margin_on_Min_GRND_FedEx))\n",
    "        except ValueError:\n",
    "            return 0\n",
    "\n",
    "    elif (col['SERVICE_LEVEL'] == 'FedEx SmartPost - Less than 1lb'): \n",
    "        try: \n",
    "            return min(10.7, col['MIN_WITH_REBATE'] / (1-Margin_on_Min_GRND_FedEx))\n",
    "        except ValueError:\n",
    "            return 0\n",
    "    \n",
    "    elif (col['SERVICE_LEVEL'] == 'FedEx SmartPost - Greater than 1lb'): \n",
    "        try: \n",
    "            return min(10.7, col['MIN_WITH_REBATE'] / (1-Margin_on_Min_GRND_FedEx))\n",
    "        except ValueError:\n",
    "            return 0\n",
    "  \n",
    "    elif col['SERVICE_LEVEL'] == 'FedEx MWT': \n",
    "        try: \n",
    "            return min(80, col['MIN_WITH_REBATE'] / (1-Margin_on_Min_GRND_FedEx))\n",
    "        except ValueError:\n",
    "            return 0\n",
    "  \n",
    "    elif (col['SERVICE_LEVEL'] == 'FedEx Ground Canada') | (col['SERVICE_LEVEL'] == 'FedEx Ground Mexico'):\n",
    "        try: \n",
    "            return min(col['MIN_WITH_REBATE'] + 4, col['MIN_WITH_REBATE'] / (1-Margin_on_Min_GRND_FedEx))\n",
    "        except ValueError:\n",
    "            return 0\n",
    "\n",
    "FedEx_Rates['CLIENT_MARGIN_MINS'] = FedEx_Rates.apply(h, axis=1)\n",
    "\n",
    "# add ADD_MARGIN_(COL_M/DECIMAL) column\n",
    "FedEx_Rates['DISCOUNT_RECIPROCAL_(BDC_PAY_ON_THE_DOLLAR)'] = 1 - FedEx_Rates['NET_CONTRACT_DISC']\n",
    "FedEx_Rates.loc[FedEx_Rates['SERVICE_LEVEL'] == 'FedEx First Overnight', 'DISCOUNT_RECIPROCAL_(BDC_PAY_ON_THE_DOLLAR)'] = None\n",
    "\n",
    "def i(col):   \n",
    "    if (col['SERVICE_LEVEL'] == 'FedEx SmartPost - Greater than 1lb') | (col['WEIGHT_BREAK'] == '70lb'):\n",
    "        return col['DISCOUNT_RECIPROCAL_(BDC_PAY_ON_THE_DOLLAR)'] / (1-Margin_on_FedEx_Parcel_Freight_AIR)\n",
    "    elif col['SERVICE_LEVEL'] == 'FedEx First Overnight':\n",
    "        return\n",
    "    elif (col['SERVICE_LEVEL'] == 'FedEx Ground') | (col['SERVICE_LEVEL'] == 'FedEx Home Delivery') | (col['SERVICE_LEVEL'] == 'FedEx SmartPost - Less than 1lb') | (col['SERVICE_LEVEL'] == 'FedEx SmartPost - Greater than 1lb'): \n",
    "        return col['DISCOUNT_RECIPROCAL_(BDC_PAY_ON_THE_DOLLAR)'] / (1-Margin_on_FedEx_Parcel_Freight_GRND)\n",
    "    else:\n",
    "        return col['DISCOUNT_RECIPROCAL_(BDC_PAY_ON_THE_DOLLAR)'] / (1-Margin_on_FedEx_Parcel_Freight_AIR)\n",
    "\n",
    "FedEx_Rates['ADD_MARGIN_(COL_M/DECIMAL)'] = FedEx_Rates.apply(i, axis=1) \n",
    "\n",
    "# add DISCOUNT_TO_CLIENT(1-COLN) column \n",
    "def j(col):   \n",
    "    if (col['SERVICE_LEVEL'] == 'Standard to Canada') | (col['SERVICE_LEVEL'] == 'Standard to Mexico'):\n",
    "        return max(0, 1 - col['ADD_MARGIN_(COL_M/DECIMAL)'])\n",
    "    elif col['SERVICE_LEVEL'] == 'FedEx First Overnight':\n",
    "        return\n",
    "    else:\n",
    "        return 1 - col['ADD_MARGIN_(COL_M/DECIMAL)']\n",
    "\n",
    "FedEx_Rates['DISCOUNT_TO_CLIENT(1-COLN)'] = FedEx_Rates.apply(j, axis=1)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "dominican-lodging",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\SLiu\\AppData\\Local\\Temp\\ipykernel_7228\\2791379907.py:14: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  tbl_clientRates_1['CLIENT_RATES_1'] = tbl_clientRates_1['PUBLISHED_RATES']*(1-DISCOUNT_TO_CLIENT_1)\n",
      "C:\\Users\\SLiu\\AppData\\Local\\Temp\\ipykernel_7228\\2791379907.py:15: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  tbl_clientRates_1['CLIENT_RATES'] = round(tbl_clientRates_1['CLIENT_RATES_1'].apply(lambda x: max(x, CLIENT_MARGIN_MINS_1)),2)\n",
      "C:\\Users\\SLiu\\AppData\\Local\\Temp\\ipykernel_7228\\2791379907.py:27: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  tbl_clientRates_2['CLIENT_RATES_1'] = tbl_clientRates_2['PUBLISHED_RATES']*(1-DISCOUNT_TO_CLIENT_2)\n",
      "C:\\Users\\SLiu\\AppData\\Local\\Temp\\ipykernel_7228\\2791379907.py:28: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  tbl_clientRates_2['CLIENT_RATES'] = round(tbl_clientRates_2['CLIENT_RATES_1'].apply(lambda x: max(x, CLIENT_MARGIN_MINS_2)),2)\n",
      "C:\\Users\\SLiu\\AppData\\Local\\Temp\\ipykernel_7228\\2791379907.py:40: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  tbl_clientRates_3['CLIENT_RATES_1'] = tbl_clientRates_3['PUBLISHED_RATES']*(1-DISCOUNT_TO_CLIENT_3)\n",
      "C:\\Users\\SLiu\\AppData\\Local\\Temp\\ipykernel_7228\\2791379907.py:41: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  tbl_clientRates_3['CLIENT_RATES'] = round(tbl_clientRates_3['CLIENT_RATES_1'].apply(lambda x: max(x, CLIENT_MARGIN_MINS_3)),2)\n",
      "C:\\Users\\SLiu\\AppData\\Local\\Temp\\ipykernel_7228\\2791379907.py:53: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  tbl_clientRates_4['CLIENT_RATES_1'] = tbl_clientRates_4['PUBLISHED_RATES']*(1-DISCOUNT_TO_CLIENT_4)\n",
      "C:\\Users\\SLiu\\AppData\\Local\\Temp\\ipykernel_7228\\2791379907.py:54: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  tbl_clientRates_4['CLIENT_RATES'] = round(tbl_clientRates_4['CLIENT_RATES_1'].apply(lambda x: max(x, CLIENT_MARGIN_MINS_4)),2)\n",
      "C:\\Users\\SLiu\\AppData\\Local\\Temp\\ipykernel_7228\\2791379907.py:66: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  tbl_clientRates_5['CLIENT_RATES_1'] = tbl_clientRates_5['PUBLISHED_RATES']*(1-DISCOUNT_TO_CLIENT_5)\n",
      "C:\\Users\\SLiu\\AppData\\Local\\Temp\\ipykernel_7228\\2791379907.py:67: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  tbl_clientRates_5['CLIENT_RATES'] = round(tbl_clientRates_5['CLIENT_RATES_1'].apply(lambda x: max(x, CLIENT_MARGIN_MINS_5)),2)\n",
      "C:\\Users\\SLiu\\AppData\\Local\\Temp\\ipykernel_7228\\2791379907.py:80: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  tbl_clientRates_6['CLIENT_RATES_1'] = tbl_clientRates_6['PUBLISHED_RATES']*(1-DISCOUNT_TO_CLIENT_6)\n",
      "C:\\Users\\SLiu\\AppData\\Local\\Temp\\ipykernel_7228\\2791379907.py:81: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  tbl_clientRates_6['CLIENT_RATES'] = round(tbl_clientRates_6['CLIENT_RATES_1'].apply(lambda x: max(x, CLIENT_MARGIN_MINS_6)),2)\n",
      "C:\\Users\\SLiu\\AppData\\Local\\Temp\\ipykernel_7228\\2791379907.py:93: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  tbl_clientRates_7['CLIENT_RATES_1'] = tbl_clientRates_7['PUBLISHED_RATES']*(1-DISCOUNT_TO_CLIENT_7)\n",
      "C:\\Users\\SLiu\\AppData\\Local\\Temp\\ipykernel_7228\\2791379907.py:94: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  tbl_clientRates_7['CLIENT_RATES'] = round(tbl_clientRates_7['CLIENT_RATES_1'].apply(lambda x: max(x, CLIENT_MARGIN_MINS_7)),2)\n",
      "C:\\Users\\SLiu\\AppData\\Local\\Temp\\ipykernel_7228\\2791379907.py:106: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  tbl_clientRates_8['CLIENT_RATES_1'] = tbl_clientRates_8['PUBLISHED_RATES']*(1-DISCOUNT_TO_CLIENT_8)\n",
      "C:\\Users\\SLiu\\AppData\\Local\\Temp\\ipykernel_7228\\2791379907.py:107: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  tbl_clientRates_8['CLIENT_RATES'] = round(tbl_clientRates_8['CLIENT_RATES_1'].apply(lambda x: max(x, CLIENT_MARGIN_MINS_8)),2)\n",
      "C:\\Users\\SLiu\\AppData\\Local\\Temp\\ipykernel_7228\\2791379907.py:119: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  tbl_clientRates_9['CLIENT_RATES_1'] = tbl_clientRates_9['PUBLISHED_RATES']*(1-DISCOUNT_TO_CLIENT_9)\n",
      "C:\\Users\\SLiu\\AppData\\Local\\Temp\\ipykernel_7228\\2791379907.py:120: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  tbl_clientRates_9['CLIENT_RATES'] = round(tbl_clientRates_9['CLIENT_RATES_1'].apply(lambda x: max(x, CLIENT_MARGIN_MINS_9)),2)\n",
      "C:\\Users\\SLiu\\AppData\\Local\\Temp\\ipykernel_7228\\2791379907.py:132: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  tbl_clientRates_10['CLIENT_RATES_1'] = tbl_clientRates_10['PUBLISHED_RATES']*(1-DISCOUNT_TO_CLIENT_10)\n",
      "C:\\Users\\SLiu\\AppData\\Local\\Temp\\ipykernel_7228\\2791379907.py:133: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  tbl_clientRates_10['CLIENT_RATES'] = round(tbl_clientRates_10['CLIENT_RATES_1'].apply(lambda x: max(x, CLIENT_MARGIN_MINS_10)),2)\n",
      "C:\\Users\\SLiu\\AppData\\Local\\Temp\\ipykernel_7228\\2791379907.py:146: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  tbl_clientRates_11['CLIENT_RATES_1'] = tbl_clientRates_11['PUBLISHED_RATES']*(1-DISCOUNT_TO_CLIENT_11)\n",
      "C:\\Users\\SLiu\\AppData\\Local\\Temp\\ipykernel_7228\\2791379907.py:147: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  tbl_clientRates_11['CLIENT_RATES'] = round(tbl_clientRates_11['CLIENT_RATES_1'].apply(lambda x: max(x, CLIENT_MARGIN_MINS_11)),2)\n",
      "C:\\Users\\SLiu\\AppData\\Local\\Temp\\ipykernel_7228\\2791379907.py:159: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  tbl_clientRates_12['CLIENT_RATES_1'] = tbl_clientRates_12['PUBLISHED_RATES']*(1-DISCOUNT_TO_CLIENT_12)\n",
      "C:\\Users\\SLiu\\AppData\\Local\\Temp\\ipykernel_7228\\2791379907.py:160: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  tbl_clientRates_12['CLIENT_RATES'] = round(tbl_clientRates_12['CLIENT_RATES_1'].apply(lambda x: max(x, CLIENT_MARGIN_MINS_12)),2)\n",
      "C:\\Users\\SLiu\\AppData\\Local\\Temp\\ipykernel_7228\\2791379907.py:172: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  tbl_clientRates_13['CLIENT_RATES_1'] = tbl_clientRates_13['PUBLISHED_RATES']*(1-DISCOUNT_TO_CLIENT_13)\n",
      "C:\\Users\\SLiu\\AppData\\Local\\Temp\\ipykernel_7228\\2791379907.py:173: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  tbl_clientRates_13['CLIENT_RATES'] = round(tbl_clientRates_13['CLIENT_RATES_1'].apply(lambda x: max(x, CLIENT_MARGIN_MINS_13)),2)\n",
      "C:\\Users\\SLiu\\AppData\\Local\\Temp\\ipykernel_7228\\2791379907.py:185: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  tbl_clientRates_14['CLIENT_RATES_1'] = tbl_clientRates_14['PUBLISHED_RATES']*(1-DISCOUNT_TO_CLIENT_14)\n",
      "C:\\Users\\SLiu\\AppData\\Local\\Temp\\ipykernel_7228\\2791379907.py:186: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  tbl_clientRates_14['CLIENT_RATES'] = round(tbl_clientRates_14['CLIENT_RATES_1'].apply(lambda x: max(x, CLIENT_MARGIN_MINS_14)),2)\n",
      "C:\\Users\\SLiu\\AppData\\Local\\Temp\\ipykernel_7228\\2791379907.py:193: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  tbl_clientRates_15['CLIENT_RATES'] = tbl_clientRates_15['PUBLISHED_RATES']\n",
      "C:\\Users\\SLiu\\AppData\\Local\\Temp\\ipykernel_7228\\2791379907.py:204: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  tbl_clientRates_16['CLIENT_RATES_1'] = tbl_clientRates_16['PUBLISHED_RATES']*(1-DISCOUNT_TO_CLIENT_16)\n",
      "C:\\Users\\SLiu\\AppData\\Local\\Temp\\ipykernel_7228\\2791379907.py:205: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  tbl_clientRates_16['CLIENT_RATES'] = round(tbl_clientRates_16['CLIENT_RATES_1'].apply(lambda x: max(x, CLIENT_MARGIN_MINS_16)),2)\n",
      "C:\\Users\\SLiu\\AppData\\Local\\Temp\\ipykernel_7228\\2791379907.py:213: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  tbl_clientRates_17['CLIENT_RATES_1'] = tbl_clientRates_17['PUBLISHED_RATES']*(1-DISCOUNT_TO_CLIENT_17)\n",
      "C:\\Users\\SLiu\\AppData\\Local\\Temp\\ipykernel_7228\\2791379907.py:214: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  tbl_clientRates_17['CLIENT_RATES'] = round(tbl_clientRates_17['CLIENT_RATES_1'].apply(lambda x: max(x, CLIENT_MARGIN_MINS_17)),2)\n",
      "C:\\Users\\SLiu\\AppData\\Local\\Temp\\ipykernel_7228\\2791379907.py:222: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  tbl_clientRates_18['CLIENT_RATES_1'] = tbl_clientRates_18['PUBLISHED_RATES']*(1-DISCOUNT_TO_CLIENT_18)\n",
      "C:\\Users\\SLiu\\AppData\\Local\\Temp\\ipykernel_7228\\2791379907.py:223: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  tbl_clientRates_18['CLIENT_RATES'] = round(tbl_clientRates_18['CLIENT_RATES_1'].apply(lambda x: max(x, CLIENT_MARGIN_MINS_18)),2)\n",
      "C:\\Users\\SLiu\\AppData\\Local\\Temp\\ipykernel_7228\\2791379907.py:235: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  tbl_clientRates_19['CLIENT_RATES_1'] = tbl_clientRates_19['PUBLISHED_RATES']*(1-DISCOUNT_TO_CLIENT_19)\n",
      "C:\\Users\\SLiu\\AppData\\Local\\Temp\\ipykernel_7228\\2791379907.py:236: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  tbl_clientRates_19['CLIENT_RATES'] = round(tbl_clientRates_19['CLIENT_RATES_1'].apply(lambda x: max(x, CLIENT_MARGIN_MINS_19)),2)\n",
      "C:\\Users\\SLiu\\AppData\\Local\\Temp\\ipykernel_7228\\2791379907.py:248: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  tbl_clientRates_20['CLIENT_RATES_1'] = tbl_clientRates_20['PUBLISHED_RATES']*(1-DISCOUNT_TO_CLIENT_20)\n",
      "C:\\Users\\SLiu\\AppData\\Local\\Temp\\ipykernel_7228\\2791379907.py:249: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  tbl_clientRates_20['CLIENT_RATES'] = round(tbl_clientRates_20['CLIENT_RATES_1'].apply(lambda x: max(x, CLIENT_MARGIN_MINS_20)),2)\n",
      "C:\\Users\\SLiu\\AppData\\Local\\Temp\\ipykernel_7228\\2791379907.py:261: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  tbl_clientRates_21['CLIENT_RATES_1'] = tbl_clientRates_21['PUBLISHED_RATES']*(1-DISCOUNT_TO_CLIENT_21)\n",
      "C:\\Users\\SLiu\\AppData\\Local\\Temp\\ipykernel_7228\\2791379907.py:262: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  tbl_clientRates_21['CLIENT_RATES'] = round(tbl_clientRates_21['CLIENT_RATES_1'].apply(lambda x: max(x, CLIENT_MARGIN_MINS_21)),2)\n"
     ]
    }
   ],
   "source": [
    "#GRND ['WEIGHT_BREAK'] == '1-5lb'\n",
    "# slice the published rates\n",
    "tbl_clientRates_1 = tbl_publishedRates[(tbl_publishedRates['SERVICE_CODE'] == 'GRND') & (tbl_publishedRates['WEIGHT_LB'] <= 5)]\n",
    "\n",
    "# get DISCOUNT_TO_CLIENT(1-COLN) and CLIENT_MARGIN_MINS\n",
    "DISCOUNT_TO_CLIENT_1 = UPS_Rates.loc[(UPS_Rates['SERVICE_CODE'] == 'GRND') & (UPS_Rates['WEIGHT_BREAK'] == '1-5lb'), 'DISCOUNT_TO_CLIENT(1-COLN)'].values[0]\n",
    "if Round_Discount == 'YES':\n",
    "    DISCOUNT_TO_CLIENT_1 = round(DISCOUNT_TO_CLIENT_1,2)\n",
    "else:\n",
    "    DISCOUNT_TO_CLIENT_1 = DISCOUNT_TO_CLIENT_1\n",
    "CLIENT_MARGIN_MINS_1 = UPS_Rates.loc[(UPS_Rates['SERVICE_CODE'] == 'GRND') & (UPS_Rates['WEIGHT_BREAK'] == '1-5lb'), 'CLIENT_MARGIN_MINS'].values[0]\n",
    "\n",
    "# use max(published rates * (1-round(DISCOUNT_TO_CLIENT)), CLIENT_MARGIN_MINS) to get the client rate\n",
    "tbl_clientRates_1['CLIENT_RATES_1'] = tbl_clientRates_1['PUBLISHED_RATES']*(1-DISCOUNT_TO_CLIENT_1)\n",
    "tbl_clientRates_1['CLIENT_RATES'] = round(tbl_clientRates_1['CLIENT_RATES_1'].apply(lambda x: max(x, CLIENT_MARGIN_MINS_1)),2)\n",
    "tbl_clientRates_1 = tbl_clientRates_1.drop('CLIENT_RATES_1', axis=1)\n",
    "\n",
    "\n",
    "#GRND ['WEIGHT_BREAK'] == '6-10lb'\n",
    "tbl_clientRates_2 = tbl_publishedRates[(tbl_publishedRates['SERVICE_CODE'] == 'GRND') & (tbl_publishedRates['WEIGHT_LB'] <= 10) & (tbl_publishedRates['WEIGHT_LB'] >= 6)]\n",
    "DISCOUNT_TO_CLIENT_2 = UPS_Rates.loc[(UPS_Rates['SERVICE_CODE'] == 'GRND') & (UPS_Rates['WEIGHT_BREAK'] == '6-10lb'), 'DISCOUNT_TO_CLIENT(1-COLN)'].values[0]\n",
    "if Round_Discount == 'YES':\n",
    "    DISCOUNT_TO_CLIENT_2 = round(DISCOUNT_TO_CLIENT_2,2)\n",
    "else:\n",
    "    DISCOUNT_TO_CLIENT_2 = DISCOUNT_TO_CLIENT_2\n",
    "CLIENT_MARGIN_MINS_2 = UPS_Rates.loc[(UPS_Rates['SERVICE_CODE'] == 'GRND') & (UPS_Rates['WEIGHT_BREAK'] == '6-10lb'), 'CLIENT_MARGIN_MINS'].values[0]\n",
    "tbl_clientRates_2['CLIENT_RATES_1'] = tbl_clientRates_2['PUBLISHED_RATES']*(1-DISCOUNT_TO_CLIENT_2)\n",
    "tbl_clientRates_2['CLIENT_RATES'] = round(tbl_clientRates_2['CLIENT_RATES_1'].apply(lambda x: max(x, CLIENT_MARGIN_MINS_2)),2)\n",
    "tbl_clientRates_2 = tbl_clientRates_2.drop('CLIENT_RATES_1', axis=1)\n",
    "\n",
    "\n",
    "#GRND ['WEIGHT_BREAK'] == '11-20lb'\n",
    "tbl_clientRates_3 = tbl_publishedRates[(tbl_publishedRates['SERVICE_CODE'] == 'GRND') & (tbl_publishedRates['WEIGHT_LB'] <= 20) & (tbl_publishedRates['WEIGHT_LB'] >= 11)]\n",
    "DISCOUNT_TO_CLIENT_3 = UPS_Rates.loc[(UPS_Rates['SERVICE_CODE'] == 'GRND') & (UPS_Rates['WEIGHT_BREAK'] == '11-20lb'), 'DISCOUNT_TO_CLIENT(1-COLN)'].values[0]\n",
    "if Round_Discount == 'YES':\n",
    "    DISCOUNT_TO_CLIENT_3 = round(DISCOUNT_TO_CLIENT_3,2)\n",
    "else:\n",
    "    DISCOUNT_TO_CLIENT_3 = DISCOUNT_TO_CLIENT_3\n",
    "CLIENT_MARGIN_MINS_3 = UPS_Rates.loc[(UPS_Rates['SERVICE_CODE'] == 'GRND') & (UPS_Rates['WEIGHT_BREAK'] == '11-20lb'), 'CLIENT_MARGIN_MINS'].values[0]\n",
    "tbl_clientRates_3['CLIENT_RATES_1'] = tbl_clientRates_3['PUBLISHED_RATES']*(1-DISCOUNT_TO_CLIENT_3)\n",
    "tbl_clientRates_3['CLIENT_RATES'] = round(tbl_clientRates_3['CLIENT_RATES_1'].apply(lambda x: max(x, CLIENT_MARGIN_MINS_3)),2)\n",
    "tbl_clientRates_3 = tbl_clientRates_3.drop('CLIENT_RATES_1', axis=1)\n",
    "\n",
    "\n",
    "#GRND ['WEIGHT_BREAK'] == '21-30lb'\n",
    "tbl_clientRates_4 = tbl_publishedRates[(tbl_publishedRates['SERVICE_CODE'] == 'GRND') & (tbl_publishedRates['WEIGHT_LB'] <= 30) & (tbl_publishedRates['WEIGHT_LB'] >= 21)]\n",
    "DISCOUNT_TO_CLIENT_4 = UPS_Rates.loc[(UPS_Rates['SERVICE_CODE'] == 'GRND') & (UPS_Rates['WEIGHT_BREAK'] == '21-30lb'), 'DISCOUNT_TO_CLIENT(1-COLN)'].values[0]\n",
    "if Round_Discount == 'YES':\n",
    "    DISCOUNT_TO_CLIENT_4 = round(DISCOUNT_TO_CLIENT_4,2)\n",
    "else:\n",
    "    DISCOUNT_TO_CLIENT_4 = DISCOUNT_TO_CLIENT_4\n",
    "CLIENT_MARGIN_MINS_4 = UPS_Rates.loc[(UPS_Rates['SERVICE_CODE'] == 'GRND') & (UPS_Rates['WEIGHT_BREAK'] == '21-30lb'), 'CLIENT_MARGIN_MINS'].values[0]\n",
    "tbl_clientRates_4['CLIENT_RATES_1'] = tbl_clientRates_4['PUBLISHED_RATES']*(1-DISCOUNT_TO_CLIENT_4)\n",
    "tbl_clientRates_4['CLIENT_RATES'] = round(tbl_clientRates_4['CLIENT_RATES_1'].apply(lambda x: max(x, CLIENT_MARGIN_MINS_4)),2)\n",
    "tbl_clientRates_4 = tbl_clientRates_4.drop('CLIENT_RATES_1', axis=1)\n",
    "\n",
    "\n",
    "#GRND ['WEIGHT_BREAK'] == '31+lb'\n",
    "tbl_clientRates_5 = tbl_publishedRates[(tbl_publishedRates['SERVICE_CODE'] == 'GRND') & (tbl_publishedRates['WEIGHT_LB'] >= 31)]\n",
    "DISCOUNT_TO_CLIENT_5 = UPS_Rates.loc[(UPS_Rates['SERVICE_CODE'] == 'GRND') & (UPS_Rates['WEIGHT_BREAK'] == '31+lb'), 'DISCOUNT_TO_CLIENT(1-COLN)'].values[0]\n",
    "if Round_Discount == 'YES':\n",
    "    DISCOUNT_TO_CLIENT_5 = round(DISCOUNT_TO_CLIENT_5,2)\n",
    "else:\n",
    "    DISCOUNT_TO_CLIENT_5 = DISCOUNT_TO_CLIENT_5\n",
    "CLIENT_MARGIN_MINS_5 = UPS_Rates.loc[(UPS_Rates['SERVICE_CODE'] == 'GRND') & (UPS_Rates['WEIGHT_BREAK'] == '31+lb'), 'CLIENT_MARGIN_MINS'].values[0]\n",
    "tbl_clientRates_5['CLIENT_RATES_1'] = tbl_clientRates_5['PUBLISHED_RATES']*(1-DISCOUNT_TO_CLIENT_5)\n",
    "tbl_clientRates_5['CLIENT_RATES'] = round(tbl_clientRates_5['CLIENT_RATES_1'].apply(lambda x: max(x, CLIENT_MARGIN_MINS_5)),2)\n",
    "tbl_clientRates_5 = tbl_clientRates_5.drop('CLIENT_RATES_1', axis=1)\n",
    "\n",
    "\n",
    "#GRES \n",
    "#GRES ['WEIGHT_BREAK'] == '1-5lb'\n",
    "tbl_clientRates_6 = tbl_publishedRates[(tbl_publishedRates['SERVICE_CODE'] == 'GRES') & (tbl_publishedRates['WEIGHT_LB'] <= 5)]\n",
    "DISCOUNT_TO_CLIENT_6 = UPS_Rates.loc[(UPS_Rates['SERVICE_CODE'] == 'GRES') & (UPS_Rates['WEIGHT_BREAK'] == '1-5lb'), 'DISCOUNT_TO_CLIENT(1-COLN)'].values[0]\n",
    "if Round_Discount == 'YES':\n",
    "    DISCOUNT_TO_CLIENT_6 = round(DISCOUNT_TO_CLIENT_6,2)\n",
    "else:\n",
    "    DISCOUNT_TO_CLIENT_6 = DISCOUNT_TO_CLIENT_6\n",
    "CLIENT_MARGIN_MINS_6 = UPS_Rates.loc[(UPS_Rates['SERVICE_CODE'] == 'GRES') & (UPS_Rates['WEIGHT_BREAK'] == '1-5lb'), 'CLIENT_MARGIN_MINS'].values[0]\n",
    "tbl_clientRates_6['CLIENT_RATES_1'] = tbl_clientRates_6['PUBLISHED_RATES']*(1-DISCOUNT_TO_CLIENT_6)\n",
    "tbl_clientRates_6['CLIENT_RATES'] = round(tbl_clientRates_6['CLIENT_RATES_1'].apply(lambda x: max(x, CLIENT_MARGIN_MINS_6)),2)\n",
    "tbl_clientRates_6 = tbl_clientRates_6.drop('CLIENT_RATES_1', axis=1)\n",
    "\n",
    "\n",
    "#GRES ['WEIGHT_BREAK'] == '6-10lb'\n",
    "tbl_clientRates_7 = tbl_publishedRates[(tbl_publishedRates['SERVICE_CODE'] == 'GRES') & (tbl_publishedRates['WEIGHT_LB'] <= 10) & (tbl_publishedRates['WEIGHT_LB'] >= 6)]\n",
    "DISCOUNT_TO_CLIENT_7 = UPS_Rates.loc[(UPS_Rates['SERVICE_CODE'] == 'GRES') & (UPS_Rates['WEIGHT_BREAK'] == '6-10lb'), 'DISCOUNT_TO_CLIENT(1-COLN)'].values[0]\n",
    "if Round_Discount == 'YES':\n",
    "    DISCOUNT_TO_CLIENT_7 = round(DISCOUNT_TO_CLIENT_7,2)\n",
    "else:\n",
    "    DISCOUNT_TO_CLIENT_7 = DISCOUNT_TO_CLIENT_7\n",
    "CLIENT_MARGIN_MINS_7 = UPS_Rates.loc[(UPS_Rates['SERVICE_CODE'] == 'GRES') & (UPS_Rates['WEIGHT_BREAK'] == '6-10lb'), 'CLIENT_MARGIN_MINS'].values[0]\n",
    "tbl_clientRates_7['CLIENT_RATES_1'] = tbl_clientRates_7['PUBLISHED_RATES']*(1-DISCOUNT_TO_CLIENT_7)\n",
    "tbl_clientRates_7['CLIENT_RATES'] = round(tbl_clientRates_7['CLIENT_RATES_1'].apply(lambda x: max(x, CLIENT_MARGIN_MINS_7)),2)\n",
    "tbl_clientRates_7 = tbl_clientRates_7.drop('CLIENT_RATES_1', axis=1)\n",
    "\n",
    "\n",
    "#GRES ['WEIGHT_BREAK'] == '11-20lb'\n",
    "tbl_clientRates_8 = tbl_publishedRates[(tbl_publishedRates['SERVICE_CODE'] == 'GRES') & (tbl_publishedRates['WEIGHT_LB'] <= 20) & (tbl_publishedRates['WEIGHT_LB'] >= 11)]\n",
    "DISCOUNT_TO_CLIENT_8 = UPS_Rates.loc[(UPS_Rates['SERVICE_CODE'] == 'GRES') & (UPS_Rates['WEIGHT_BREAK'] == '11-20lb'), 'DISCOUNT_TO_CLIENT(1-COLN)'].values[0]\n",
    "if Round_Discount == 'YES':\n",
    "    DISCOUNT_TO_CLIENT_8 = round(DISCOUNT_TO_CLIENT_8,2)\n",
    "else:\n",
    "    DISCOUNT_TO_CLIENT_8 = DISCOUNT_TO_CLIENT_8\n",
    "CLIENT_MARGIN_MINS_8 = UPS_Rates.loc[(UPS_Rates['SERVICE_CODE'] == 'GRES') & (UPS_Rates['WEIGHT_BREAK'] == '11-20lb'), 'CLIENT_MARGIN_MINS'].values[0]\n",
    "tbl_clientRates_8['CLIENT_RATES_1'] = tbl_clientRates_8['PUBLISHED_RATES']*(1-DISCOUNT_TO_CLIENT_8)\n",
    "tbl_clientRates_8['CLIENT_RATES'] = round(tbl_clientRates_8['CLIENT_RATES_1'].apply(lambda x: max(x, CLIENT_MARGIN_MINS_8)),2)\n",
    "tbl_clientRates_8 = tbl_clientRates_8.drop('CLIENT_RATES_1', axis=1)\n",
    "\n",
    "\n",
    "#GRES ['WEIGHT_BREAK'] == '21-30lb'\n",
    "tbl_clientRates_9 = tbl_publishedRates[(tbl_publishedRates['SERVICE_CODE'] == 'GRES') & (tbl_publishedRates['WEIGHT_LB'] <= 30) & (tbl_publishedRates['WEIGHT_LB'] >= 21)]\n",
    "DISCOUNT_TO_CLIENT_9 = UPS_Rates.loc[(UPS_Rates['SERVICE_CODE'] == 'GRES') & (UPS_Rates['WEIGHT_BREAK'] == '21-30lb'), 'DISCOUNT_TO_CLIENT(1-COLN)'].values[0]\n",
    "if Round_Discount == 'YES':\n",
    "    DISCOUNT_TO_CLIENT_9 = round(DISCOUNT_TO_CLIENT_9,2)\n",
    "else:\n",
    "    DISCOUNT_TO_CLIENT_9 = DISCOUNT_TO_CLIENT_9\n",
    "CLIENT_MARGIN_MINS_9 = UPS_Rates.loc[(UPS_Rates['SERVICE_CODE'] == 'GRES') & (UPS_Rates['WEIGHT_BREAK'] == '21-30lb'), 'CLIENT_MARGIN_MINS'].values[0]\n",
    "tbl_clientRates_9['CLIENT_RATES_1'] = tbl_clientRates_9['PUBLISHED_RATES']*(1-DISCOUNT_TO_CLIENT_9)\n",
    "tbl_clientRates_9['CLIENT_RATES'] = round(tbl_clientRates_9['CLIENT_RATES_1'].apply(lambda x: max(x, CLIENT_MARGIN_MINS_9)),2)\n",
    "tbl_clientRates_9 = tbl_clientRates_9.drop('CLIENT_RATES_1', axis=1)\n",
    "\n",
    "\n",
    "#GRES ['WEIGHT_BREAK'] == '31+lb'\n",
    "tbl_clientRates_10 = tbl_publishedRates[(tbl_publishedRates['SERVICE_CODE'] == 'GRES') & (tbl_publishedRates['WEIGHT_LB'] >= 31)]\n",
    "DISCOUNT_TO_CLIENT_10 = UPS_Rates.loc[(UPS_Rates['SERVICE_CODE'] == 'GRES') & (UPS_Rates['WEIGHT_BREAK'] == '31+lb'), 'DISCOUNT_TO_CLIENT(1-COLN)'].values[0]\n",
    "if Round_Discount == 'YES':\n",
    "    DISCOUNT_TO_CLIENT_10 = round(DISCOUNT_TO_CLIENT_10,2)\n",
    "else:\n",
    "    DISCOUNT_TO_CLIENT_10 = DISCOUNT_TO_CLIENT_10\n",
    "CLIENT_MARGIN_MINS_10 = UPS_Rates.loc[(UPS_Rates['SERVICE_CODE'] == 'GRES') & (UPS_Rates['WEIGHT_BREAK'] == '31+lb'), 'CLIENT_MARGIN_MINS'].values[0]\n",
    "tbl_clientRates_10['CLIENT_RATES_1'] = tbl_clientRates_10['PUBLISHED_RATES']*(1-DISCOUNT_TO_CLIENT_10)\n",
    "tbl_clientRates_10['CLIENT_RATES'] = round(tbl_clientRates_10['CLIENT_RATES_1'].apply(lambda x: max(x, CLIENT_MARGIN_MINS_10)),2)\n",
    "tbl_clientRates_10 = tbl_clientRates_10.drop('CLIENT_RATES_1', axis=1)\n",
    "\n",
    "\n",
    "#SRPT\n",
    "#SRPT ['WEIGHT_BREAK'] == '1-3lb'\n",
    "tbl_clientRates_11 = tbl_publishedRates[(tbl_publishedRates['SERVICE_CODE'] == 'SRPT') & (tbl_publishedRates['WEIGHT_LB'] <= 3)]\n",
    "DISCOUNT_TO_CLIENT_11 = UPS_Rates.loc[(UPS_Rates['SERVICE_CODE'] == 'SRPT') & (UPS_Rates['WEIGHT_BREAK'] == '1-3lb'), 'DISCOUNT_TO_CLIENT(1-COLN)'].values[0]\n",
    "if Round_Discount == 'YES':\n",
    "    DISCOUNT_TO_CLIENT_11 = round(DISCOUNT_TO_CLIENT_11,2)\n",
    "else:\n",
    "    DISCOUNT_TO_CLIENT_11 = DISCOUNT_TO_CLIENT_11\n",
    "CLIENT_MARGIN_MINS_11 = UPS_Rates.loc[(UPS_Rates['SERVICE_CODE'] == 'SRPT') & (UPS_Rates['WEIGHT_BREAK'] == '1-3lb'), 'CLIENT_MARGIN_MINS'].values[0]\n",
    "tbl_clientRates_11['CLIENT_RATES_1'] = tbl_clientRates_11['PUBLISHED_RATES']*(1-DISCOUNT_TO_CLIENT_11)\n",
    "tbl_clientRates_11['CLIENT_RATES'] = round(tbl_clientRates_11['CLIENT_RATES_1'].apply(lambda x: max(x, CLIENT_MARGIN_MINS_11)),2)\n",
    "tbl_clientRates_11 = tbl_clientRates_11.drop('CLIENT_RATES_1', axis=1)\n",
    "\n",
    "\n",
    "#SRPT ['WEIGHT_BREAK'] == '4-6lb'\n",
    "tbl_clientRates_12 = tbl_publishedRates[(tbl_publishedRates['SERVICE_CODE'] == 'SRPT') & (tbl_publishedRates['WEIGHT_LB'] >= 4) & (tbl_publishedRates['WEIGHT_LB'] <= 6)]\n",
    "DISCOUNT_TO_CLIENT_12 = UPS_Rates.loc[(UPS_Rates['SERVICE_CODE'] == 'SRPT') & (UPS_Rates['WEIGHT_BREAK'] == '4-6lb'), 'DISCOUNT_TO_CLIENT(1-COLN)'].values[0]\n",
    "if Round_Discount == 'YES':\n",
    "    DISCOUNT_TO_CLIENT_12 = round(DISCOUNT_TO_CLIENT_12,2)\n",
    "else:\n",
    "    DISCOUNT_TO_CLIENT_12 = DISCOUNT_TO_CLIENT_12\n",
    "CLIENT_MARGIN_MINS_12 = UPS_Rates.loc[(UPS_Rates['SERVICE_CODE'] == 'SRPT') & (UPS_Rates['WEIGHT_BREAK'] == '4-6lb'), 'CLIENT_MARGIN_MINS'].values[0]\n",
    "tbl_clientRates_12['CLIENT_RATES_1'] = tbl_clientRates_12['PUBLISHED_RATES']*(1-DISCOUNT_TO_CLIENT_12)\n",
    "tbl_clientRates_12['CLIENT_RATES'] = round(tbl_clientRates_12['CLIENT_RATES_1'].apply(lambda x: max(x, CLIENT_MARGIN_MINS_12)),2)\n",
    "tbl_clientRates_12 = tbl_clientRates_12.drop('CLIENT_RATES_1', axis=1)\n",
    "\n",
    "\n",
    "#SRPT ['WEIGHT_BREAK'] == '7-8lb'\n",
    "tbl_clientRates_13 = tbl_publishedRates[(tbl_publishedRates['SERVICE_CODE'] == 'SRPT') & (tbl_publishedRates['WEIGHT_LB'] >= 7) & (tbl_publishedRates['WEIGHT_LB'] <= 8)]\n",
    "DISCOUNT_TO_CLIENT_13 = UPS_Rates.loc[(UPS_Rates['SERVICE_CODE'] == 'SRPT') & (UPS_Rates['WEIGHT_BREAK'] == '7-8lb'), 'DISCOUNT_TO_CLIENT(1-COLN)'].values[0]\n",
    "if Round_Discount == 'YES':\n",
    "    DISCOUNT_TO_CLIENT_13 = round(DISCOUNT_TO_CLIENT_13,2)\n",
    "else:\n",
    "    DISCOUNT_TO_CLIENT_13 = DISCOUNT_TO_CLIENT_13\n",
    "CLIENT_MARGIN_MINS_13 = UPS_Rates.loc[(UPS_Rates['SERVICE_CODE'] == 'SRPT') & (UPS_Rates['WEIGHT_BREAK'] == '7-8lb'), 'CLIENT_MARGIN_MINS'].values[0]\n",
    "tbl_clientRates_13['CLIENT_RATES_1'] = tbl_clientRates_13['PUBLISHED_RATES']*(1-DISCOUNT_TO_CLIENT_13)\n",
    "tbl_clientRates_13['CLIENT_RATES'] = round(tbl_clientRates_13['CLIENT_RATES_1'].apply(lambda x: max(x, CLIENT_MARGIN_MINS_13)),2)\n",
    "tbl_clientRates_13 = tbl_clientRates_13.drop('CLIENT_RATES_1', axis=1)\n",
    "\n",
    "\n",
    "#SRPT ['WEIGHT_BREAK'] == '9lb'\n",
    "tbl_clientRates_14 = tbl_publishedRates[(tbl_publishedRates['SERVICE_CODE'] == 'SRPT') & (tbl_publishedRates['WEIGHT_LB'] == 9)]\n",
    "DISCOUNT_TO_CLIENT_14 = UPS_Rates.loc[(UPS_Rates['SERVICE_CODE'] == 'SRPT') & (UPS_Rates['WEIGHT_BREAK'] == '9lb'), 'DISCOUNT_TO_CLIENT(1-COLN)'].values[0]\n",
    "if Round_Discount == 'YES':\n",
    "    DISCOUNT_TO_CLIENT_14 = round(DISCOUNT_TO_CLIENT_14,2)\n",
    "else:\n",
    "    DISCOUNT_TO_CLIENT_14 = DISCOUNT_TO_CLIENT_14\n",
    "CLIENT_MARGIN_MINS_14 = UPS_Rates.loc[(UPS_Rates['SERVICE_CODE'] == 'SRPT') & (UPS_Rates['WEIGHT_BREAK'] == '9lb'), 'CLIENT_MARGIN_MINS'].values[0]\n",
    "tbl_clientRates_14['CLIENT_RATES_1'] = tbl_clientRates_14['PUBLISHED_RATES']*(1-DISCOUNT_TO_CLIENT_14)\n",
    "tbl_clientRates_14['CLIENT_RATES'] = round(tbl_clientRates_14['CLIENT_RATES_1'].apply(lambda x: max(x, CLIENT_MARGIN_MINS_14)),2)\n",
    "tbl_clientRates_14 = tbl_clientRates_14.drop('CLIENT_RATES_1', axis=1)\n",
    "\n",
    "\n",
    "#Others:\n",
    "#UPS REDE, there is no discount, therefore REDE's client rate = published rate\n",
    "tbl_clientRates_15 = tbl_publishedRates[(tbl_publishedRates['SERVICE_CODE'] == 'REDE')]\n",
    "tbl_clientRates_15['CLIENT_RATES'] = tbl_clientRates_15['PUBLISHED_RATES']\n",
    "\n",
    "\n",
    "#UPS RED\n",
    "tbl_clientRates_16 = tbl_publishedRates[(tbl_publishedRates['SERVICE_CODE'] == 'RED')]\n",
    "DISCOUNT_TO_CLIENT_16 = UPS_Rates.loc[(UPS_Rates['SERVICE_CODE'] == 'RED'), 'DISCOUNT_TO_CLIENT(1-COLN)'].values[0]\n",
    "if Round_Discount == 'YES':\n",
    "    DISCOUNT_TO_CLIENT_16 = round(DISCOUNT_TO_CLIENT_16,2)\n",
    "else:\n",
    "    DISCOUNT_TO_CLIENT_16 = DISCOUNT_TO_CLIENT_16\n",
    "CLIENT_MARGIN_MINS_16 = UPS_Rates.loc[(UPS_Rates['SERVICE_CODE'] == 'RED'), 'CLIENT_MARGIN_MINS'].values[0]\n",
    "tbl_clientRates_16['CLIENT_RATES_1'] = tbl_clientRates_16['PUBLISHED_RATES']*(1-DISCOUNT_TO_CLIENT_16)\n",
    "tbl_clientRates_16['CLIENT_RATES'] = round(tbl_clientRates_16['CLIENT_RATES_1'].apply(lambda x: max(x, CLIENT_MARGIN_MINS_16)),2)\n",
    "tbl_clientRates_16 = tbl_clientRates_16.drop('CLIENT_RATES_1', axis=1)\n",
    "\n",
    "\n",
    "#UPS REDS, this does not have round up option (1/2)\n",
    "tbl_clientRates_17 = tbl_publishedRates[(tbl_publishedRates['SERVICE_CODE'] == 'REDS')]\n",
    "DISCOUNT_TO_CLIENT_17 = UPS_Rates.loc[(UPS_Rates['SERVICE_CODE'] == 'REDS'), 'DISCOUNT_TO_CLIENT(1-COLN)'].values[0]\n",
    "CLIENT_MARGIN_MINS_17 = UPS_Rates.loc[(UPS_Rates['SERVICE_CODE'] == 'REDS'), 'CLIENT_MARGIN_MINS'].values[0]\n",
    "tbl_clientRates_17['CLIENT_RATES_1'] = tbl_clientRates_17['PUBLISHED_RATES']*(1-DISCOUNT_TO_CLIENT_17)\n",
    "tbl_clientRates_17['CLIENT_RATES'] = round(tbl_clientRates_17['CLIENT_RATES_1'].apply(lambda x: max(x, CLIENT_MARGIN_MINS_17)),2)\n",
    "tbl_clientRates_17 = tbl_clientRates_17.drop('CLIENT_RATES_1', axis=1)\n",
    "\n",
    "\n",
    "#UPS 2DAM, this does not have round up option (2/2)\n",
    "tbl_clientRates_18 = tbl_publishedRates[(tbl_publishedRates['SERVICE_CODE'] == '2DAM')]\n",
    "DISCOUNT_TO_CLIENT_18 = UPS_Rates.loc[(UPS_Rates['SERVICE_CODE'] == '2DAM'), 'DISCOUNT_TO_CLIENT(1-COLN)'].values[0]\n",
    "CLIENT_MARGIN_MINS_18 = UPS_Rates.loc[(UPS_Rates['SERVICE_CODE'] == '2DAM'), 'CLIENT_MARGIN_MINS'].values[0]\n",
    "tbl_clientRates_18['CLIENT_RATES_1'] = tbl_clientRates_18['PUBLISHED_RATES']*(1-DISCOUNT_TO_CLIENT_18)\n",
    "tbl_clientRates_18['CLIENT_RATES'] = round(tbl_clientRates_18['CLIENT_RATES_1'].apply(lambda x: max(x, CLIENT_MARGIN_MINS_18)),2)\n",
    "tbl_clientRates_18 = tbl_clientRates_18.drop('CLIENT_RATES_1', axis=1)\n",
    "\n",
    "\n",
    "#UPS BLUE\n",
    "tbl_clientRates_19 = tbl_publishedRates[(tbl_publishedRates['SERVICE_CODE'] == 'BLUE')]\n",
    "DISCOUNT_TO_CLIENT_19 = UPS_Rates.loc[(UPS_Rates['SERVICE_CODE'] == 'BLUE'), 'DISCOUNT_TO_CLIENT(1-COLN)'].values[0]\n",
    "if Round_Discount == 'YES':\n",
    "    DISCOUNT_TO_CLIENT_19 = round(DISCOUNT_TO_CLIENT_19,2)\n",
    "else:\n",
    "    DISCOUNT_TO_CLIENT_19 = DISCOUNT_TO_CLIENT_19\n",
    "CLIENT_MARGIN_MINS_19 = UPS_Rates.loc[(UPS_Rates['SERVICE_CODE'] == 'BLUE') , 'CLIENT_MARGIN_MINS'].values[0]\n",
    "tbl_clientRates_19['CLIENT_RATES_1'] = tbl_clientRates_19['PUBLISHED_RATES']*(1-DISCOUNT_TO_CLIENT_19)\n",
    "tbl_clientRates_19['CLIENT_RATES'] = round(tbl_clientRates_19['CLIENT_RATES_1'].apply(lambda x: max(x, CLIENT_MARGIN_MINS_19)),2)\n",
    "tbl_clientRates_19 = tbl_clientRates_19.drop('CLIENT_RATES_1', axis=1)\n",
    "\n",
    "\n",
    "#UPS ORNG\n",
    "tbl_clientRates_20 = tbl_publishedRates[(tbl_publishedRates['SERVICE_CODE'] == 'ORNG')]\n",
    "DISCOUNT_TO_CLIENT_20 = UPS_Rates.loc[(UPS_Rates['SERVICE_CODE'] == 'ORNG'), 'DISCOUNT_TO_CLIENT(1-COLN)'].values[0]\n",
    "if Round_Discount == 'YES':\n",
    "    DISCOUNT_TO_CLIENT_20 = round(DISCOUNT_TO_CLIENT_20,2)\n",
    "else:\n",
    "    DISCOUNT_TO_CLIENT_20 = DISCOUNT_TO_CLIENT_20\n",
    "CLIENT_MARGIN_MINS_20 = UPS_Rates.loc[(UPS_Rates['SERVICE_CODE'] == 'ORNG'), 'CLIENT_MARGIN_MINS'].values[0]\n",
    "tbl_clientRates_20['CLIENT_RATES_1'] = tbl_clientRates_20['PUBLISHED_RATES']*(1-DISCOUNT_TO_CLIENT_20)\n",
    "tbl_clientRates_20['CLIENT_RATES'] = round(tbl_clientRates_20['CLIENT_RATES_1'].apply(lambda x: max(x, CLIENT_MARGIN_MINS_20)),2)\n",
    "tbl_clientRates_20 = tbl_clientRates_20.drop('CLIENT_RATES_1', axis=1)\n",
    "\n",
    "\n",
    "#UPS SRPT<1\n",
    "tbl_clientRates_21 = tbl_publishedRates[(tbl_publishedRates['SERVICE_CODE'] == 'SRPT<1')]\n",
    "DISCOUNT_TO_CLIENT_21 = UPS_Rates.loc[(UPS_Rates['SERVICE_CODE'] == 'SRPT<1'), 'DISCOUNT_TO_CLIENT(1-COLN)'].values[0]\n",
    "if Round_Discount == 'YES':\n",
    "    DISCOUNT_TO_CLIENT_21 = round(DISCOUNT_TO_CLIENT_21,2)\n",
    "else:\n",
    "    DISCOUNT_TO_CLIENT_21 = DISCOUNT_TO_CLIENT_21\n",
    "CLIENT_MARGIN_MINS_21 = UPS_Rates.loc[(UPS_Rates['SERVICE_CODE'] == 'SRPT<1'), 'CLIENT_MARGIN_MINS'].values[0]\n",
    "tbl_clientRates_21['CLIENT_RATES_1'] = tbl_clientRates_21['PUBLISHED_RATES']*(1-DISCOUNT_TO_CLIENT_21)\n",
    "tbl_clientRates_21['CLIENT_RATES'] = round(tbl_clientRates_21['CLIENT_RATES_1'].apply(lambda x: max(x, CLIENT_MARGIN_MINS_21)),2)\n",
    "tbl_clientRates_21 = tbl_clientRates_21.drop('CLIENT_RATES_1', axis=1)\n",
    "\n",
    "\n",
    "\n",
    "tbl_ups_clientRates = pd.concat([tbl_clientRates_1, tbl_clientRates_2, tbl_clientRates_3, tbl_clientRates_4, \n",
    "                    tbl_clientRates_5, tbl_clientRates_6, tbl_clientRates_7, tbl_clientRates_8,\n",
    "                    tbl_clientRates_9, tbl_clientRates_10, tbl_clientRates_11, tbl_clientRates_12,\n",
    "                    tbl_clientRates_13, tbl_clientRates_14, tbl_clientRates_15, tbl_clientRates_16,\n",
    "                    tbl_clientRates_17, tbl_clientRates_18, tbl_clientRates_19, tbl_clientRates_20,\n",
    "                    tbl_clientRates_21])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "b27b88e3-d8fb-42ab-8bf6-4e2bd9ead084",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\SLiu\\AppData\\Local\\Temp\\ipykernel_7228\\1008874764.py:14: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  tbl_clientRates_23['CLIENT_RATES_1'] = tbl_clientRates_23['PUBLISHED_RATES']*(1-DISCOUNT_TO_CLIENT_1)\n",
      "C:\\Users\\SLiu\\AppData\\Local\\Temp\\ipykernel_7228\\1008874764.py:15: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  tbl_clientRates_23['CLIENT_RATES'] = round(tbl_clientRates_23['CLIENT_RATES_1'].apply(lambda x: max(x, CLIENT_MARGIN_MINS_1)),2)\n",
      "C:\\Users\\SLiu\\AppData\\Local\\Temp\\ipykernel_7228\\1008874764.py:27: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  tbl_clientRates_24['CLIENT_RATES_1'] = tbl_clientRates_24['PUBLISHED_RATES']*(1-DISCOUNT_TO_CLIENT_2)\n",
      "C:\\Users\\SLiu\\AppData\\Local\\Temp\\ipykernel_7228\\1008874764.py:28: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  tbl_clientRates_24['CLIENT_RATES'] = round(tbl_clientRates_24['CLIENT_RATES_1'].apply(lambda x: max(x, CLIENT_MARGIN_MINS_2)),2)\n",
      "C:\\Users\\SLiu\\AppData\\Local\\Temp\\ipykernel_7228\\1008874764.py:40: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  tbl_clientRates_25['CLIENT_RATES_1'] = tbl_clientRates_25['PUBLISHED_RATES']*(1-DISCOUNT_TO_CLIENT_3)\n",
      "C:\\Users\\SLiu\\AppData\\Local\\Temp\\ipykernel_7228\\1008874764.py:41: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  tbl_clientRates_25['CLIENT_RATES'] = round(tbl_clientRates_25['CLIENT_RATES_1'].apply(lambda x: max(x, CLIENT_MARGIN_MINS_3)),2)\n",
      "C:\\Users\\SLiu\\AppData\\Local\\Temp\\ipykernel_7228\\1008874764.py:53: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  tbl_clientRates_26['CLIENT_RATES_1'] = tbl_clientRates_26['PUBLISHED_RATES']*(1-DISCOUNT_TO_CLIENT_4)\n",
      "C:\\Users\\SLiu\\AppData\\Local\\Temp\\ipykernel_7228\\1008874764.py:54: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  tbl_clientRates_26['CLIENT_RATES'] = round(tbl_clientRates_26['CLIENT_RATES_1'].apply(lambda x: max(x, CLIENT_MARGIN_MINS_4)),2)\n",
      "C:\\Users\\SLiu\\AppData\\Local\\Temp\\ipykernel_7228\\1008874764.py:66: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  tbl_clientRates_27['CLIENT_RATES_1'] = tbl_clientRates_27['PUBLISHED_RATES']*(1-DISCOUNT_TO_CLIENT_5)\n",
      "C:\\Users\\SLiu\\AppData\\Local\\Temp\\ipykernel_7228\\1008874764.py:67: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  tbl_clientRates_27['CLIENT_RATES'] = round(tbl_clientRates_27['CLIENT_RATES_1'].apply(lambda x: max(x, CLIENT_MARGIN_MINS_5)),2)\n",
      "C:\\Users\\SLiu\\AppData\\Local\\Temp\\ipykernel_7228\\1008874764.py:80: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  tbl_clientRates_28['CLIENT_RATES_1'] = tbl_clientRates_28['PUBLISHED_RATES']*(1-DISCOUNT_TO_CLIENT_6)\n",
      "C:\\Users\\SLiu\\AppData\\Local\\Temp\\ipykernel_7228\\1008874764.py:81: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  tbl_clientRates_28['CLIENT_RATES'] = round(tbl_clientRates_28['CLIENT_RATES_1'].apply(lambda x: max(x, CLIENT_MARGIN_MINS_6)),2)\n",
      "C:\\Users\\SLiu\\AppData\\Local\\Temp\\ipykernel_7228\\1008874764.py:93: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  tbl_clientRates_29['CLIENT_RATES_1'] = tbl_clientRates_29['PUBLISHED_RATES']*(1-DISCOUNT_TO_CLIENT_7)\n",
      "C:\\Users\\SLiu\\AppData\\Local\\Temp\\ipykernel_7228\\1008874764.py:94: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  tbl_clientRates_29['CLIENT_RATES'] = round(tbl_clientRates_29['CLIENT_RATES_1'].apply(lambda x: max(x, CLIENT_MARGIN_MINS_7)),2)\n",
      "C:\\Users\\SLiu\\AppData\\Local\\Temp\\ipykernel_7228\\1008874764.py:106: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  tbl_clientRates_30['CLIENT_RATES_1'] = tbl_clientRates_30['PUBLISHED_RATES']*(1-DISCOUNT_TO_CLIENT_8)\n",
      "C:\\Users\\SLiu\\AppData\\Local\\Temp\\ipykernel_7228\\1008874764.py:107: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  tbl_clientRates_30['CLIENT_RATES'] = round(tbl_clientRates_30['CLIENT_RATES_1'].apply(lambda x: max(x, CLIENT_MARGIN_MINS_8)),2)\n",
      "C:\\Users\\SLiu\\AppData\\Local\\Temp\\ipykernel_7228\\1008874764.py:119: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  tbl_clientRates_31['CLIENT_RATES_1'] = tbl_clientRates_31['PUBLISHED_RATES']*(1-DISCOUNT_TO_CLIENT_9)\n",
      "C:\\Users\\SLiu\\AppData\\Local\\Temp\\ipykernel_7228\\1008874764.py:120: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  tbl_clientRates_31['CLIENT_RATES'] = round(tbl_clientRates_31['CLIENT_RATES_1'].apply(lambda x: max(x, CLIENT_MARGIN_MINS_9)),2)\n",
      "C:\\Users\\SLiu\\AppData\\Local\\Temp\\ipykernel_7228\\1008874764.py:132: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  tbl_clientRates_32['CLIENT_RATES_1'] = tbl_clientRates_32['PUBLISHED_RATES']*(1-DISCOUNT_TO_CLIENT_10)\n",
      "C:\\Users\\SLiu\\AppData\\Local\\Temp\\ipykernel_7228\\1008874764.py:133: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  tbl_clientRates_32['CLIENT_RATES'] = round(tbl_clientRates_32['CLIENT_RATES_1'].apply(lambda x: max(x, CLIENT_MARGIN_MINS_10)),2)\n",
      "C:\\Users\\SLiu\\AppData\\Local\\Temp\\ipykernel_7228\\1008874764.py:146: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  tbl_clientRates_33['CLIENT_RATES_1'] = tbl_clientRates_33['PUBLISHED_RATES']*(1-DISCOUNT_TO_CLIENT_11)\n",
      "C:\\Users\\SLiu\\AppData\\Local\\Temp\\ipykernel_7228\\1008874764.py:147: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  tbl_clientRates_33['CLIENT_RATES'] = round(tbl_clientRates_33['CLIENT_RATES_1'].apply(lambda x: max(x, CLIENT_MARGIN_MINS_11)),2)\n",
      "C:\\Users\\SLiu\\AppData\\Local\\Temp\\ipykernel_7228\\1008874764.py:160: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  tbl_clientRates_34['CLIENT_RATES_1'] = tbl_clientRates_34['PUBLISHED_RATES']*(1-DISCOUNT_TO_CLIENT_12)\n",
      "C:\\Users\\SLiu\\AppData\\Local\\Temp\\ipykernel_7228\\1008874764.py:161: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  tbl_clientRates_34['CLIENT_RATES'] = round(tbl_clientRates_34['CLIENT_RATES_1'].apply(lambda x: max(x, CLIENT_MARGIN_MINS_12)),2)\n",
      "C:\\Users\\SLiu\\AppData\\Local\\Temp\\ipykernel_7228\\1008874764.py:173: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  tbl_clientRates_35['CLIENT_RATES_1'] = tbl_clientRates_35['PUBLISHED_RATES']*(1-DISCOUNT_TO_CLIENT_13)\n",
      "C:\\Users\\SLiu\\AppData\\Local\\Temp\\ipykernel_7228\\1008874764.py:174: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  tbl_clientRates_35['CLIENT_RATES'] = round(tbl_clientRates_35['CLIENT_RATES_1'].apply(lambda x: max(x, CLIENT_MARGIN_MINS_13)),2)\n",
      "C:\\Users\\SLiu\\AppData\\Local\\Temp\\ipykernel_7228\\1008874764.py:194: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  tbl_clientRates_37['CLIENT_RATES'] = tbl_clientRates_37['PUBLISHED_RATES']*(1-BarrettDisc_1STO_FedEx)\n",
      "C:\\Users\\SLiu\\AppData\\Local\\Temp\\ipykernel_7228\\1008874764.py:205: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  tbl_clientRates_38['CLIENT_RATES_1'] = tbl_clientRates_38['PUBLISHED_RATES']*(1-DISCOUNT_TO_CLIENT_16)\n",
      "C:\\Users\\SLiu\\AppData\\Local\\Temp\\ipykernel_7228\\1008874764.py:206: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  tbl_clientRates_38['CLIENT_RATES'] = round(tbl_clientRates_38['CLIENT_RATES_1'].apply(lambda x: max(x, CLIENT_MARGIN_MINS_16)),2)\n",
      "C:\\Users\\SLiu\\AppData\\Local\\Temp\\ipykernel_7228\\1008874764.py:214: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  tbl_clientRates_39['CLIENT_RATES_1'] = tbl_clientRates_39['PUBLISHED_RATES']*(1-DISCOUNT_TO_CLIENT_17)\n",
      "C:\\Users\\SLiu\\AppData\\Local\\Temp\\ipykernel_7228\\1008874764.py:215: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  tbl_clientRates_39['CLIENT_RATES'] = round(tbl_clientRates_39['CLIENT_RATES_1'].apply(lambda x: max(x, CLIENT_MARGIN_MINS_17)),2)\n",
      "C:\\Users\\SLiu\\AppData\\Local\\Temp\\ipykernel_7228\\1008874764.py:223: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  tbl_clientRates_40['CLIENT_RATES_1'] = tbl_clientRates_40['PUBLISHED_RATES']*(1-DISCOUNT_TO_CLIENT_18)\n",
      "C:\\Users\\SLiu\\AppData\\Local\\Temp\\ipykernel_7228\\1008874764.py:224: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  tbl_clientRates_40['CLIENT_RATES'] = round(tbl_clientRates_40['CLIENT_RATES_1'].apply(lambda x: max(x, CLIENT_MARGIN_MINS_18)),2)\n",
      "C:\\Users\\SLiu\\AppData\\Local\\Temp\\ipykernel_7228\\1008874764.py:236: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  tbl_clientRates_41['CLIENT_RATES_1'] = tbl_clientRates_41['PUBLISHED_RATES']*(1-DISCOUNT_TO_CLIENT_19)\n",
      "C:\\Users\\SLiu\\AppData\\Local\\Temp\\ipykernel_7228\\1008874764.py:237: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  tbl_clientRates_41['CLIENT_RATES'] = round(tbl_clientRates_41['CLIENT_RATES_1'].apply(lambda x: max(x, CLIENT_MARGIN_MINS_19)),2)\n",
      "C:\\Users\\SLiu\\AppData\\Local\\Temp\\ipykernel_7228\\1008874764.py:249: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  tbl_clientRates_42['CLIENT_RATES_1'] = tbl_clientRates_42['PUBLISHED_RATES']*(1-DISCOUNT_TO_CLIENT_20)\n",
      "C:\\Users\\SLiu\\AppData\\Local\\Temp\\ipykernel_7228\\1008874764.py:250: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  tbl_clientRates_42['CLIENT_RATES'] = round(tbl_clientRates_42['CLIENT_RATES_1'].apply(lambda x: max(x, CLIENT_MARGIN_MINS_20)),2)\n",
      "C:\\Users\\SLiu\\AppData\\Local\\Temp\\ipykernel_7228\\1008874764.py:262: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  tbl_clientRates_43['CLIENT_RATES_1'] = tbl_clientRates_43['PUBLISHED_RATES']*(1-DISCOUNT_TO_CLIENT_21)\n",
      "C:\\Users\\SLiu\\AppData\\Local\\Temp\\ipykernel_7228\\1008874764.py:263: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  tbl_clientRates_43['CLIENT_RATES'] = round(tbl_clientRates_43['CLIENT_RATES_1'].apply(lambda x: max(x, CLIENT_MARGIN_MINS_21)),2)\n",
      "C:\\Users\\SLiu\\AppData\\Local\\Temp\\ipykernel_7228\\1008874764.py:276: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  tbl_clientRates_44['CLIENT_RATES_1'] = tbl_clientRates_44['PUBLISHED_RATES']*(1-DISCOUNT_TO_CLIENT_22)\n",
      "C:\\Users\\SLiu\\AppData\\Local\\Temp\\ipykernel_7228\\1008874764.py:277: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  tbl_clientRates_44['CLIENT_RATES'] = round(tbl_clientRates_44['CLIENT_RATES_1'].apply(lambda x: max(x, CLIENT_MARGIN_MINS_22)),2)\n"
     ]
    }
   ],
   "source": [
    "#GRNDF ['WEIGHT_BREAK'] == '1-5lb'\n",
    "# slice the published rates\n",
    "tbl_clientRates_23 = tbl_publishedRates[(tbl_publishedRates['SERVICE_CODE'] == 'GRNDF') & (tbl_publishedRates['WEIGHT_LB'] <= 5)]\n",
    "\n",
    "# get DISCOUNT_TO_CLIENT(1-COLN) and CLIENT_MARGIN_MINS\n",
    "DISCOUNT_TO_CLIENT_1 = FedEx_Rates.loc[(FedEx_Rates['SERVICE_CODE'] == 'GRNDF') & (FedEx_Rates['WEIGHT_BREAK'] == '1-5lb'), 'DISCOUNT_TO_CLIENT(1-COLN)'].values[0]\n",
    "if Round_Discount == 'YES':\n",
    "    DISCOUNT_TO_CLIENT_1 = round(DISCOUNT_TO_CLIENT_1,2)\n",
    "else:\n",
    "    DISCOUNT_TO_CLIENT_1 = DISCOUNT_TO_CLIENT_1\n",
    "CLIENT_MARGIN_MINS_1 = FedEx_Rates.loc[(FedEx_Rates['SERVICE_CODE'] == 'GRNDF') & (FedEx_Rates['WEIGHT_BREAK'] == '1-5lb'), 'CLIENT_MARGIN_MINS'].values[0]\n",
    "\n",
    "# use max(published rates * (1-round(DISCOUNT_TO_CLIENT)), CLIENT_MARGIN_MINS) to get the client rate\n",
    "tbl_clientRates_23['CLIENT_RATES_1'] = tbl_clientRates_23['PUBLISHED_RATES']*(1-DISCOUNT_TO_CLIENT_1)\n",
    "tbl_clientRates_23['CLIENT_RATES'] = round(tbl_clientRates_23['CLIENT_RATES_1'].apply(lambda x: max(x, CLIENT_MARGIN_MINS_1)),2)\n",
    "tbl_clientRates_23 = tbl_clientRates_23.drop('CLIENT_RATES_1', axis=1)\n",
    "\n",
    "\n",
    "#GRNDF ['WEIGHT_BREAK'] == '6-10lb'\n",
    "tbl_clientRates_24 = tbl_publishedRates[(tbl_publishedRates['SERVICE_CODE'] == 'GRNDF') & (tbl_publishedRates['WEIGHT_LB'] <= 10) & (tbl_publishedRates['WEIGHT_LB'] >= 6)]\n",
    "DISCOUNT_TO_CLIENT_2 = FedEx_Rates.loc[(FedEx_Rates['SERVICE_CODE'] == 'GRNDF') & (FedEx_Rates['WEIGHT_BREAK'] == '6-10lb'), 'DISCOUNT_TO_CLIENT(1-COLN)'].values[0]\n",
    "if Round_Discount == 'YES':\n",
    "    DISCOUNT_TO_CLIENT_2 = round(DISCOUNT_TO_CLIENT_2,2)\n",
    "else:\n",
    "    DISCOUNT_TO_CLIENT_2 = DISCOUNT_TO_CLIENT_2\n",
    "CLIENT_MARGIN_MINS_2 = FedEx_Rates.loc[(FedEx_Rates['SERVICE_CODE'] == 'GRNDF') & (FedEx_Rates['WEIGHT_BREAK'] == '6-10lb'), 'CLIENT_MARGIN_MINS'].values[0]\n",
    "tbl_clientRates_24['CLIENT_RATES_1'] = tbl_clientRates_24['PUBLISHED_RATES']*(1-DISCOUNT_TO_CLIENT_2)\n",
    "tbl_clientRates_24['CLIENT_RATES'] = round(tbl_clientRates_24['CLIENT_RATES_1'].apply(lambda x: max(x, CLIENT_MARGIN_MINS_2)),2)\n",
    "tbl_clientRates_24 = tbl_clientRates_24.drop('CLIENT_RATES_1', axis=1)\n",
    "\n",
    "\n",
    "#GRNDF ['WEIGHT_BREAK'] == '11-20lb'\n",
    "tbl_clientRates_25 = tbl_publishedRates[(tbl_publishedRates['SERVICE_CODE'] == 'GRNDF') & (tbl_publishedRates['WEIGHT_LB'] <= 20) & (tbl_publishedRates['WEIGHT_LB'] >= 11)]\n",
    "DISCOUNT_TO_CLIENT_3 = FedEx_Rates.loc[(FedEx_Rates['SERVICE_CODE'] == 'GRNDF') & (FedEx_Rates['WEIGHT_BREAK'] == '11-20lb'), 'DISCOUNT_TO_CLIENT(1-COLN)'].values[0]\n",
    "if Round_Discount == 'YES':\n",
    "    DISCOUNT_TO_CLIENT_3 = round(DISCOUNT_TO_CLIENT_3,2)\n",
    "else:\n",
    "    DISCOUNT_TO_CLIENT_3 = DISCOUNT_TO_CLIENT_3\n",
    "CLIENT_MARGIN_MINS_3 = FedEx_Rates.loc[(FedEx_Rates['SERVICE_CODE'] == 'GRNDF') & (FedEx_Rates['WEIGHT_BREAK'] == '11-20lb'), 'CLIENT_MARGIN_MINS'].values[0]\n",
    "tbl_clientRates_25['CLIENT_RATES_1'] = tbl_clientRates_25['PUBLISHED_RATES']*(1-DISCOUNT_TO_CLIENT_3)\n",
    "tbl_clientRates_25['CLIENT_RATES'] = round(tbl_clientRates_25['CLIENT_RATES_1'].apply(lambda x: max(x, CLIENT_MARGIN_MINS_3)),2)\n",
    "tbl_clientRates_25 = tbl_clientRates_25.drop('CLIENT_RATES_1', axis=1)\n",
    "\n",
    "\n",
    "#GRNDF ['WEIGHT_BREAK'] == '21-30lb'\n",
    "tbl_clientRates_26 = tbl_publishedRates[(tbl_publishedRates['SERVICE_CODE'] == 'GRNDF') & (tbl_publishedRates['WEIGHT_LB'] <= 30) & (tbl_publishedRates['WEIGHT_LB'] >= 21)]\n",
    "DISCOUNT_TO_CLIENT_4 = FedEx_Rates.loc[(FedEx_Rates['SERVICE_CODE'] == 'GRNDF') & (FedEx_Rates['WEIGHT_BREAK'] == '21-30lb'), 'DISCOUNT_TO_CLIENT(1-COLN)'].values[0]\n",
    "if Round_Discount == 'YES':\n",
    "    DISCOUNT_TO_CLIENT_4 = round(DISCOUNT_TO_CLIENT_4,2)\n",
    "else:\n",
    "    DISCOUNT_TO_CLIENT_4 = DISCOUNT_TO_CLIENT_4\n",
    "CLIENT_MARGIN_MINS_4 = FedEx_Rates.loc[(FedEx_Rates['SERVICE_CODE'] == 'GRNDF') & (FedEx_Rates['WEIGHT_BREAK'] == '21-30lb'), 'CLIENT_MARGIN_MINS'].values[0]\n",
    "tbl_clientRates_26['CLIENT_RATES_1'] = tbl_clientRates_26['PUBLISHED_RATES']*(1-DISCOUNT_TO_CLIENT_4)\n",
    "tbl_clientRates_26['CLIENT_RATES'] = round(tbl_clientRates_26['CLIENT_RATES_1'].apply(lambda x: max(x, CLIENT_MARGIN_MINS_4)),2)\n",
    "tbl_clientRates_26 = tbl_clientRates_26.drop('CLIENT_RATES_1', axis=1)\n",
    "\n",
    "\n",
    "#GRNDF ['WEIGHT_BREAK'] == '31+lb'\n",
    "tbl_clientRates_27 = tbl_publishedRates[(tbl_publishedRates['SERVICE_CODE'] == 'GRNDF') & (tbl_publishedRates['WEIGHT_LB'] >= 31)]\n",
    "DISCOUNT_TO_CLIENT_5 = FedEx_Rates.loc[(FedEx_Rates['SERVICE_CODE'] == 'GRNDF') & (FedEx_Rates['WEIGHT_BREAK'] == '31+lb'), 'DISCOUNT_TO_CLIENT(1-COLN)'].values[0]\n",
    "if Round_Discount == 'YES':\n",
    "    DISCOUNT_TO_CLIENT_5 = round(DISCOUNT_TO_CLIENT_5,2)\n",
    "else:\n",
    "    DISCOUNT_TO_CLIENT_5 = DISCOUNT_TO_CLIENT_5\n",
    "CLIENT_MARGIN_MINS_5 = FedEx_Rates.loc[(FedEx_Rates['SERVICE_CODE'] == 'GRNDF') & (FedEx_Rates['WEIGHT_BREAK'] == '31+lb'), 'CLIENT_MARGIN_MINS'].values[0]\n",
    "tbl_clientRates_27['CLIENT_RATES_1'] = tbl_clientRates_27['PUBLISHED_RATES']*(1-DISCOUNT_TO_CLIENT_5)\n",
    "tbl_clientRates_27['CLIENT_RATES'] = round(tbl_clientRates_27['CLIENT_RATES_1'].apply(lambda x: max(x, CLIENT_MARGIN_MINS_5)),2)\n",
    "tbl_clientRates_27 = tbl_clientRates_27.drop('CLIENT_RATES_1', axis=1)\n",
    "\n",
    "\n",
    "#HOME \n",
    "#HOME ['WEIGHT_BREAK'] == '1-5lb'\n",
    "tbl_clientRates_28 = tbl_publishedRates[(tbl_publishedRates['SERVICE_CODE'] == 'HOME') & (tbl_publishedRates['WEIGHT_LB'] <= 5)]\n",
    "DISCOUNT_TO_CLIENT_6 = FedEx_Rates.loc[(FedEx_Rates['SERVICE_CODE'] == 'HOME') & (FedEx_Rates['WEIGHT_BREAK'] == '1-5lb'), 'DISCOUNT_TO_CLIENT(1-COLN)'].values[0]\n",
    "if Round_Discount == 'YES':\n",
    "    DISCOUNT_TO_CLIENT_6 = round(DISCOUNT_TO_CLIENT_6,2)\n",
    "else:\n",
    "    DISCOUNT_TO_CLIENT_6 = DISCOUNT_TO_CLIENT_6\n",
    "CLIENT_MARGIN_MINS_6 = FedEx_Rates.loc[(FedEx_Rates['SERVICE_CODE'] == 'HOME') & (FedEx_Rates['WEIGHT_BREAK'] == '1-5lb'), 'CLIENT_MARGIN_MINS'].values[0]\n",
    "tbl_clientRates_28['CLIENT_RATES_1'] = tbl_clientRates_28['PUBLISHED_RATES']*(1-DISCOUNT_TO_CLIENT_6)\n",
    "tbl_clientRates_28['CLIENT_RATES'] = round(tbl_clientRates_28['CLIENT_RATES_1'].apply(lambda x: max(x, CLIENT_MARGIN_MINS_6)),2)\n",
    "tbl_clientRates_28 = tbl_clientRates_28.drop('CLIENT_RATES_1', axis=1)\n",
    "\n",
    "\n",
    "#HOME ['WEIGHT_BREAK'] == '6-10lb'\n",
    "tbl_clientRates_29 = tbl_publishedRates[(tbl_publishedRates['SERVICE_CODE'] == 'HOME') & (tbl_publishedRates['WEIGHT_LB'] <= 10) & (tbl_publishedRates['WEIGHT_LB'] >= 6)]\n",
    "DISCOUNT_TO_CLIENT_7 = FedEx_Rates.loc[(FedEx_Rates['SERVICE_CODE'] == 'HOME') & (FedEx_Rates['WEIGHT_BREAK'] == '6-10lb'), 'DISCOUNT_TO_CLIENT(1-COLN)'].values[0]\n",
    "if Round_Discount == 'YES':\n",
    "    DISCOUNT_TO_CLIENT_7 = round(DISCOUNT_TO_CLIENT_7,2)\n",
    "else:\n",
    "    DISCOUNT_TO_CLIENT_7 = DISCOUNT_TO_CLIENT_7\n",
    "CLIENT_MARGIN_MINS_7 = FedEx_Rates.loc[(FedEx_Rates['SERVICE_CODE'] == 'HOME') & (FedEx_Rates['WEIGHT_BREAK'] == '6-10lb'), 'CLIENT_MARGIN_MINS'].values[0]\n",
    "tbl_clientRates_29['CLIENT_RATES_1'] = tbl_clientRates_29['PUBLISHED_RATES']*(1-DISCOUNT_TO_CLIENT_7)\n",
    "tbl_clientRates_29['CLIENT_RATES'] = round(tbl_clientRates_29['CLIENT_RATES_1'].apply(lambda x: max(x, CLIENT_MARGIN_MINS_7)),2)\n",
    "tbl_clientRates_29 = tbl_clientRates_29.drop('CLIENT_RATES_1', axis=1)\n",
    "\n",
    "\n",
    "#HOME ['WEIGHT_BREAK'] == '11-20lb'\n",
    "tbl_clientRates_30 = tbl_publishedRates[(tbl_publishedRates['SERVICE_CODE'] == 'HOME') & (tbl_publishedRates['WEIGHT_LB'] <= 20) & (tbl_publishedRates['WEIGHT_LB'] >= 11)]\n",
    "DISCOUNT_TO_CLIENT_8 = FedEx_Rates.loc[(FedEx_Rates['SERVICE_CODE'] == 'HOME') & (FedEx_Rates['WEIGHT_BREAK'] == '11-20lb'), 'DISCOUNT_TO_CLIENT(1-COLN)'].values[0]\n",
    "if Round_Discount == 'YES':\n",
    "    DISCOUNT_TO_CLIENT_8 = round(DISCOUNT_TO_CLIENT_8,2)\n",
    "else:\n",
    "    DISCOUNT_TO_CLIENT_8 = DISCOUNT_TO_CLIENT_8\n",
    "CLIENT_MARGIN_MINS_8 = FedEx_Rates.loc[(FedEx_Rates['SERVICE_CODE'] == 'HOME') & (FedEx_Rates['WEIGHT_BREAK'] == '11-20lb'), 'CLIENT_MARGIN_MINS'].values[0]\n",
    "tbl_clientRates_30['CLIENT_RATES_1'] = tbl_clientRates_30['PUBLISHED_RATES']*(1-DISCOUNT_TO_CLIENT_8)\n",
    "tbl_clientRates_30['CLIENT_RATES'] = round(tbl_clientRates_30['CLIENT_RATES_1'].apply(lambda x: max(x, CLIENT_MARGIN_MINS_8)),2)\n",
    "tbl_clientRates_30 = tbl_clientRates_30.drop('CLIENT_RATES_1', axis=1)\n",
    "\n",
    "\n",
    "#HOME ['WEIGHT_BREAK'] == '21-30lb'\n",
    "tbl_clientRates_31 = tbl_publishedRates[(tbl_publishedRates['SERVICE_CODE'] == 'HOME') & (tbl_publishedRates['WEIGHT_LB'] <= 30) & (tbl_publishedRates['WEIGHT_LB'] >= 21)]\n",
    "DISCOUNT_TO_CLIENT_9 = FedEx_Rates.loc[(FedEx_Rates['SERVICE_CODE'] == 'HOME') & (FedEx_Rates['WEIGHT_BREAK'] == '21-30lb'), 'DISCOUNT_TO_CLIENT(1-COLN)'].values[0]\n",
    "if Round_Discount == 'YES':\n",
    "    DISCOUNT_TO_CLIENT_9 = round(DISCOUNT_TO_CLIENT_9,2)\n",
    "else:\n",
    "    DISCOUNT_TO_CLIENT_9 = DISCOUNT_TO_CLIENT_9\n",
    "CLIENT_MARGIN_MINS_9 = FedEx_Rates.loc[(FedEx_Rates['SERVICE_CODE'] == 'HOME') & (FedEx_Rates['WEIGHT_BREAK'] == '21-30lb'), 'CLIENT_MARGIN_MINS'].values[0]\n",
    "tbl_clientRates_31['CLIENT_RATES_1'] = tbl_clientRates_31['PUBLISHED_RATES']*(1-DISCOUNT_TO_CLIENT_9)\n",
    "tbl_clientRates_31['CLIENT_RATES'] = round(tbl_clientRates_31['CLIENT_RATES_1'].apply(lambda x: max(x, CLIENT_MARGIN_MINS_9)),2)\n",
    "tbl_clientRates_31 = tbl_clientRates_31.drop('CLIENT_RATES_1', axis=1)\n",
    "\n",
    "\n",
    "#HOME ['WEIGHT_BREAK'] == '31+lb'\n",
    "tbl_clientRates_32 = tbl_publishedRates[(tbl_publishedRates['SERVICE_CODE'] == 'HOME') & (tbl_publishedRates['WEIGHT_LB'] >= 31)]\n",
    "DISCOUNT_TO_CLIENT_10 = FedEx_Rates.loc[(FedEx_Rates['SERVICE_CODE'] == 'HOME') & (FedEx_Rates['WEIGHT_BREAK'] == '31+lb'), 'DISCOUNT_TO_CLIENT(1-COLN)'].values[0]\n",
    "if Round_Discount == 'YES':\n",
    "    DISCOUNT_TO_CLIENT_10 = round(DISCOUNT_TO_CLIENT_10,2)\n",
    "else:\n",
    "    DISCOUNT_TO_CLIENT_10 = DISCOUNT_TO_CLIENT_10\n",
    "CLIENT_MARGIN_MINS_10 = FedEx_Rates.loc[(FedEx_Rates['SERVICE_CODE'] == 'HOME') & (FedEx_Rates['WEIGHT_BREAK'] == '31+lb'), 'CLIENT_MARGIN_MINS'].values[0]\n",
    "tbl_clientRates_32['CLIENT_RATES_1'] = tbl_clientRates_32['PUBLISHED_RATES']*(1-DISCOUNT_TO_CLIENT_10)\n",
    "tbl_clientRates_32['CLIENT_RATES'] = round(tbl_clientRates_32['CLIENT_RATES_1'].apply(lambda x: max(x, CLIENT_MARGIN_MINS_10)),2)\n",
    "tbl_clientRates_32 = tbl_clientRates_32.drop('CLIENT_RATES_1', axis=1)\n",
    "\n",
    "\n",
    "#SPST\n",
    "#SPST ['WEIGHT_BREAK'] == '1-2lb'\n",
    "tbl_clientRates_33 = tbl_publishedRates[(tbl_publishedRates['SERVICE_CODE'] == 'SPST') & (tbl_publishedRates['WEIGHT_LB'] <= 2)]\n",
    "DISCOUNT_TO_CLIENT_11 = FedEx_Rates.loc[(FedEx_Rates['SERVICE_CODE'] == 'SPST') & (FedEx_Rates['WEIGHT_BREAK'] == '1-2lbs'), 'DISCOUNT_TO_CLIENT(1-COLN)'].values[0]\n",
    "if Round_Discount == 'YES':\n",
    "    DISCOUNT_TO_CLIENT_11 = round(DISCOUNT_TO_CLIENT_11,2)\n",
    "else:\n",
    "    DISCOUNT_TO_CLIENT_11 = DISCOUNT_TO_CLIENT_11\n",
    "CLIENT_MARGIN_MINS_11 = FedEx_Rates.loc[(FedEx_Rates['SERVICE_CODE'] == 'SPST') & (FedEx_Rates['WEIGHT_BREAK'] == '1-2lbs'), 'CLIENT_MARGIN_MINS'].values[0]\n",
    "tbl_clientRates_33['CLIENT_RATES_1'] = tbl_clientRates_33['PUBLISHED_RATES']*(1-DISCOUNT_TO_CLIENT_11)\n",
    "tbl_clientRates_33['CLIENT_RATES'] = round(tbl_clientRates_33['CLIENT_RATES_1'].apply(lambda x: max(x, CLIENT_MARGIN_MINS_11)),2)\n",
    "tbl_clientRates_33 = tbl_clientRates_33.drop('CLIENT_RATES_1', axis=1)\n",
    "\n",
    "\n",
    "\n",
    "#SPST ['WEIGHT_BREAK'] == '3lb'\n",
    "tbl_clientRates_34 = tbl_publishedRates[(tbl_publishedRates['SERVICE_CODE'] == 'SPST') & (tbl_publishedRates['WEIGHT_LB'] >= 3) & (tbl_publishedRates['WEIGHT_LB'] <= 3)]\n",
    "DISCOUNT_TO_CLIENT_12 = FedEx_Rates.loc[(FedEx_Rates['SERVICE_CODE'] == 'SPST') & (FedEx_Rates['WEIGHT_BREAK'] == '3 lbs'), 'DISCOUNT_TO_CLIENT(1-COLN)'].values[0]\n",
    "if Round_Discount == 'YES':\n",
    "    DISCOUNT_TO_CLIENT_12 = round(DISCOUNT_TO_CLIENT_12,2)\n",
    "else:\n",
    "    DISCOUNT_TO_CLIENT_12 = DISCOUNT_TO_CLIENT_12\n",
    "CLIENT_MARGIN_MINS_12 = FedEx_Rates.loc[(FedEx_Rates['SERVICE_CODE'] == 'SPST') & (FedEx_Rates['WEIGHT_BREAK'] == '3 lbs'), 'CLIENT_MARGIN_MINS'].values[0]\n",
    "tbl_clientRates_34['CLIENT_RATES_1'] = tbl_clientRates_34['PUBLISHED_RATES']*(1-DISCOUNT_TO_CLIENT_12)\n",
    "tbl_clientRates_34['CLIENT_RATES'] = round(tbl_clientRates_34['CLIENT_RATES_1'].apply(lambda x: max(x, CLIENT_MARGIN_MINS_12)),2)\n",
    "tbl_clientRates_34 = tbl_clientRates_34.drop('CLIENT_RATES_1', axis=1)\n",
    "\n",
    "\n",
    "#SPST ['WEIGHT_BREAK'] == '4-70lb'\n",
    "tbl_clientRates_35 = tbl_publishedRates[(tbl_publishedRates['SERVICE_CODE'] == 'SPST') & (tbl_publishedRates['WEIGHT_LB'] >= 4) & (tbl_publishedRates['WEIGHT_LB'] <= 70)]\n",
    "DISCOUNT_TO_CLIENT_13 = FedEx_Rates.loc[(FedEx_Rates['SERVICE_CODE'] == 'SPST') & (FedEx_Rates['WEIGHT_BREAK'] == '4-70lbs'), 'DISCOUNT_TO_CLIENT(1-COLN)'].values[0]\n",
    "if Round_Discount == 'YES':\n",
    "    DISCOUNT_TO_CLIENT_13 = round(DISCOUNT_TO_CLIENT_13,2)\n",
    "else:\n",
    "    DISCOUNT_TO_CLIENT_13 = DISCOUNT_TO_CLIENT_13\n",
    "CLIENT_MARGIN_MINS_13 = FedEx_Rates.loc[(FedEx_Rates['SERVICE_CODE'] == 'SPST') & (FedEx_Rates['WEIGHT_BREAK'] == '4-70lbs'), 'CLIENT_MARGIN_MINS'].values[0]\n",
    "tbl_clientRates_35['CLIENT_RATES_1'] = tbl_clientRates_35['PUBLISHED_RATES']*(1-DISCOUNT_TO_CLIENT_13)\n",
    "tbl_clientRates_35['CLIENT_RATES'] = round(tbl_clientRates_35['CLIENT_RATES_1'].apply(lambda x: max(x, CLIENT_MARGIN_MINS_13)),2)\n",
    "tbl_clientRates_35 = tbl_clientRates_35.drop('CLIENT_RATES_1', axis=1)\n",
    "\n",
    "\n",
    "#SPST ['WEIGHT_BREAK'] == '70lb+'\n",
    "tbl_clientRates_36 = tbl_publishedRates[(tbl_publishedRates['SERVICE_CODE'] == 'SPST') & (tbl_publishedRates['WEIGHT_LB'] == 70)]\n",
    "DISCOUNT_TO_CLIENT_14 = FedEx_Rates.loc[(FedEx_Rates['SERVICE_CODE'] == 'SPST') & (FedEx_Rates['WEIGHT_BREAK'] == '70+lbs'), 'DISCOUNT_TO_CLIENT(1-COLN)'].values[0]\n",
    "if Round_Discount == 'YES':\n",
    "    DISCOUNT_TO_CLIENT_14 = round(DISCOUNT_TO_CLIENT_14,2)\n",
    "else:\n",
    "    DISCOUNT_TO_CLIENT_14 = DISCOUNT_TO_CLIENT_14\n",
    "CLIENT_MARGIN_MINS_14 = FedEx_Rates.loc[(FedEx_Rates['SERVICE_CODE'] == 'SPST') & (FedEx_Rates['WEIGHT_BREAK'] == '70+lbs'), 'CLIENT_MARGIN_MINS'].values[0]\n",
    "tbl_clientRates_36['CLIENT_RATES_1'] = tbl_clientRates_36['PUBLISHED_RATES']*(1-DISCOUNT_TO_CLIENT_14)\n",
    "tbl_clientRates_36['CLIENT_RATES'] = round(tbl_clientRates_36['CLIENT_RATES_1'].apply(lambda x: max(x, CLIENT_MARGIN_MINS_14)),2)\n",
    "tbl_clientRates_36 = tbl_clientRates_36.drop('CLIENT_RATES_1', axis=1)\n",
    "\n",
    "\n",
    "#Others:\n",
    "#FedEx 1STO, there is no discount, therefore 1STO's client rate = published rate\n",
    "tbl_clientRates_37 = tbl_publishedRates[(tbl_publishedRates['SERVICE_CODE'] == '1STO')]\n",
    "tbl_clientRates_37['CLIENT_RATES'] = tbl_clientRates_37['PUBLISHED_RATES']*(1-BarrettDisc_1STO_FedEx)\n",
    "\n",
    "\n",
    "#FedEx PROV\n",
    "tbl_clientRates_38 = tbl_publishedRates[(tbl_publishedRates['SERVICE_CODE'] == 'PROV')]\n",
    "DISCOUNT_TO_CLIENT_16 = FedEx_Rates.loc[(FedEx_Rates['SERVICE_CODE'] == 'PROV'), 'DISCOUNT_TO_CLIENT(1-COLN)'].values[0]\n",
    "if Round_Discount == 'YES':\n",
    "    DISCOUNT_TO_CLIENT_16 = round(DISCOUNT_TO_CLIENT_16,2)\n",
    "else:\n",
    "    DISCOUNT_TO_CLIENT_16 = DISCOUNT_TO_CLIENT_16\n",
    "CLIENT_MARGIN_MINS_16 = FedEx_Rates.loc[(FedEx_Rates['SERVICE_CODE'] == 'PROV'), 'CLIENT_MARGIN_MINS'].values[0]\n",
    "tbl_clientRates_38['CLIENT_RATES_1'] = tbl_clientRates_38['PUBLISHED_RATES']*(1-DISCOUNT_TO_CLIENT_16)\n",
    "tbl_clientRates_38['CLIENT_RATES'] = round(tbl_clientRates_38['CLIENT_RATES_1'].apply(lambda x: max(x, CLIENT_MARGIN_MINS_16)),2)\n",
    "tbl_clientRates_38 = tbl_clientRates_38.drop('CLIENT_RATES_1', axis=1)\n",
    "\n",
    "\n",
    "#FedEx STOV, this does not have round up option (1/2)\n",
    "tbl_clientRates_39 = tbl_publishedRates[(tbl_publishedRates['SERVICE_CODE'] == 'STOV')]\n",
    "DISCOUNT_TO_CLIENT_17 = FedEx_Rates.loc[(FedEx_Rates['SERVICE_CODE'] == 'STOV'), 'DISCOUNT_TO_CLIENT(1-COLN)'].values[0]\n",
    "CLIENT_MARGIN_MINS_17 = FedEx_Rates.loc[(FedEx_Rates['SERVICE_CODE'] == 'STOV'), 'CLIENT_MARGIN_MINS'].values[0]\n",
    "tbl_clientRates_39['CLIENT_RATES_1'] = tbl_clientRates_39['PUBLISHED_RATES']*(1-DISCOUNT_TO_CLIENT_17)\n",
    "tbl_clientRates_39['CLIENT_RATES'] = round(tbl_clientRates_39['CLIENT_RATES_1'].apply(lambda x: max(x, CLIENT_MARGIN_MINS_17)),2)\n",
    "tbl_clientRates_39 = tbl_clientRates_39.drop('CLIENT_RATES_1', axis=1)\n",
    "\n",
    "\n",
    "#FedEx 2AM, this does not have round up option (2/2)\n",
    "tbl_clientRates_40 = tbl_publishedRates[(tbl_publishedRates['SERVICE_CODE'] == '2AM')]\n",
    "DISCOUNT_TO_CLIENT_18 = FedEx_Rates.loc[(FedEx_Rates['SERVICE_CODE'] == '2AM'), 'DISCOUNT_TO_CLIENT(1-COLN)'].values[0]\n",
    "CLIENT_MARGIN_MINS_18 = FedEx_Rates.loc[(FedEx_Rates['SERVICE_CODE'] == '2AM'), 'CLIENT_MARGIN_MINS'].values[0]\n",
    "tbl_clientRates_40['CLIENT_RATES_1'] = tbl_clientRates_40['PUBLISHED_RATES']*(1-DISCOUNT_TO_CLIENT_18)\n",
    "tbl_clientRates_40['CLIENT_RATES'] = round(tbl_clientRates_40['CLIENT_RATES_1'].apply(lambda x: max(x, CLIENT_MARGIN_MINS_18)),2)\n",
    "tbl_clientRates_40 = tbl_clientRates_40.drop('CLIENT_RATES_1', axis=1)\n",
    "\n",
    "\n",
    "#FedEx 2DAY\n",
    "tbl_clientRates_41 = tbl_publishedRates[(tbl_publishedRates['SERVICE_CODE'] == '2DAY')]\n",
    "DISCOUNT_TO_CLIENT_19 = FedEx_Rates.loc[(FedEx_Rates['SERVICE_CODE'] == '2DAY'), 'DISCOUNT_TO_CLIENT(1-COLN)'].values[0]\n",
    "if Round_Discount == 'YES':\n",
    "    DISCOUNT_TO_CLIENT_19 = round(DISCOUNT_TO_CLIENT_19,2)\n",
    "else:\n",
    "    DISCOUNT_TO_CLIENT_19 = DISCOUNT_TO_CLIENT_19\n",
    "CLIENT_MARGIN_MINS_19 = FedEx_Rates.loc[(FedEx_Rates['SERVICE_CODE'] == '2DAY') , 'CLIENT_MARGIN_MINS'].values[0]\n",
    "tbl_clientRates_41['CLIENT_RATES_1'] = tbl_clientRates_41['PUBLISHED_RATES']*(1-DISCOUNT_TO_CLIENT_19)\n",
    "tbl_clientRates_41['CLIENT_RATES'] = round(tbl_clientRates_41['CLIENT_RATES_1'].apply(lambda x: max(x, CLIENT_MARGIN_MINS_19)),2)\n",
    "tbl_clientRates_41 = tbl_clientRates_41.drop('CLIENT_RATES_1', axis=1)\n",
    "\n",
    "\n",
    "#FedEx EXSA\n",
    "tbl_clientRates_42 = tbl_publishedRates[(tbl_publishedRates['SERVICE_CODE'] == 'EXSA')]\n",
    "DISCOUNT_TO_CLIENT_20 = FedEx_Rates.loc[(FedEx_Rates['SERVICE_CODE'] == 'EXSA'), 'DISCOUNT_TO_CLIENT(1-COLN)'].values[0]\n",
    "if Round_Discount == 'YES':\n",
    "    DISCOUNT_TO_CLIENT_20 = round(DISCOUNT_TO_CLIENT_20,2)\n",
    "else:\n",
    "    DISCOUNT_TO_CLIENT_20 = DISCOUNT_TO_CLIENT_20\n",
    "CLIENT_MARGIN_MINS_20 = FedEx_Rates.loc[(FedEx_Rates['SERVICE_CODE'] == 'EXSA'), 'CLIENT_MARGIN_MINS'].values[0]\n",
    "tbl_clientRates_42['CLIENT_RATES_1'] = tbl_clientRates_42['PUBLISHED_RATES']*(1-DISCOUNT_TO_CLIENT_20)\n",
    "tbl_clientRates_42['CLIENT_RATES'] = round(tbl_clientRates_42['CLIENT_RATES_1'].apply(lambda x: max(x, CLIENT_MARGIN_MINS_20)),2)\n",
    "tbl_clientRates_42 = tbl_clientRates_42.drop('CLIENT_RATES_1', axis=1)\n",
    "\n",
    "\n",
    "#FedEx SPST<1 1-3oz\n",
    "tbl_clientRates_43 = tbl_publishedRates[(tbl_publishedRates['SERVICE_CODE'] == 'SPST<1') & (tbl_publishedRates['WEIGHT_OZ'] <= 3)]\n",
    "DISCOUNT_TO_CLIENT_21 = FedEx_Rates.loc[(FedEx_Rates['SERVICE_CODE'] == 'SPST<1') & (FedEx_Rates['WEIGHT_BREAK'] == '1-3oz'), 'DISCOUNT_TO_CLIENT(1-COLN)'].values[0]\n",
    "if Round_Discount == 'YES':\n",
    "    DISCOUNT_TO_CLIENT_21 = round(DISCOUNT_TO_CLIENT_21,2)\n",
    "else:\n",
    "    DISCOUNT_TO_CLIENT_21 = DISCOUNT_TO_CLIENT_21\n",
    "CLIENT_MARGIN_MINS_21 = FedEx_Rates.loc[(FedEx_Rates['SERVICE_CODE'] == 'SPST<1') & (FedEx_Rates['WEIGHT_BREAK'] == '1-3oz'), 'CLIENT_MARGIN_MINS'].values[0]\n",
    "tbl_clientRates_43['CLIENT_RATES_1'] = tbl_clientRates_43['PUBLISHED_RATES']*(1-DISCOUNT_TO_CLIENT_21)\n",
    "tbl_clientRates_43['CLIENT_RATES'] = round(tbl_clientRates_43['CLIENT_RATES_1'].apply(lambda x: max(x, CLIENT_MARGIN_MINS_21)),2)\n",
    "tbl_clientRates_43 = tbl_clientRates_43.drop('CLIENT_RATES_1', axis=1)\n",
    "\n",
    "\n",
    "\n",
    "#FedEx SPST<1 4-16oz\n",
    "tbl_clientRates_44 = tbl_publishedRates[(tbl_publishedRates['SERVICE_CODE'] == 'SPST<1') & (tbl_publishedRates['WEIGHT_OZ'] <= 16) & (tbl_publishedRates['WEIGHT_OZ'] >= 4)]\n",
    "DISCOUNT_TO_CLIENT_22 = FedEx_Rates.loc[(FedEx_Rates['SERVICE_CODE'] == 'SPST<1') & (FedEx_Rates['WEIGHT_BREAK'] == '4-16oz'), 'DISCOUNT_TO_CLIENT(1-COLN)'].values[0]\n",
    "if Round_Discount == 'YES':\n",
    "    DISCOUNT_TO_CLIENT_22 = round(DISCOUNT_TO_CLIENT_22,2)\n",
    "else:\n",
    "    DISCOUNT_TO_CLIENT_22 = DISCOUNT_TO_CLIENT_22\n",
    "CLIENT_MARGIN_MINS_22 = FedEx_Rates.loc[(FedEx_Rates['SERVICE_CODE'] == 'SPST<1') & (FedEx_Rates['WEIGHT_BREAK'] == '4-16oz'), 'CLIENT_MARGIN_MINS'].values[0]\n",
    "tbl_clientRates_44['CLIENT_RATES_1'] = tbl_clientRates_44['PUBLISHED_RATES']*(1-DISCOUNT_TO_CLIENT_22)\n",
    "tbl_clientRates_44['CLIENT_RATES'] = round(tbl_clientRates_44['CLIENT_RATES_1'].apply(lambda x: max(x, CLIENT_MARGIN_MINS_22)),2)\n",
    "tbl_clientRates_44 = tbl_clientRates_44.drop('CLIENT_RATES_1', axis=1)\n",
    "\n",
    "\n",
    "tbl_fedex_clientRates = pd.concat([tbl_clientRates_23, tbl_clientRates_24, tbl_clientRates_25, tbl_clientRates_26, \n",
    "                    tbl_clientRates_27, tbl_clientRates_28, tbl_clientRates_29, tbl_clientRates_30,\n",
    "                    tbl_clientRates_31, tbl_clientRates_32, tbl_clientRates_33, tbl_clientRates_34,\n",
    "                    tbl_clientRates_35, tbl_clientRates_36, tbl_clientRates_37, tbl_clientRates_38,\n",
    "                    tbl_clientRates_39, tbl_clientRates_40, tbl_clientRates_41, tbl_clientRates_42,\n",
    "                    tbl_clientRates_43, tbl_clientRates_44])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "7cbbcb44-29fc-4cba-9a8f-037ca5dc1333",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#overwrite SPST client rates\n",
    "DISCOUNT_TO_CLIENT_1 = FedEx_Rates.loc[(FedEx_Rates['SERVICE_CODE'] == 'SPST') & (FedEx_Rates['WEIGHT_BREAK'] == '1-2lbs'), 'DISCOUNT_TO_CLIENT(1-COLN)'].values[0]\n",
    "DISCOUNT_TO_CLIENT_2 = FedEx_Rates.loc[(FedEx_Rates['SERVICE_CODE'] == 'SPST') & (FedEx_Rates['WEIGHT_BREAK'] == '3 lbs'), 'DISCOUNT_TO_CLIENT(1-COLN)'].values[0]\n",
    "DISCOUNT_TO_CLIENT_3 = FedEx_Rates.loc[(FedEx_Rates['SERVICE_CODE'] == 'SPST') & (FedEx_Rates['WEIGHT_BREAK'] == '4-70lbs'), 'DISCOUNT_TO_CLIENT(1-COLN)'].values[0]\n",
    "DISCOUNT_TO_CLIENT_4 = FedEx_Rates.loc[(FedEx_Rates['SERVICE_CODE'] == 'SPST') & (FedEx_Rates['WEIGHT_BREAK'] == '70+lbs'), 'DISCOUNT_TO_CLIENT(1-COLN)'].values[0]\n",
    "\n",
    "# Define the conditions\n",
    "condition1 = (tbl_fedex_clientRates['SERVICE_CODE'] == 'SPST') & (tbl_fedex_clientRates['ZONE'].isin(['17', '9', '10', '26', '99'])) & (tbl_fedex_clientRates['WEIGHT_LB'] <= 2)\n",
    "condition2 = (tbl_fedex_clientRates['SERVICE_CODE'] == 'SPST') & (tbl_fedex_clientRates['ZONE'].isin(['17', '9', '10', '26', '99'])) & (tbl_fedex_clientRates['WEIGHT_LB'] > 2) & (tbl_fedex_clientRates['WEIGHT_LB'] < 4)\n",
    "condition3 = (tbl_fedex_clientRates['SERVICE_CODE'] == 'SPST') & (tbl_fedex_clientRates['ZONE'].isin(['17', '9', '10', '26', '99'])) & (tbl_fedex_clientRates['WEIGHT_LB'] >= 4) & (tbl_fedex_clientRates['WEIGHT_LB'] < 70)\n",
    "condition4 = (tbl_fedex_clientRates['SERVICE_CODE'] == 'SPST') & (tbl_fedex_clientRates['ZONE'].isin(['17', '9', '10', '26', '99'])) & (tbl_fedex_clientRates['WEIGHT_LB'] >= 70)\n",
    "\n",
    "\n",
    "# Overwrite values where the condition is met\n",
    "tbl_fedex_clientRates.loc[condition1, 'CLIENT_RATES'] = tbl_fedex_clientRates.loc[condition1, 'PUBLISHED_RATES'].apply(lambda x: max(18.28, x * (1 - DISCOUNT_TO_CLIENT_1)))\n",
    "tbl_fedex_clientRates.loc[condition2, 'CLIENT_RATES'] = tbl_fedex_clientRates.loc[condition2, 'PUBLISHED_RATES'].apply(lambda x: max(18.28, x * (1 - DISCOUNT_TO_CLIENT_2)))\n",
    "tbl_fedex_clientRates.loc[condition3, 'CLIENT_RATES'] = tbl_fedex_clientRates.loc[condition3, 'PUBLISHED_RATES'].apply(lambda x: max(18.28, x * (1 - DISCOUNT_TO_CLIENT_3)))\n",
    "tbl_fedex_clientRates.loc[condition4, 'CLIENT_RATES'] = tbl_fedex_clientRates.loc[condition4, 'PUBLISHED_RATES'].apply(lambda x: max(18.28, x * (1 - DISCOUNT_TO_CLIENT_4)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "governmental-perspective",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\SLiu\\AppData\\Local\\Temp\\ipykernel_7228\\2179308695.py:18: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  tbl_dhl_clientRates['CLIENT_RATES'] = round(tbl_dhl_clientRates['BARRETT_RATES']/(1 - Margin_on_DHL),2)\n",
      "C:\\Users\\SLiu\\AppData\\Local\\Temp\\ipykernel_7228\\2179308695.py:19: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  tbl_usps_clientRates['CLIENT_RATES'] = round(tbl_usps_clientRates['BARRETT_RATES']/(1 - Margin_on_USPS),2)\n",
      "C:\\Users\\SLiu\\AppData\\Local\\Temp\\ipykernel_7228\\2179308695.py:20: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  tbl_mi_clientRates['CLIENT_RATES'] = round(tbl_mi_clientRates['BARRETT_RATES']/(1 - Margin_on_MI),2)\n"
     ]
    }
   ],
   "source": [
    "# Create DHL and USPS Client Rate\n",
    "tbl_dhl_clientRates = tbl_publishedRates[(tbl_publishedRates['SERVICE_CODE'].str.startswith('DHLG_')) |\n",
    "                                         (tbl_publishedRates['SERVICE_CODE'].str.startswith('DHLG<1_')) | \n",
    "                                         (tbl_publishedRates['SERVICE_CODE'].str.startswith('DHLE_')) |\n",
    "                                         (tbl_publishedRates['SERVICE_CODE'].str.startswith('DHLE<1_')) |\n",
    "                                         (tbl_publishedRates['SERVICE_CODE'].str.startswith('DHLEM_'))]\n",
    "\n",
    "tbl_usps_clientRates = tbl_publishedRates[(tbl_publishedRates['SERVICE_CODE'] == 'USPSAG') | \n",
    "                                          (tbl_publishedRates['SERVICE_CODE'] == 'USPSAP') |\n",
    "                                          (tbl_publishedRates['SERVICE_CODE'] == 'USPSAG_CI') | \n",
    "                                          (tbl_publishedRates['SERVICE_CODE'] == 'USPSAP_CI')]\n",
    "\n",
    "\n",
    "tbl_mi_clientRates = tbl_publishedRates[(tbl_publishedRates['SERVICE_CODE'] == 'MIPLE') | \n",
    "                                          (tbl_publishedRates['SERVICE_CODE'] == 'MIPH')]\n",
    "\n",
    "\n",
    "tbl_dhl_clientRates['CLIENT_RATES'] = round(tbl_dhl_clientRates['BARRETT_RATES']/(1 - Margin_on_DHL),2)\n",
    "tbl_usps_clientRates['CLIENT_RATES'] = round(tbl_usps_clientRates['BARRETT_RATES']/(1 - Margin_on_USPS),2)\n",
    "tbl_mi_clientRates['CLIENT_RATES'] = round(tbl_mi_clientRates['BARRETT_RATES']/(1 - Margin_on_MI),2)\n",
    "\n",
    "\n",
    "tbl_clientRates = pd.concat([tbl_ups_clientRates, tbl_fedex_clientRates, tbl_dhl_clientRates, tbl_usps_clientRates, tbl_mi_clientRates])\n",
    "tbl_clientRates = tbl_clientRates.reindex(columns=['SERVICE_CODE', 'SERVICE_PLAINTEXT', 'ZONE', 'WEIGHT_OZ', 'WEIGHT_LB', 'CUBIC_FT', 'PUBLISHED_RATES', 'CLIENT_RATES', 'VERSION', 'START_DATE', 'END_DATE'])\n",
    "\n",
    "#get the finalized tbl_publishedRates\n",
    "tbl_clientRates = tbl_BarrettRates.merge(tbl_clientRates[['SERVICE_CODE', 'SERVICE_PLAINTEXT', 'ZONE', 'WEIGHT_OZ', 'WEIGHT_LB', 'CUBIC_FT', 'VERSION', 'START_DATE', 'END_DATE', 'CLIENT_RATES']], left_on=['SERVICE_CODE', 'SERVICE_PLAINTEXT', 'ZONE', 'WEIGHT_OZ', 'WEIGHT_LB', 'VERSION', 'START_DATE', 'END_DATE', 'CUBIC_FT'], right_on=['SERVICE_CODE', 'SERVICE_PLAINTEXT', 'ZONE', 'WEIGHT_OZ', 'WEIGHT_LB', 'VERSION', 'START_DATE', 'END_DATE', 'CUBIC_FT'], how='left')\n",
    "#tbl_clientRates.to_csv('test.csv')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hollow-unemployment",
   "metadata": {},
   "source": [
    "# Step 2 - a. Create UPS accessorial rates\n",
    "*This rate is separated from freight rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "english-chester",
   "metadata": {},
   "outputs": [],
   "source": [
    "# UPSAccessorial_2023 has 2 tabs, UPS_Rates and UPS_Accessorial\n",
    "xls = pd.ExcelFile('Accessorial_2024.xlsx')\n",
    "UPS_Accessorial = pd.read_excel(xls, 'UPS_Accessorial')\n",
    "\n",
    "# Add Net_charge column\n",
    "UPS_Accessorial['NET_CHARGE'] = UPS_Accessorial['PUBLISHED_CHARGE']*(1-UPS_Accessorial['DISCOUNT'])\n",
    "\n",
    "# Add ADD_MARGIN column\n",
    "def k(col):   \n",
    "    if UPS_Accessorial['ACCESSORIAL'].str.contains('Air').any():\n",
    "        return col['NET_CHARGE']/(1-Margin_on_Value_Add_AIR_UPS)\n",
    "    else:\n",
    "        return col['NET_CHARGE']/(1-Margin_on_Value_Add_GRND_UPS)\n",
    "\n",
    "UPS_Accessorial['ADD_MARGIN'] = UPS_Accessorial.apply(k, axis=1)  \n",
    "\n",
    "\n",
    "# Assign a number to a specific cell based on column name and other row values\n",
    "column_name = 'ADD_MARGIN'\n",
    "condition1 = UPS_Accessorial['ACCESSORIAL'] == 'Remote Area' \n",
    "condition2 = UPS_Accessorial['ACCESSORIAL'] == 'Remote Area Alaska' \n",
    "condition3 = UPS_Accessorial['ACCESSORIAL'] == 'Remote Area Hawaii' \n",
    "\n",
    "UPS_Accessorial.loc[condition1, column_name] = 14.15\n",
    "UPS_Accessorial.loc[condition2, column_name] = 40.50\n",
    "UPS_Accessorial.loc[condition3, column_name] = 14.15\n",
    "\n",
    "\n",
    "# Add PERCENTAGE column\n",
    "def l(col):   \n",
    "    if Round_Discount == 'YES':\n",
    "        return 1 - round((col['ADD_MARGIN']/col['PUBLISHED_CHARGE']),2)\n",
    "    else:\n",
    "        return 1 - col['ADD_MARGIN']/col['PUBLISHED_CHARGE']\n",
    "\n",
    "UPS_Accessorial['PERCENTAGE'] = UPS_Accessorial.apply(l, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "a67e9e66-a998-4d97-aeb2-5ff7b93f23c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# UPSAccessorial_2023 has 2 tabs, UPS_Rates and UPS_Accessorial\n",
    "xls = pd.ExcelFile('Accessorial_2024.xlsx')\n",
    "FedEx_Accessorial = pd.read_excel(xls, 'FedEx_Accessorial')\n",
    "\n",
    "# Add Net_charge column\n",
    "FedEx_Accessorial['NET_CHARGE'] = round(FedEx_Accessorial['PUBLISHED_CHARGE']*(1-FedEx_Accessorial['DISCOUNT']),2)\n",
    "\n",
    "# Add ADD_MARGIN column\n",
    "def k(col):   \n",
    "    if 'Air' in col['ACCESSORIAL'] or 'Express' in col['ACCESSORIAL']:\n",
    "        return col['NET_CHARGE']/(1-Margin_on_Value_Add_AIR_FedEx)\n",
    "    else:\n",
    "        return col['NET_CHARGE']/(1-Margin_on_Value_Add_GRND_FedEx)\n",
    "\n",
    "FedEx_Accessorial['ADD_MARGIN'] = FedEx_Accessorial.apply(k, axis=1)  \n",
    "\n",
    "# Add PERCENTAGE column\n",
    "def l(col):   \n",
    "    if Round_Discount == 'YES':\n",
    "        return 1 - round((col['ADD_MARGIN']/col['PUBLISHED_CHARGE']),2)\n",
    "    else:\n",
    "        return 1 - round((col['ADD_MARGIN']/col['PUBLISHED_CHARGE']),2)\n",
    "\n",
    "FedEx_Accessorial['PERCENTAGE'] = FedEx_Accessorial.apply(l, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "quick-belarus",
   "metadata": {},
   "source": [
    "# Step 2 - b. Prepare all other rates for rerate\n",
    "*These rates do not need a table to house"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "actual-calendar",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\SLiu\\AppData\\Local\\Temp\\ipykernel_7228\\3619121388.py:54: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '2026-08-22' has dtype incompatible with datetime64[ns], please explicitly cast to a compatible dtype first.\n",
      "  DHL_FSC.loc[DHL_FSC['SHIP_DATE_START'] == max(DHL_FSC['SHIP_DATE_START']), ['SHIP_DATE_END']] = future_date\n"
     ]
    }
   ],
   "source": [
    "# List all UPS Surcharge rates for rerate\n",
    "Resi_Surcharge_Air = UPS_Accessorial.loc[(UPS_Accessorial['ACCESSORIAL'] == 'Resi Surcharge (Air)'), 'ADD_MARGIN'].values[0]\n",
    "Resi_Surcharge_Ground = UPS_Accessorial.loc[(UPS_Accessorial['ACCESSORIAL'] == 'Resi Surcharge (Ground)'), 'ADD_MARGIN'].values[0]\n",
    "DAS_Comm_Ground = UPS_Accessorial.loc[(UPS_Accessorial['ACCESSORIAL'] == 'DAS Comm (Ground)'), 'ADD_MARGIN'].values[0]\n",
    "DAS_Comm_Extended_Ground = UPS_Accessorial.loc[(UPS_Accessorial['ACCESSORIAL'] == 'DAS Comm Extended (Ground)'), 'ADD_MARGIN'].values[0]\n",
    "DAS_Resi_Ground = UPS_Accessorial.loc[(UPS_Accessorial['ACCESSORIAL'] == 'DAS Resi (Ground)'), 'ADD_MARGIN'].values[0]\n",
    "DAS_Resi_Extended_Ground = UPS_Accessorial.loc[(UPS_Accessorial['ACCESSORIAL'] == 'DAS Resi Extended (Ground)'), 'ADD_MARGIN'].values[0]\n",
    "DAS_Comm_Air = UPS_Accessorial.loc[(UPS_Accessorial['ACCESSORIAL'] == 'DAS Comm (Air)'), 'ADD_MARGIN'].values[0]\n",
    "DAS_Comm_Extended_Air = UPS_Accessorial.loc[(UPS_Accessorial['ACCESSORIAL'] == 'DAS Comm Extended (Air)'), 'ADD_MARGIN'].values[0]\n",
    "DAS_Resi_Air = UPS_Accessorial.loc[(UPS_Accessorial['ACCESSORIAL'] == 'DAS Resi (Air)'), 'ADD_MARGIN'].values[0]\n",
    "DAS_Resi_Extended_Air = UPS_Accessorial.loc[(UPS_Accessorial['ACCESSORIAL'] == 'DAS Resi Extended (Air)'), 'ADD_MARGIN'].values[0]\n",
    "DAS_SurePost = UPS_Accessorial.loc[(UPS_Accessorial['ACCESSORIAL'] == 'DAS SurePost'), 'ADD_MARGIN'].values[0]\n",
    "DAS_Extended_SurePost = UPS_Accessorial.loc[(UPS_Accessorial['ACCESSORIAL'] == 'DAS Extended SurePost'), 'ADD_MARGIN'].values[0]\n",
    "Remote_Area = 14.15\n",
    "Remote_Area_Alaska = 40.50\n",
    "Remote_Area_Hawaii = 14.15\n",
    "\n",
    "\n",
    "# Get FSC ready for rerate\n",
    "xls = pd.ExcelFile('FSC.xlsx')\n",
    "UPS_FSC = pd.read_excel(xls, 'UPS_FSC')\n",
    "DHL_FSC = pd.read_excel(xls, 'DHL_FSC')\n",
    "FedEx_FSC = pd.read_excel(xls, 'FedEx_FSC')\n",
    "\n",
    "#Get UPS_FSC doc ready\n",
    "UPS_FSC['SHIP_DATE_START'] = UPS_FSC['Ship Date']\n",
    "UPS_FSC['Ship Date'] = pd.to_datetime(UPS_FSC['Ship Date']) \n",
    "UPS_FSC['SHIP_DATE_END'] = UPS_FSC['Ship Date'] + pd.Timedelta(days=6)\n",
    "UPS_FSC = UPS_FSC.drop('Ship Date', axis=1)\n",
    "\n",
    "UPS_FSC['SHIP_DATE_START'] = pd.to_datetime(UPS_FSC['SHIP_DATE_START']).dt.date\n",
    "UPS_FSC['SHIP_DATE_END'] = pd.to_datetime(UPS_FSC['SHIP_DATE_END']).dt.date\n",
    "\n",
    "#Get DHL_FSC doc ready\n",
    "from datetime import timedelta\n",
    "from datetime import date\n",
    "DHL_FSC = DHL_FSC.sort_values(by='Ship Date', ascending=True)\n",
    "DHL_FSC['Ship Date'] = pd.to_datetime(DHL_FSC['Ship Date']) \n",
    "DHL_FSC['SHIP_DATE_START'] = DHL_FSC['Ship Date']\n",
    "DHL_FSC['SHIP_DATE_END'] = DHL_FSC['SHIP_DATE_START'].shift(-1) - timedelta(days=1)\n",
    "\n",
    "#Get FedEx_FSC doc ready\n",
    "FedEx_FSC['SHIP_DATE_START'] = FedEx_FSC['Ship Date']\n",
    "FedEx_FSC['Ship Date'] = pd.to_datetime(FedEx_FSC['Ship Date']) \n",
    "FedEx_FSC['SHIP_DATE_END'] = FedEx_FSC['Ship Date'] + pd.Timedelta(days=6)\n",
    "FedEx_FSC = FedEx_FSC.drop('Ship Date', axis=1)\n",
    "\n",
    "FedEx_FSC['SHIP_DATE_START'] = pd.to_datetime(FedEx_FSC['SHIP_DATE_START']).dt.date\n",
    "FedEx_FSC['SHIP_DATE_END'] = pd.to_datetime(FedEx_FSC['SHIP_DATE_END']).dt.date\n",
    "\n",
    "# Calculate a date that is 2 years in the future\n",
    "future_date = date.today().replace(year=date.today().year + 2)\n",
    "# Update the 'SHIP_DATE_END' column for rows with the latest 'SHIP_DATE_START'\n",
    "DHL_FSC.loc[DHL_FSC['SHIP_DATE_START'] == max(DHL_FSC['SHIP_DATE_START']), ['SHIP_DATE_END']] = future_date\n",
    "\n",
    "DHL_FSC['SHIP_DATE_END'] = pd.to_datetime(DHL_FSC['SHIP_DATE_END']) \n",
    "DHL_FSC = DHL_FSC.drop('Ship Date', axis=1)\n",
    "\n",
    "# List UPS AHS rates for rerate\n",
    "AH_Girth_2_UPS = UPS_Accessorial.loc[(UPS_Accessorial['ACCESSORIAL'] == 'Additional Handling Girth 2'), 'ADD_MARGIN'].values[0]\n",
    "AH_Girth_3_4_UPS = UPS_Accessorial.loc[(UPS_Accessorial['ACCESSORIAL'] == 'Additional Handling Girth 3-4'), 'ADD_MARGIN'].values[0]\n",
    "AH_Girth_5_up_UPS = UPS_Accessorial.loc[(UPS_Accessorial['ACCESSORIAL'] == 'Additional Handling Girth 5+'), 'ADD_MARGIN'].values[0]\n",
    "\n",
    "AH_L_2_UPS = UPS_Accessorial.loc[(UPS_Accessorial['ACCESSORIAL'] == 'Additional Handling L 2'), 'ADD_MARGIN'].values[0]\n",
    "AH_L_3_4_UPS = UPS_Accessorial.loc[(UPS_Accessorial['ACCESSORIAL'] == 'Additional Handling L 3-4'), 'ADD_MARGIN'].values[0]\n",
    "AH_L_5_up_UPS = UPS_Accessorial.loc[(UPS_Accessorial['ACCESSORIAL'] == 'Additional Handling L 5+'), 'ADD_MARGIN'].values[0]\n",
    "\n",
    "AH_W_2_UPS = UPS_Accessorial.loc[(UPS_Accessorial['ACCESSORIAL'] == 'Additional Handling W 2'), 'ADD_MARGIN'].values[0]\n",
    "AH_W_3_4_UPS = UPS_Accessorial.loc[(UPS_Accessorial['ACCESSORIAL'] == 'Additional Handling W 3-4'), 'ADD_MARGIN'].values[0]\n",
    "AH_W_5_up_UPS = UPS_Accessorial.loc[(UPS_Accessorial['ACCESSORIAL'] == 'Additional Handling W 5+'), 'ADD_MARGIN'].values[0]\n",
    "\n",
    "AH_WGT_2_UPS = UPS_Accessorial.loc[(UPS_Accessorial['ACCESSORIAL'] == 'Additional Handling WGT 2'), 'ADD_MARGIN'].values[0]\n",
    "AH_WGT_3_4_UPS = UPS_Accessorial.loc[(UPS_Accessorial['ACCESSORIAL'] == 'Additional Handling WGT 3-4'), 'ADD_MARGIN'].values[0]\n",
    "AH_WGT_5_up_UPS = UPS_Accessorial.loc[(UPS_Accessorial['ACCESSORIAL'] == 'Additional Handling WGT 5+'), 'ADD_MARGIN'].values[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "7c166ce6-0e98-4056-934a-5f936cfaa0b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List all FedEx Surcharge rates for rerate\n",
    "Resi_Surcharge_Air_FedEx = FedEx_Accessorial.loc[(FedEx_Accessorial['ACCESSORIAL'] == 'FedEx Residential Air Surcharge'), 'ADD_MARGIN'].values[0]\n",
    "Resi_Surcharge_Ground_FedEx = FedEx_Accessorial.loc[(FedEx_Accessorial['ACCESSORIAL'] == 'FedEx Residential Ground Surcharge'), 'ADD_MARGIN'].values[0]\n",
    "DAS_Comm_Ground_FedEx = FedEx_Accessorial.loc[(FedEx_Accessorial['ACCESSORIAL'] == 'FedEx DAS Commercial(Ground)'), 'ADD_MARGIN'].values[0]\n",
    "DAS_Comm_Extended_Ground_FedEx = FedEx_Accessorial.loc[(FedEx_Accessorial['ACCESSORIAL'] == 'FedEx DAS Extended Commercial(Ground)'), 'ADD_MARGIN'].values[0]\n",
    "DAS_Resi_Ground_FedEx = FedEx_Accessorial.loc[(FedEx_Accessorial['ACCESSORIAL'] == 'FedEx DAS Residential(Ground)'), 'ADD_MARGIN'].values[0]\n",
    "DAS_Resi_Extended_Ground_FedEx = FedEx_Accessorial.loc[(FedEx_Accessorial['ACCESSORIAL'] == 'FedEx DAS Extended Residential(Ground)'), 'ADD_MARGIN'].values[0]\n",
    "DAS_Resi_Air_FedEx = FedEx_Accessorial.loc[(FedEx_Accessorial['ACCESSORIAL'] == 'FedEx DAS Residential(Express)'), 'ADD_MARGIN'].values[0]\n",
    "DAS_Resi_Extended_Air_FedEx = FedEx_Accessorial.loc[(FedEx_Accessorial['ACCESSORIAL'] == 'FedEx DAS Extended Residential(Express)'), 'ADD_MARGIN'].values[0]\n",
    "Remote_Area_FedEx = FedEx_Accessorial.loc[(FedEx_Accessorial['ACCESSORIAL'] == 'FedEx Remote Area Surcharge (Commercial and Residential)'), 'ADD_MARGIN'].values[0]\n",
    "Remote_Area_Alaska_FedEx = FedEx_Accessorial.loc[(FedEx_Accessorial['ACCESSORIAL'] == 'FedEx Remote Area Alaska'), 'ADD_MARGIN'].values[0]\n",
    "Remote_Area_Hawaii_FedEx = FedEx_Accessorial.loc[(FedEx_Accessorial['ACCESSORIAL'] == 'FedEx Remote Area Hawaii'), 'ADD_MARGIN'].values[0]\n",
    "\n",
    "# List FedEx AHS rates for rerate\n",
    "AH_Dim_2_FedEx = FedEx_Accessorial.loc[(FedEx_Accessorial['ACCESSORIAL'] == 'FedEx Additional Handling Dimension 2'), 'ADD_MARGIN'].values[0]\n",
    "AH_Dim_3_4_FedEx = FedEx_Accessorial.loc[(FedEx_Accessorial['ACCESSORIAL'] == 'FedEx Additional Handling Dimension 3-4'), 'ADD_MARGIN'].values[0]\n",
    "AH_Dim_5_6_FedEx = FedEx_Accessorial.loc[(FedEx_Accessorial['ACCESSORIAL'] == 'FedEx Additional Handling Dimension 5-6'), 'ADD_MARGIN'].values[0]\n",
    "AH_Dim_7_up_FedEx = FedEx_Accessorial.loc[(FedEx_Accessorial['ACCESSORIAL'] == 'FedEx Additional Handling Dimension 7+'), 'ADD_MARGIN'].values[0]\n",
    "\n",
    "AH_WGT_2_FedEx = FedEx_Accessorial.loc[(FedEx_Accessorial['ACCESSORIAL'] == 'FedEx Additional Handling Weight 2'), 'ADD_MARGIN'].values[0]\n",
    "AH_WGT_3_4_FedEx = FedEx_Accessorial.loc[(FedEx_Accessorial['ACCESSORIAL'] == 'FedEx Additional Handling Weight 3-4'), 'ADD_MARGIN'].values[0]\n",
    "AH_WGT_5_6_FedEx = FedEx_Accessorial.loc[(FedEx_Accessorial['ACCESSORIAL'] == 'FedEx Additional Handling Weight 5-6'), 'ADD_MARGIN'].values[0]\n",
    "AH_WGT_7_up_FedEx = FedEx_Accessorial.loc[(FedEx_Accessorial['ACCESSORIAL'] == 'FedEx Additional Handling Weight 7+'), 'ADD_MARGIN'].values[0]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "induced-monroe",
   "metadata": {},
   "source": [
    "# Step 3 - a. Prepare PLD for rerate\n",
    "* Regardless of various types of PLD, Client PLD, Barrett PLD, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "earlier-grenada",
   "metadata": {},
   "source": [
    "# Load PLD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "lasting-primary",
   "metadata": {},
   "outputs": [],
   "source": [
    "#change the corresponding column names to below names before upload. \n",
    "#Requested columns and the corresponding type:     \n",
    "# ---  ------             --------------  -----         \n",
    "#1   TRACKING_NUMBER    \n",
    "#2   SHIP_DATE             \n",
    "#3   OLD_SERVICE --(optional)      \n",
    "#4   ZIP_CODE               \n",
    "#5   ZONE --(optional)               \n",
    "#6   DIMENSIONS or LWH           \n",
    "#7   ACTUAL_WEIGHT               \n",
    "#8   RESIDENTIAL_FLAG       \n",
    "#9   INTERNATIONAL_FLAG    \n",
    "\n",
    "#Load Sample PLD\n",
    "xls = pd.ExcelFile('pld_rate_model_FM_DHL&MI.xlsx')\n",
    "tbl_BP_Master = pd.read_excel(xls, 'Sheet1')\n",
    "#tbl_BP.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "c3c60bdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#When the original file is too large, make a copy\n",
    "tbl_BP = tbl_BP_Master.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "4996b6fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean the empties if necessary\n",
    "tbl_BP = tbl_BP.dropna(axis = 0, how = 'all')\n",
    "tbl_BP = tbl_BP.dropna(axis = 1, how = 'all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "4ed09bdf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(104086, 30)"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tbl_BP.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5931f8f",
   "metadata": {},
   "source": [
    "# Clean PLD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 672,
   "id": "90c89f16",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Separate the correct data from those error data. \n",
    "tbl_BP_delete = tbl_BP[#(tbl_BP.TRACKING_NUMBER.isnull()) |\n",
    "                       (tbl_BP.SHIP_DATE.isnull()) | \n",
    "                        (tbl_BP.ZIP_CODE.isnull()) | \n",
    "                        (tbl_BP.DIMENSIONS.isnull()) | \n",
    "                        (tbl_BP.ACTUAL_WEIGHT.isnull()) | \n",
    "                        (tbl_BP.ACTUAL_WEIGHT == 0) | \n",
    "                       # (tbl_BP.ZONE.isnull()) |\n",
    "                        (tbl_BP.RESIDENTIAL_FLAG.isnull()) #| #will not be rated under ups except SRPT\n",
    "                        #(tbl_BP.L.isnull()) | \n",
    "                        #(tbl_BP.L == 0) |  \n",
    "                        #(tbl_BP.W.isnull()) |\n",
    "                        #(tbl_BP.W == 0) | \n",
    "                        #(tbl_BP.H.isnull()) | \n",
    "                        #(tbl_BP.H == 0) \n",
    "                       #| \n",
    "                        #(tbl_BP['INTERNATIONAL_FLAG'] == True)\n",
    "                      ]\n",
    "\n",
    "tbl_BP = tbl_BP[~(#(tbl_BP.TRACKING_NUMBER.isnull()) |\n",
    "                 (tbl_BP.SHIP_DATE.isnull()) | \n",
    "                 (tbl_BP.ZIP_CODE.isnull()) | \n",
    "                 (tbl_BP.DIMENSIONS.isnull()) | \n",
    "                 (tbl_BP.ACTUAL_WEIGHT.isnull()) | \n",
    "                 (tbl_BP.ACTUAL_WEIGHT == 0) | \n",
    "                # (tbl_BP.ZONE.isnull()) |\n",
    "                 (tbl_BP.RESIDENTIAL_FLAG.isnull()) #|\n",
    "                # (tbl_BP.L.isnull()) | \n",
    "                # (tbl_BP.L == 0) |  \n",
    "                # (tbl_BP.W.isnull()) |\n",
    "                # (tbl_BP.W == 0) | \n",
    "                # (tbl_BP.H.isnull()) | \n",
    "                # (tbl_BP.H == 0)\n",
    "                  #| \n",
    "                 #(tbl_BP['INTERNATIONAL_FLAG'] == True)\n",
    ")]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 673,
   "id": "c0c3ec5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Output those incomplete PLD that cannot be used in this model\n",
    "tbl_BP_delete.to_csv('tbl_BP_delete.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "e684aa3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 104086 entries, 0 to 104085\n",
      "Data columns (total 30 columns):\n",
      " #   Column              Non-Null Count   Dtype         \n",
      "---  ------              --------------   -----         \n",
      " 0   CustomerID          104086 non-null  int64         \n",
      " 1   CustomerName        104086 non-null  object        \n",
      " 2   Facility            104086 non-null  object        \n",
      " 3   BarrettOrderNumber  104086 non-null  object        \n",
      " 4   Reference           104086 non-null  object        \n",
      " 5   PoNumber            1863 non-null    object        \n",
      " 6   TrackingNumber      1863 non-null    object        \n",
      " 7   ShipDate            104086 non-null  datetime64[ns]\n",
      " 8   Service             104086 non-null  object        \n",
      " 9   Shipper             1863 non-null    object        \n",
      " 10  ShipToName          1863 non-null    object        \n",
      " 11  ShipToContact       28 non-null      object        \n",
      " 12  ShipToCity          104086 non-null  object        \n",
      " 13  ShipToState         104086 non-null  object        \n",
      " 14  ZipCode             104086 non-null  int64         \n",
      " 15  ShipToCountry       104086 non-null  object        \n",
      " 16  Zone                104086 non-null  int64         \n",
      " 17  Quantity            103738 non-null  float64       \n",
      " 18  Dimensions          104086 non-null  object        \n",
      " 19  ActualWeight        104086 non-null  float64       \n",
      " 20  ResidentialFlag     104086 non-null  bool          \n",
      " 21  JoChannel           291 non-null     object        \n",
      " 22  TRACKING_NUMBER     104086 non-null  int64         \n",
      " 23  SHIP_DATE           104086 non-null  datetime64[ns]\n",
      " 24  OLD_SERVICE         104086 non-null  object        \n",
      " 25  ZIP_CODE            104086 non-null  int64         \n",
      " 26  ZONE                104086 non-null  int64         \n",
      " 27  DIMENSIONS          104086 non-null  object        \n",
      " 28  ACTUAL_WEIGHT       104086 non-null  float64       \n",
      " 29  RESIDENTIAL_FLAG    104086 non-null  bool          \n",
      "dtypes: bool(2), datetime64[ns](2), float64(3), int64(6), object(17)\n",
      "memory usage: 22.4+ MB\n"
     ]
    }
   ],
   "source": [
    "#The column type must match\n",
    "#Requested columns and the corresponding type:     \n",
    "# ---  ------             --------------  -----         \n",
    "#1   TRACKING_NUMBER        ---------------------      int64 (or object) \"can manually create one before upload\"       \n",
    "#2   SHIP_DATE              ---------------------      datetime64[ns] \"can manually fill up before upload\"\n",
    "#3   OLD_SERVICE (optional) ---------------------      object        \n",
    "#4   ZIP_CODE               ---------------------      object\n",
    "#5   ZONE (optional)        ---------------------      object          \n",
    "#6   DIMENSIONS (or LWH)    ---------------------      object (or float64 or int64)       \n",
    "#7   ACTUAL_WEIGHT (lb as default)---------------      float64       \n",
    "#8   RESIDENTIAL_FLAG       ---------------------      bool   \n",
    "tbl_BP.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "4558a9d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#update column type\n",
    "#tbl_BP['ZIP_CODE'] = tbl_BP['ZIP_CODE'].astype(str)\n",
    "tbl_BP['ZONE'] = tbl_BP['ZONE'].astype(str)\n",
    "#tbl_BP['ZONE'] = pd.to_numeric(tbl_BP['ZONE'], errors='coerce')\n",
    "# Convert float values to integers\n",
    "#tbl_BP['ZONE']= tbl_BP['ZONE'].fillna(0).astype(int)\n",
    "\n",
    "#tbl_BP['RESIDENTIAL_FLAG'] = tbl_BP['RESIDENTIAL_FLAG'].astype('bool')\n",
    "tbl_BP['ZIP_CODE'] = tbl_BP['ZIP_CODE'].astype(str)\n",
    "#tbl_BP['SHIP_DATE'] = pd.to_datetime(tbl_BP['SHIP_DATE'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "69616d63",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['10011', '21113', '77365', ..., '68767', '73772', '63845'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Check the Zip_Code values\n",
    "tbl_BP['ZIP_CODE'].unique() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 424,
   "id": "ca339d7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sliu\\AppData\\Local\\Temp\\ipykernel_9692\\2751569339.py:2: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  tbl_BP['ZIP_CODE'] = tbl_BP['ZIP_CODE'].astype(str).str.replace('\\.0$', '').astype(int)\n"
     ]
    }
   ],
   "source": [
    "tbl_BP['ZIP_CODE'] = tbl_BP['ZIP_CODE'].replace('nan', np.nan).astype(float).fillna(0).astype(int)\n",
    "tbl_BP['ZIP_CODE'] = tbl_BP['ZIP_CODE'].astype(str).str.replace('\\.0$', '').astype(int)\n",
    "tbl_BP['ZIP_CODE'] = tbl_BP['ZIP_CODE'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "d056af57",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['5', '4', '7', '6', '8', '2', '3'], dtype=object)"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Check the value of Zone if exists\n",
    "tbl_BP['ZONE'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "6a9e15cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "tbl_BP['ZONE'] = tbl_BP['ZONE'].replace('nan', np.nan).astype(float).fillna(0).astype(int)\n",
    "tbl_BP['ZONE'] = tbl_BP['ZONE'].astype(str).str.replace('\\.0$', '').astype(int)\n",
    "tbl_BP['ZONE'] = tbl_BP['ZONE'].astype(str)\n",
    "#tbl_BP['ZONE'] = tbl_BP['ZONE'].str.replace('102', '2')\n",
    "#tbl_BP['ZONE'] = tbl_BP['ZONE'].str.replace('108', '8')\n",
    "#tbl_BP['ZONE'] = tbl_BP['ZONE'].str.replace('104', '4')\n",
    "#tbl_BP['ZONE'] = tbl_BP['ZONE'].str.replace('44', '9')\n",
    "#tbl_BP['ZONE'] = tbl_BP['ZONE'].str.replace('46', '11')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "exact-candle",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Drop duplicates based on ID column by only keeping the unique ID associated with the most recent Ship Date.  \n",
    "#If there are rows where both tracking no and ship date are duplicated, only keep the first one.\n",
    "tbl_BP = tbl_BP.sort_values('SHIP_DATE', ascending=False)\n",
    "tbl_BP = tbl_BP.drop_duplicates(subset='TRACKING_NUMBER', keep='first')\n",
    "tbl_BP = tbl_BP.drop_duplicates(subset='TRACKING_NUMBER', keep='first')\n",
    "#tbl_BP.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adjusted-period",
   "metadata": {},
   "source": [
    "# Format PLD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "satisfied-mathematics",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Skip this if LWH Columns exist already. This line of code needs improvement. \n",
    "#turn dimensions into L W H columns. no need to run this if they are already separate. \n",
    "tbl_BP[['L', 'W', 'H']] = tbl_BP['DIMENSIONS'].str.split('x', expand=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "01ccea89",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Skip this if LWH Columns are integers. \n",
    "tbl_BP[['L', 'W', 'H']] = tbl_BP[['L', 'W', 'H']].apply(pd.to_numeric)\n",
    "tbl_BP['L'] = tbl_BP['L'].apply(lambda x: round(x,0) if x - math.floor(x) < 0.5 else np.ceil(x))\n",
    "tbl_BP['W'] = tbl_BP['W'].apply(lambda x: round(x,0) if x - math.floor(x) < 0.5 else np.ceil(x))\n",
    "tbl_BP['H'] = tbl_BP['H'].apply(lambda x: round(x,0) if x - math.floor(x) < 0.5 else np.ceil(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "fuzzy-telephone",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure that the columns are of a numeric type (float)\n",
    "tbl_BP['L'] = tbl_BP['L'].astype(float)\n",
    "tbl_BP['W'] = tbl_BP['W'].astype(float)\n",
    "tbl_BP['H'] = tbl_BP['H'].astype(float)\n",
    "\n",
    "# Create a temporary DataFrame with the columns 'L', 'W', and 'H'\n",
    "temp_df = tbl_BP[['L', 'W', 'H']]\n",
    "\n",
    "# Sort each row in descending order and reverse the columns to maintain the order L, W, H\n",
    "sorted_temp_df = np.sort(temp_df.values, axis=1)[:, ::-1]\n",
    "\n",
    "# Assign the sorted values back to the original DataFrame\n",
    "tbl_BP[['L', 'W', 'H']] = sorted_temp_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e68872ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "tbl_BP[['L_CI', 'W_CI', 'H_CI']] = tbl_BP['DIMENSIONS'].str.split('x', expand=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d7c585d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure that the columns are of a numeric type (float)\n",
    "tbl_BP['L_CI'] = tbl_BP['L_CI'].astype(float)\n",
    "tbl_BP['W_CI'] = tbl_BP['W_CI'].astype(float)\n",
    "tbl_BP['H_CI'] = tbl_BP['H_CI'].astype(float)\n",
    "\n",
    "# Create a temporary DataFrame with the columns 'L', 'W', and 'H'\n",
    "temp_df = tbl_BP[['L_CI', 'W_CI', 'H_CI']]\n",
    "\n",
    "# Sort each row in descending order and reverse the columns to maintain the order L, W, H\n",
    "sorted_temp_df = np.sort(temp_df.values, axis=1)[:, ::-1]\n",
    "\n",
    "# Assign the sorted values back to the original DataFrame\n",
    "tbl_BP[['L_CI', 'W_CI', 'H_CI']] = sorted_temp_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d97db02e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to round down to the nearest quarter\n",
    "def round_down_to_quarter_inch(value):\n",
    "    return (value // 0.25) * 0.25\n",
    "\n",
    "# Assuming df1 is your DataFrame with the columns 'Length', 'Width', and 'Height'\n",
    "# Apply the function to each of the three columns\n",
    "tbl_BP['L_CI'] = tbl_BP['L_CI'].apply(round_down_to_quarter_inch)\n",
    "tbl_BP['W_CI'] = tbl_BP['W_CI'].apply(round_down_to_quarter_inch)\n",
    "tbl_BP['H_CI'] = tbl_BP['H_CI'].apply(round_down_to_quarter_inch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "aquatic-white",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create columns of GIRTH_AND_L and CUBIC_INCH\n",
    "tbl_BP['GIRTH_AND_L'] = round(tbl_BP['W'] * 2 + tbl_BP['H'] * 2 + tbl_BP['L'],0)\n",
    "tbl_BP['CUBIC_INCH'] = round(tbl_BP['W'] * tbl_BP['H'] * tbl_BP['L'],0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "modern-league",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create ACTUAL_WEIGHT_LB(rounded up to the nearest lb) and ACTUAL_WEIGHT_OZ(rounded up to the nearest oz). \n",
    "# Still keep the same value if the cell is empty string or has any missing values.  \n",
    "tbl_BP['ACTUAL_WEIGHT_LB'] = tbl_BP['ACTUAL_WEIGHT'].apply(lambda x: np.ceil(x) if pd.notna(x) and x != '' else x)\n",
    "tbl_BP['ACTUAL_WEIGHT_OZ_1'] = tbl_BP['ACTUAL_WEIGHT']*16\n",
    "tbl_BP['ACTUAL_WEIGHT_OZ'] = tbl_BP['ACTUAL_WEIGHT_OZ_1'].apply(lambda x: np.ceil(x) if pd.notna(x) and x != '' else x)\n",
    "tbl_BP = tbl_BP.drop('ACTUAL_WEIGHT_OZ_1', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "concerned-temperature",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Skip this if the left 5 digits are provided already\n",
    "# get the left 5 digits of the zip code or fill the zip to 5 digits\n",
    "#import re\n",
    "#split_col = tbl_BP['ZIP_CODE'].str.split('-')\n",
    "#left_col = split_col.str[0]\n",
    "#tbl_BP['ZIP_CODE'] = left_col\n",
    "#tbl_BP['ZIP_CODE'] = tbl_BP['ZIP_CODE'].apply(lambda x: re.match(r'\\d{5}', str(x)).group(0) if (pd.notna(x) and re.match(r'\\d{5}', str(x))) else '')\n",
    "# fill the 3 and 4 digits zip to 5 digits\n",
    "tbl_BP['ZIP_CODE'] = tbl_BP['ZIP_CODE'].astype(str)\n",
    "tbl_BP['ZIP_CODE'] = tbl_BP['ZIP_CODE'].apply(lambda x: '0' + x if len(x) == 4 else ('00' + x if (len(x) == 3) else x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "bd9c9033",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add Cubic Inch column for regular pack and L+W column for soft pack\n",
    "tbl_BP['CUBIC_FT_CI'] = np.ceil(((tbl_BP['L_CI'] * tbl_BP['W_CI'] * tbl_BP['H_CI'])/1728)*10)/10\n",
    "tbl_BP['L_AND_W_CI'] = np.ceil((tbl_BP['L_CI'] + tbl_BP['W_CI'])*10)/10\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffd5990f",
   "metadata": {},
   "source": [
    "# Decide the facility to use (Optional)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3f0dacf",
   "metadata": {},
   "source": [
    "#Decide by most likely State"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "876f5660",
   "metadata": {},
   "outputs": [],
   "source": [
    "# When PLD does not have Zone column, the origin facility needs to be decided based on destination State. \n",
    "# Specify the columns you want to read\n",
    "columns_to_read = ['Zipcode', 'State']\n",
    "\n",
    "# Read the CSV file and select the desired columns\n",
    "tbl_ZiptoState = pd.read_excel('ZiptoState.xlsx', usecols=columns_to_read)\n",
    "\n",
    "# Drop duplicates based on specific columns\n",
    "tbl_ZiptoState = tbl_ZiptoState.drop_duplicates(subset=['Zipcode'])\n",
    "\n",
    "#Add State column to PLD\n",
    "tbl_ZiptoState['Zipcode'] = tbl_ZiptoState['Zipcode'].astype(str)\n",
    "tbl_BP = tbl_BP.merge(tbl_ZiptoState[[\"Zipcode\", \"State\"]], left_on='ZIP_CODE', right_on='Zipcode', how='left').rename(columns={'State': 'STATE'}).drop('Zipcode', axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "828b1f6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_tbl_BP = tbl_BP.groupby('STATE')['TRACKING_NUMBER'].count()\n",
    "ranked_tbl_BP = grouped_tbl_BP.sort_values(ascending=False)\n",
    "#If this facility selection is not obvious, use the extended modal to calculate everything and calculate. \n",
    "ranked_tbl_BP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18c87812",
   "metadata": {},
   "source": [
    "#Decide by Zone Count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "25482c21",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sliu\\AppData\\Local\\Temp\\ipykernel_17780\\2900919867.py:15: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  tbl_BP['ZONE_02038'] = tbl_BP['ZONE_02038'].astype(str).str.replace('\\.0$', '').astype(int)\n",
      "C:\\Users\\sliu\\AppData\\Local\\Temp\\ipykernel_17780\\2900919867.py:31: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  tbl_BP['ZONE_08873'] = tbl_BP['ZONE_08873'].astype(str).str.replace('\\.0$', '').astype(int)\n",
      "C:\\Users\\sliu\\AppData\\Local\\Temp\\ipykernel_17780\\2900919867.py:47: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  tbl_BP['ZONE_21226'] = tbl_BP['ZONE_21226'].astype(str).str.replace('\\.0$', '').astype(int)\n",
      "C:\\Users\\sliu\\AppData\\Local\\Temp\\ipykernel_17780\\2900919867.py:63: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  tbl_BP['ZONE_38141'] = tbl_BP['ZONE_38141'].astype(str).str.replace('\\.0$', '').astype(int)\n",
      "C:\\Users\\sliu\\AppData\\Local\\Temp\\ipykernel_17780\\2900919867.py:79: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  tbl_BP['ZONE_38654'] = tbl_BP['ZONE_38654'].astype(str).str.replace('\\.0$', '').astype(int)\n",
      "C:\\Users\\sliu\\AppData\\Local\\Temp\\ipykernel_17780\\2900919867.py:95: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  tbl_BP['ZONE_75041'] = tbl_BP['ZONE_75041'].astype(str).str.replace('\\.0$', '').astype(int)\n",
      "C:\\Users\\sliu\\AppData\\Local\\Temp\\ipykernel_17780\\2900919867.py:111: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  tbl_BP['ZONE_90640'] = tbl_BP['ZONE_90640'].astype(str).str.replace('\\.0$', '').astype(int)\n"
     ]
    }
   ],
   "source": [
    "# select facilities based on the total Zone Count. \n",
    "tbl_BP['FROM_ZIP_02038'] = '2038'\n",
    "tbl_BP['FROM_ZIP_02038'] = tbl_BP['FROM_ZIP_02038'].astype(str)\n",
    "OB_STATE['Origin'] = OB_STATE['Origin'].astype(str)\n",
    "OB_STATE['Dest Zip'] = OB_STATE['Dest Zip'].astype(str)\n",
    "OB_STATE['Dest Zip'] = OB_STATE['Dest Zip'].apply(lambda x: '0' + x if len(x) == 4 else ('00' + x if (len(x) == 3) else x))\n",
    "\n",
    "tbl_BP = tbl_BP.merge(OB_STATE[[\"Origin\", \"Dest Zip\", \"GNDZone\"]], left_on=['FROM_ZIP_02038', 'ZIP_CODE'], right_on=['Origin','Dest Zip'], how='left').rename(columns={'GNDZone': 'ZONE_02038'}).drop(['Origin','Dest Zip'], axis=1)\n",
    "tbl_BP['ZONE_02038'] = tbl_BP['ZONE_02038'].astype('str')\n",
    "tbl_BP['ZONE_02038'] = tbl_BP['ZONE_02038'].str.replace('44', '9')\n",
    "tbl_BP['ZONE_02038'] = tbl_BP['ZONE_02038'].str.replace('45', '10')\n",
    "tbl_BP['ZONE_02038'] = tbl_BP['ZONE_02038'].str.replace('46', '11')\n",
    "tbl_BP['ZONE_02038'] = tbl_BP['ZONE_02038'].replace('nan', np.nan).astype(float).fillna(0).astype(int)\n",
    "tbl_BP['ZONE_02038'] = tbl_BP['ZONE_02038'].astype(str).str.replace('\\.0$', '').astype(int)\n",
    "\n",
    "\n",
    "tbl_BP['FROM_ZIP_08873'] = '8873'\n",
    "tbl_BP['FROM_ZIP_08873'] = tbl_BP['FROM_ZIP_08873'].astype(str)\n",
    "OB_STATE['Origin'] = OB_STATE['Origin'].astype(str)\n",
    "OB_STATE['Dest Zip'] = OB_STATE['Dest Zip'].astype(str)\n",
    "OB_STATE['Dest Zip'] = OB_STATE['Dest Zip'].apply(lambda x: '0' + x if len(x) == 4 else ('00' + x if (len(x) == 3) else x))\n",
    "\n",
    "tbl_BP = tbl_BP.merge(OB_STATE[[\"Origin\", \"Dest Zip\", \"GNDZone\"]], left_on=['FROM_ZIP_08873', 'ZIP_CODE'], right_on=['Origin','Dest Zip'], how='left').rename(columns={'GNDZone': 'ZONE_08873'}).drop(['Origin','Dest Zip'], axis=1)\n",
    "tbl_BP['ZONE_08873'] = tbl_BP['ZONE_08873'].astype('str')\n",
    "tbl_BP['ZONE_08873'] = tbl_BP['ZONE_08873'].str.replace('44', '9')\n",
    "tbl_BP['ZONE_08873'] = tbl_BP['ZONE_08873'].str.replace('45', '10')\n",
    "tbl_BP['ZONE_08873'] = tbl_BP['ZONE_08873'].str.replace('46', '11')\n",
    "tbl_BP['ZONE_08873'] = tbl_BP['ZONE_08873'].replace('nan', np.nan).astype(float).fillna(0).astype(int)\n",
    "tbl_BP['ZONE_08873'] = tbl_BP['ZONE_08873'].astype(str).str.replace('\\.0$', '').astype(int)\n",
    "\n",
    "\n",
    "tbl_BP['FROM_ZIP_21226'] = '21226'\n",
    "tbl_BP['FROM_ZIP_21226'] = tbl_BP['FROM_ZIP_21226'].astype(str)\n",
    "OB_STATE['Origin'] = OB_STATE['Origin'].astype(str)\n",
    "OB_STATE['Dest Zip'] = OB_STATE['Dest Zip'].astype(str)\n",
    "OB_STATE['Dest Zip'] = OB_STATE['Dest Zip'].apply(lambda x: '0' + x if len(x) == 4 else ('00' + x if (len(x) == 3) else x))\n",
    "\n",
    "tbl_BP = tbl_BP.merge(OB_STATE[[\"Origin\", \"Dest Zip\", \"GNDZone\"]], left_on=['FROM_ZIP_21226', 'ZIP_CODE'], right_on=['Origin','Dest Zip'], how='left').rename(columns={'GNDZone': 'ZONE_21226'}).drop(['Origin','Dest Zip'], axis=1)\n",
    "tbl_BP['ZONE_21226'] = tbl_BP['ZONE_21226'].astype('str')\n",
    "tbl_BP['ZONE_21226'] = tbl_BP['ZONE_21226'].str.replace('44', '9')\n",
    "tbl_BP['ZONE_21226'] = tbl_BP['ZONE_21226'].str.replace('45', '10')\n",
    "tbl_BP['ZONE_21226'] = tbl_BP['ZONE_21226'].str.replace('46', '11')\n",
    "tbl_BP['ZONE_21226'] = tbl_BP['ZONE_21226'].replace('nan', np.nan).astype(float).fillna(0).astype(int)\n",
    "tbl_BP['ZONE_21226'] = tbl_BP['ZONE_21226'].astype(str).str.replace('\\.0$', '').astype(int)\n",
    "\n",
    "\n",
    "tbl_BP['FROM_ZIP_38141'] = '38141'\n",
    "tbl_BP['FROM_ZIP_38141'] = tbl_BP['FROM_ZIP_38141'].astype(str)\n",
    "OB_STATE['Origin'] = OB_STATE['Origin'].astype(str)\n",
    "OB_STATE['Dest Zip'] = OB_STATE['Dest Zip'].astype(str)\n",
    "OB_STATE['Dest Zip'] = OB_STATE['Dest Zip'].apply(lambda x: '0' + x if len(x) == 4 else ('00' + x if (len(x) == 3) else x))\n",
    "\n",
    "tbl_BP = tbl_BP.merge(OB_STATE[[\"Origin\", \"Dest Zip\", \"GNDZone\"]], left_on=['FROM_ZIP_38141', 'ZIP_CODE'], right_on=['Origin','Dest Zip'], how='left').rename(columns={'GNDZone': 'ZONE_38141'}).drop(['Origin','Dest Zip'], axis=1)\n",
    "tbl_BP['ZONE_38141'] = tbl_BP['ZONE_38141'].astype('str')\n",
    "tbl_BP['ZONE_38141'] = tbl_BP['ZONE_38141'].str.replace('44', '9')\n",
    "tbl_BP['ZONE_38141'] = tbl_BP['ZONE_38141'].str.replace('45', '10')\n",
    "tbl_BP['ZONE_38141'] = tbl_BP['ZONE_38141'].str.replace('46', '11')\n",
    "tbl_BP['ZONE_38141'] = tbl_BP['ZONE_38141'].replace('nan', np.nan).astype(float).fillna(0).astype(int)\n",
    "tbl_BP['ZONE_38141'] = tbl_BP['ZONE_38141'].astype(str).str.replace('\\.0$', '').astype(int)\n",
    "\n",
    "\n",
    "tbl_BP['FROM_ZIP_38654'] = '38654'\n",
    "tbl_BP['FROM_ZIP_38654'] = tbl_BP['FROM_ZIP_38654'].astype(str)\n",
    "OB_STATE['Origin'] = OB_STATE['Origin'].astype(str)\n",
    "OB_STATE['Dest Zip'] = OB_STATE['Dest Zip'].astype(str)\n",
    "OB_STATE['Dest Zip'] = OB_STATE['Dest Zip'].apply(lambda x: '0' + x if len(x) == 4 else ('00' + x if (len(x) == 3) else x))\n",
    "\n",
    "tbl_BP = tbl_BP.merge(OB_STATE[[\"Origin\", \"Dest Zip\", \"GNDZone\"]], left_on=['FROM_ZIP_38654', 'ZIP_CODE'], right_on=['Origin','Dest Zip'], how='left').rename(columns={'GNDZone': 'ZONE_38654'}).drop(['Origin','Dest Zip'], axis=1)\n",
    "tbl_BP['ZONE_38654'] = tbl_BP['ZONE_38654'].astype('str')\n",
    "tbl_BP['ZONE_38654'] = tbl_BP['ZONE_38654'].str.replace('44', '9')\n",
    "tbl_BP['ZONE_38654'] = tbl_BP['ZONE_38654'].str.replace('45', '10')\n",
    "tbl_BP['ZONE_38654'] = tbl_BP['ZONE_38654'].str.replace('46', '11')\n",
    "tbl_BP['ZONE_38654'] = tbl_BP['ZONE_38654'].replace('nan', np.nan).astype(float).fillna(0).astype(int)\n",
    "tbl_BP['ZONE_38654'] = tbl_BP['ZONE_38654'].astype(str).str.replace('\\.0$', '').astype(int)\n",
    "\n",
    "\n",
    "tbl_BP['FROM_ZIP_75041'] = '75041'\n",
    "tbl_BP['FROM_ZIP_75041'] = tbl_BP['FROM_ZIP_75041'].astype(str)\n",
    "OB_STATE['Origin'] = OB_STATE['Origin'].astype(str)\n",
    "OB_STATE['Dest Zip'] = OB_STATE['Dest Zip'].astype(str)\n",
    "OB_STATE['Dest Zip'] = OB_STATE['Dest Zip'].apply(lambda x: '0' + x if len(x) == 4 else ('00' + x if (len(x) == 3) else x))\n",
    "\n",
    "tbl_BP = tbl_BP.merge(OB_STATE[[\"Origin\", \"Dest Zip\", \"GNDZone\"]], left_on=['FROM_ZIP_75041', 'ZIP_CODE'], right_on=['Origin','Dest Zip'], how='left').rename(columns={'GNDZone': 'ZONE_75041'}).drop(['Origin','Dest Zip'], axis=1)\n",
    "tbl_BP['ZONE_75041'] = tbl_BP['ZONE_75041'].astype('str')\n",
    "tbl_BP['ZONE_75041'] = tbl_BP['ZONE_75041'].str.replace('44', '9')\n",
    "tbl_BP['ZONE_75041'] = tbl_BP['ZONE_75041'].str.replace('45', '10')\n",
    "tbl_BP['ZONE_75041'] = tbl_BP['ZONE_75041'].str.replace('46', '11')\n",
    "tbl_BP['ZONE_75041'] = tbl_BP['ZONE_75041'].replace('nan', np.nan).astype(float).fillna(0).astype(int)\n",
    "tbl_BP['ZONE_75041'] = tbl_BP['ZONE_75041'].astype(str).str.replace('\\.0$', '').astype(int)\n",
    "\n",
    "\n",
    "tbl_BP['FROM_ZIP_90640'] = '90640'\n",
    "tbl_BP['FROM_ZIP_90640'] = tbl_BP['FROM_ZIP_90640'].astype(str)\n",
    "OB_STATE['Origin'] = OB_STATE['Origin'].astype(str)\n",
    "OB_STATE['Dest Zip'] = OB_STATE['Dest Zip'].astype(str)\n",
    "OB_STATE['Dest Zip'] = OB_STATE['Dest Zip'].apply(lambda x: '0' + x if len(x) == 4 else ('00' + x if (len(x) == 3) else x))\n",
    "\n",
    "tbl_BP = tbl_BP.merge(OB_STATE[[\"Origin\", \"Dest Zip\", \"GNDZone\"]], left_on=['FROM_ZIP_90640', 'ZIP_CODE'], right_on=['Origin','Dest Zip'], how='left').rename(columns={'GNDZone': 'ZONE_90640'}).drop(['Origin','Dest Zip'], axis=1)\n",
    "tbl_BP['ZONE_90640'] = tbl_BP['ZONE_90640'].astype('str')\n",
    "tbl_BP['ZONE_90640'] = tbl_BP['ZONE_90640'].str.replace('44', '9')\n",
    "tbl_BP['ZONE_90640'] = tbl_BP['ZONE_90640'].str.replace('45', '10')\n",
    "tbl_BP['ZONE_90640'] = tbl_BP['ZONE_90640'].str.replace('46', '11')\n",
    "tbl_BP['ZONE_90640'] = tbl_BP['ZONE_90640'].replace('nan', np.nan).astype(float).fillna(0).astype(int)\n",
    "tbl_BP['ZONE_90640'] = tbl_BP['ZONE_90640'].astype(str).str.replace('\\.0$', '').astype(int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "be613dec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of columns to consider\n",
    "columns_to_sum = ['ZONE_02038', 'ZONE_08873', 'ZONE_21226', 'ZONE_38141', 'ZONE_38654', 'ZONE_75041', 'ZONE_90640']\n",
    "\n",
    "# Calculate the sum for each column and get the index of the minimum value\n",
    "min_index = np.argmin([tbl_BP[col].sum() for col in columns_to_sum])\n",
    "\n",
    "# Get the column name with the minimum sum\n",
    "column_with_min_value = columns_to_sum[min_index]\n",
    "\n",
    "# Get the minimum sum value\n",
    "min_value = tbl_BP[column_with_min_value].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "68235a35",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15633"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "min_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "684244e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ZONE_21226'"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "column_with_min_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "d83a2a13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of columns to drop\n",
    "columns_to_drop = ['ZONE_02038', 'FROM_ZIP_02038', 'ZONE_08873', 'FROM_ZIP_08873', 'ZONE_38141', 'FROM_ZIP_38141', 'ZONE_38654', 'FROM_ZIP_38654', 'ZONE_75041', 'FROM_ZIP_75041', 'ZONE_90640', 'FROM_ZIP_90640']\n",
    "\n",
    "# Drop the specified columns\n",
    "tbl_BP.drop(columns=columns_to_drop, inplace=True)\n",
    "\n",
    "tbl_BP.rename(columns={'ZONE_21226': 'ZONE'}, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "85c6a797",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Payor</th>\n",
       "      <th>Ground Tracking ID Prefix</th>\n",
       "      <th>Transportation Charge Amount</th>\n",
       "      <th>Net Charge Amount</th>\n",
       "      <th>POD Delivery Date</th>\n",
       "      <th>POD Delivery Time</th>\n",
       "      <th>POD Service Area Code</th>\n",
       "      <th>Actual Weight Units</th>\n",
       "      <th>Rated Weight Amount</th>\n",
       "      <th>Rated Weight Units</th>\n",
       "      <th>...</th>\n",
       "      <th>H</th>\n",
       "      <th>ACTUAL_WEIGHT</th>\n",
       "      <th>RESIDENTIAL_FLAG</th>\n",
       "      <th>GIRTH_AND_L</th>\n",
       "      <th>CUBIC_INCH</th>\n",
       "      <th>ACTUAL_WEIGHT_LB</th>\n",
       "      <th>ACTUAL_WEIGHT_OZ</th>\n",
       "      <th>STATE</th>\n",
       "      <th>FROM_ZIP_21226</th>\n",
       "      <th>ZONE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Shipper</td>\n",
       "      <td>NaN</td>\n",
       "      <td>87.58</td>\n",
       "      <td>28.90</td>\n",
       "      <td>20230425.0</td>\n",
       "      <td>10:06</td>\n",
       "      <td>A2</td>\n",
       "      <td>lbs</td>\n",
       "      <td>2.0</td>\n",
       "      <td>lbs</td>\n",
       "      <td>...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>True</td>\n",
       "      <td>34.0</td>\n",
       "      <td>350.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>21226</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Shipper</td>\n",
       "      <td>NaN</td>\n",
       "      <td>125.60</td>\n",
       "      <td>39.42</td>\n",
       "      <td>20230425.0</td>\n",
       "      <td>10:40</td>\n",
       "      <td>A1</td>\n",
       "      <td>lbs</td>\n",
       "      <td>9.0</td>\n",
       "      <td>lbs</td>\n",
       "      <td>...</td>\n",
       "      <td>7.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>True</td>\n",
       "      <td>56.0</td>\n",
       "      <td>1456.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>144.0</td>\n",
       "      <td>IL</td>\n",
       "      <td>21226</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Shipper</td>\n",
       "      <td>NaN</td>\n",
       "      <td>85.04</td>\n",
       "      <td>28.20</td>\n",
       "      <td>20230425.0</td>\n",
       "      <td>10:45</td>\n",
       "      <td>A2</td>\n",
       "      <td>lbs</td>\n",
       "      <td>2.0</td>\n",
       "      <td>lbs</td>\n",
       "      <td>...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>True</td>\n",
       "      <td>34.0</td>\n",
       "      <td>350.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>MD</td>\n",
       "      <td>21226</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Shipper</td>\n",
       "      <td>NaN</td>\n",
       "      <td>139.33</td>\n",
       "      <td>43.22</td>\n",
       "      <td>20230425.0</td>\n",
       "      <td>09:25</td>\n",
       "      <td>A1</td>\n",
       "      <td>lbs</td>\n",
       "      <td>10.0</td>\n",
       "      <td>lbs</td>\n",
       "      <td>...</td>\n",
       "      <td>11.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>True</td>\n",
       "      <td>62.0</td>\n",
       "      <td>2112.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>48.0</td>\n",
       "      <td>SD</td>\n",
       "      <td>21226</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Shipper</td>\n",
       "      <td>NaN</td>\n",
       "      <td>193.80</td>\n",
       "      <td>58.28</td>\n",
       "      <td>20230425.0</td>\n",
       "      <td>11:38</td>\n",
       "      <td>AA</td>\n",
       "      <td>lbs</td>\n",
       "      <td>11.0</td>\n",
       "      <td>lbs</td>\n",
       "      <td>...</td>\n",
       "      <td>7.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>True</td>\n",
       "      <td>67.0</td>\n",
       "      <td>2261.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>80.0</td>\n",
       "      <td>CA</td>\n",
       "      <td>21226</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 55 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Payor  Ground Tracking ID Prefix  Transportation Charge Amount  \\\n",
       "0  Shipper                        NaN                         87.58   \n",
       "1  Shipper                        NaN                        125.60   \n",
       "2  Shipper                        NaN                         85.04   \n",
       "3  Shipper                        NaN                        139.33   \n",
       "4  Shipper                        NaN                        193.80   \n",
       "\n",
       "   Net Charge Amount  POD Delivery Date POD Delivery Time  \\\n",
       "0              28.90         20230425.0             10:06   \n",
       "1              39.42         20230425.0             10:40   \n",
       "2              28.20         20230425.0             10:45   \n",
       "3              43.22         20230425.0             09:25   \n",
       "4              58.28         20230425.0             11:38   \n",
       "\n",
       "  POD Service Area Code Actual Weight Units  Rated Weight Amount  \\\n",
       "0                    A2                 lbs                  2.0   \n",
       "1                    A1                 lbs                  9.0   \n",
       "2                    A2                 lbs                  2.0   \n",
       "3                    A1                 lbs                 10.0   \n",
       "4                    AA                 lbs                 11.0   \n",
       "\n",
       "  Rated Weight Units  ...     H  ACTUAL_WEIGHT  RESIDENTIAL_FLAG GIRTH_AND_L  \\\n",
       "0                lbs  ...   5.0            2.0              True        34.0   \n",
       "1                lbs  ...   7.0            9.0              True        56.0   \n",
       "2                lbs  ...   5.0            1.0              True        34.0   \n",
       "3                lbs  ...  11.0            3.0              True        62.0   \n",
       "4                lbs  ...   7.0            5.0              True        67.0   \n",
       "\n",
       "   CUBIC_INCH ACTUAL_WEIGHT_LB ACTUAL_WEIGHT_OZ STATE FROM_ZIP_21226  ZONE  \n",
       "0       350.0              2.0             32.0   NaN          21226     3  \n",
       "1      1456.0              9.0            144.0    IL          21226     4  \n",
       "2       350.0              1.0             16.0    MD          21226     2  \n",
       "3      2112.0              3.0             48.0    SD          21226     6  \n",
       "4      2261.0              5.0             80.0    CA          21226     8  \n",
       "\n",
       "[5 rows x 55 columns]"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tbl_BP.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02064950",
   "metadata": {},
   "source": [
    "# Specify the number of facilities to rerate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51d09ea5",
   "metadata": {},
   "source": [
    "**specific one facility rerate**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 535,
   "id": "fd837ec8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#specify FedEx facility\n",
    "OB_STATE = pd.read_csv('C:\\\\Users\\\\SLiu\\\\BD\\\\model\\\\RateQuote\\\\OB_STATE_FedEx.csv')\n",
    "OB_STATE[['Origin', 'Service']] = OB_STATE['FileID'].str.split('_', expand=True)\n",
    "OB_STATE.rename(columns={' Dest Zip/Postal Code': 'Dest Zip'}, inplace=True)\n",
    "\n",
    "tbl_BP['FROM_ZIP'] = '38141' # change this to the correct \"ship from\" facility\n",
    "tbl_BP['FROM_ZIP'] = tbl_BP['FROM_ZIP'].astype(str)\n",
    "OB_STATE['Origin'] = OB_STATE['Origin'].astype(str)\n",
    "OB_STATE['Dest Zip'] = OB_STATE['Dest Zip'].astype(str)\n",
    "OB_STATE['Dest Zip'] = OB_STATE['Dest Zip'].apply(lambda x: '0' + x if len(x) == 4 else ('00' + x if (len(x) == 3) else x))\n",
    "\n",
    "OB_STATE1 = OB_STATE[(OB_STATE['Service'] == 'FXG')]\n",
    "OB_STATE2 = OB_STATE[(OB_STATE['Service'] == 'FHD')]\n",
    "tbl_BP1 = tbl_BP[(tbl_BP['RESIDENTIAL_FLAG'] == False)]\n",
    "tbl_BP2 = tbl_BP[(tbl_BP['RESIDENTIAL_FLAG'] == True)]\n",
    "\n",
    "tbl_BP1 = tbl_BP1.merge(OB_STATE1[[\"Origin\", \"Dest Zip\", \" Zone\"]], left_on=['FROM_ZIP', 'ZIP_CODE'], right_on=['Origin','Dest Zip'], how='left').rename(columns={' Zone': 'ZONE'}).drop(['Origin','Dest Zip'], axis=1)\n",
    "tbl_BP1['ZONE'] = tbl_BP1['ZONE'].astype('str')\n",
    "#tbl_BP1['ZONE'] = tbl_BP1['ZONE'].str.replace('44', '9')\n",
    "#tbl_BP1['ZONE'] = tbl_BP1['ZONE'].str.replace('45', '10')\n",
    "#tbl_BP1['ZONE'] = tbl_BP1['ZONE'].str.replace('46', '11')\n",
    "#tbl_BP1['ZONE'] = tbl_BP1['ZONE'].replace('nan', np.nan).astype(float).fillna(0).astype(int)\n",
    "#tbl_BP1['ZONE'] = tbl_BP1['ZONE'].astype(str).str.replace('\\.0$', '').astype(int)\n",
    "\n",
    "tbl_BP2 = tbl_BP2.merge(OB_STATE2[[\"Origin\", \"Dest Zip\", \" Zone\"]], left_on=['FROM_ZIP', 'ZIP_CODE'], right_on=['Origin','Dest Zip'], how='left').rename(columns={' Zone': 'ZONE'}).drop(['Origin','Dest Zip'], axis=1)\n",
    "tbl_BP2['ZONE'] = tbl_BP2['ZONE'].astype('str')\n",
    "#tbl_BP2['ZONE'] = tbl_BP2['ZONE'].str.replace('44', '9')\n",
    "#tbl_BP2['ZONE'] = tbl_BP2['ZONE'].str.replace('45', '10')\n",
    "#tbl_BP2['ZONE'] = tbl_BP2['ZONE'].str.replace('46', '11')\n",
    "#tbl_BP2['ZONE'] = tbl_BP2['ZONE'].replace('nan', np.nan).astype(float).fillna(0).astype(int)\n",
    "#tbl_BP2['ZONE'] = tbl_BP2['ZONE'].astype(str).str.replace('\\.0$', '').astype(int)\n",
    "\n",
    "tbl_BP = pd.concat([tbl_BP1, tbl_BP2])\n",
    "\n",
    "tbl_BP['ZONE'] = tbl_BP['ZONE'].replace('nan', np.nan).astype(float).fillna(0).astype(int)\n",
    "tbl_BP['ZONE'] = tbl_BP['ZONE'].astype(str).str.replace('\\.0$', '').astype(int)\n",
    "tbl_BP['ZONE'] = tbl_BP['ZONE'].astype(str)\n",
    "\n",
    "tbl_BP1 = tbl_BP[(tbl_BP['ZONE'] != '0')]\n",
    "tbl_BP3 = tbl_BP[(tbl_BP['ZONE'] == '0')]\n",
    "# List of columns to drop\n",
    "columns_to_drop = ['ZONE']\n",
    "\n",
    "# Drop the specified columns\n",
    "tbl_BP3.drop(columns=columns_to_drop, inplace=True)\n",
    "tbl_BP3 = tbl_BP3.merge(OB_STATE1[[\"Origin\", \"Dest Zip\", \" Zone\"]], left_on=['FROM_ZIP', 'ZIP_CODE'], right_on=['Origin','Dest Zip'], how='left').rename(columns={' Zone': 'ZONE'}).drop(['Origin','Dest Zip'], axis=1)\n",
    "tbl_BP3['ZONE'] = tbl_BP3['ZONE'].astype('str')\n",
    "\n",
    "tbl_BP = pd.concat([tbl_BP1, tbl_BP3])\n",
    "tbl_BP['ZONE'] = tbl_BP['ZONE'].replace('nan', np.nan).astype(float).fillna(0).astype(int)\n",
    "tbl_BP['ZONE'] = tbl_BP['ZONE'].astype(str).str.replace('\\.0$', '').astype(int)\n",
    "tbl_BP['ZONE'] = tbl_BP['ZONE'].astype(str)\n",
    "\n",
    "tbl_BP.rename(columns={'ZONE': 'ZONE_FedEx'}, inplace=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 537,
   "id": "f36a13bb-6a18-4687-abd3-9634716ad1e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\SLiu\\AppData\\Local\\Temp\\ipykernel_2256\\2498246324.py:1: DtypeWarning: Columns (6,7,9,10) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  OB_STATE = pd.read_csv('C:\\\\Users\\\\SLiu\\\\BD\\\\model\\\\RateQuote\\\\OB_STATE_UPS.csv')\n"
     ]
    }
   ],
   "source": [
    "#specify UPS facility\n",
    "OB_STATE = pd.read_csv('C:\\\\Users\\\\SLiu\\\\BD\\\\model\\\\RateQuote\\\\OB_STATE_UPS.csv')\n",
    "tbl_BP['FROM_ZIP'] = '38141'\n",
    "tbl_BP['FROM_ZIP'] = tbl_BP['FROM_ZIP'].astype(str)\n",
    "OB_STATE['Origin'] = OB_STATE['Origin'].astype(str)\n",
    "OB_STATE['Dest Zip'] = OB_STATE['Dest Zip'].astype(str)\n",
    "OB_STATE['Dest Zip'] = OB_STATE['Dest Zip'].apply(lambda x: '0' + x if len(x) == 4 else ('00' + x if (len(x) == 3) else x))\n",
    "\n",
    "tbl_BP = tbl_BP.merge(OB_STATE[[\"Origin\", \"Dest Zip\", \"GNDZone\"]], left_on=['FROM_ZIP', 'ZIP_CODE'], right_on=['Origin','Dest Zip'], how='left').rename(columns={'GNDZone': 'ZONE'}).drop(['Origin','Dest Zip'], axis=1)\n",
    "tbl_BP['ZONE'] = tbl_BP['ZONE'].astype('str')\n",
    "tbl_BP['ZONE'] = tbl_BP['ZONE'].str.replace('44', '9')\n",
    "tbl_BP['ZONE'] = tbl_BP['ZONE'].str.replace('45', '10')\n",
    "tbl_BP['ZONE'] = tbl_BP['ZONE'].str.replace('46', '11')\n",
    "tbl_BP['ZONE'] = tbl_BP['ZONE'].replace('nan', np.nan).astype(float).fillna(0).astype(int)\n",
    "tbl_BP['ZONE'] = tbl_BP['ZONE'].astype(str).str.replace('\\.0$', '').astype(int)\n",
    "\n",
    "tbl_BP.rename(columns={'ZONE': 'ZONE_UPS'}, inplace=True)\n",
    "tbl_BP['ZONE_UPS'] = tbl_BP['ZONE_UPS'].astype('str')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73c60ca4",
   "metadata": {},
   "source": [
    "**specific 2 facilities rerate**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "id": "8d03d57a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sliu\\AppData\\Local\\Temp\\ipykernel_12636\\3153116086.py:13: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  tbl_BP['ZONE'] = tbl_BP['ZONE'].astype(str).str.replace('\\.0$', '').astype(int)\n"
     ]
    }
   ],
   "source": [
    "tbl_BP1 = tbl_BP[(tbl_BP['FACILITY'] == 'JER')]\n",
    "tbl_BP2 = tbl_BP[(tbl_BP['FACILITY'] == 'CA3')]\n",
    "\n",
    "tbl_BP1['From Zip'] = '08873'\n",
    "tbl_BP2['From Zip'] = '90640'\n",
    "\n",
    "tbl_BP = pd.concat([tbl_BP1, tbl_BP2])\n",
    "\n",
    "tbl_BP['From Zip'] = tbl_BP['From Zip'].astype(str)\n",
    "OB_STATE['Origin'] = OB_STATE['Origin'].astype(str)\n",
    "OB_STATE['Dest Zip'] = OB_STATE['Dest Zip'].astype(str)\n",
    "OB_STATE['Dest Zip'] = OB_STATE['Dest Zip'].apply(lambda x: '0' + x if len(x) == 4 else ('00' + x if (len(x) == 3) else x))\n",
    "\n",
    "tbl_BP = tbl_BP.merge(OB_STATE[[\"Origin\", \"Dest Zip\", \"GNDZone\"]], left_on=['From Zip', 'ZIP_CODE'], right_on=['Origin','Dest Zip'], how='left').rename(columns={'GNDZone': 'ZONE'}).drop(['Origin','Dest Zip'], axis=1)\n",
    "tbl_BP['ZONE'] = tbl_BP['ZONE'].astype('str')\n",
    "tbl_BP['ZONE'] = tbl_BP['ZONE'].str.replace('44', '9')\n",
    "tbl_BP['ZONE'] = tbl_BP['ZONE'].str.replace('45', '10')\n",
    "tbl_BP['ZONE'] = tbl_BP['ZONE'].str.replace('46', '11')\n",
    "tbl_BP['ZONE'] = tbl_BP['ZONE'].replace('nan', np.nan).astype(float).fillna(0).astype(int)\n",
    "tbl_BP['ZONE'] = tbl_BP['ZONE'].astype(str).str.replace('\\.0$', '').astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcaf9d2a",
   "metadata": {},
   "source": [
    "**Specify all facility rerate**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "d914560d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sliu\\AppData\\Local\\Temp\\ipykernel_6600\\2846403398.py:12: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  tbl_BP['ZONE'] = tbl_BP['ZONE'].astype(str).str.replace('\\.0$', '').astype(int)\n"
     ]
    }
   ],
   "source": [
    "tbl_BP['From Zip'] = tbl_BP['From Zip'].astype(str)\n",
    "OB_STATE['Origin'] = OB_STATE['Origin'].astype(str)\n",
    "OB_STATE['Dest Zip'] = OB_STATE['Dest Zip'].astype(str)\n",
    "OB_STATE['Dest Zip'] = OB_STATE['Dest Zip'].apply(lambda x: '0' + x if len(x) == 4 else ('00' + x if (len(x) == 3) else x))\n",
    "\n",
    "tbl_BP = tbl_BP.merge(OB_STATE[[\"Origin\", \"Dest Zip\", \"GNDZone\"]], left_on=['From Zip', 'ZIP_CODE'], right_on=['Origin','Dest Zip'], how='left').rename(columns={'GNDZone': 'ZONE'}).drop(['Origin','Dest Zip'], axis=1)\n",
    "tbl_BP['ZONE'] = tbl_BP['ZONE'].astype('str')\n",
    "tbl_BP['ZONE'] = tbl_BP['ZONE'].str.replace('44', '9')\n",
    "tbl_BP['ZONE'] = tbl_BP['ZONE'].str.replace('45', '10')\n",
    "tbl_BP['ZONE'] = tbl_BP['ZONE'].str.replace('46', '11')\n",
    "tbl_BP['ZONE'] = tbl_BP['ZONE'].replace('nan', np.nan).astype(float).fillna(0).astype(int)\n",
    "tbl_BP['ZONE'] = tbl_BP['ZONE'].astype(str).str.replace('\\.0$', '').astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "expected-assignment",
   "metadata": {},
   "source": [
    "# Step 3 - b. Rerate Begin\n",
    "#====================UPS====================\n",
    "\n",
    "#REDE (UPS NDA Early): commercial & residential, lb    \n",
    "\n",
    "#RED (UPS NDA): commercial & residential, lb\n",
    "\n",
    "#REDS (UPS NDA Saver): commercial & residential, lb\n",
    "\n",
    "#2DAM (UPS 2DA A.M.): commercial & residential, lb\n",
    "\n",
    "#BLUE (UPS 2DA): commercial & residential, lb\n",
    "\n",
    "#ORNG (UPS 3DA): commercial & residential, lb\n",
    "\n",
    "#GRND (UPS Ground Commercial): commercial only, lb\n",
    "\n",
    "#GRES (UPS Ground Residential): residential only, lb\n",
    "\n",
    "#SRPT<1 (UPS Surepost 1#>): commercial & residential, oz\n",
    "\n",
    "#SRPT (UPS Surepost): commercial & residential, lb\n",
    "\n",
    "#====================DHL====================\n",
    "\n",
    "#DHLG (DHL SmartMail Parcel Plus Ground 1-25): commercial & residential, lb\n",
    "\n",
    "#DHLG<1 (DHL SmartMail Parcel Ground < 1lb): commercial & residential, oz\n",
    "\n",
    "#DHLE (DHL SmartMail Parcel Plus Expedited 1-25): commercial & residential, lb\n",
    "\n",
    "#DHLE<1 (DHL SmartMail Parcel Expedited  < 1lb): commercial & residential, oz\n",
    "\n",
    "#DHLEM (DHL SmartMail Parcel Expedited Max): commercial & residential, both oz and lb\n",
    "\n",
    "#====================USPS====================\n",
    "\n",
    "#USPSAG (USPS Auctane GRND): commercial & residential, lb\n",
    "\n",
    "#USPSAP (USPS Auctane PM): commercial & residential, lb\n",
    "\n",
    "#==========================MI===========================================\n",
    "\n",
    "#MIPLE (MI Parcel Select Lightweight Expedited): commercial & residential, oz\n",
    "\n",
    "#MIPH (MI Parcel Select Heavyweight): commercial & residential, lb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e5e8b31",
   "metadata": {},
   "source": [
    "# ========= UPS & FedEx Section =========="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 539,
   "id": "c94beed7-b986-4a13-97dd-191b8e7090ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add a column named DAS_CATEGORY to PLD with ups accessorial category DAS, DASE, RA, HI, or AK. One package (or row) can only have one single category.\n",
    "# create UPS Das\n",
    "xls = pd.ExcelFile('C:\\\\Users\\\\SLiu\\\\BD\\\\model\\\\RateQuote\\\\5.ZipToDas_2024.xlsx')\n",
    "tbl_area_surcharge_UPS = pd.read_excel(xls, sheet_name='UPS')\n",
    "tbl_area_surcharge_FedEx = pd.read_excel(xls, sheet_name='FedEx')\n",
    "\n",
    "tbl_area_surcharge_UPS['Zip'] = tbl_area_surcharge_UPS['Zip'].astype(str)\n",
    "tbl_area_surcharge_UPS['Zip'] = tbl_area_surcharge_UPS['Zip'].apply(lambda x: '0' + x if len(x) == 4 else ('00' + x if (len(x) == 3) else x))\n",
    "\n",
    "tbl_BP = tbl_BP.merge(tbl_area_surcharge_UPS[[\"Zip\", \"Type\"]], left_on='ZIP_CODE', right_on='Zip', how='left')\n",
    "tbl_BP['DAS_CATEGORY_UPS'] = tbl_BP['Type']\n",
    "tbl_BP = tbl_BP.drop('Type', axis=1)\n",
    "tbl_BP = tbl_BP.drop('Zip', axis=1)\n",
    "#tbl_BP.info()\n",
    "#tbl_BP['ZONE'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 540,
   "id": "3dccb490-19ff-4f1d-9ff9-b3add87601eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create FedEx Das\n",
    "tbl_area_surcharge_FedEx['Zip'] = tbl_area_surcharge_FedEx['Zip'].astype(str)\n",
    "tbl_area_surcharge_FedEx['Zip'] = tbl_area_surcharge_FedEx['Zip'].apply(lambda x: '0' + x if len(x) == 4 else ('00' + x if (len(x) == 3) else x))\n",
    "\n",
    "tbl_BP = tbl_BP.merge(tbl_area_surcharge_FedEx[[\"Zip\", \"Type\"]], left_on='ZIP_CODE', right_on='Zip', how='left')\n",
    "tbl_BP['DAS_CATEGORY_FedEx'] = tbl_BP['Type']\n",
    "tbl_BP = tbl_BP.drop('Type', axis=1)\n",
    "tbl_BP = tbl_BP.drop('Zip', axis=1)\n",
    "#tbl_BP.info()\n",
    "#tbl_BP['ZONE'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dying-interview",
   "metadata": {},
   "source": [
    "# REDE (UPS NDA Early) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 541,
   "id": "9133892c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get the right client rate and the corresponding PLD\n",
    "tbl_clientRates_1 = tbl_clientRates[(tbl_clientRates['SERVICE_CODE'] == 'REDE')]\n",
    "tbl_PLD_original_REDE = tbl_BP.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 542,
   "id": "d9d60753",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Rerate start\n",
    "#add billed weight column to PLD\n",
    "tbl_PLD_original_REDE['BILLED_WEIGHT_LB'] = tbl_PLD_original_REDE.apply(lambda row: max(row['ACTUAL_WEIGHT_LB'], np.ceil(row['CUBIC_INCH']/dim_factor_ups)), axis=1)\n",
    "\n",
    "\n",
    "#add Freight column to PLD\n",
    "tbl_PLD_original_REDE['ZONE_UPS'] = tbl_PLD_original_REDE['ZONE_UPS'].astype(str)\n",
    "tbl_PLD_original_REDE = tbl_PLD_original_REDE.merge(tbl_clientRates_1[[\"WEIGHT_LB\", \"ZONE\", \"CLIENT_RATES\"]], left_on=['BILLED_WEIGHT_LB', 'ZONE_UPS'], right_on=['WEIGHT_LB', 'ZONE'], how='left')\n",
    "tbl_PLD_original_REDE = tbl_PLD_original_REDE.drop('WEIGHT_LB', axis=1)\n",
    "tbl_PLD_original_REDE = tbl_PLD_original_REDE.rename(columns={'CLIENT_RATES': 'FRT_REDE'})\n",
    "   \n",
    "\n",
    "#add Residential Surcharge column to PLD\n",
    "tbl_PLD_original_REDE['RES_REDE'] = tbl_PLD_original_REDE.apply(lambda row: Resi_Surcharge_Air if row['RESIDENTIAL_FLAG'] is True else 0, axis=1)\n",
    "\n",
    "\n",
    "#add DAS column to PLD, Pay attention to the flags under Residential Flag\n",
    "tbl_PLD_original_REDE['DAS_REDE'] = tbl_PLD_original_REDE.apply(lambda row: \n",
    "                                                            DAS_Comm_Air if (row['DAS_CATEGORY_UPS'] == 'DASUPS' and row['RESIDENTIAL_FLAG'] is False) else \n",
    "                                                           (DAS_Comm_Extended_Air if (row['DAS_CATEGORY_UPS'] == 'DASEUPS' and row['RESIDENTIAL_FLAG'] is False) else\n",
    "                                                           (DAS_Resi_Air if (row['DAS_CATEGORY_UPS'] == 'DASUPS' and row['RESIDENTIAL_FLAG'] is True) else \n",
    "                                                           (DAS_Resi_Extended_Air if (row['DAS_CATEGORY_UPS'] == 'DASEUPS' and row['RESIDENTIAL_FLAG'] is True) else \n",
    "                                                           (Remote_Area if (row['DAS_CATEGORY_UPS'] == 'RAUPS') else \n",
    "                                                           (Remote_Area_Alaska if (row['DAS_CATEGORY_UPS'] == 'AKUPS') else\n",
    "                                                           (Remote_Area_Hawaii if (row['DAS_CATEGORY_UPS'] == 'HIUPS') else 0)))))), axis=1)\n",
    "\n",
    "\n",
    "#add fuel surcharge % column to PLD\n",
    "tbl_PLD_original_REDE['SHIP_DATE'] = pd.to_datetime(tbl_PLD_original_REDE['SHIP_DATE']).dt.date\n",
    "# get unique values from column 'SHIP_DATE' and create a new dataframe\n",
    "new_df = pd.DataFrame({'SHIP_DATE': tbl_PLD_original_REDE['SHIP_DATE'].unique()})\n",
    "# use this new dataframe to merge to FSC\n",
    "df_merge = new_df.merge(UPS_FSC, how='cross')\n",
    "\n",
    "\n",
    "\n",
    "#slice the merged dataframe to make sure ship_date is between the ship_data_start and end. Now we have a FSC file with Ship_date col to merge back to pld so that the pld can get FSC.\n",
    "df_merge['SHIP_DATE'] = pd.to_datetime(df_merge['SHIP_DATE']).dt.date\n",
    "df_merge = df_merge.query('SHIP_DATE >= SHIP_DATE_START and SHIP_DATE <= SHIP_DATE_END')\n",
    "df_merge = df_merge.drop_duplicates()\n",
    "# Get Domestic Air column as FSC. \n",
    "tbl_PLD_original_REDE = tbl_PLD_original_REDE.merge(df_merge, on=['SHIP_DATE'], how='left').drop(['SHIP_DATE_START', 'SHIP_DATE_END', 'Ground'], axis=1)\n",
    "tbl_PLD_original_REDE.rename(columns={'Domestic Air': 'FSC%_REDE'}, inplace=True)\n",
    "\n",
    "\n",
    "\n",
    "#add AHS column to PLD\n",
    "tbl_PLD_original_REDE['ZONE_UPS'] = pd.to_numeric(tbl_PLD_original_REDE['ZONE_UPS'], errors='coerce').fillna(0).astype(int)\n",
    "\n",
    "tbl_PLD_original_REDE['AHS_1'] = tbl_PLD_original_REDE.apply(lambda row: \n",
    "                                                          AH_Girth_2_UPS if (row['GIRTH_AND_L'] > 105 and row['ZONE_UPS'] == 2) else \n",
    "                                                           (AH_Girth_3_4_UPS if (row['GIRTH_AND_L'] > 105 and row['ZONE_UPS'] > 2 and row['ZONE_UPS'] < 5) else\n",
    "                                                           (AH_Girth_5_up_UPS if (row['GIRTH_AND_L'] > 105 and row['ZONE_UPS'] > 4) else 0)), axis=1)\n",
    "                                                    \n",
    "tbl_PLD_original_REDE['AHS_2'] = tbl_PLD_original_REDE.apply(lambda row: \n",
    "                                                           AH_L_2_UPS if (row['L'] > 48 and row['ZONE_UPS'] == 2) else \n",
    "                                                           (AH_L_3_4_UPS if (row['L'] > 48 and row['ZONE_UPS'] > 2 and row['ZONE_UPS'] < 5) else \n",
    "                                                           (AH_L_5_up_UPS if (row['L'] > 48 and row['ZONE_UPS'] > 4) else 0)), axis=1)\n",
    "\n",
    "tbl_PLD_original_REDE['AHS_3'] = tbl_PLD_original_REDE.apply(lambda row:\n",
    "                                                           AH_W_2_UPS if (row['W'] > 30 and row['ZONE_UPS'] == 2) else \n",
    "                                                           (AH_W_3_4_UPS if (row['W'] > 30 and row['ZONE_UPS'] > 2 and row['ZONE_UPS'] < 5) else \n",
    "                                                           (AH_W_5_up_UPS if (row['W'] > 30 and row['ZONE_UPS'] > 4) else 0)), axis=1)\n",
    "                                        \n",
    "tbl_PLD_original_REDE['AHS_4'] = tbl_PLD_original_REDE.apply(lambda row:\n",
    "                                                           AH_WGT_2_UPS if (row['ACTUAL_WEIGHT'] > 50 and row['ZONE_UPS'] == 2) else \n",
    "                                                           (AH_WGT_3_4_UPS if (row['ACTUAL_WEIGHT'] > 50 and row['ZONE_UPS'] > 2 and row['ZONE_UPS'] < 5) else \n",
    "                                                           (AH_WGT_5_up_UPS if (row['ACTUAL_WEIGHT'] > 50 and row['ZONE_UPS'] > 4) else 0)), axis=1)\n",
    "\n",
    "tbl_PLD_original_REDE['AHS_REDE'] = tbl_PLD_original_REDE['AHS_1'] + tbl_PLD_original_REDE['AHS_2'] + tbl_PLD_original_REDE['AHS_3'] + tbl_PLD_original_REDE['AHS_4']\n",
    "tbl_PLD_original_REDE = tbl_PLD_original_REDE.drop(['AHS_1','AHS_2','AHS_3','AHS_4'], axis=1)\n",
    "\n",
    "#Get Total rate. Set total rate to null if the freight rate is null and/or some package parameters are out of carrier handling capability.  \n",
    "tbl_PLD_original_REDE['TOTAL_REDE'] = round(tbl_PLD_original_REDE.apply(lambda row: \n",
    "                                                                  (1 + row['FSC%_REDE']*(1-BarrettDisc_Fsc_UPS)) * (row['FRT_REDE'] + row['RES_REDE'] + row['DAS_REDE'] + row['AHS_REDE']) if not pd.isna(row['FRT_REDE']) and row['FRT_REDE'] != '' else np.nan, axis=1),2)\n",
    "#tbl_PLD_original_REDE.to_csv('test.csv')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56e9330f-851f-4016-a4fb-9d3d74c49fc2",
   "metadata": {},
   "source": [
    "# 1STO (FedEx First Overnight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "bf1afc55-da0a-4430-a3ad-a62b3f986c75",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get the right client rate and the corresponding PLD\n",
    "tbl_clientRates_1 = tbl_clientRates[(tbl_clientRates['SERVICE_CODE'] == '1STO')]\n",
    "tbl_PLD_original_1STO = tbl_BP.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "718ab834-f647-44d9-a5e3-0657c820f4f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Rerate start\n",
    "#add billed weight column to PLD\n",
    "tbl_PLD_original_1STO['BILLED_WEIGHT_LB'] = tbl_PLD_original_1STO.apply(lambda row: max(row['ACTUAL_WEIGHT_LB'], np.ceil(row['CUBIC_INCH']/dim_factor_fedex)), axis=1)\n",
    "\n",
    "\n",
    "#add Freight column to PLD\n",
    "tbl_PLD_original_1STO['ZONE_FedEx'] = tbl_PLD_original_1STO['ZONE_FedEx'].replace('17', '9')\n",
    "tbl_PLD_original_1STO['ZONE_FedEx'] = tbl_PLD_original_1STO['ZONE_FedEx'].astype(str)\n",
    "tbl_PLD_original_1STO = tbl_PLD_original_1STO.merge(tbl_clientRates_1[[\"WEIGHT_LB\", \"ZONE\", \"CLIENT_RATES\"]], left_on=['BILLED_WEIGHT_LB', 'ZONE_FedEx'], right_on=['WEIGHT_LB', 'ZONE'], how='left')\n",
    "tbl_PLD_original_1STO = tbl_PLD_original_1STO.drop(['WEIGHT_LB', 'ZONE'], axis=1)\n",
    "tbl_PLD_original_1STO = tbl_PLD_original_1STO.rename(columns={'CLIENT_RATES': 'FRT_1STO'})\n",
    "\n",
    "\n",
    "#add Residential Surcharge column to PLD\n",
    "tbl_PLD_original_1STO['RES_1STO'] = tbl_PLD_original_1STO.apply(lambda row: Resi_Surcharge_Air_FedEx if row['RESIDENTIAL_FLAG'] is True else 0, axis=1)\n",
    "\n",
    "\n",
    "#add DAS column to PLD, Pay attention to the flags under Residential Flag\n",
    "tbl_PLD_original_1STO['DAS_1STO'] = tbl_PLD_original_1STO.apply(lambda row: \n",
    "                                                            0 if (row['DAS_CATEGORY_FedEx'] == 'DASFDX' and row['RESIDENTIAL_FLAG'] is False) else \n",
    "                                                           (0 if (row['DAS_CATEGORY_FedEx'] == 'DASEFDX' and row['RESIDENTIAL_FLAG'] is False) else\n",
    "                                                           (DAS_Resi_Air_FedEx if (row['DAS_CATEGORY_FedEx'] == 'DASFDX' and row['RESIDENTIAL_FLAG'] is True) else \n",
    "                                                           (DAS_Resi_Extended_Air_FedEx if (row['DAS_CATEGORY_FedEx'] == 'DASEFDX' and row['RESIDENTIAL_FLAG'] is True) else \n",
    "                                                           (Remote_Area_FedEx if (row['DAS_CATEGORY_FedEx'] == 'RAFDX') else \n",
    "                                                           (Remote_Area_Alaska_FedEx if (row['DAS_CATEGORY_FedEx'] == 'AKFDX') else\n",
    "                                                           (Remote_Area_Hawaii_FedEx if (row['DAS_CATEGORY_FedEx'] == 'HIFDX') else 0)))))), axis=1)\n",
    "\n",
    "\n",
    "#add fuel surcharge % column to PLD\n",
    "tbl_PLD_original_1STO['SHIP_DATE'] = pd.to_datetime(tbl_PLD_original_1STO['SHIP_DATE']).dt.date\n",
    "# get unique values from column 'SHIP_DATE' and create a new dataframe\n",
    "new_df = pd.DataFrame({'SHIP_DATE': tbl_PLD_original_1STO['SHIP_DATE'].unique()})\n",
    "# use this new dataframe to merge to FSC\n",
    "df_merge = new_df.merge(FedEx_FSC, how='cross')\n",
    "\n",
    "\n",
    "#slice the merged dataframe to make sure ship_date is between the ship_data_start and end. Now we have a FSC file with Ship_date col to merge back to pld so that the pld can get FSC.\n",
    "df_merge['SHIP_DATE'] = pd.to_datetime(df_merge['SHIP_DATE']).dt.date\n",
    "df_merge = df_merge.query('SHIP_DATE >= SHIP_DATE_START and SHIP_DATE <= SHIP_DATE_END')\n",
    "df_merge = df_merge.drop_duplicates()\n",
    "# Get Domestic Air column as FSC. \n",
    "tbl_PLD_original_1STO = tbl_PLD_original_1STO.merge(df_merge, on=['SHIP_DATE'], how='left').drop(['SHIP_DATE_START', 'SHIP_DATE_END', 'Ground FedEx'], axis=1)\n",
    "tbl_PLD_original_1STO.rename(columns={'Domestic Air FedEx': 'FSC%_1STO'}, inplace=True)\n",
    "\n",
    "\n",
    "#add AHS column to PLD\n",
    "# Define the conditions and their corresponding values\n",
    "def calculate_ahs1(row):\n",
    "    if any([row['GIRTH_AND_L'] > 105, row['L'] > 48, row['W'] > 30]):\n",
    "        if row['ZONE_FedEx'] == '2':\n",
    "            return AH_Dim_2_FedEx\n",
    "        elif row['ZONE_FedEx'] in ('3','4'):\n",
    "            return AH_Dim_3_4_FedEx\n",
    "        elif row['ZONE_FedEx'] in ('5','6'):\n",
    "            return AH_Dim_5_6_FedEx\n",
    "        elif row['ZONE_FedEx'] in ('7','8','9','10','11','12','17','26','9','10','17','99'):\n",
    "            return AH_Dim_7_up_FedEx\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def calculate_ahs2(row):\n",
    "    if row['ACTUAL_WEIGHT'] > 50:\n",
    "        if row['ZONE_FedEx'] == '2':\n",
    "            return AH_WGT_2_FedEx\n",
    "        elif row['ZONE_FedEx'] in ('3','4'):\n",
    "            return AH_WGT_3_4_FedEx\n",
    "        elif row['ZONE_FedEx'] in ('5','6'):\n",
    "            return AH_WGT_5_6_FedEx\n",
    "        elif row['ZONE_FedEx'] in ('7','8','9','10','11','12','17','26','9','10','17','99'):\n",
    "            return AH_WGT_7_up_FedEx\n",
    "\n",
    "    else:\n",
    "        return 0\n",
    "        \n",
    "# Apply the function to the DataFrame\n",
    "tbl_PLD_original_1STO['AHS_1STO_1'] = tbl_PLD_original_1STO.apply(calculate_ahs1, axis=1)\n",
    "tbl_PLD_original_1STO['AHS_1STO_2'] = tbl_PLD_original_1STO.apply(calculate_ahs2, axis=1)\n",
    "tbl_PLD_original_1STO['AHS_1STO'] = tbl_PLD_original_1STO['AHS_1STO_1'] + tbl_PLD_original_1STO['AHS_1STO_2']\n",
    "tbl_PLD_original_1STO = tbl_PLD_original_1STO.drop(['AHS_1STO_1','AHS_1STO_2'], axis=1)\n",
    "\n",
    "\n",
    "#Get Total rate. Set total rate to null if the freight rate is null and/or some package parameters are out of carrier handling capability.  \n",
    "tbl_PLD_original_1STO['TOTAL_1STO'] = round(tbl_PLD_original_1STO.apply(lambda row: \n",
    "                                                                  (1 + row['FSC%_1STO']*(1-BarrettDisc_Fsc_FedEx)) * (row['FRT_1STO'] + row['RES_1STO'] + row['DAS_1STO'] + row['AHS_1STO']) if not pd.isna(row['FRT_1STO']) and row['FRT_1STO'] != '' else np.nan, axis=1),2)\n",
    "#tbl_PLD_original_1STO.to_csv('test.csv')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "broken-safety",
   "metadata": {},
   "source": [
    "# RED (UPS NDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 549,
   "id": "92c04bae",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get the right client rate and the corresponding PLD\n",
    "tbl_clientRates_2 = tbl_clientRates[(tbl_clientRates['SERVICE_CODE'] == 'RED')]\n",
    "tbl_PLD_original_RED = tbl_BP.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 550,
   "id": "meaning-force",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Rerate start\n",
    "#add billed weight column to PLD\n",
    "tbl_PLD_original_RED['BILLED_WEIGHT_LB'] = tbl_PLD_original_RED.apply(lambda row: max(row['ACTUAL_WEIGHT_LB'], np.ceil(row['CUBIC_INCH']/dim_factor_ups)), axis=1)\n",
    "\n",
    "#add Freight column to PLD\n",
    "tbl_PLD_original_RED['ZONE_UPS'] = tbl_PLD_original_RED['ZONE_UPS'].astype(str)\n",
    "tbl_PLD_original_RED = tbl_PLD_original_RED.merge(tbl_clientRates_2[[\"WEIGHT_LB\", \"ZONE\", \"CLIENT_RATES\"]], left_on=['BILLED_WEIGHT_LB', 'ZONE_UPS'], right_on=['WEIGHT_LB', 'ZONE'], how='left')\n",
    "tbl_PLD_original_RED = tbl_PLD_original_RED.drop('WEIGHT_LB', axis=1)\n",
    "tbl_PLD_original_RED = tbl_PLD_original_RED.rename(columns={'CLIENT_RATES': 'FRT_RED'})\n",
    "   \n",
    "    \n",
    "#add Residential Surcharge column to PLD\n",
    "tbl_PLD_original_RED['RES_RED'] = tbl_PLD_original_RED.apply(lambda row: Resi_Surcharge_Air if row['RESIDENTIAL_FLAG'] is True else 0, axis=1)\n",
    "\n",
    "#add DAS column to PLD, Pay attention to the flags under Residential Flag\n",
    "tbl_PLD_original_RED['DAS_RED'] = tbl_PLD_original_RED.apply(lambda row: \n",
    "                                                            DAS_Comm_Air if (row['DAS_CATEGORY_UPS'] == 'DASUPS' and row['RESIDENTIAL_FLAG'] is False) else \n",
    "                                                           (DAS_Comm_Extended_Air if (row['DAS_CATEGORY_UPS'] == 'DASEUPS' and row['RESIDENTIAL_FLAG'] is False) else\n",
    "                                                           (DAS_Resi_Air if (row['DAS_CATEGORY_UPS'] == 'DASUPS' and row['RESIDENTIAL_FLAG'] is True) else \n",
    "                                                           (DAS_Resi_Extended_Air if (row['DAS_CATEGORY_UPS'] == 'DASEUPS' and row['RESIDENTIAL_FLAG'] is True) else \n",
    "                                                           (Remote_Area if (row['DAS_CATEGORY_UPS'] == 'RAUPS') else \n",
    "                                                           (Remote_Area_Alaska if (row['DAS_CATEGORY_UPS'] == 'AKUPS') else\n",
    "                                                           (Remote_Area_Hawaii if (row['DAS_CATEGORY_UPS'] == 'HIUPS') else 0)))))), axis=1)\n",
    "\n",
    "\n",
    "#add fuel surcharge % column to PLD\n",
    "tbl_PLD_original_RED['SHIP_DATE'] = pd.to_datetime(tbl_PLD_original_RED['SHIP_DATE']).dt.date\n",
    "# get unique values from column 'SHIP_DATE' and create a new dataframe\n",
    "new_df = pd.DataFrame({'SHIP_DATE': tbl_PLD_original_RED['SHIP_DATE'].unique()})\n",
    "# use this new dataframe to merge to FSC\n",
    "df_merge = new_df.merge(UPS_FSC, how='cross')\n",
    "df_merge['SHIP_DATE'] = pd.to_datetime(df_merge['SHIP_DATE']).dt.date\n",
    "#slice the merged dataframe to make sure ship_date is between the ship_data_start and end. Now we have a FSC file with Ship_date col to merge back to pld so that the pld can get FSC.\n",
    "df_merge = df_merge.query('SHIP_DATE >= SHIP_DATE_START and SHIP_DATE <= SHIP_DATE_END')\n",
    "df_merge = df_merge.drop_duplicates()\n",
    "# Get Domestic Air column as FSC. \n",
    "tbl_PLD_original_RED = tbl_PLD_original_RED.merge(df_merge, on=['SHIP_DATE'], how='left').drop(['SHIP_DATE_START', 'SHIP_DATE_END', 'Ground'], axis=1)\n",
    "tbl_PLD_original_RED.rename(columns={'Domestic Air': 'FSC%_RED'}, inplace=True)\n",
    "\n",
    "\n",
    "#add AHS column to PLD\n",
    "tbl_PLD_original_RED['ZONE_UPS'] = pd.to_numeric(tbl_PLD_original_RED['ZONE_UPS'], errors='coerce').fillna(0).astype(int)\n",
    "\n",
    "tbl_PLD_original_RED['AHS_1'] = tbl_PLD_original_RED.apply(lambda row: \n",
    "                                                           AH_Girth_2_UPS if (row['GIRTH_AND_L'] > 105 and row['ZONE_UPS'] == 2) else \n",
    "                                                           (AH_Girth_3_4_UPS if (row['GIRTH_AND_L'] > 105 and row['ZONE_UPS'] > 2 and row['ZONE_UPS'] < 5) else\n",
    "                                                           (AH_Girth_5_up_UPS if (row['GIRTH_AND_L'] > 105 and row['ZONE_UPS'] > 4) else 0)), axis=1)\n",
    "                                                    \n",
    "tbl_PLD_original_RED['AHS_2'] = tbl_PLD_original_RED.apply(lambda row: \n",
    "                                                           AH_L_2_UPS if (row['L'] > 48 and row['ZONE_UPS'] == 2) else \n",
    "                                                           (AH_L_3_4_UPS if (row['L'] > 48 and row['ZONE_UPS'] > 2 and row['ZONE_UPS'] < 5) else \n",
    "                                                           (AH_L_5_up_UPS if (row['L'] > 48 and row['ZONE_UPS'] > 4) else 0)), axis=1)\n",
    "\n",
    "tbl_PLD_original_RED['AHS_3'] = tbl_PLD_original_RED.apply(lambda row:\n",
    "                                                           AH_W_2_UPS if (row['W'] > 30 and row['ZONE_UPS'] == 2) else \n",
    "                                                           (AH_W_3_4_UPS if (row['W'] > 30 and row['ZONE_UPS'] > 2 and row['ZONE_UPS'] < 5) else \n",
    "                                                           (AH_W_5_up_UPS if (row['W'] > 30 and row['ZONE_UPS'] > 4) else 0)), axis=1)\n",
    "                                        \n",
    "tbl_PLD_original_RED['AHS_4'] = tbl_PLD_original_RED.apply(lambda row:\n",
    "                                                           AH_WGT_2_UPS if (row['ACTUAL_WEIGHT'] > 50 and row['ZONE_UPS'] == 2) else \n",
    "                                                           (AH_WGT_3_4_UPS if (row['ACTUAL_WEIGHT'] > 50 and row['ZONE_UPS'] > 2 and row['ZONE_UPS'] < 5) else \n",
    "                                                           (AH_WGT_5_up_UPS if (row['ACTUAL_WEIGHT'] > 50 and row['ZONE_UPS'] > 4) else 0)), axis=1)\n",
    "\n",
    "tbl_PLD_original_RED['AHS_RED'] = tbl_PLD_original_RED['AHS_1'] + tbl_PLD_original_RED['AHS_2'] + tbl_PLD_original_RED['AHS_3'] + tbl_PLD_original_RED['AHS_4']\n",
    "tbl_PLD_original_RED = tbl_PLD_original_RED.drop(['AHS_1','AHS_2','AHS_3','AHS_4'], axis=1)\n",
    "\n",
    "#Get Total rate. Set total rate to null if the freight rate is null and/or some package parameters are out of carrier handling capability.  \n",
    "tbl_PLD_original_RED['TOTAL_RED'] = round(tbl_PLD_original_RED.apply(lambda row: \n",
    "                                                                  (1 + row['FSC%_RED']*(1-BarrettDisc_Fsc_UPS)) * (row['FRT_RED'] + row['RES_RED'] + row['DAS_RED'] + row['AHS_RED']) if not pd.isna(row['FRT_RED']) and row['FRT_RED'] != '' else np.nan, axis=1),2)\n",
    "#tbl_PLD_original_RED.to_csv('test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1b05b4f-ec10-4b26-af7b-f01e1ac985ec",
   "metadata": {},
   "source": [
    "# PROV (FedEx Next Day Air by 10a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "f32ce0a3-66f5-4045-a791-b615faccbeb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get the right client rate and the corresponding PLD\n",
    "tbl_clientRates_2 = tbl_clientRates[(tbl_clientRates['SERVICE_CODE'] == 'PROV')]\n",
    "tbl_PLD_original_PROV = tbl_BP.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "b246b795-a778-4797-9d25-6c69febf9fb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Rerate start\n",
    "#add billed weight column to PLD\n",
    "tbl_PLD_original_PROV['BILLED_WEIGHT_LB'] = tbl_PLD_original_PROV.apply(lambda row: max(row['ACTUAL_WEIGHT_LB'], np.ceil(row['CUBIC_INCH']/dim_factor_fedex)), axis=1)\n",
    "\n",
    "\n",
    "#add Freight column to PLD\n",
    "tbl_PLD_original_PROV['ZONE_FedEx'] = tbl_PLD_original_PROV['ZONE_FedEx'].replace('17', '9')\n",
    "tbl_PLD_original_PROV['ZONE_FedEx'] = tbl_PLD_original_PROV['ZONE_FedEx'].astype(str)\n",
    "tbl_PLD_original_PROV = tbl_PLD_original_PROV.merge(tbl_clientRates_2[[\"WEIGHT_LB\", \"ZONE\", \"CLIENT_RATES\"]], left_on=['BILLED_WEIGHT_LB', 'ZONE_FedEx'], right_on=['WEIGHT_LB', 'ZONE'], how='left')\n",
    "tbl_PLD_original_PROV = tbl_PLD_original_PROV.drop(['WEIGHT_LB', 'ZONE'], axis=1)\n",
    "tbl_PLD_original_PROV = tbl_PLD_original_PROV.rename(columns={'CLIENT_RATES': 'FRT_PROV'})\n",
    "\n",
    "\n",
    "#add Residential Surcharge column to PLD\n",
    "tbl_PLD_original_PROV['RES_PROV'] = tbl_PLD_original_PROV.apply(lambda row: Resi_Surcharge_Air_FedEx if row['RESIDENTIAL_FLAG'] is True else 0, axis=1)\n",
    "\n",
    "\n",
    "#add DAS column to PLD, Pay attention to the flags under Residential Flag\n",
    "tbl_PLD_original_PROV['DAS_PROV'] = tbl_PLD_original_PROV.apply(lambda row: \n",
    "                                                            0 if (row['DAS_CATEGORY_FedEx'] == 'DASFDX' and row['RESIDENTIAL_FLAG'] is False) else \n",
    "                                                           (0 if (row['DAS_CATEGORY_FedEx'] == 'DASEFDX' and row['RESIDENTIAL_FLAG'] is False) else\n",
    "                                                           (DAS_Resi_Air_FedEx if (row['DAS_CATEGORY_FedEx'] == 'DASFDX' and row['RESIDENTIAL_FLAG'] is True) else \n",
    "                                                           (DAS_Resi_Extended_Air_FedEx if (row['DAS_CATEGORY_FedEx'] == 'DASEFDX' and row['RESIDENTIAL_FLAG'] is True) else \n",
    "                                                           (Remote_Area_FedEx if (row['DAS_CATEGORY_FedEx'] == 'RAFDX') else \n",
    "                                                           (Remote_Area_Alaska_FedEx if (row['DAS_CATEGORY_FedEx'] == 'AKFDX') else\n",
    "                                                           (Remote_Area_Hawaii_FedEx if (row['DAS_CATEGORY_FedEx'] == 'HIFDX') else 0)))))), axis=1)\n",
    "\n",
    "\n",
    "#add fuel surcharge % column to PLD\n",
    "tbl_PLD_original_PROV['SHIP_DATE'] = pd.to_datetime(tbl_PLD_original_PROV['SHIP_DATE']).dt.date\n",
    "# get unique values from column 'SHIP_DATE' and create a new dataframe\n",
    "new_df = pd.DataFrame({'SHIP_DATE': tbl_PLD_original_PROV['SHIP_DATE'].unique()})\n",
    "# use this new dataframe to merge to FSC\n",
    "df_merge = new_df.merge(FedEx_FSC, how='cross')\n",
    "\n",
    "\n",
    "#slice the merged dataframe to make sure ship_date is between the ship_data_start and end. Now we have a FSC file with Ship_date col to merge back to pld so that the pld can get FSC.\n",
    "df_merge['SHIP_DATE'] = pd.to_datetime(df_merge['SHIP_DATE']).dt.date\n",
    "df_merge = df_merge.query('SHIP_DATE >= SHIP_DATE_START and SHIP_DATE <= SHIP_DATE_END')\n",
    "df_merge = df_merge.drop_duplicates()\n",
    "# Get Domestic Air column as FSC. \n",
    "tbl_PLD_original_PROV = tbl_PLD_original_PROV.merge(df_merge, on=['SHIP_DATE'], how='left').drop(['SHIP_DATE_START', 'SHIP_DATE_END', 'Ground FedEx'], axis=1)\n",
    "tbl_PLD_original_PROV.rename(columns={'Domestic Air FedEx': 'FSC%_PROV'}, inplace=True)\n",
    "\n",
    "\n",
    "#add AHS column to PLD\n",
    "# Define the conditions and their corresponding values\n",
    "def calculate_ahs1(row):\n",
    "    if any([row['GIRTH_AND_L'] > 105, row['L'] > 48, row['W'] > 30]):\n",
    "        if row['ZONE_FedEx'] == '2':\n",
    "            return AH_Dim_2_FedEx\n",
    "        elif row['ZONE_FedEx'] in ('3','4'):\n",
    "            return AH_Dim_3_4_FedEx\n",
    "        elif row['ZONE_FedEx'] in ('5','6'):\n",
    "            return AH_Dim_5_6_FedEx\n",
    "        elif row['ZONE_FedEx'] in ('7','8','9','10','11','12','17','26','9','10','17','99'):\n",
    "            return AH_Dim_7_up_FedEx\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def calculate_ahs2(row):\n",
    "    if row['ACTUAL_WEIGHT'] > 50:\n",
    "        if row['ZONE_FedEx'] == '2':\n",
    "            return AH_WGT_2_FedEx\n",
    "        elif row['ZONE_FedEx'] in ('3','4'):\n",
    "            return AH_WGT_3_4_FedEx\n",
    "        elif row['ZONE_FedEx'] in ('5','6'):\n",
    "            return AH_WGT_5_6_FedEx\n",
    "        elif row['ZONE_FedEx'] in ('7','8','9','10','11','12','17','26','9','10','17','99'):\n",
    "            return AH_WGT_7_up_FedEx\n",
    "\n",
    "    else:\n",
    "        return 0\n",
    "        \n",
    "# Apply the function to the DataFrame\n",
    "tbl_PLD_original_PROV['AHS_PROV_1'] = tbl_PLD_original_PROV.apply(calculate_ahs1, axis=1)\n",
    "tbl_PLD_original_PROV['AHS_PROV_2'] = tbl_PLD_original_PROV.apply(calculate_ahs2, axis=1)\n",
    "tbl_PLD_original_PROV['AHS_PROV'] = tbl_PLD_original_PROV['AHS_PROV_1'] + tbl_PLD_original_PROV['AHS_PROV_2']\n",
    "tbl_PLD_original_PROV = tbl_PLD_original_PROV.drop(['AHS_PROV_1','AHS_PROV_2'], axis=1)\n",
    "\n",
    "\n",
    "#Get Total rate. Set total rate to null if the freight rate is null and/or some package parameters are out of carrier handling capability.  \n",
    "tbl_PLD_original_PROV['TOTAL_PROV'] = round(tbl_PLD_original_PROV.apply(lambda row: \n",
    "                                                                  (1 + row['FSC%_PROV']*(1-BarrettDisc_Fsc_FedEx)) * (row['FRT_PROV'] + row['RES_PROV'] + row['DAS_PROV'] + row['AHS_PROV']) if not pd.isna(row['FRT_PROV']) and row['FRT_PROV'] != '' else np.nan, axis=1),2)\n",
    "#tbl_PLD_original_PROV.to_csv('test.csv')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "raised-departure",
   "metadata": {},
   "source": [
    "# REDS (UPS NDA Saver)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 552,
   "id": "ef6fe621",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get the right client rate and the corresponding PLD\n",
    "tbl_clientRates_3 = tbl_clientRates[(tbl_clientRates['SERVICE_CODE'] == 'REDS')]\n",
    "tbl_PLD_original_REDS = tbl_BP.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 553,
   "id": "foster-stockholm",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Rerate start\n",
    "#add billed weight column to PLD\n",
    "tbl_PLD_original_REDS['BILLED_WEIGHT_LB'] = tbl_PLD_original_REDS.apply(lambda row: max(row['ACTUAL_WEIGHT_LB'], np.ceil(row['CUBIC_INCH']/dim_factor_ups)), axis=1)\n",
    "\n",
    "#add Freight column to PLD\n",
    "tbl_PLD_original_REDS['ZONE_UPS'] = tbl_PLD_original_REDS['ZONE_UPS'].astype(str)\n",
    "tbl_PLD_original_REDS = tbl_PLD_original_REDS.merge(tbl_clientRates_3[[\"WEIGHT_LB\", \"ZONE\", \"CLIENT_RATES\"]], left_on=['BILLED_WEIGHT_LB', 'ZONE_UPS'], right_on=['WEIGHT_LB', 'ZONE'], how='left')\n",
    "tbl_PLD_original_REDS = tbl_PLD_original_REDS.drop('WEIGHT_LB', axis=1)\n",
    "tbl_PLD_original_REDS = tbl_PLD_original_REDS.rename(columns={'CLIENT_RATES': 'FRT_REDS'})\n",
    "   \n",
    "    \n",
    "#add Residential Surcharge column to PLD\n",
    "tbl_PLD_original_REDS['RES_REDS'] = tbl_PLD_original_REDS.apply(lambda row: Resi_Surcharge_Air if row['RESIDENTIAL_FLAG'] is True else 0, axis=1)\n",
    "\n",
    "#add DAS column to PLD, Pay attention to the flags under Residential Flag\n",
    "tbl_PLD_original_REDS['DAS_REDS'] = tbl_PLD_original_REDS.apply(lambda row: \n",
    "                                                            DAS_Comm_Air if (row['DAS_CATEGORY_UPS'] == 'DASUPS' and row['RESIDENTIAL_FLAG'] is False) else \n",
    "                                                           (DAS_Comm_Extended_Air if (row['DAS_CATEGORY_UPS'] == 'DASEUPS' and row['RESIDENTIAL_FLAG'] is False) else\n",
    "                                                           (DAS_Resi_Air if (row['DAS_CATEGORY_UPS'] == 'DASUPS' and row['RESIDENTIAL_FLAG'] is True) else \n",
    "                                                           (DAS_Resi_Extended_Air if (row['DAS_CATEGORY_UPS'] == 'DASEUPS' and row['RESIDENTIAL_FLAG'] is True) else \n",
    "                                                           (Remote_Area if (row['DAS_CATEGORY_UPS'] == 'RAUPS') else \n",
    "                                                           (Remote_Area_Alaska if (row['DAS_CATEGORY_UPS'] == 'AKUPS') else\n",
    "                                                           (Remote_Area_Hawaii if (row['DAS_CATEGORY_UPS'] == 'HIUPS') else 0)))))), axis=1)\n",
    "\n",
    "\n",
    "#add fuel surcharge % column to PLD\n",
    "tbl_PLD_original_REDS['SHIP_DATE'] = pd.to_datetime(tbl_PLD_original_REDS['SHIP_DATE']).dt.date\n",
    "# get unique values from column 'SHIP_DATE' and create a new dataframe\n",
    "new_df = pd.DataFrame({'SHIP_DATE': tbl_PLD_original_REDS['SHIP_DATE'].unique()})\n",
    "# use this new dataframe to merge to FSC\n",
    "df_merge = new_df.merge(UPS_FSC, how='cross')\n",
    "df_merge['SHIP_DATE'] = pd.to_datetime(df_merge['SHIP_DATE']).dt.date\n",
    "#slice the merged dataframe to make sure ship_date is between the ship_data_start and end. Now we have a FSC file with Ship_date col to merge back to pld so that the pld can get FSC.\n",
    "df_merge = df_merge.query('SHIP_DATE >= SHIP_DATE_START and SHIP_DATE <= SHIP_DATE_END')\n",
    "df_merge = df_merge.drop_duplicates()\n",
    "# Get Domestic Air column as FSC. \n",
    "tbl_PLD_original_REDS = tbl_PLD_original_REDS.merge(df_merge, on=['SHIP_DATE'], how='left').drop(['SHIP_DATE_START', 'SHIP_DATE_END', 'Ground'], axis=1)\n",
    "tbl_PLD_original_REDS.rename(columns={'Domestic Air': 'FSC%_REDS'}, inplace=True)\n",
    "\n",
    "\n",
    "#add AHS column to PLD\n",
    "tbl_PLD_original_REDS['ZONE_UPS'] = pd.to_numeric(tbl_PLD_original_REDS['ZONE_UPS'], errors='coerce').fillna(0).astype(int)\n",
    "\n",
    "tbl_PLD_original_REDS['AHS_1'] = tbl_PLD_original_REDS.apply(lambda row: \n",
    "                                                           AH_Girth_2_UPS if (row['GIRTH_AND_L'] > 105 and row['ZONE_UPS'] == 2) else \n",
    "                                                           (AH_Girth_3_4_UPS if (row['GIRTH_AND_L'] > 105 and row['ZONE_UPS'] > 2 and row['ZONE_UPS'] < 5) else\n",
    "                                                           (AH_Girth_5_up_UPS if (row['GIRTH_AND_L'] > 105 and row['ZONE_UPS'] > 4) else 0)), axis=1)\n",
    "                                                    \n",
    "tbl_PLD_original_REDS['AHS_2'] = tbl_PLD_original_REDS.apply(lambda row: \n",
    "                                                           AH_L_2_UPS if (row['L'] > 48 and row['ZONE_UPS'] == 2) else \n",
    "                                                           (AH_L_3_4_UPS if (row['L'] > 48 and row['ZONE_UPS'] > 2 and row['ZONE_UPS'] < 5) else \n",
    "                                                           (AH_L_5_up_UPS if (row['L'] > 48 and row['ZONE_UPS'] > 4) else 0)), axis=1)\n",
    "\n",
    "tbl_PLD_original_REDS['AHS_3'] = tbl_PLD_original_REDS.apply(lambda row:\n",
    "                                                           AH_W_2_UPS if (row['W'] > 30 and row['ZONE_UPS'] == 2) else \n",
    "                                                           (AH_W_3_4_UPS if (row['W'] > 30 and row['ZONE_UPS'] > 2 and row['ZONE_UPS'] < 5) else \n",
    "                                                           (AH_W_5_up_UPS if (row['W'] > 30 and row['ZONE_UPS'] > 4) else 0)), axis=1)\n",
    "                                        \n",
    "tbl_PLD_original_REDS['AHS_4'] = tbl_PLD_original_REDS.apply(lambda row:\n",
    "                                                           AH_WGT_2_UPS if (row['ACTUAL_WEIGHT'] > 50 and row['ZONE_UPS'] == 2) else \n",
    "                                                           (AH_WGT_3_4_UPS if (row['ACTUAL_WEIGHT'] > 50 and row['ZONE_UPS'] > 2 and row['ZONE_UPS'] < 5) else \n",
    "                                                           (AH_WGT_5_up_UPS if (row['ACTUAL_WEIGHT'] > 50 and row['ZONE_UPS'] > 4) else 0)), axis=1)\n",
    "\n",
    "tbl_PLD_original_REDS['AHS_REDS'] = tbl_PLD_original_REDS['AHS_1'] + tbl_PLD_original_REDS['AHS_2'] + tbl_PLD_original_REDS['AHS_3'] + tbl_PLD_original_REDS['AHS_4']\n",
    "tbl_PLD_original_REDS = tbl_PLD_original_REDS.drop(['AHS_1','AHS_2','AHS_3','AHS_4'], axis=1)\n",
    "\n",
    "#Get Total rate. Set total rate to null if the freight rate is null and/or some package parameters are out of carrier handling capability.  \n",
    "tbl_PLD_original_REDS['TOTAL_REDS'] = round(tbl_PLD_original_REDS.apply(lambda row: \n",
    "                                                                  (1 + row['FSC%_REDS']*(1-BarrettDisc_Fsc_UPS)) * (row['FRT_REDS'] + row['RES_REDS'] + row['DAS_REDS'] + row['AHS_REDS']) if not pd.isna(row['FRT_REDS']) and row['FRT_REDS'] != '' else np.nan, axis=1),2)\n",
    "#tbl_PLD_original_REDS.to_csv('test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecfb6b6c-ec54-4eaf-8634-0198b8be0798",
   "metadata": {},
   "source": [
    "# STOV (FedEx Next Day Air by 3p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "aaca7275-31cf-41e2-9c19-df8a13eefb29",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get the right client rate and the corresponding PLD\n",
    "tbl_clientRates_3 = tbl_clientRates[(tbl_clientRates['SERVICE_CODE'] == 'STOV')]\n",
    "tbl_PLD_original_STOV = tbl_BP.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "c94865d6-48c4-48f1-b15a-0bb7d1650b1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Rerate start\n",
    "#add billed weight column to PLD\n",
    "tbl_PLD_original_STOV['BILLED_WEIGHT_LB'] = tbl_PLD_original_STOV.apply(lambda row: max(row['ACTUAL_WEIGHT_LB'], np.ceil(row['CUBIC_INCH']/dim_factor_fedex)), axis=1)\n",
    "\n",
    "\n",
    "#add Freight column to PLD\n",
    "tbl_PLD_original_STOV['ZONE_FedEx'] = tbl_PLD_original_STOV['ZONE_FedEx'].replace('17', '9')\n",
    "tbl_PLD_original_STOV['ZONE_FedEx'] = tbl_PLD_original_STOV['ZONE_FedEx'].astype(str)\n",
    "tbl_PLD_original_STOV = tbl_PLD_original_STOV.merge(tbl_clientRates_3[[\"WEIGHT_LB\", \"ZONE\", \"CLIENT_RATES\"]], left_on=['BILLED_WEIGHT_LB', 'ZONE_FedEx'], right_on=['WEIGHT_LB', 'ZONE'], how='left')\n",
    "tbl_PLD_original_STOV = tbl_PLD_original_STOV.drop(['WEIGHT_LB', 'ZONE'], axis=1)\n",
    "tbl_PLD_original_STOV = tbl_PLD_original_STOV.rename(columns={'CLIENT_RATES': 'FRT_STOV'})\n",
    "\n",
    "\n",
    "#add Residential Surcharge column to PLD\n",
    "tbl_PLD_original_STOV['RES_STOV'] = tbl_PLD_original_STOV.apply(lambda row: Resi_Surcharge_Air_FedEx if row['RESIDENTIAL_FLAG'] is True else 0, axis=1)\n",
    "\n",
    "\n",
    "#add DAS column to PLD, Pay attention to the flags under Residential Flag\n",
    "tbl_PLD_original_STOV['DAS_STOV'] = tbl_PLD_original_STOV.apply(lambda row: \n",
    "                                                            0 if (row['DAS_CATEGORY_FedEx'] == 'DASFDX' and row['RESIDENTIAL_FLAG'] is False) else \n",
    "                                                           (0 if (row['DAS_CATEGORY_FedEx'] == 'DASEFDX' and row['RESIDENTIAL_FLAG'] is False) else\n",
    "                                                           (DAS_Resi_Air_FedEx if (row['DAS_CATEGORY_FedEx'] == 'DASFDX' and row['RESIDENTIAL_FLAG'] is True) else \n",
    "                                                           (DAS_Resi_Extended_Air_FedEx if (row['DAS_CATEGORY_FedEx'] == 'DASEFDX' and row['RESIDENTIAL_FLAG'] is True) else \n",
    "                                                           (Remote_Area_FedEx if (row['DAS_CATEGORY_FedEx'] == 'RAFDX') else \n",
    "                                                           (Remote_Area_Alaska_FedEx if (row['DAS_CATEGORY_FedEx'] == 'AKFDX') else\n",
    "                                                           (Remote_Area_Hawaii_FedEx if (row['DAS_CATEGORY_FedEx'] == 'HIFDX') else 0)))))), axis=1)\n",
    "\n",
    "\n",
    "#add fuel surcharge % column to PLD\n",
    "tbl_PLD_original_STOV['SHIP_DATE'] = pd.to_datetime(tbl_PLD_original_STOV['SHIP_DATE']).dt.date\n",
    "# get unique values from column 'SHIP_DATE' and create a new dataframe\n",
    "new_df = pd.DataFrame({'SHIP_DATE': tbl_PLD_original_STOV['SHIP_DATE'].unique()})\n",
    "# use this new dataframe to merge to FSC\n",
    "df_merge = new_df.merge(FedEx_FSC, how='cross')\n",
    "\n",
    "\n",
    "#slice the merged dataframe to make sure ship_date is between the ship_data_start and end. Now we have a FSC file with Ship_date col to merge back to pld so that the pld can get FSC.\n",
    "df_merge['SHIP_DATE'] = pd.to_datetime(df_merge['SHIP_DATE']).dt.date\n",
    "df_merge = df_merge.query('SHIP_DATE >= SHIP_DATE_START and SHIP_DATE <= SHIP_DATE_END')\n",
    "df_merge = df_merge.drop_duplicates()\n",
    "# Get Domestic Air column as FSC. \n",
    "tbl_PLD_original_STOV = tbl_PLD_original_STOV.merge(df_merge, on=['SHIP_DATE'], how='left').drop(['SHIP_DATE_START', 'SHIP_DATE_END', 'Ground FedEx'], axis=1)\n",
    "tbl_PLD_original_STOV.rename(columns={'Domestic Air FedEx': 'FSC%_STOV'}, inplace=True)\n",
    "\n",
    "\n",
    "#add AHS column to PLD\n",
    "# Define the conditions and their corresponding values\n",
    "def calculate_ahs1(row):\n",
    "    if any([row['GIRTH_AND_L'] > 105, row['L'] > 48, row['W'] > 30]):\n",
    "        if row['ZONE_FedEx'] == '2':\n",
    "            return AH_Dim_2_FedEx\n",
    "        elif row['ZONE_FedEx'] in ('3','4'):\n",
    "            return AH_Dim_3_4_FedEx\n",
    "        elif row['ZONE_FedEx'] in ('5','6'):\n",
    "            return AH_Dim_5_6_FedEx\n",
    "        elif row['ZONE_FedEx'] in ('7','8','9','10','11','12','17','26','9','10','17','99'):\n",
    "            return AH_Dim_7_up_FedEx\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def calculate_ahs2(row):\n",
    "    if row['ACTUAL_WEIGHT'] > 50:\n",
    "        if row['ZONE_FedEx'] == '2':\n",
    "            return AH_WGT_2_FedEx\n",
    "        elif row['ZONE_FedEx'] in ('3','4'):\n",
    "            return AH_WGT_3_4_FedEx\n",
    "        elif row['ZONE_FedEx'] in ('5','6'):\n",
    "            return AH_WGT_5_6_FedEx\n",
    "        elif row['ZONE_FedEx'] in ('7','8','9','10','11','12','17','26','9','10','17','99'):\n",
    "            return AH_WGT_7_up_FedEx\n",
    "\n",
    "    else:\n",
    "        return 0\n",
    "        \n",
    "# Apply the function to the DataFrame\n",
    "tbl_PLD_original_STOV['AHS_STOV_1'] = tbl_PLD_original_STOV.apply(calculate_ahs1, axis=1)\n",
    "tbl_PLD_original_STOV['AHS_STOV_2'] = tbl_PLD_original_STOV.apply(calculate_ahs2, axis=1)\n",
    "tbl_PLD_original_STOV['AHS_STOV'] = tbl_PLD_original_STOV['AHS_STOV_1'] + tbl_PLD_original_STOV['AHS_STOV_2']\n",
    "tbl_PLD_original_STOV = tbl_PLD_original_STOV.drop(['AHS_STOV_1','AHS_STOV_2'], axis=1)\n",
    "\n",
    "\n",
    "#Get Total rate. Set total rate to null if the freight rate is null and/or some package parameters are out of carrier handling capability.  \n",
    "tbl_PLD_original_STOV['TOTAL_STOV'] = round(tbl_PLD_original_STOV.apply(lambda row: \n",
    "                                                                  (1 + row['FSC%_STOV']*(1-BarrettDisc_Fsc_FedEx)) * (row['FRT_STOV'] + row['RES_STOV'] + row['DAS_STOV'] + row['AHS_STOV']) if not pd.isna(row['FRT_STOV']) and row['FRT_STOV'] != '' else np.nan, axis=1),2)\n",
    "#tbl_PLD_original_STOV.to_csv('test.csv')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "scenic-transaction",
   "metadata": {},
   "source": [
    "# 2DAM (UPS 2DA A.M.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 555,
   "id": "f87cf032",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get the right client rate and the corresponding PLD\n",
    "tbl_clientRates_4 = tbl_clientRates[(tbl_clientRates['SERVICE_CODE'] == '2DAM')]\n",
    "tbl_PLD_original_2DAM = tbl_BP.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 556,
   "id": "elegant-nicaragua",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Rerate start\n",
    "#add billed weight column to PLD\n",
    "tbl_PLD_original_2DAM['BILLED_WEIGHT_LB'] = tbl_PLD_original_2DAM.apply(lambda row: max(row['ACTUAL_WEIGHT_LB'], np.ceil(row['CUBIC_INCH']/dim_factor_ups)), axis=1)\n",
    "\n",
    "#add Freight column to PLD\n",
    "tbl_PLD_original_2DAM['ZONE_UPS'] = tbl_PLD_original_2DAM['ZONE_UPS'].astype(str)\n",
    "tbl_PLD_original_2DAM = tbl_PLD_original_2DAM.merge(tbl_clientRates_4[[\"WEIGHT_LB\", \"ZONE\", \"CLIENT_RATES\"]], left_on=['BILLED_WEIGHT_LB', 'ZONE_UPS'], right_on=['WEIGHT_LB', 'ZONE'], how='left')\n",
    "tbl_PLD_original_2DAM = tbl_PLD_original_2DAM.drop('WEIGHT_LB', axis=1)\n",
    "tbl_PLD_original_2DAM = tbl_PLD_original_2DAM.rename(columns={'CLIENT_RATES': 'FRT_2DAM'})\n",
    "   \n",
    "    \n",
    "#add Residential Surcharge column to PLD\n",
    "tbl_PLD_original_2DAM['RES_2DAM'] = tbl_PLD_original_2DAM.apply(lambda row: Resi_Surcharge_Air if row['RESIDENTIAL_FLAG'] is True else 0, axis=1)\n",
    "\n",
    "#add DAS column to PLD, Pay attention to the flags under Residential Flag\n",
    "tbl_PLD_original_2DAM['DAS_2DAM'] = tbl_PLD_original_2DAM.apply(lambda row: \n",
    "                                                            DAS_Comm_Air if (row['DAS_CATEGORY_UPS'] == 'DASUPS' and row['RESIDENTIAL_FLAG'] is False) else \n",
    "                                                           (DAS_Comm_Extended_Air if (row['DAS_CATEGORY_UPS'] == 'DASEUPS' and row['RESIDENTIAL_FLAG'] is False) else\n",
    "                                                           (DAS_Resi_Air if (row['DAS_CATEGORY_UPS'] == 'DASUPS' and row['RESIDENTIAL_FLAG'] is True) else \n",
    "                                                           (DAS_Resi_Extended_Air if (row['DAS_CATEGORY_UPS'] == 'DASEUPS' and row['RESIDENTIAL_FLAG'] is True) else \n",
    "                                                           (Remote_Area if (row['DAS_CATEGORY_UPS'] == 'RAUPS') else \n",
    "                                                           (Remote_Area_Alaska if (row['DAS_CATEGORY_UPS'] == 'AKUPS') else\n",
    "                                                           (Remote_Area_Hawaii if (row['DAS_CATEGORY_UPS'] == 'HIUPS') else 0)))))), axis=1)\n",
    "\n",
    "\n",
    "#add fuel surcharge % column to PLD\n",
    "tbl_PLD_original_2DAM['SHIP_DATE'] = pd.to_datetime(tbl_PLD_original_2DAM['SHIP_DATE']).dt.date\n",
    "# get unique values from column 'SHIP_DATE' and create a new dataframe\n",
    "new_df = pd.DataFrame({'SHIP_DATE': tbl_PLD_original_2DAM['SHIP_DATE'].unique()})\n",
    "# use this new dataframe to merge to FSC\n",
    "df_merge = new_df.merge(UPS_FSC, how='cross')\n",
    "df_merge['SHIP_DATE'] = pd.to_datetime(df_merge['SHIP_DATE']).dt.date\n",
    "#slice the merged dataframe to make sure ship_date is between the ship_data_start and end. Now we have a FSC file with Ship_date col to merge back to pld so that the pld can get FSC.\n",
    "df_merge = df_merge.query('SHIP_DATE >= SHIP_DATE_START and SHIP_DATE <= SHIP_DATE_END')\n",
    "df_merge = df_merge.drop_duplicates()\n",
    "# Get Domestic Air column as FSC. \n",
    "tbl_PLD_original_2DAM = tbl_PLD_original_2DAM.merge(df_merge, on=['SHIP_DATE'], how='left').drop(['SHIP_DATE_START', 'SHIP_DATE_END', 'Ground'], axis=1)\n",
    "tbl_PLD_original_2DAM.rename(columns={'Domestic Air': 'FSC%_2DAM'}, inplace=True)\n",
    "\n",
    "\n",
    "#add AHS column to PLD\n",
    "tbl_PLD_original_2DAM['ZONE_UPS'] = pd.to_numeric(tbl_PLD_original_2DAM['ZONE_UPS'], errors='coerce').fillna(0).astype(int)\n",
    "\n",
    "tbl_PLD_original_2DAM['AHS_1'] = tbl_PLD_original_2DAM.apply(lambda row: \n",
    "                                                           AH_Girth_2_UPS if (row['GIRTH_AND_L'] > 105 and row['ZONE_UPS'] == 2) else \n",
    "                                                           (AH_Girth_3_4_UPS if (row['GIRTH_AND_L'] > 105 and row['ZONE_UPS'] > 2 and row['ZONE_UPS'] < 5) else\n",
    "                                                           (AH_Girth_5_up_UPS if (row['GIRTH_AND_L'] > 105 and row['ZONE_UPS'] > 4) else 0)), axis=1)\n",
    "                                                    \n",
    "tbl_PLD_original_2DAM['AHS_2'] = tbl_PLD_original_2DAM.apply(lambda row: \n",
    "                                                           AH_L_2_UPS if (row['L'] > 48 and row['ZONE_UPS'] == 2) else \n",
    "                                                           (AH_L_3_4_UPS if (row['L'] > 48 and row['ZONE_UPS'] > 2 and row['ZONE_UPS'] < 5) else \n",
    "                                                           (AH_L_5_up_UPS if (row['L'] > 48 and row['ZONE_UPS'] > 4) else 0)), axis=1)\n",
    "\n",
    "tbl_PLD_original_2DAM['AHS_3'] = tbl_PLD_original_2DAM.apply(lambda row:\n",
    "                                                           AH_W_2_UPS if (row['W'] > 30 and row['ZONE_UPS'] == 2) else \n",
    "                                                           (AH_W_3_4_UPS if (row['W'] > 30 and row['ZONE_UPS'] > 2 and row['ZONE_UPS'] < 5) else \n",
    "                                                           (AH_W_5_up_UPS if (row['W'] > 30 and row['ZONE_UPS'] > 4) else 0)), axis=1)\n",
    "                                        \n",
    "tbl_PLD_original_2DAM['AHS_4'] = tbl_PLD_original_2DAM.apply(lambda row:\n",
    "                                                           AH_WGT_2_UPS if (row['ACTUAL_WEIGHT'] > 50 and row['ZONE_UPS'] == 2) else \n",
    "                                                           (AH_WGT_3_4_UPS if (row['ACTUAL_WEIGHT'] > 50 and row['ZONE_UPS'] > 2 and row['ZONE_UPS'] < 5) else \n",
    "                                                           (AH_WGT_5_up_UPS if (row['ACTUAL_WEIGHT'] > 50 and row['ZONE_UPS'] > 4) else 0)), axis=1)\n",
    "\n",
    "tbl_PLD_original_2DAM['AHS_2DAM'] = tbl_PLD_original_2DAM['AHS_1'] + tbl_PLD_original_2DAM['AHS_2'] + tbl_PLD_original_2DAM['AHS_3'] + tbl_PLD_original_2DAM['AHS_4']\n",
    "tbl_PLD_original_2DAM = tbl_PLD_original_2DAM.drop(['AHS_1','AHS_2','AHS_3','AHS_4'], axis=1)\n",
    "\n",
    "#Get Total rate. Set total rate to null if the freight rate is null and/or some package parameters are out of carrier handling capability.  \n",
    "tbl_PLD_original_2DAM['TOTAL_2DAM'] = round(tbl_PLD_original_2DAM.apply(lambda row: \n",
    "                                                                  (1 + row['FSC%_2DAM']*(1-BarrettDisc_Fsc_UPS)) * (row['FRT_2DAM'] + row['RES_2DAM'] + row['DAS_2DAM'] + row['AHS_2DAM']) if not pd.isna(row['FRT_2DAM']) and row['FRT_2DAM'] != '' else np.nan, axis=1),2)\n",
    "#tbl_PLD_original_2DAM.to_csv('test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e17bca60-6e84-4f6d-89a7-4942260953d3",
   "metadata": {},
   "source": [
    "# 2AM (FedEx 2nd Day AM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "eb750ab9-25d5-4633-86b3-222a44682906",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get the right client rate and the corresponding PLD\n",
    "tbl_clientRates_4 = tbl_clientRates[(tbl_clientRates['SERVICE_CODE'] == '2AM')]\n",
    "tbl_PLD_original_2AM = tbl_BP.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "83a144df-2e84-4527-8cd5-be80a2fdc515",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Rerate start\n",
    "#add billed weight column to PLD\n",
    "tbl_PLD_original_2AM['BILLED_WEIGHT_LB'] = tbl_PLD_original_2AM.apply(lambda row: max(row['ACTUAL_WEIGHT_LB'], np.ceil(row['CUBIC_INCH']/dim_factor_fedex)), axis=1)\n",
    "\n",
    "\n",
    "#add Freight column to PLD\n",
    "tbl_PLD_original_2AM['ZONE_FedEx'] = tbl_PLD_original_2AM['ZONE_FedEx'].replace('17', '9')\n",
    "tbl_PLD_original_2AM['ZONE_FedEx'] = tbl_PLD_original_2AM['ZONE_FedEx'].astype(str)\n",
    "tbl_PLD_original_2AM = tbl_PLD_original_2AM.merge(tbl_clientRates_4[[\"WEIGHT_LB\", \"ZONE\", \"CLIENT_RATES\"]], left_on=['BILLED_WEIGHT_LB', 'ZONE_FedEx'], right_on=['WEIGHT_LB', 'ZONE'], how='left')\n",
    "tbl_PLD_original_2AM = tbl_PLD_original_2AM.drop(['WEIGHT_LB', 'ZONE'], axis=1)\n",
    "tbl_PLD_original_2AM = tbl_PLD_original_2AM.rename(columns={'CLIENT_RATES': 'FRT_2AM'})\n",
    "\n",
    "\n",
    "#add Residential Surcharge column to PLD\n",
    "tbl_PLD_original_2AM['RES_2AM'] = tbl_PLD_original_2AM.apply(lambda row: Resi_Surcharge_Air_FedEx if row['RESIDENTIAL_FLAG'] is True else 0, axis=1)\n",
    "\n",
    "\n",
    "#add DAS column to PLD, Pay attention to the flags under Residential Flag\n",
    "tbl_PLD_original_2AM['DAS_2AM'] = tbl_PLD_original_2AM.apply(lambda row: \n",
    "                                                            0 if (row['DAS_CATEGORY_FedEx'] == 'DASFDX' and row['RESIDENTIAL_FLAG'] is False) else \n",
    "                                                           (0 if (row['DAS_CATEGORY_FedEx'] == 'DASEFDX' and row['RESIDENTIAL_FLAG'] is False) else\n",
    "                                                           (DAS_Resi_Air_FedEx if (row['DAS_CATEGORY_FedEx'] == 'DASFDX' and row['RESIDENTIAL_FLAG'] is True) else \n",
    "                                                           (DAS_Resi_Extended_Air_FedEx if (row['DAS_CATEGORY_FedEx'] == 'DASEFDX' and row['RESIDENTIAL_FLAG'] is True) else \n",
    "                                                           (Remote_Area_FedEx if (row['DAS_CATEGORY_FedEx'] == 'RAFDX') else \n",
    "                                                           (Remote_Area_Alaska_FedEx if (row['DAS_CATEGORY_FedEx'] == 'AKFDX') else\n",
    "                                                           (Remote_Area_Hawaii_FedEx if (row['DAS_CATEGORY_FedEx'] == 'HIFDX') else 0)))))), axis=1)\n",
    "\n",
    "\n",
    "#add fuel surcharge % column to PLD\n",
    "tbl_PLD_original_2AM['SHIP_DATE'] = pd.to_datetime(tbl_PLD_original_2AM['SHIP_DATE']).dt.date\n",
    "# get unique values from column 'SHIP_DATE' and create a new dataframe\n",
    "new_df = pd.DataFrame({'SHIP_DATE': tbl_PLD_original_2AM['SHIP_DATE'].unique()})\n",
    "# use this new dataframe to merge to FSC\n",
    "df_merge = new_df.merge(FedEx_FSC, how='cross')\n",
    "\n",
    "\n",
    "#slice the merged dataframe to make sure ship_date is between the ship_data_start and end. Now we have a FSC file with Ship_date col to merge back to pld so that the pld can get FSC.\n",
    "df_merge['SHIP_DATE'] = pd.to_datetime(df_merge['SHIP_DATE']).dt.date\n",
    "df_merge = df_merge.query('SHIP_DATE >= SHIP_DATE_START and SHIP_DATE <= SHIP_DATE_END')\n",
    "df_merge = df_merge.drop_duplicates()\n",
    "# Get Domestic Air column as FSC. \n",
    "tbl_PLD_original_2AM = tbl_PLD_original_2AM.merge(df_merge, on=['SHIP_DATE'], how='left').drop(['SHIP_DATE_START', 'SHIP_DATE_END', 'Ground FedEx'], axis=1)\n",
    "tbl_PLD_original_2AM.rename(columns={'Domestic Air FedEx': 'FSC%_2AM'}, inplace=True)\n",
    "\n",
    "\n",
    "#add AHS column to PLD\n",
    "# Define the conditions and their corresponding values\n",
    "def calculate_ahs1(row):\n",
    "    if any([row['GIRTH_AND_L'] > 105, row['L'] > 48, row['W'] > 30]):\n",
    "        if row['ZONE_FedEx'] == '2':\n",
    "            return AH_Dim_2_FedEx\n",
    "        elif row['ZONE_FedEx'] in ('3','4'):\n",
    "            return AH_Dim_3_4_FedEx\n",
    "        elif row['ZONE_FedEx'] in ('5','6'):\n",
    "            return AH_Dim_5_6_FedEx\n",
    "        elif row['ZONE_FedEx'] in ('7','8','9','10','11','12','17','26','9','10','17','99'):\n",
    "            return AH_Dim_7_up_FedEx\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def calculate_ahs2(row):\n",
    "    if row['ACTUAL_WEIGHT'] > 50:\n",
    "        if row['ZONE_FedEx'] == '2':\n",
    "            return AH_WGT_2_FedEx\n",
    "        elif row['ZONE_FedEx'] in ('3','4'):\n",
    "            return AH_WGT_3_4_FedEx\n",
    "        elif row['ZONE_FedEx'] in ('5','6'):\n",
    "            return AH_WGT_5_6_FedEx\n",
    "        elif row['ZONE_FedEx'] in ('7','8','9','10','11','12','17','26','9','10','17','99'):\n",
    "            return AH_WGT_7_up_FedEx\n",
    "\n",
    "    else:\n",
    "        return 0\n",
    "        \n",
    "# Apply the function to the DataFrame\n",
    "tbl_PLD_original_2AM['AHS_2AM_1'] = tbl_PLD_original_2AM.apply(calculate_ahs1, axis=1)\n",
    "tbl_PLD_original_2AM['AHS_2AM_2'] = tbl_PLD_original_2AM.apply(calculate_ahs2, axis=1)\n",
    "tbl_PLD_original_2AM['AHS_2AM'] = tbl_PLD_original_2AM['AHS_2AM_1'] + tbl_PLD_original_2AM['AHS_2AM_2']\n",
    "tbl_PLD_original_2AM = tbl_PLD_original_2AM.drop(['AHS_2AM_1','AHS_2AM_2'], axis=1)\n",
    "\n",
    "\n",
    "#Get Total rate. Set total rate to null if the freight rate is null and/or some package parameters are out of carrier handling capability.  \n",
    "tbl_PLD_original_2AM['TOTAL_2AM'] = round(tbl_PLD_original_2AM.apply(lambda row: \n",
    "                                                                  (1 + row['FSC%_2AM']*(1-BarrettDisc_Fsc_FedEx)) * (row['FRT_2AM'] + row['RES_2AM'] + row['DAS_2AM'] + row['AHS_2AM']) if not pd.isna(row['FRT_2AM']) and row['FRT_2AM'] != '' else np.nan, axis=1),2)\n",
    "#tbl_PLD_original_2AM.to_csv('test.csv')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "insured-stations",
   "metadata": {},
   "source": [
    "# BLUE (UPS 2DA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 570,
   "id": "954a4bf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get the right client rate and the corresponding PLD\n",
    "tbl_clientRates_5 = tbl_clientRates[(tbl_clientRates['SERVICE_CODE'] == 'BLUE')]\n",
    "tbl_PLD_original_BLUE = tbl_BP.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 571,
   "id": "systematic-working",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Rerate start\n",
    "#add billed weight column to PLD\n",
    "tbl_PLD_original_BLUE['BILLED_WEIGHT_LB'] = tbl_PLD_original_BLUE.apply(lambda row: max(row['ACTUAL_WEIGHT_LB'], np.ceil(row['CUBIC_INCH']/dim_factor_ups)), axis=1)\n",
    "\n",
    "#add Freight column to PLD\n",
    "tbl_PLD_original_BLUE['ZONE_UPS'] = tbl_PLD_original_BLUE['ZONE_UPS'].astype(str)\n",
    "tbl_PLD_original_BLUE = tbl_PLD_original_BLUE.merge(tbl_clientRates_5[[\"WEIGHT_LB\", \"ZONE\", \"CLIENT_RATES\"]], left_on=['BILLED_WEIGHT_LB', 'ZONE_UPS'], right_on=['WEIGHT_LB', 'ZONE'], how='left')\n",
    "tbl_PLD_original_BLUE = tbl_PLD_original_BLUE.drop('WEIGHT_LB', axis=1)\n",
    "tbl_PLD_original_BLUE = tbl_PLD_original_BLUE.rename(columns={'CLIENT_RATES': 'FRT_BLUE'})\n",
    "   \n",
    "    \n",
    "#add Residential Surcharge column to PLD\n",
    "tbl_PLD_original_BLUE['RES_BLUE'] = tbl_PLD_original_BLUE.apply(lambda row: Resi_Surcharge_Air if row['RESIDENTIAL_FLAG'] is True else 0, axis=1)\n",
    "\n",
    "#add DAS column to PLD, Pay attention to the flags under Residential Flag\n",
    "tbl_PLD_original_BLUE['DAS_BLUE'] = tbl_PLD_original_BLUE.apply(lambda row: \n",
    "                                                            DAS_Comm_Air if (row['DAS_CATEGORY_UPS'] == 'DASUPS' and row['RESIDENTIAL_FLAG'] is False) else \n",
    "                                                           (DAS_Comm_Extended_Air if (row['DAS_CATEGORY_UPS'] == 'DASEUPS' and row['RESIDENTIAL_FLAG'] is False) else\n",
    "                                                           (DAS_Resi_Air if (row['DAS_CATEGORY_UPS'] == 'DASUPS' and row['RESIDENTIAL_FLAG'] is True) else \n",
    "                                                           (DAS_Resi_Extended_Air if (row['DAS_CATEGORY_UPS'] == 'DASEUPS' and row['RESIDENTIAL_FLAG'] is True) else \n",
    "                                                           (Remote_Area if (row['DAS_CATEGORY_UPS'] == 'RAUPS') else \n",
    "                                                           (Remote_Area_Alaska if (row['DAS_CATEGORY_UPS'] == 'AKUPS') else\n",
    "                                                           (Remote_Area_Hawaii if (row['DAS_CATEGORY_UPS'] == 'HIUPS') else 0)))))), axis=1)\n",
    "\n",
    "\n",
    "#add fuel surcharge % column to PLD\n",
    "tbl_PLD_original_BLUE['SHIP_DATE'] = pd.to_datetime(tbl_PLD_original_BLUE['SHIP_DATE']).dt.date\n",
    "# get unique values from column 'SHIP_DATE' and create a new dataframe\n",
    "new_df = pd.DataFrame({'SHIP_DATE': tbl_PLD_original_BLUE['SHIP_DATE'].unique()})\n",
    "# use this new dataframe to merge to FSC\n",
    "df_merge = new_df.merge(UPS_FSC, how='cross')\n",
    "\n",
    "df_merge['SHIP_DATE'] = pd.to_datetime(df_merge['SHIP_DATE']).dt.date\n",
    "\n",
    "#slice the merged dataframe to make sure ship_date is between the ship_data_start and end. Now we have a FSC file with Ship_date col to merge back to pld so that the pld can get FSC.\n",
    "df_merge = df_merge.query('SHIP_DATE >= SHIP_DATE_START and SHIP_DATE <= SHIP_DATE_END')\n",
    "df_merge = df_merge.drop_duplicates()\n",
    "# Get Domestic Air column as FSC. \n",
    "tbl_PLD_original_BLUE = tbl_PLD_original_BLUE.merge(df_merge, on=['SHIP_DATE'], how='left').drop(['SHIP_DATE_START', 'SHIP_DATE_END', 'Ground'], axis=1)\n",
    "tbl_PLD_original_BLUE.rename(columns={'Domestic Air': 'FSC%_BLUE'}, inplace=True)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#add AHS column to PLD\n",
    "tbl_PLD_original_BLUE['ZONE_UPS'] = pd.to_numeric(tbl_PLD_original_BLUE['ZONE_UPS'], errors='coerce').fillna(0).astype(int)\n",
    "\n",
    "tbl_PLD_original_BLUE['AHS_1'] = tbl_PLD_original_BLUE.apply(lambda row: \n",
    "                                                           AH_Girth_2_UPS if (row['GIRTH_AND_L'] > 105 and row['ZONE_UPS'] == 2) else \n",
    "                                                           (AH_Girth_3_4_UPS if (row['GIRTH_AND_L'] > 105 and row['ZONE_UPS'] > 2 and row['ZONE_UPS'] < 5) else\n",
    "                                                           (AH_Girth_5_up_UPS if (row['GIRTH_AND_L'] > 105 and row['ZONE_UPS'] > 4) else 0)), axis=1)\n",
    "                                                    \n",
    "tbl_PLD_original_BLUE['AHS_2'] = tbl_PLD_original_BLUE.apply(lambda row: \n",
    "                                                           AH_L_2_UPS if (row['L'] > 48 and row['ZONE_UPS'] == 2) else \n",
    "                                                           (AH_L_3_4_UPS if (row['L'] > 48 and row['ZONE_UPS'] > 2 and row['ZONE_UPS'] < 5) else \n",
    "                                                           (AH_L_5_up_UPS if (row['L'] > 48 and row['ZONE_UPS'] > 4) else 0)), axis=1)\n",
    "\n",
    "tbl_PLD_original_BLUE['AHS_3'] = tbl_PLD_original_BLUE.apply(lambda row:\n",
    "                                                           AH_W_2_UPS if (row['W'] > 30 and row['ZONE_UPS'] == 2) else \n",
    "                                                           (AH_W_3_4_UPS if (row['W'] > 30 and row['ZONE_UPS'] > 2 and row['ZONE_UPS'] < 5) else \n",
    "                                                           (AH_W_5_up_UPS if (row['W'] > 30 and row['ZONE_UPS'] > 4) else 0)), axis=1)\n",
    "                                        \n",
    "tbl_PLD_original_BLUE['AHS_4'] = tbl_PLD_original_BLUE.apply(lambda row:\n",
    "                                                           AH_WGT_2_UPS if (row['ACTUAL_WEIGHT'] > 50 and row['ZONE_UPS'] == 2) else \n",
    "                                                           (AH_WGT_3_4_UPS if (row['ACTUAL_WEIGHT'] > 50 and row['ZONE_UPS'] > 2 and row['ZONE_UPS'] < 5) else \n",
    "                                                           (AH_WGT_5_up_UPS if (row['ACTUAL_WEIGHT'] > 50 and row['ZONE_UPS'] > 4) else 0)), axis=1)\n",
    "\n",
    "tbl_PLD_original_BLUE['AHS_BLUE'] = tbl_PLD_original_BLUE['AHS_1'] + tbl_PLD_original_BLUE['AHS_2'] + tbl_PLD_original_BLUE['AHS_3'] + tbl_PLD_original_BLUE['AHS_4']\n",
    "tbl_PLD_original_BLUE = tbl_PLD_original_BLUE.drop(['AHS_1','AHS_2','AHS_3','AHS_4'], axis=1)\n",
    "\n",
    "#Get Total rate. Set total rate to null if the freight rate is null and/or some package parameters are out of carrier handling capability.  \n",
    "tbl_PLD_original_BLUE['TOTAL_BLUE'] = round(tbl_PLD_original_BLUE.apply(lambda row: \n",
    "                                                                  (1 + row['FSC%_BLUE']*(1-BarrettDisc_Fsc_UPS)) * (row['FRT_BLUE'] + row['RES_BLUE'] + row['DAS_BLUE'] + row['AHS_BLUE']) if not pd.isna(row['FRT_BLUE']) and row['FRT_BLUE'] != '' else np.nan, axis=1),2)\n",
    "#tbl_PLD_original_BLUE.to_csv('test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb83a866-c0c3-4aa5-a807-586ab72ac6b6",
   "metadata": {},
   "source": [
    "# 2DAY (FedEx 2 Day Air)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "96d96c76-4f63-4fa1-9792-ec4bead65a96",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get the right client rate and the corresponding PLD\n",
    "tbl_clientRates_5 = tbl_clientRates[(tbl_clientRates['SERVICE_CODE'] == '2DAY')]\n",
    "tbl_PLD_original_2DAY = tbl_BP.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "45d24de2-44a4-4943-afe5-b857dae2a3b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Rerate start\n",
    "#add billed weight column to PLD\n",
    "tbl_PLD_original_2DAY['BILLED_WEIGHT_LB'] = tbl_PLD_original_2DAY.apply(lambda row: max(row['ACTUAL_WEIGHT_LB'], np.ceil(row['CUBIC_INCH']/dim_factor_fedex)), axis=1)\n",
    "\n",
    "\n",
    "#add Freight column to PLD\n",
    "tbl_PLD_original_2DAY['ZONE_FedEx'] = tbl_PLD_original_2DAY['ZONE_FedEx'].replace('17', '9')\n",
    "tbl_PLD_original_2DAY['ZONE_FedEx'] = tbl_PLD_original_2DAY['ZONE_FedEx'].astype(str)\n",
    "tbl_PLD_original_2DAY = tbl_PLD_original_2DAY.merge(tbl_clientRates_5[[\"WEIGHT_LB\", \"ZONE\", \"CLIENT_RATES\"]], left_on=['BILLED_WEIGHT_LB', 'ZONE_FedEx'], right_on=['WEIGHT_LB', 'ZONE'], how='left')\n",
    "tbl_PLD_original_2DAY = tbl_PLD_original_2DAY.drop(['WEIGHT_LB', 'ZONE'], axis=1)\n",
    "tbl_PLD_original_2DAY = tbl_PLD_original_2DAY.rename(columns={'CLIENT_RATES': 'FRT_2DAY'})\n",
    "\n",
    "\n",
    "#add Residential Surcharge column to PLD\n",
    "tbl_PLD_original_2DAY['RES_2DAY'] = tbl_PLD_original_2DAY.apply(lambda row: Resi_Surcharge_Air_FedEx if row['RESIDENTIAL_FLAG'] is True else 0, axis=1)\n",
    "\n",
    "\n",
    "#add DAS column to PLD, Pay attention to the flags under Residential Flag\n",
    "tbl_PLD_original_2DAY['DAS_2DAY'] = tbl_PLD_original_2DAY.apply(lambda row: \n",
    "                                                            0 if (row['DAS_CATEGORY_FedEx'] == 'DASFDX' and row['RESIDENTIAL_FLAG'] is False) else \n",
    "                                                           (0 if (row['DAS_CATEGORY_FedEx'] == 'DASEFDX' and row['RESIDENTIAL_FLAG'] is False) else\n",
    "                                                           (DAS_Resi_Air_FedEx if (row['DAS_CATEGORY_FedEx'] == 'DASFDX' and row['RESIDENTIAL_FLAG'] is True) else \n",
    "                                                           (DAS_Resi_Extended_Air_FedEx if (row['DAS_CATEGORY_FedEx'] == 'DASEFDX' and row['RESIDENTIAL_FLAG'] is True) else \n",
    "                                                           (Remote_Area_FedEx if (row['DAS_CATEGORY_FedEx'] == 'RAFDX') else \n",
    "                                                           (Remote_Area_Alaska_FedEx if (row['DAS_CATEGORY_FedEx'] == 'AKFDX') else\n",
    "                                                           (Remote_Area_Hawaii_FedEx if (row['DAS_CATEGORY_FedEx'] == 'HIFDX') else 0)))))), axis=1)\n",
    "\n",
    "\n",
    "#add fuel surcharge % column to PLD\n",
    "tbl_PLD_original_2DAY['SHIP_DATE'] = pd.to_datetime(tbl_PLD_original_2DAY['SHIP_DATE']).dt.date\n",
    "# get unique values from column 'SHIP_DATE' and create a new dataframe\n",
    "new_df = pd.DataFrame({'SHIP_DATE': tbl_PLD_original_2DAY['SHIP_DATE'].unique()})\n",
    "# use this new dataframe to merge to FSC\n",
    "df_merge = new_df.merge(FedEx_FSC, how='cross')\n",
    "\n",
    "\n",
    "#slice the merged dataframe to make sure ship_date is between the ship_data_start and end. Now we have a FSC file with Ship_date col to merge back to pld so that the pld can get FSC.\n",
    "df_merge['SHIP_DATE'] = pd.to_datetime(df_merge['SHIP_DATE']).dt.date\n",
    "df_merge = df_merge.query('SHIP_DATE >= SHIP_DATE_START and SHIP_DATE <= SHIP_DATE_END')\n",
    "df_merge = df_merge.drop_duplicates()\n",
    "# Get Domestic Air column as FSC. \n",
    "tbl_PLD_original_2DAY = tbl_PLD_original_2DAY.merge(df_merge, on=['SHIP_DATE'], how='left').drop(['SHIP_DATE_START', 'SHIP_DATE_END', 'Ground FedEx'], axis=1)\n",
    "tbl_PLD_original_2DAY.rename(columns={'Domestic Air FedEx': 'FSC%_2DAY'}, inplace=True)\n",
    "\n",
    "\n",
    "#add AHS column to PLD\n",
    "# Define the conditions and their corresponding values\n",
    "def calculate_ahs1(row):\n",
    "    if any([row['GIRTH_AND_L'] > 105, row['L'] > 48, row['W'] > 30]):\n",
    "        if row['ZONE_FedEx'] == '2':\n",
    "            return AH_Dim_2_FedEx\n",
    "        elif row['ZONE_FedEx'] in ('3','4'):\n",
    "            return AH_Dim_3_4_FedEx\n",
    "        elif row['ZONE_FedEx'] in ('5','6'):\n",
    "            return AH_Dim_5_6_FedEx\n",
    "        elif row['ZONE_FedEx'] in ('7','8','9','10','11','12','17','26','9','10','17','99'):\n",
    "            return AH_Dim_7_up_FedEx\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def calculate_ahs2(row):\n",
    "    if row['ACTUAL_WEIGHT'] > 50:\n",
    "        if row['ZONE_FedEx'] == '2':\n",
    "            return AH_WGT_2_FedEx\n",
    "        elif row['ZONE_FedEx'] in ('3','4'):\n",
    "            return AH_WGT_3_4_FedEx\n",
    "        elif row['ZONE_FedEx'] in ('5','6'):\n",
    "            return AH_WGT_5_6_FedEx\n",
    "        elif row['ZONE_FedEx'] in ('7','8','9','10','11','12','17','26','9','10','17','99'):\n",
    "            return AH_WGT_7_up_FedEx\n",
    "\n",
    "    else:\n",
    "        return 0\n",
    "        \n",
    "# Apply the function to the DataFrame\n",
    "tbl_PLD_original_2DAY['AHS_2DAY_1'] = tbl_PLD_original_2DAY.apply(calculate_ahs1, axis=1)\n",
    "tbl_PLD_original_2DAY['AHS_2DAY_2'] = tbl_PLD_original_2DAY.apply(calculate_ahs2, axis=1)\n",
    "tbl_PLD_original_2DAY['AHS_2DAY'] = tbl_PLD_original_2DAY['AHS_2DAY_1'] + tbl_PLD_original_2DAY['AHS_2DAY_2']\n",
    "tbl_PLD_original_2DAY = tbl_PLD_original_2DAY.drop(['AHS_2DAY_1','AHS_2DAY_2'], axis=1)\n",
    "\n",
    "\n",
    "#Get Total rate. Set total rate to null if the freight rate is null and/or some package parameters are out of carrier handling capability.  \n",
    "tbl_PLD_original_2DAY['TOTAL_2DAY'] = round(tbl_PLD_original_2DAY.apply(lambda row: \n",
    "                                                                  (1 + row['FSC%_2DAY']*(1-BarrettDisc_Fsc_FedEx)) * (row['FRT_2DAY'] + row['RES_2DAY'] + row['DAS_2DAY'] + row['AHS_2DAY']) if not pd.isna(row['FRT_2DAY']) and row['FRT_2DAY'] != '' else np.nan, axis=1),2)\n",
    "#tbl_PLD_original_2DAY.to_csv('test.csv')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "final-third",
   "metadata": {},
   "source": [
    "# ORNG (UPS 3DA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 573,
   "id": "59483f8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get the right client rate and the corresponding PLD\n",
    "tbl_clientRates_6 = tbl_clientRates[(tbl_clientRates['SERVICE_CODE'] == 'ORNG')]\n",
    "tbl_PLD_original_ORNG = tbl_BP.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 574,
   "id": "empty-illustration",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Rerate start\n",
    "#add billed weight column to PLD\n",
    "tbl_PLD_original_ORNG['BILLED_WEIGHT_LB'] = tbl_PLD_original_ORNG.apply(lambda row: max(row['ACTUAL_WEIGHT_LB'], np.ceil(row['CUBIC_INCH']/dim_factor_ups)), axis=1)\n",
    "\n",
    "#add Freight column to PLD\n",
    "tbl_PLD_original_ORNG['ZONE_UPS'] = tbl_PLD_original_ORNG['ZONE_UPS'].astype(str)\n",
    "tbl_PLD_original_ORNG = tbl_PLD_original_ORNG.merge(tbl_clientRates_6[[\"WEIGHT_LB\", \"ZONE\", \"CLIENT_RATES\"]], left_on=['BILLED_WEIGHT_LB', 'ZONE_UPS'], right_on=['WEIGHT_LB', 'ZONE'], how='left')\n",
    "tbl_PLD_original_ORNG = tbl_PLD_original_ORNG.drop('WEIGHT_LB', axis=1)\n",
    "tbl_PLD_original_ORNG = tbl_PLD_original_ORNG.rename(columns={'CLIENT_RATES': 'FRT_ORNG'})\n",
    "   \n",
    "    \n",
    "#add Residential Surcharge column to PLD\n",
    "tbl_PLD_original_ORNG['RES_ORNG'] = tbl_PLD_original_ORNG.apply(lambda row: Resi_Surcharge_Air if row['RESIDENTIAL_FLAG'] is True else 0, axis=1)\n",
    "\n",
    "#add DAS column to PLD, Pay attention to the flags under Residential Flag\n",
    "tbl_PLD_original_ORNG['DAS_ORNG'] = tbl_PLD_original_ORNG.apply(lambda row: \n",
    "                                                            DAS_Comm_Air if (row['DAS_CATEGORY_UPS'] == 'DASUPS' and row['RESIDENTIAL_FLAG'] is False) else \n",
    "                                                           (DAS_Comm_Extended_Air if (row['DAS_CATEGORY_UPS'] == 'DASEUPS' and row['RESIDENTIAL_FLAG'] is False) else\n",
    "                                                           (DAS_Resi_Air if (row['DAS_CATEGORY_UPS'] == 'DASUPS' and row['RESIDENTIAL_FLAG'] is True) else \n",
    "                                                           (DAS_Resi_Extended_Air if (row['DAS_CATEGORY_UPS'] == 'DASEUPS' and row['RESIDENTIAL_FLAG'] is True) else \n",
    "                                                           (Remote_Area if (row['DAS_CATEGORY_UPS'] == 'RAUPS') else \n",
    "                                                           (Remote_Area_Alaska if (row['DAS_CATEGORY_UPS'] == 'AKUPS') else\n",
    "                                                           (Remote_Area_Hawaii if (row['DAS_CATEGORY_UPS'] == 'HIUPS') else 0)))))), axis=1)\n",
    "\n",
    "\n",
    "#add fuel surcharge % column to PLD\n",
    "tbl_PLD_original_ORNG['SHIP_DATE'] = pd.to_datetime(tbl_PLD_original_ORNG['SHIP_DATE']).dt.date\n",
    "# get unique values from column 'SHIP_DATE' and create a new dataframe\n",
    "new_df = pd.DataFrame({'SHIP_DATE': tbl_PLD_original_ORNG['SHIP_DATE'].unique()})\n",
    "# use this new dataframe to merge to FSC\n",
    "df_merge = new_df.merge(UPS_FSC, how='cross')\n",
    "df_merge['SHIP_DATE'] = pd.to_datetime(df_merge['SHIP_DATE']).dt.date\n",
    "#slice the merged dataframe to make sure ship_date is between the ship_data_start and end. Now we have a FSC file with Ship_date col to merge back to pld so that the pld can get FSC.\n",
    "df_merge = df_merge.query('SHIP_DATE >= SHIP_DATE_START and SHIP_DATE <= SHIP_DATE_END')\n",
    "df_merge = df_merge.drop_duplicates()\n",
    "# Get Domestic Air column as FSC. \n",
    "tbl_PLD_original_ORNG = tbl_PLD_original_ORNG.merge(df_merge, on=['SHIP_DATE'], how='left').drop(['SHIP_DATE_START', 'SHIP_DATE_END', 'Ground'], axis=1)\n",
    "tbl_PLD_original_ORNG.rename(columns={'Domestic Air': 'FSC%_ORNG'}, inplace=True)\n",
    "\n",
    "\n",
    "\n",
    "#add AHS column to PLD\n",
    "tbl_PLD_original_ORNG['ZONE_UPS'] = pd.to_numeric(tbl_PLD_original_ORNG['ZONE_UPS'], errors='coerce').fillna(0).astype(int)\n",
    "\n",
    "tbl_PLD_original_ORNG['AHS_1'] = tbl_PLD_original_ORNG.apply(lambda row: \n",
    "                                                           AH_Girth_2_UPS if (row['GIRTH_AND_L'] > 105 and row['ZONE_UPS'] == 2) else \n",
    "                                                           (AH_Girth_3_4_UPS if (row['GIRTH_AND_L'] > 105 and row['ZONE_UPS'] > 2 and row['ZONE_UPS'] < 5) else\n",
    "                                                           (AH_Girth_5_up_UPS if (row['GIRTH_AND_L'] > 105 and row['ZONE_UPS'] > 4) else 0)), axis=1)\n",
    "                                                    \n",
    "tbl_PLD_original_ORNG['AHS_2'] = tbl_PLD_original_ORNG.apply(lambda row: \n",
    "                                                           AH_L_2_UPS if (row['L'] > 48 and row['ZONE_UPS'] == 2) else \n",
    "                                                           (AH_L_3_4_UPS if (row['L'] > 48 and row['ZONE_UPS'] > 2 and row['ZONE_UPS'] < 5) else \n",
    "                                                           (AH_L_5_up_UPS if (row['L'] > 48 and row['ZONE_UPS'] > 4) else 0)), axis=1)\n",
    "\n",
    "tbl_PLD_original_ORNG['AHS_3'] = tbl_PLD_original_ORNG.apply(lambda row:\n",
    "                                                           AH_W_2_UPS if (row['W'] > 30 and row['ZONE_UPS'] == 2) else \n",
    "                                                           (AH_W_3_4_UPS if (row['W'] > 30 and row['ZONE_UPS'] > 2 and row['ZONE_UPS'] < 5) else \n",
    "                                                           (AH_W_5_up_UPS if (row['W'] > 30 and row['ZONE_UPS'] > 4) else 0)), axis=1)\n",
    "                                        \n",
    "tbl_PLD_original_ORNG['AHS_4'] = tbl_PLD_original_ORNG.apply(lambda row:\n",
    "                                                           AH_WGT_2_UPS if (row['ACTUAL_WEIGHT'] > 50 and row['ZONE_UPS'] == 2) else \n",
    "                                                           (AH_WGT_3_4_UPS if (row['ACTUAL_WEIGHT'] > 50 and row['ZONE_UPS'] > 2 and row['ZONE_UPS'] < 5) else \n",
    "                                                           (AH_WGT_5_up_UPS if (row['ACTUAL_WEIGHT'] > 50 and row['ZONE_UPS'] > 4) else 0)), axis=1)\n",
    "\n",
    "tbl_PLD_original_ORNG['AHS_ORNG'] = tbl_PLD_original_ORNG['AHS_1'] + tbl_PLD_original_ORNG['AHS_2'] + tbl_PLD_original_ORNG['AHS_3'] + tbl_PLD_original_ORNG['AHS_4']\n",
    "tbl_PLD_original_ORNG = tbl_PLD_original_ORNG.drop(['AHS_1','AHS_2','AHS_3','AHS_4'], axis=1)\n",
    "\n",
    "#Get Total rate. Set total rate to null if the freight rate is null and/or some package parameters are out of carrier handling capability.  \n",
    "tbl_PLD_original_ORNG['TOTAL_ORNG'] = round(tbl_PLD_original_ORNG.apply(lambda row: \n",
    "                                                                  (1 + row['FSC%_ORNG']*(1-BarrettDisc_Fsc_UPS)) * (row['FRT_ORNG'] + row['RES_ORNG'] + row['DAS_ORNG'] + row['AHS_ORNG']) if not pd.isna(row['FRT_ORNG']) and row['FRT_ORNG'] != '' else np.nan, axis=1),2)\n",
    "#tbl_PLD_original_ORNG.to_csv('test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5b56ded-5cd0-446e-9c93-7ffd213d427a",
   "metadata": {},
   "source": [
    "# EXSA (FedEx 3 Day Air)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "f39c4eb3-38f0-4c31-ad92-9443f626da07",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get the right client rate and the corresponding PLD\n",
    "tbl_clientRates_6 = tbl_clientRates[(tbl_clientRates['SERVICE_CODE'] == 'EXSA')]\n",
    "tbl_PLD_original_EXSA = tbl_BP.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "b6b830df-e412-48f9-8288-427ae3635b8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Rerate start\n",
    "#add billed weight column to PLD\n",
    "tbl_PLD_original_EXSA['BILLED_WEIGHT_LB'] = tbl_PLD_original_EXSA.apply(lambda row: max(row['ACTUAL_WEIGHT_LB'], np.ceil(row['CUBIC_INCH']/dim_factor_fedex)), axis=1)\n",
    "\n",
    "\n",
    "#add Freight column to PLD\n",
    "tbl_PLD_original_EXSA['ZONE_FedEx'] = tbl_PLD_original_EXSA['ZONE_FedEx'].replace('17', '9')\n",
    "tbl_PLD_original_EXSA['ZONE_FedEx'] = tbl_PLD_original_EXSA['ZONE_FedEx'].astype(str)\n",
    "tbl_PLD_original_EXSA = tbl_PLD_original_EXSA.merge(tbl_clientRates_6[[\"WEIGHT_LB\", \"ZONE\", \"CLIENT_RATES\"]], left_on=['BILLED_WEIGHT_LB', 'ZONE_FedEx'], right_on=['WEIGHT_LB', 'ZONE'], how='left')\n",
    "tbl_PLD_original_EXSA = tbl_PLD_original_EXSA.drop(['WEIGHT_LB', 'ZONE'], axis=1)\n",
    "tbl_PLD_original_EXSA = tbl_PLD_original_EXSA.rename(columns={'CLIENT_RATES': 'FRT_EXSA'})\n",
    "\n",
    "\n",
    "#add Residential Surcharge column to PLD\n",
    "tbl_PLD_original_EXSA['RES_EXSA'] = tbl_PLD_original_EXSA.apply(lambda row: Resi_Surcharge_Air_FedEx if row['RESIDENTIAL_FLAG'] is True else 0, axis=1)\n",
    "\n",
    "\n",
    "#add DAS column to PLD, Pay attention to the flags under Residential Flag\n",
    "tbl_PLD_original_EXSA['DAS_EXSA'] = tbl_PLD_original_EXSA.apply(lambda row: \n",
    "                                                            0 if (row['DAS_CATEGORY_FedEx'] == 'DASFDX' and row['RESIDENTIAL_FLAG'] is False) else \n",
    "                                                           (0 if (row['DAS_CATEGORY_FedEx'] == 'DASEFDX' and row['RESIDENTIAL_FLAG'] is False) else\n",
    "                                                           (DAS_Resi_Air_FedEx if (row['DAS_CATEGORY_FedEx'] == 'DASFDX' and row['RESIDENTIAL_FLAG'] is True) else \n",
    "                                                           (DAS_Resi_Extended_Air_FedEx if (row['DAS_CATEGORY_FedEx'] == 'DASEFDX' and row['RESIDENTIAL_FLAG'] is True) else \n",
    "                                                           (Remote_Area_FedEx if (row['DAS_CATEGORY_FedEx'] == 'RAFDX') else \n",
    "                                                           (Remote_Area_Alaska_FedEx if (row['DAS_CATEGORY_FedEx'] == 'AKFDX') else\n",
    "                                                           (Remote_Area_Hawaii_FedEx if (row['DAS_CATEGORY_FedEx'] == 'HIFDX') else 0)))))), axis=1)\n",
    "\n",
    "\n",
    "#add fuel surcharge % column to PLD\n",
    "tbl_PLD_original_EXSA['SHIP_DATE'] = pd.to_datetime(tbl_PLD_original_EXSA['SHIP_DATE']).dt.date\n",
    "# get unique values from column 'SHIP_DATE' and create a new dataframe\n",
    "new_df = pd.DataFrame({'SHIP_DATE': tbl_PLD_original_EXSA['SHIP_DATE'].unique()})\n",
    "# use this new dataframe to merge to FSC\n",
    "df_merge = new_df.merge(FedEx_FSC, how='cross')\n",
    "\n",
    "\n",
    "#slice the merged dataframe to make sure ship_date is between the ship_data_start and end. Now we have a FSC file with Ship_date col to merge back to pld so that the pld can get FSC.\n",
    "df_merge['SHIP_DATE'] = pd.to_datetime(df_merge['SHIP_DATE']).dt.date\n",
    "df_merge = df_merge.query('SHIP_DATE >= SHIP_DATE_START and SHIP_DATE <= SHIP_DATE_END')\n",
    "df_merge = df_merge.drop_duplicates()\n",
    "# Get Domestic Air column as FSC. \n",
    "tbl_PLD_original_EXSA = tbl_PLD_original_EXSA.merge(df_merge, on=['SHIP_DATE'], how='left').drop(['SHIP_DATE_START', 'SHIP_DATE_END', 'Ground FedEx'], axis=1)\n",
    "tbl_PLD_original_EXSA.rename(columns={'Domestic Air FedEx': 'FSC%_EXSA'}, inplace=True)\n",
    "\n",
    "\n",
    "#add AHS column to PLD\n",
    "# Define the conditions and their corresponding values\n",
    "def calculate_ahs1(row):\n",
    "    if any([row['GIRTH_AND_L'] > 105, row['L'] > 48, row['W'] > 30]):\n",
    "        if row['ZONE_FedEx'] == '2':\n",
    "            return AH_Dim_2_FedEx\n",
    "        elif row['ZONE_FedEx'] in ('3','4'):\n",
    "            return AH_Dim_3_4_FedEx\n",
    "        elif row['ZONE_FedEx'] in ('5','6'):\n",
    "            return AH_Dim_5_6_FedEx\n",
    "        elif row['ZONE_FedEx'] in ('7','8','9','10','11','12','17','26','9','10','17','99'):\n",
    "            return AH_Dim_7_up_FedEx\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def calculate_ahs2(row):\n",
    "    if row['ACTUAL_WEIGHT'] > 50:\n",
    "        if row['ZONE_FedEx'] == '2':\n",
    "            return AH_WGT_2_FedEx\n",
    "        elif row['ZONE_FedEx'] in ('3','4'):\n",
    "            return AH_WGT_3_4_FedEx\n",
    "        elif row['ZONE_FedEx'] in ('5','6'):\n",
    "            return AH_WGT_5_6_FedEx\n",
    "        elif row['ZONE_FedEx'] in ('7','8','9','10','11','12','17','26','9','10','17','99'):\n",
    "            return AH_WGT_7_up_FedEx\n",
    "\n",
    "    else:\n",
    "        return 0\n",
    "        \n",
    "# Apply the function to the DataFrame\n",
    "tbl_PLD_original_EXSA['AHS_EXSA_1'] = tbl_PLD_original_EXSA.apply(calculate_ahs1, axis=1)\n",
    "tbl_PLD_original_EXSA['AHS_EXSA_2'] = tbl_PLD_original_EXSA.apply(calculate_ahs2, axis=1)\n",
    "tbl_PLD_original_EXSA['AHS_EXSA'] = tbl_PLD_original_EXSA['AHS_EXSA_1'] + tbl_PLD_original_EXSA['AHS_EXSA_2']\n",
    "tbl_PLD_original_EXSA = tbl_PLD_original_EXSA.drop(['AHS_EXSA_1','AHS_EXSA_2'], axis=1)\n",
    "\n",
    "\n",
    "#Get Total rate. Set total rate to null if the freight rate is null and/or some package parameters are out of carrier handling capability.  \n",
    "tbl_PLD_original_EXSA['TOTAL_EXSA'] = round(tbl_PLD_original_EXSA.apply(lambda row: \n",
    "                                                                  (1 + row['FSC%_EXSA']*(1-BarrettDisc_Fsc_FedEx)) * (row['FRT_EXSA'] + row['RES_EXSA'] + row['DAS_EXSA'] + row['AHS_EXSA']) if not pd.isna(row['FRT_EXSA']) and row['FRT_EXSA'] != '' else np.nan, axis=1),2)\n",
    "#tbl_PLD_original_EXSA.to_csv('test.csv')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "registered-prison",
   "metadata": {},
   "source": [
    "# GRND (UPS Ground Commercial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 578,
   "id": "235ec428",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get the right client rate and the corresponding PLD\n",
    "tbl_clientRates_7 = tbl_clientRates[(tbl_clientRates['SERVICE_CODE'] == 'GRND')]\n",
    "tbl_PLD_original_GRND = tbl_BP[(tbl_BP['RESIDENTIAL_FLAG'] == False)].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 579,
   "id": "linear-rwanda",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Rerate start\n",
    "#add billed weight column to PLD\n",
    "tbl_PLD_original_GRND['BILLED_WEIGHT_LB'] = tbl_PLD_original_GRND.apply(lambda row: max(row['ACTUAL_WEIGHT_LB'], np.ceil(row['CUBIC_INCH']/dim_factor_ups)), axis=1)\n",
    "\n",
    "#add Freight column to PLD\n",
    "tbl_PLD_original_GRND['ZONE_UPS'] = tbl_PLD_original_GRND['ZONE_UPS'].astype(str)\n",
    "tbl_PLD_original_GRND = tbl_PLD_original_GRND.merge(tbl_clientRates_7[[\"WEIGHT_LB\", \"ZONE\", \"CLIENT_RATES\"]], left_on=['BILLED_WEIGHT_LB', 'ZONE_UPS'], right_on=['WEIGHT_LB', 'ZONE'], how='left')\n",
    "tbl_PLD_original_GRND = tbl_PLD_original_GRND.drop('WEIGHT_LB', axis=1)\n",
    "tbl_PLD_original_GRND = tbl_PLD_original_GRND.rename(columns={'CLIENT_RATES': 'FRT_GRND'})\n",
    "   \n",
    "    \n",
    "#add Residential Surcharge column to PLD\n",
    "tbl_PLD_original_GRND['RES_GRND'] = tbl_PLD_original_GRND.apply(lambda row: Resi_Surcharge_Ground if row['RESIDENTIAL_FLAG'] is True else 0, axis=1)\n",
    "\n",
    "#add DAS column to PLD, Pay attention to the flags under Residential Flag\n",
    "tbl_PLD_original_GRND['DAS_GRND'] = tbl_PLD_original_GRND.apply(lambda row: \n",
    "                                                            DAS_Comm_Ground if (row['DAS_CATEGORY_UPS'] == 'DASUPS' and row['RESIDENTIAL_FLAG'] is False) else \n",
    "                                                           (DAS_Comm_Extended_Ground if (row['DAS_CATEGORY_UPS'] == 'DASEUPS' and row['RESIDENTIAL_FLAG'] is False) else\n",
    "                                                           (DAS_Resi_Ground if (row['DAS_CATEGORY_UPS'] == 'DASUPS' and row['RESIDENTIAL_FLAG'] is True) else \n",
    "                                                           (DAS_Resi_Extended_Ground if (row['DAS_CATEGORY_UPS'] == 'DASEUPS' and row['RESIDENTIAL_FLAG'] is True) else \n",
    "                                                           (Remote_Area if (row['DAS_CATEGORY_UPS'] == 'RAUPS') else \n",
    "                                                           (Remote_Area_Alaska if (row['DAS_CATEGORY_UPS'] == 'AKUPS') else\n",
    "                                                           (Remote_Area_Hawaii if (row['DAS_CATEGORY_UPS'] == 'HIUPS') else 0)))))), axis=1)\n",
    "\n",
    "\n",
    "#add fuel surcharge % column to PLD\n",
    "tbl_PLD_original_GRND['SHIP_DATE'] = pd.to_datetime(tbl_PLD_original_GRND['SHIP_DATE']).dt.date\n",
    "# get unique values from column 'SHIP_DATE' and create a new dataframe\n",
    "new_df = pd.DataFrame({'SHIP_DATE': tbl_PLD_original_GRND['SHIP_DATE'].unique()})\n",
    "# use this new dataframe to merge to FSC\n",
    "df_merge = new_df.merge(UPS_FSC, how='cross')\n",
    "df_merge['SHIP_DATE'] = pd.to_datetime(df_merge['SHIP_DATE']).dt.date\n",
    "#slice the merged dataframe to make sure ship_date is between the ship_data_start and end. Now we have a FSC file with Ship_date col to merge back to pld so that the pld can get FSC.\n",
    "df_merge = df_merge.query('SHIP_DATE >= SHIP_DATE_START and SHIP_DATE <= SHIP_DATE_END')\n",
    "df_merge = df_merge.drop_duplicates()\n",
    "# Get Ground column as FSC. \n",
    "tbl_PLD_original_GRND = tbl_PLD_original_GRND.merge(df_merge, on=['SHIP_DATE'], how='left').drop(['SHIP_DATE_START', 'SHIP_DATE_END', 'Domestic Air'], axis=1)\n",
    "tbl_PLD_original_GRND.rename(columns={'Ground': 'FSC%_GRND'}, inplace=True)\n",
    "\n",
    "\n",
    "\n",
    "#add AHS column to PLD\n",
    "tbl_PLD_original_GRND['ZONE_UPS'] = pd.to_numeric(tbl_PLD_original_GRND['ZONE_UPS'], errors='coerce').fillna(0).astype(int)\n",
    "\n",
    "tbl_PLD_original_GRND['AHS_1'] = tbl_PLD_original_GRND.apply(lambda row: \n",
    "                                                           AH_Girth_2_UPS if (row['GIRTH_AND_L'] > 105 and row['ZONE_UPS'] == 2) else \n",
    "                                                           (AH_Girth_3_4_UPS if (row['GIRTH_AND_L'] > 105 and row['ZONE_UPS'] > 2 and row['ZONE_UPS'] < 5) else\n",
    "                                                           (AH_Girth_5_up_UPS if (row['GIRTH_AND_L'] > 105 and row['ZONE_UPS'] > 4) else 0)), axis=1)\n",
    "                                                    \n",
    "tbl_PLD_original_GRND['AHS_2'] = tbl_PLD_original_GRND.apply(lambda row: \n",
    "                                                           AH_L_2_UPS if (row['L'] > 48 and row['ZONE_UPS'] == 2) else \n",
    "                                                           (AH_L_3_4_UPS if (row['L'] > 48 and row['ZONE_UPS'] > 2 and row['ZONE_UPS'] < 5) else \n",
    "                                                           (AH_L_5_up_UPS if (row['L'] > 48 and row['ZONE_UPS'] > 4) else 0)), axis=1)\n",
    "\n",
    "tbl_PLD_original_GRND['AHS_3'] = tbl_PLD_original_GRND.apply(lambda row:\n",
    "                                                           AH_W_2_UPS if (row['W'] > 30 and row['ZONE_UPS'] == 2) else \n",
    "                                                           (AH_W_3_4_UPS if (row['W'] > 30 and row['ZONE_UPS'] > 2 and row['ZONE_UPS'] < 5) else \n",
    "                                                           (AH_W_5_up_UPS if (row['W'] > 30 and row['ZONE_UPS'] > 4) else 0)), axis=1)\n",
    "                                        \n",
    "tbl_PLD_original_GRND['AHS_4'] = tbl_PLD_original_GRND.apply(lambda row:\n",
    "                                                           AH_WGT_2_UPS if (row['ACTUAL_WEIGHT'] > 50 and row['ZONE_UPS'] == 2) else \n",
    "                                                           (AH_WGT_3_4_UPS if (row['ACTUAL_WEIGHT'] > 50 and row['ZONE_UPS'] > 2 and row['ZONE_UPS'] < 5) else \n",
    "                                                           (AH_WGT_5_up_UPS if (row['ACTUAL_WEIGHT'] > 50 and row['ZONE_UPS'] > 4) else 0)), axis=1)\n",
    "\n",
    "tbl_PLD_original_GRND['AHS_GRND'] = tbl_PLD_original_GRND['AHS_1'] + tbl_PLD_original_GRND['AHS_2'] + tbl_PLD_original_GRND['AHS_3'] + tbl_PLD_original_GRND['AHS_4']\n",
    "tbl_PLD_original_GRND = tbl_PLD_original_GRND.drop(['AHS_1','AHS_2','AHS_3','AHS_4'], axis=1)\n",
    "\n",
    "#Get Total rate. Set total rate to null if the freight rate is null and/or some package parameters are out of carrier handling capability.  \n",
    "tbl_PLD_original_GRND['TOTAL_GRND'] = round(tbl_PLD_original_GRND.apply(lambda row: \n",
    "                                                                  (1 + row['FSC%_GRND']*(1-BarrettDisc_Fsc_UPS)) * (row['FRT_GRND'] + row['RES_GRND'] + row['DAS_GRND'] + row['AHS_GRND']) if not pd.isna(row['FRT_GRND']) and row['FRT_GRND'] != '' else np.nan, axis=1),2)\n",
    "#tbl_PLD_original_GRND.to_csv('test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a272a65e-057c-4434-b981-987e5d7ed663",
   "metadata": {},
   "source": [
    "# GRNDF (FedEx Ground)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "9d85097f-f95c-4be6-9feb-1aec88d8881e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get the right client rate and the corresponding PLD\n",
    "tbl_clientRates_7 = tbl_clientRates[(tbl_clientRates['SERVICE_CODE'] == 'GRNDF')]\n",
    "tbl_PLD_original_GRNDF = tbl_BP.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "07ba942d-40e7-4819-92ae-aed5a767add4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Rerate start\n",
    "#add billed weight column to PLD\n",
    "tbl_PLD_original_GRNDF['BILLED_WEIGHT_LB'] = tbl_PLD_original_GRNDF.apply(lambda row: max(row['ACTUAL_WEIGHT_LB'], np.ceil(row['CUBIC_INCH']/dim_factor_fedex)), axis=1)\n",
    "\n",
    "\n",
    "#add Freight column to PLD\n",
    "tbl_PLD_original_GRNDF['ZONE_FedEx'] = tbl_PLD_original_GRNDF['ZONE_FedEx'].astype(str)\n",
    "tbl_PLD_original_GRNDF = tbl_PLD_original_GRNDF.merge(tbl_clientRates_7[[\"WEIGHT_LB\", \"ZONE\", \"CLIENT_RATES\"]], left_on=['BILLED_WEIGHT_LB', 'ZONE_FedEx'], right_on=['WEIGHT_LB', 'ZONE'], how='left')\n",
    "tbl_PLD_original_GRNDF = tbl_PLD_original_GRNDF.drop(['WEIGHT_LB', 'ZONE'], axis=1)\n",
    "tbl_PLD_original_GRNDF = tbl_PLD_original_GRNDF.rename(columns={'CLIENT_RATES': 'FRT_GRNDF'})\n",
    "\n",
    "\n",
    "#add Residential Surcharge column to PLD\n",
    "tbl_PLD_original_GRNDF['RES_GRNDF'] = tbl_PLD_original_GRNDF.apply(lambda row: Resi_Surcharge_Ground_FedEx if row['RESIDENTIAL_FLAG'] is True else 0, axis=1)\n",
    "\n",
    "\n",
    "#add DAS column to PLD, Pay attention to the flags under Residential Flag\n",
    "tbl_PLD_original_GRNDF['DAS_GRNDF'] = tbl_PLD_original_GRNDF.apply(lambda row: \n",
    "                                                            DAS_Comm_Ground_FedEx if (row['DAS_CATEGORY_FedEx'] == 'DASFDX' and row['RESIDENTIAL_FLAG'] is False) else \n",
    "                                                           (DAS_Comm_Extended_Ground_FedEx if (row['DAS_CATEGORY_FedEx'] == 'DASEFDX' and row['RESIDENTIAL_FLAG'] is False) else\n",
    "                                                           (DAS_Resi_Ground_FedEx if (row['DAS_CATEGORY_FedEx'] == 'DASFDX' and row['RESIDENTIAL_FLAG'] is True) else \n",
    "                                                           (DAS_Resi_Extended_Ground_FedEx if (row['DAS_CATEGORY_FedEx'] == 'DASEFDX' and row['RESIDENTIAL_FLAG'] is True) else \n",
    "                                                           (Remote_Area_FedEx if (row['DAS_CATEGORY_FedEx'] == 'RAFDX') else \n",
    "                                                           (Remote_Area_Alaska_FedEx if (row['DAS_CATEGORY_FedEx'] == 'AKFDX') else\n",
    "                                                           (Remote_Area_Hawaii_FedEx if (row['DAS_CATEGORY_FedEx'] == 'HIFDX') else 0)))))), axis=1)\n",
    "\n",
    "\n",
    "#add fuel surcharge % column to PLD\n",
    "tbl_PLD_original_GRNDF['SHIP_DATE'] = pd.to_datetime(tbl_PLD_original_GRNDF['SHIP_DATE']).dt.date\n",
    "# get unique values from column 'SHIP_DATE' and create a new dataframe\n",
    "new_df = pd.DataFrame({'SHIP_DATE': tbl_PLD_original_GRNDF['SHIP_DATE'].unique()})\n",
    "# use this new dataframe to merge to FSC\n",
    "df_merge = new_df.merge(FedEx_FSC, how='cross')\n",
    "\n",
    "\n",
    "#slice the merged dataframe to make sure ship_date is between the ship_data_start and end. Now we have a FSC file with Ship_date col to merge back to pld so that the pld can get FSC.\n",
    "df_merge['SHIP_DATE'] = pd.to_datetime(df_merge['SHIP_DATE']).dt.date\n",
    "df_merge = df_merge.query('SHIP_DATE >= SHIP_DATE_START and SHIP_DATE <= SHIP_DATE_END')\n",
    "df_merge = df_merge.drop_duplicates()\n",
    "# Get Domestic Air column as FSC. \n",
    "tbl_PLD_original_GRNDF = tbl_PLD_original_GRNDF.merge(df_merge, on=['SHIP_DATE'], how='left').drop(['SHIP_DATE_START', 'SHIP_DATE_END', 'Domestic Air FedEx'], axis=1)\n",
    "tbl_PLD_original_GRNDF.rename(columns={'Ground FedEx': 'FSC%_GRNDF'}, inplace=True)\n",
    "\n",
    "\n",
    "#add AHS column to PLD\n",
    "# Define the conditions and their corresponding values\n",
    "def calculate_ahs1(row):\n",
    "    if any([row['GIRTH_AND_L'] > 105, row['L'] > 48, row['W'] > 30]):\n",
    "        if row['ZONE_FedEx'] == '2':\n",
    "            return AH_Dim_2_FedEx\n",
    "        elif row['ZONE_FedEx'] in ('3','4'):\n",
    "            return AH_Dim_3_4_FedEx\n",
    "        elif row['ZONE_FedEx'] in ('5','6'):\n",
    "            return AH_Dim_5_6_FedEx\n",
    "        elif row['ZONE_FedEx'] in ('7','8','9','10','11','12','17','26','9','10','17','99'):\n",
    "            return AH_Dim_7_up_FedEx\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def calculate_ahs2(row):\n",
    "    if row['ACTUAL_WEIGHT'] > 50:\n",
    "        if row['ZONE_FedEx'] == '2':\n",
    "            return AH_WGT_2_FedEx\n",
    "        elif row['ZONE_FedEx'] in ('3','4'):\n",
    "            return AH_WGT_3_4_FedEx\n",
    "        elif row['ZONE_FedEx'] in ('5','6'):\n",
    "            return AH_WGT_5_6_FedEx\n",
    "        elif row['ZONE_FedEx'] in ('7','8','9','10','11','12','17','26','9','10','17','99'):\n",
    "            return AH_WGT_7_up_FedEx\n",
    "\n",
    "    else:\n",
    "        return 0\n",
    "        \n",
    "# Apply the function to the DataFrame\n",
    "tbl_PLD_original_GRNDF['AHS_GRNDF_1'] = tbl_PLD_original_GRNDF.apply(calculate_ahs1, axis=1)\n",
    "tbl_PLD_original_GRNDF['AHS_GRNDF_2'] = tbl_PLD_original_GRNDF.apply(calculate_ahs2, axis=1)\n",
    "tbl_PLD_original_GRNDF['AHS_GRNDF'] = tbl_PLD_original_GRNDF['AHS_GRNDF_1'] + tbl_PLD_original_GRNDF['AHS_GRNDF_2']\n",
    "tbl_PLD_original_GRNDF = tbl_PLD_original_GRNDF.drop(['AHS_GRNDF_1','AHS_GRNDF_2'], axis=1)\n",
    "\n",
    "\n",
    "#Get Total rate. Set total rate to null if the freight rate is null and/or some package parameters are out of carrier handling capability.  \n",
    "tbl_PLD_original_GRNDF['TOTAL_GRNDF'] = round(tbl_PLD_original_GRNDF.apply(lambda row: \n",
    "                                                                  (1 + row['FSC%_GRNDF']*(1-BarrettDisc_Fsc_FedEx)) * (row['FRT_GRNDF'] + row['RES_GRNDF'] + row['DAS_GRNDF'] + row['AHS_GRNDF']) if not pd.isna(row['FRT_GRNDF']) and row['FRT_GRNDF'] != '' else np.nan, axis=1),2)\n",
    "#tbl_PLD_original_GRNDF.to_csv('test.csv')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sensitive-belgium",
   "metadata": {},
   "source": [
    "# GRES (UPS Ground Residential)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 581,
   "id": "51a4c030",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get the right client rate and the corresponding PLD\n",
    "tbl_clientRates_8 = tbl_clientRates[(tbl_clientRates['SERVICE_CODE'] == 'GRES')]\n",
    "tbl_PLD_original_GRES = tbl_BP[(tbl_BP['RESIDENTIAL_FLAG'] == True)].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 582,
   "id": "durable-liver",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Rerate start\n",
    "#add billed weight column to PLD\n",
    "tbl_PLD_original_GRES['BILLED_WEIGHT_LB'] = tbl_PLD_original_GRES.apply(lambda row: max(row['ACTUAL_WEIGHT_LB'], np.ceil(row['CUBIC_INCH']/dim_factor_ups)), axis=1)\n",
    "\n",
    "#add Freight column to PLD\n",
    "tbl_PLD_original_GRES['ZONE_UPS'] = tbl_PLD_original_GRES['ZONE_UPS'].astype(str)\n",
    "tbl_PLD_original_GRES = tbl_PLD_original_GRES.merge(tbl_clientRates_8[[\"WEIGHT_LB\", \"ZONE\", \"CLIENT_RATES\"]], left_on=['BILLED_WEIGHT_LB', 'ZONE_UPS'], right_on=['WEIGHT_LB', 'ZONE'], how='left')\n",
    "tbl_PLD_original_GRES = tbl_PLD_original_GRES.drop('WEIGHT_LB', axis=1)\n",
    "tbl_PLD_original_GRES = tbl_PLD_original_GRES.rename(columns={'CLIENT_RATES': 'FRT_GRES'})\n",
    "   \n",
    "    \n",
    "#add Residential Surcharge column to PLD\n",
    "tbl_PLD_original_GRES['RES_GRES'] = tbl_PLD_original_GRES.apply(lambda row: Resi_Surcharge_Ground if row['RESIDENTIAL_FLAG'] is True else 0, axis=1)\n",
    "\n",
    "#add DAS column to PLD, Pay attention to the flags under Residential Flag\n",
    "tbl_PLD_original_GRES['DAS_GRES'] = tbl_PLD_original_GRES.apply(lambda row: \n",
    "                                                            DAS_Comm_Ground if (row['DAS_CATEGORY_UPS'] == 'DASUPS' and row['RESIDENTIAL_FLAG'] is False) else \n",
    "                                                           (DAS_Comm_Extended_Ground if (row['DAS_CATEGORY_UPS'] == 'DASEUPS' and row['RESIDENTIAL_FLAG'] is False) else\n",
    "                                                           (DAS_Resi_Ground if (row['DAS_CATEGORY_UPS'] == 'DASUPS' and row['RESIDENTIAL_FLAG'] is True) else \n",
    "                                                           (DAS_Resi_Extended_Ground if (row['DAS_CATEGORY_UPS'] == 'DASEUPS' and row['RESIDENTIAL_FLAG'] is True) else \n",
    "                                                           (Remote_Area if (row['DAS_CATEGORY_UPS'] == 'RAUPS') else \n",
    "                                                           (Remote_Area_Alaska if (row['DAS_CATEGORY_UPS'] == 'AKUPS') else\n",
    "                                                           (Remote_Area_Hawaii if (row['DAS_CATEGORY_UPS'] == 'HIUPS') else 0)))))), axis=1)\n",
    "\n",
    "\n",
    "#add fuel surcharge % column to PLD\n",
    "tbl_PLD_original_GRES['SHIP_DATE'] = pd.to_datetime(tbl_PLD_original_GRES['SHIP_DATE']).dt.date\n",
    "# get unique values from column 'SHIP_DATE' and create a new dataframe\n",
    "new_df = pd.DataFrame({'SHIP_DATE': tbl_PLD_original_GRES['SHIP_DATE'].unique()})\n",
    "# use this new dataframe to merge to FSC\n",
    "df_merge = new_df.merge(UPS_FSC, how='cross')\n",
    "df_merge['SHIP_DATE'] = pd.to_datetime(df_merge['SHIP_DATE']).dt.date\n",
    "#slice the merged dataframe to make sure ship_date is between the ship_data_start and end. Now we have a FSC file with Ship_date col to merge back to pld so that the pld can get FSC.\n",
    "df_merge = df_merge.query('SHIP_DATE >= SHIP_DATE_START and SHIP_DATE <= SHIP_DATE_END')\n",
    "df_merge = df_merge.drop_duplicates()\n",
    "# Get Ground column as FSC. \n",
    "tbl_PLD_original_GRES = tbl_PLD_original_GRES.merge(df_merge, on=['SHIP_DATE'], how='left').drop(['SHIP_DATE_START', 'SHIP_DATE_END', 'Domestic Air'], axis=1)\n",
    "tbl_PLD_original_GRES.rename(columns={'Ground': 'FSC%_GRES'}, inplace=True)\n",
    "\n",
    "\n",
    "\n",
    "#add AHS column to PLD\n",
    "tbl_PLD_original_GRES['ZONE_UPS'] = pd.to_numeric(tbl_PLD_original_GRES['ZONE_UPS'], errors='coerce').fillna(0).astype(int)\n",
    "\n",
    "tbl_PLD_original_GRES['AHS_1'] = tbl_PLD_original_GRES.apply(lambda row: \n",
    "                                                           AH_Girth_2_UPS if (row['GIRTH_AND_L'] > 105 and row['ZONE_UPS'] == 2) else \n",
    "                                                           (AH_Girth_3_4_UPS if (row['GIRTH_AND_L'] > 105 and row['ZONE_UPS'] > 2 and row['ZONE_UPS'] < 5) else\n",
    "                                                           (AH_Girth_5_up_UPS if (row['GIRTH_AND_L'] > 105 and row['ZONE_UPS'] > 4) else 0)), axis=1)\n",
    "                                                    \n",
    "tbl_PLD_original_GRES['AHS_2'] = tbl_PLD_original_GRES.apply(lambda row: \n",
    "                                                           AH_L_2_UPS if (row['L'] > 48 and row['ZONE_UPS'] == 2) else \n",
    "                                                           (AH_L_3_4_UPS if (row['L'] > 48 and row['ZONE_UPS'] > 2 and row['ZONE_UPS'] < 5) else \n",
    "                                                           (AH_L_5_up_UPS if (row['L'] > 48 and row['ZONE_UPS'] > 4) else 0)), axis=1)\n",
    "\n",
    "tbl_PLD_original_GRES['AHS_3'] = tbl_PLD_original_GRES.apply(lambda row:\n",
    "                                                           AH_W_2_UPS if (row['W'] > 30 and row['ZONE_UPS'] == 2) else \n",
    "                                                           (AH_W_3_4_UPS if (row['W'] > 30 and row['ZONE_UPS'] > 2 and row['ZONE_UPS'] < 5) else \n",
    "                                                           (AH_W_5_up_UPS if (row['W'] > 30 and row['ZONE_UPS'] > 4) else 0)), axis=1)\n",
    "                                        \n",
    "tbl_PLD_original_GRES['AHS_4'] = tbl_PLD_original_GRES.apply(lambda row:\n",
    "                                                           AH_WGT_2_UPS if (row['ACTUAL_WEIGHT'] > 50 and row['ZONE_UPS'] == 2) else \n",
    "                                                           (AH_WGT_3_4_UPS if (row['ACTUAL_WEIGHT'] > 50 and row['ZONE_UPS'] > 2 and row['ZONE_UPS'] < 5) else \n",
    "                                                           (AH_WGT_5_up_UPS if (row['ACTUAL_WEIGHT'] > 50 and row['ZONE_UPS'] > 4) else 0)), axis=1)\n",
    "\n",
    "tbl_PLD_original_GRES['AHS_GRES'] = tbl_PLD_original_GRES['AHS_1'] + tbl_PLD_original_GRES['AHS_2'] + tbl_PLD_original_GRES['AHS_3'] + tbl_PLD_original_GRES['AHS_4']\n",
    "tbl_PLD_original_GRES = tbl_PLD_original_GRES.drop(['AHS_1','AHS_2','AHS_3','AHS_4'], axis=1)\n",
    "\n",
    "#Get Total rate. Set total rate to null if the freight rate is null and/or some package parameters are out of carrier handling capability.  \n",
    "tbl_PLD_original_GRES['TOTAL_GRES'] = round(tbl_PLD_original_GRES.apply(lambda row: \n",
    "                                                                  (1 + row['FSC%_GRES']*(1-BarrettDisc_Fsc_UPS)) * (row['FRT_GRES'] + row['RES_GRES'] + row['DAS_GRES'] + row['AHS_GRES']) if not pd.isna(row['FRT_GRES']) and row['FRT_GRES'] != '' else np.nan, axis=1),2)\n",
    "#tbl_PLD_original_GRES.to_csv('test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "611c3762-68af-4e8e-a179-4eb1d91644d6",
   "metadata": {},
   "source": [
    "# HOME (FedEx Residential Ground)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "cbdf1938-0baa-4864-991f-c9bd5597aa5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get the right client rate and the corresponding PLD\n",
    "tbl_clientRates_8 = tbl_clientRates[(tbl_clientRates['SERVICE_CODE'] == 'HOME')]\n",
    "tbl_PLD_original_HOME = tbl_BP.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "b6f2cf50-ac8b-440c-89ec-b2f7f16246c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Rerate start\n",
    "#add billed weight column to PLD\n",
    "tbl_PLD_original_HOME['BILLED_WEIGHT_LB'] = tbl_PLD_original_HOME.apply(lambda row: max(row['ACTUAL_WEIGHT_LB'], np.ceil(row['CUBIC_INCH']/dim_factor_fedex)), axis=1)\n",
    "\n",
    "\n",
    "#add Freight column to PLD\n",
    "tbl_PLD_original_HOME['ZONE_FedEx'] = tbl_PLD_original_HOME['ZONE_FedEx'].astype(str)\n",
    "tbl_PLD_original_HOME = tbl_PLD_original_HOME.merge(tbl_clientRates_8[[\"WEIGHT_LB\", \"ZONE\", \"CLIENT_RATES\"]], left_on=['BILLED_WEIGHT_LB', 'ZONE_FedEx'], right_on=['WEIGHT_LB', 'ZONE'], how='left')\n",
    "tbl_PLD_original_HOME = tbl_PLD_original_HOME.drop(['WEIGHT_LB', 'ZONE'], axis=1)\n",
    "tbl_PLD_original_HOME = tbl_PLD_original_HOME.rename(columns={'CLIENT_RATES': 'FRT_HOME'})\n",
    "\n",
    "\n",
    "#add Residential Surcharge column to PLD\n",
    "tbl_PLD_original_HOME['RES_HOME'] = tbl_PLD_original_HOME.apply(lambda row: Resi_Surcharge_Ground_FedEx if row['RESIDENTIAL_FLAG'] is True else 0, axis=1)\n",
    "\n",
    "\n",
    "#add DAS column to PLD, Pay attention to the flags under Residential Flag\n",
    "tbl_PLD_original_HOME['DAS_HOME'] = tbl_PLD_original_HOME.apply(lambda row: \n",
    "                                                            DAS_Comm_Ground_FedEx if (row['DAS_CATEGORY_FedEx'] == 'DASFDX' and row['RESIDENTIAL_FLAG'] is False) else \n",
    "                                                           (DAS_Comm_Extended_Ground_FedEx if (row['DAS_CATEGORY_FedEx'] == 'DASEFDX' and row['RESIDENTIAL_FLAG'] is False) else\n",
    "                                                           (DAS_Resi_Ground_FedEx if (row['DAS_CATEGORY_FedEx'] == 'DASFDX' and row['RESIDENTIAL_FLAG'] is True) else \n",
    "                                                           (DAS_Resi_Extended_Ground_FedEx if (row['DAS_CATEGORY_FedEx'] == 'DASEFDX' and row['RESIDENTIAL_FLAG'] is True) else \n",
    "                                                           (Remote_Area_FedEx if (row['DAS_CATEGORY_FedEx'] == 'RAFDX') else \n",
    "                                                           (Remote_Area_Alaska_FedEx if (row['DAS_CATEGORY_FedEx'] == 'AKFDX') else\n",
    "                                                           (Remote_Area_Hawaii_FedEx if (row['DAS_CATEGORY_FedEx'] == 'HIFDX') else 0)))))), axis=1)\n",
    "\n",
    "\n",
    "#add fuel surcharge % column to PLD\n",
    "tbl_PLD_original_HOME['SHIP_DATE'] = pd.to_datetime(tbl_PLD_original_HOME['SHIP_DATE']).dt.date\n",
    "# get unique values from column 'SHIP_DATE' and create a new dataframe\n",
    "new_df = pd.DataFrame({'SHIP_DATE': tbl_PLD_original_HOME['SHIP_DATE'].unique()})\n",
    "# use this new dataframe to merge to FSC\n",
    "df_merge = new_df.merge(FedEx_FSC, how='cross')\n",
    "\n",
    "\n",
    "#slice the merged dataframe to make sure ship_date is between the ship_data_start and end. Now we have a FSC file with Ship_date col to merge back to pld so that the pld can get FSC.\n",
    "df_merge['SHIP_DATE'] = pd.to_datetime(df_merge['SHIP_DATE']).dt.date\n",
    "df_merge = df_merge.query('SHIP_DATE >= SHIP_DATE_START and SHIP_DATE <= SHIP_DATE_END')\n",
    "df_merge = df_merge.drop_duplicates()\n",
    "# Get Domestic Air column as FSC. \n",
    "tbl_PLD_original_HOME = tbl_PLD_original_HOME.merge(df_merge, on=['SHIP_DATE'], how='left').drop(['SHIP_DATE_START', 'SHIP_DATE_END', 'Domestic Air FedEx'], axis=1)\n",
    "tbl_PLD_original_HOME.rename(columns={'Ground FedEx': 'FSC%_HOME'}, inplace=True)\n",
    "\n",
    "\n",
    "#add AHS column to PLD\n",
    "# Define the conditions and their corresponding values\n",
    "def calculate_ahs1(row):\n",
    "    if any([row['GIRTH_AND_L'] > 105, row['L'] > 48, row['W'] > 30]):\n",
    "        if row['ZONE_FedEx'] == '2':\n",
    "            return AH_Dim_2_FedEx\n",
    "        elif row['ZONE_FedEx'] in ('3','4'):\n",
    "            return AH_Dim_3_4_FedEx\n",
    "        elif row['ZONE_FedEx'] in ('5','6'):\n",
    "            return AH_Dim_5_6_FedEx\n",
    "        elif row['ZONE_FedEx'] in ('7','8','9','10','11','12','17','26','9','10','17','99'):\n",
    "            return AH_Dim_7_up_FedEx\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def calculate_ahs2(row):\n",
    "    if row['ACTUAL_WEIGHT'] > 50:\n",
    "        if row['ZONE_FedEx'] == '2':\n",
    "            return AH_WGT_2_FedEx\n",
    "        elif row['ZONE_FedEx'] in ('3','4'):\n",
    "            return AH_WGT_3_4_FedEx\n",
    "        elif row['ZONE_FedEx'] in ('5','6'):\n",
    "            return AH_WGT_5_6_FedEx\n",
    "        elif row['ZONE_FedEx'] in ('7','8','9','10','11','12','17','26','9','10','17','99'):\n",
    "            return AH_WGT_7_up_FedEx\n",
    "\n",
    "    else:\n",
    "        return 0\n",
    "        \n",
    "# Apply the function to the DataFrame\n",
    "tbl_PLD_original_HOME['AHS_HOME_1'] = tbl_PLD_original_HOME.apply(calculate_ahs1, axis=1)\n",
    "tbl_PLD_original_HOME['AHS_HOME_2'] = tbl_PLD_original_HOME.apply(calculate_ahs2, axis=1)\n",
    "tbl_PLD_original_HOME['AHS_HOME'] = tbl_PLD_original_HOME['AHS_HOME_1'] + tbl_PLD_original_HOME['AHS_HOME_2']\n",
    "tbl_PLD_original_HOME = tbl_PLD_original_HOME.drop(['AHS_HOME_1','AHS_HOME_2'], axis=1)\n",
    "\n",
    "\n",
    "#Get Total rate. Set total rate to null if the freight rate is null and/or some package parameters are out of carrier handling capability.  \n",
    "tbl_PLD_original_HOME['TOTAL_HOME'] = round(tbl_PLD_original_HOME.apply(lambda row: \n",
    "                                                                  (1 + row['FSC%_HOME']*(1-BarrettDisc_Fsc_FedEx)) * (row['FRT_HOME'] + row['RES_HOME'] + row['DAS_HOME'] + row['AHS_HOME']) if not pd.isna(row['FRT_HOME']) and row['FRT_HOME'] != '' else np.nan, axis=1),2)\n",
    "#tbl_PLD_original_HOME.to_csv('test.csv')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hungarian-fishing",
   "metadata": {},
   "source": [
    "# SRPT<1 (UPS Surepost 1#>)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 586,
   "id": "4cb0cab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get the right client rate and the corresponding PLD\n",
    "tbl_clientRates_9 = tbl_clientRates[(tbl_clientRates['SERVICE_CODE'] == 'SRPT<1')]\n",
    "tbl_PLD_original_SRPT1 = tbl_BP[(tbl_BP['ACTUAL_WEIGHT'] < 1)].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 587,
   "id": "listed-graduation",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Rerate start\n",
    "#add billed weight column to PLD\n",
    "tbl_PLD_original_SRPT1['BILLED_WEIGHT_OZ'] = tbl_PLD_original_SRPT1.apply(lambda row: max(row['ACTUAL_WEIGHT_LB'], np.ceil(row['CUBIC_INCH']/dim_factor_ups_srpt))*16 if (row['CUBIC_INCH'] >= 3456) else\n",
    "                                                                          (row['ACTUAL_WEIGHT_OZ']), axis=1)\n",
    "\n",
    "\n",
    "#add Freight column to PLD\n",
    "tbl_PLD_original_SRPT1['ZONE_UPS'] = tbl_PLD_original_SRPT1['ZONE_UPS'].astype(str)\n",
    "tbl_PLD_original_SRPT1 = tbl_PLD_original_SRPT1.merge(tbl_clientRates_9[[\"WEIGHT_OZ\", \"ZONE\", \"CLIENT_RATES\"]], left_on=['BILLED_WEIGHT_OZ', 'ZONE_UPS'], right_on=['WEIGHT_OZ', 'ZONE'], how='left')\n",
    "tbl_PLD_original_SRPT1 = tbl_PLD_original_SRPT1.drop('WEIGHT_OZ', axis=1)\n",
    "tbl_PLD_original_SRPT1 = tbl_PLD_original_SRPT1.rename(columns={'CLIENT_RATES': 'FRT_SRPT1'})\n",
    "   \n",
    "    \n",
    "#add Residential Surcharge column to PLD\n",
    "tbl_PLD_original_SRPT1['RES_SRPT1'] = tbl_PLD_original_SRPT1.apply(lambda row: 0 if row['RESIDENTIAL_FLAG'] == True else 0, axis=1)\n",
    "\n",
    "#add DAS column to PLD, Pay attention to the flags under Residential Flag\n",
    "tbl_PLD_original_SRPT1['DAS_SRPT1'] = tbl_PLD_original_SRPT1.apply(lambda row: \n",
    "                                                            DAS_SurePost if (row['DAS_CATEGORY_UPS'] == 'DASUPS') else \n",
    "                                                           (DAS_Extended_SurePost if (row['DAS_CATEGORY_UPS'] == 'DASEUPS') else \n",
    "                                                           (Remote_Area if (row['DAS_CATEGORY_UPS'] == 'RAUPS') else \n",
    "                                                           (Remote_Area_Alaska if (row['DAS_CATEGORY_UPS'] == 'AKUPS') else\n",
    "                                                           (Remote_Area_Hawaii if (row['DAS_CATEGORY_UPS'] == 'HIUPS') else 0)))), axis=1)\n",
    "\n",
    "\n",
    "\n",
    "#add fuel surcharge % column to PLD\n",
    "tbl_PLD_original_SRPT1['SHIP_DATE'] = pd.to_datetime(tbl_PLD_original_SRPT1['SHIP_DATE']).dt.date\n",
    "# get unique values from column 'SHIP_DATE' and create a new dataframe\n",
    "new_df = pd.DataFrame({'SHIP_DATE': tbl_PLD_original_SRPT1['SHIP_DATE'].unique()})\n",
    "# use this new dataframe to merge to FSC\n",
    "df_merge = new_df.merge(UPS_FSC, how='cross')\n",
    "df_merge['SHIP_DATE'] = pd.to_datetime(df_merge['SHIP_DATE']).dt.date\n",
    "#slice the merged dataframe to make sure ship_date is between the ship_data_start and end. Now we have a FSC file with Ship_date col to merge back to pld so that the pld can get FSC.\n",
    "df_merge = df_merge.query('SHIP_DATE >= SHIP_DATE_START and SHIP_DATE <= SHIP_DATE_END')\n",
    "df_merge = df_merge.drop_duplicates()\n",
    "# Get Ground column as FSC. \n",
    "tbl_PLD_original_SRPT1 = tbl_PLD_original_SRPT1.merge(df_merge, on=['SHIP_DATE'], how='left').drop(['SHIP_DATE_START', 'SHIP_DATE_END', 'Domestic Air'], axis=1)\n",
    "tbl_PLD_original_SRPT1.rename(columns={'Ground': 'FSC%_SRPT1'}, inplace=True)\n",
    "\n",
    "\n",
    "\n",
    "#add AHS column to PLD\n",
    "tbl_PLD_original_SRPT1['ZONE_UPS'] = pd.to_numeric(tbl_PLD_original_SRPT1['ZONE_UPS'], errors='coerce').fillna(0).astype(int)\n",
    "\n",
    "tbl_PLD_original_SRPT1['AHS_1'] = tbl_PLD_original_SRPT1.apply(lambda row: \n",
    "                                                           AH_Girth_2_UPS if (row['GIRTH_AND_L'] > 105 and row['ZONE_UPS'] == 2) else \n",
    "                                                           (AH_Girth_3_4_UPS if (row['GIRTH_AND_L'] > 105 and row['ZONE_UPS'] > 2 and row['ZONE_UPS'] < 5) else\n",
    "                                                           (AH_Girth_5_up_UPS if (row['GIRTH_AND_L'] > 105 and row['ZONE_UPS'] > 4) else 0)), axis=1)\n",
    "                                                    \n",
    "tbl_PLD_original_SRPT1['AHS_2'] = tbl_PLD_original_SRPT1.apply(lambda row: \n",
    "                                                           AH_L_2_UPS if (row['L'] > 48 and row['ZONE_UPS'] == 2) else \n",
    "                                                           (AH_L_3_4_UPS if (row['L'] > 48 and row['ZONE_UPS'] > 2 and row['ZONE_UPS'] < 5) else \n",
    "                                                           (AH_L_5_up_UPS if (row['L'] > 48 and row['ZONE_UPS'] > 4) else 0)), axis=1)\n",
    "\n",
    "tbl_PLD_original_SRPT1['AHS_3'] = tbl_PLD_original_SRPT1.apply(lambda row:\n",
    "                                                           AH_W_2_UPS if (row['W'] > 30 and row['ZONE_UPS'] == 2) else \n",
    "                                                           (AH_W_3_4_UPS if (row['W'] > 30 and row['ZONE_UPS'] > 2 and row['ZONE_UPS'] < 5) else \n",
    "                                                           (AH_W_5_up_UPS if (row['W'] > 30 and row['ZONE_UPS'] > 4) else 0)), axis=1)\n",
    "                                        \n",
    "tbl_PLD_original_SRPT1['AHS_4'] = tbl_PLD_original_SRPT1.apply(lambda row:\n",
    "                                                           AH_WGT_2_UPS if (row['ACTUAL_WEIGHT'] > 50 and row['ZONE_UPS'] == 2) else \n",
    "                                                           (AH_WGT_3_4_UPS if (row['ACTUAL_WEIGHT'] > 50 and row['ZONE_UPS'] > 2 and row['ZONE_UPS'] < 5) else \n",
    "                                                           (AH_WGT_5_up_UPS if (row['ACTUAL_WEIGHT'] > 50 and row['ZONE_UPS'] > 4) else 0)), axis=1)\n",
    "\n",
    "tbl_PLD_original_SRPT1['AHS_SRPT1'] = tbl_PLD_original_SRPT1['AHS_1'] + tbl_PLD_original_SRPT1['AHS_2'] + tbl_PLD_original_SRPT1['AHS_3'] + tbl_PLD_original_SRPT1['AHS_4']\n",
    "tbl_PLD_original_SRPT1 = tbl_PLD_original_SRPT1.drop(['AHS_1','AHS_2','AHS_3','AHS_4'], axis=1)\n",
    "\n",
    "#Get Total rate. Set total rate to null if the freight rate is null and/or some package parameters are out of carrier handling capability.  \n",
    "tbl_PLD_original_SRPT1['TOTAL_SRPT1'] = round(tbl_PLD_original_SRPT1.apply(lambda row: \n",
    "                                                                  (1 + row['FSC%_SRPT1']*(1-BarrettDisc_Fsc_UPS)) * (row['FRT_SRPT1'] + row['RES_SRPT1'] + row['DAS_SRPT1'] + row['AHS_SRPT1']) if not pd.isna(row['FRT_SRPT1']) and row['FRT_SRPT1'] != '' else np.nan, axis=1),2)\n",
    "#tbl_PLD_original_SRPT1.to_csv('test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c6f5cf0-777e-4081-a862-bd492b41f214",
   "metadata": {},
   "source": [
    "# SPST<1 (FedEx Deferred Ground <1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 452,
   "id": "1c78e712-9a2d-4c58-9693-8597c77f4392",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get the right client rate and the corresponding PLD\n",
    "tbl_clientRates_9 = tbl_clientRates[(tbl_clientRates['SERVICE_CODE'] == 'SPST<1')]\n",
    "tbl_PLD_original_SPST1 = tbl_BP[(tbl_BP['ACTUAL_WEIGHT'] < 1)].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 453,
   "id": "f12e9f99-cbb9-4e2a-b7b5-fcc481040707",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Rerate start\n",
    "#add billed weight column to PLD\n",
    "tbl_PLD_original_SPST1['BILLED_WEIGHT_OZ'] = tbl_PLD_original_SPST1.apply(lambda row: \n",
    "                                                                          max(np.ceil(row['ACTUAL_WEIGHT']*16), np.ceil((row['CUBIC_INCH']/dim_factor_fedex_spst)*16)) if (row['CUBIC_INCH'] < 3456) else \n",
    "                                                                          max(np.ceil(row['ACTUAL_WEIGHT']*16), np.ceil((row['CUBIC_INCH']/dim_factor_fedex)*16)), axis=1)\n",
    "\n",
    "\n",
    "#add Freight column to PLD\n",
    "tbl_PLD_original_SPST1['ZONE_FedEx'] = tbl_PLD_original_SPST1['ZONE_FedEx'].astype(str)\n",
    "tbl_PLD_original_SPST1 = tbl_PLD_original_SPST1.merge(tbl_clientRates_9[[\"WEIGHT_OZ\", \"ZONE\", \"CLIENT_RATES\"]], left_on=['BILLED_WEIGHT_OZ', 'ZONE_FedEx'], right_on=['WEIGHT_OZ', 'ZONE'], how='left')\n",
    "tbl_PLD_original_SPST1 = tbl_PLD_original_SPST1.drop(['WEIGHT_OZ', 'ZONE'], axis=1)\n",
    "tbl_PLD_original_SPST1 = tbl_PLD_original_SPST1.rename(columns={'CLIENT_RATES': 'FRT_SPST1'})\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 455,
   "id": "346ce771-830a-4782-890f-9defb486bd94",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#add Residential Surcharge column to PLD\n",
    "tbl_PLD_original_SPST1['RES_SPST1'] = tbl_PLD_original_SPST1.apply(lambda row: 0 if row['RESIDENTIAL_FLAG'] == True else 0, axis=1)\n",
    "\n",
    "#add DAS column to PLD, Pay attention to the flags under Residential Flag\n",
    "tbl_PLD_original_SPST1['DAS_SPST1'] = tbl_PLD_original_SPST1.apply(lambda row: \n",
    "                                                            DAS_Comm_Ground_FedEx if (row['DAS_CATEGORY_FedEx'] == 'DASFDX' and row['RESIDENTIAL_FLAG'] is False) else \n",
    "                                                           (DAS_Comm_Extended_Ground_FedEx if (row['DAS_CATEGORY_FedEx'] == 'DASEFDX' and row['RESIDENTIAL_FLAG'] is False) else\n",
    "                                                           (DAS_Resi_Ground_FedEx if (row['DAS_CATEGORY_FedEx'] == 'DASFDX' and row['RESIDENTIAL_FLAG'] is True) else \n",
    "                                                           (DAS_Resi_Extended_Ground_FedEx if (row['DAS_CATEGORY_FedEx'] == 'DASEFDX' and row['RESIDENTIAL_FLAG'] is True) else \n",
    "                                                           (Remote_Area_FedEx if (row['DAS_CATEGORY_FedEx'] == 'RAFDX') else \n",
    "                                                           (Remote_Area_Alaska_FedEx if (row['DAS_CATEGORY_FedEx'] == 'AKFDX') else\n",
    "                                                           (Remote_Area_Hawaii_FedEx if (row['DAS_CATEGORY_FedEx'] == 'HIFDX') else 0)))))), axis=1)\n",
    "\n",
    "\n",
    "#add fuel surcharge % column to PLD\n",
    "tbl_PLD_original_SPST1['SHIP_DATE'] = pd.to_datetime(tbl_PLD_original_SPST1['SHIP_DATE']).dt.date\n",
    "# get unique values from column 'SHIP_DATE' and create a new dataframe\n",
    "new_df = pd.DataFrame({'SHIP_DATE': tbl_PLD_original_SPST1['SHIP_DATE'].unique()})\n",
    "# use this new dataframe to merge to FSC\n",
    "df_merge = new_df.merge(FedEx_FSC, how='cross')\n",
    "\n",
    "\n",
    "#slice the merged dataframe to make sure ship_date is between the ship_data_start and end. Now we have a FSC file with Ship_date col to merge back to pld so that the pld can get FSC.\n",
    "df_merge['SHIP_DATE'] = pd.to_datetime(df_merge['SHIP_DATE']).dt.date\n",
    "df_merge = df_merge.query('SHIP_DATE >= SHIP_DATE_START and SHIP_DATE <= SHIP_DATE_END')\n",
    "df_merge = df_merge.drop_duplicates()\n",
    "# Get Domestic Air column as FSC. \n",
    "tbl_PLD_original_SPST1 = tbl_PLD_original_SPST1.merge(df_merge, on=['SHIP_DATE'], how='left').drop(['SHIP_DATE_START', 'SHIP_DATE_END', 'Domestic Air FedEx'], axis=1)\n",
    "tbl_PLD_original_SPST1.rename(columns={'Ground FedEx': 'FSC%_SPST1'}, inplace=True)\n",
    "\n",
    "\n",
    "#add AHS column to PLD\n",
    "# Define the conditions and their corresponding values\n",
    "def calculate_ahs1(row):\n",
    "    if any([row['GIRTH_AND_L'] > 105, row['L'] > 48, row['W'] > 30]):\n",
    "        if row['ZONE_FedEx'] == '2':\n",
    "            return AH_Dim_2_FedEx\n",
    "        elif row['ZONE_FedEx'] in ('3','4'):\n",
    "            return AH_Dim_3_4_FedEx\n",
    "        elif row['ZONE_FedEx'] in ('5','6'):\n",
    "            return AH_Dim_5_6_FedEx\n",
    "        elif row['ZONE_FedEx'] in ('7','8','9','10','11','12','17','26','9','10','17','99'):\n",
    "            return AH_Dim_7_up_FedEx\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def calculate_ahs2(row):\n",
    "    if row['ACTUAL_WEIGHT'] > 50:\n",
    "        if row['ZONE_FedEx'] == '2':\n",
    "            return AH_WGT_2_FedEx\n",
    "        elif row['ZONE_FedEx'] in ('3','4'):\n",
    "            return AH_WGT_3_4_FedEx\n",
    "        elif row['ZONE_FedEx'] in ('5','6'):\n",
    "            return AH_WGT_5_6_FedEx\n",
    "        elif row['ZONE_FedEx'] in ('7','8','9','10','11','12','17','26','9','10','17','99'):\n",
    "            return AH_WGT_7_up_FedEx\n",
    "\n",
    "    else:\n",
    "        return 0\n",
    "        \n",
    "# Apply the function to the DataFrame\n",
    "tbl_PLD_original_SPST1['AHS_SPST1_1'] = tbl_PLD_original_SPST1.apply(calculate_ahs1, axis=1)\n",
    "tbl_PLD_original_SPST1['AHS_SPST1_2'] = tbl_PLD_original_SPST1.apply(calculate_ahs2, axis=1)\n",
    "tbl_PLD_original_SPST1['AHS_SPST1'] = tbl_PLD_original_SPST1['AHS_SPST1_1'] + tbl_PLD_original_SPST1['AHS_SPST1_2']\n",
    "tbl_PLD_original_SPST1 = tbl_PLD_original_SPST1.drop(['AHS_SPST1_1','AHS_SPST1_2'], axis=1)\n",
    "\n",
    "\n",
    "#Get Total rate. Set total rate to null if the freight rate is null and/or some package parameters are out of carrier handling capability.  \n",
    "tbl_PLD_original_SPST1['TOTAL_SPST1'] = round(tbl_PLD_original_SPST1.apply(lambda row: \n",
    "                                                                  (1 + row['FSC%_SPST1']*(1-BarrettDisc_Fsc_FedEx)) * (row['FRT_SPST1'] + row['RES_SPST1'] + row['DAS_SPST1'] + row['AHS_SPST1']) if not pd.isna(row['FRT_SPST1']) and row['FRT_SPST1'] != '' else np.nan, axis=1),2)\n",
    "#tbl_PLD_original_SPST1.to_csv('test.csv')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sorted-tobacco",
   "metadata": {},
   "source": [
    "# SRPT (UPS Surepost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 589,
   "id": "c395a83b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get the right client rate and the corresponding PLD\n",
    "#get client rate and a copy of PLD\n",
    "tbl_clientRates_10 = tbl_clientRates[(tbl_clientRates['SERVICE_CODE'] == 'SRPT')]\n",
    "tbl_PLD_original_SRPT = tbl_BP.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 590,
   "id": "quarterly-taiwan",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Rerate start\n",
    "#add billed weight column to PLD\n",
    "tbl_PLD_original_SRPT['BILLED_WEIGHT_LB'] = tbl_PLD_original_SRPT.apply(lambda row: max(row['ACTUAL_WEIGHT_LB'], np.ceil(row['CUBIC_INCH']/dim_factor_ups_srpt)) if (row['CUBIC_INCH'] >= 3456) else\n",
    "                                                                          (row['ACTUAL_WEIGHT_LB']), axis=1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#add Freight column to PLD\n",
    "tbl_PLD_original_SRPT['ZONE_UPS'] = tbl_PLD_original_SRPT['ZONE_UPS'].astype(str)\n",
    "tbl_PLD_original_SRPT = tbl_PLD_original_SRPT.merge(tbl_clientRates_10[[\"WEIGHT_LB\", \"ZONE\", \"CLIENT_RATES\"]], left_on=['BILLED_WEIGHT_LB', 'ZONE_UPS'], right_on=['WEIGHT_LB', 'ZONE'], how='left')\n",
    "tbl_PLD_original_SRPT = tbl_PLD_original_SRPT.drop('WEIGHT_LB', axis=1)\n",
    "tbl_PLD_original_SRPT = tbl_PLD_original_SRPT.rename(columns={'CLIENT_RATES': 'FRT_SRPT'})\n",
    "   \n",
    "\n",
    "#add Residential Surcharge column to PLD\n",
    "tbl_PLD_original_SRPT['RES_SRPT'] = tbl_PLD_original_SRPT.apply(lambda row: 0 if row['RESIDENTIAL_FLAG'] == True else 0, axis=1)\n",
    "\n",
    "#add DAS column to PLD, Pay attention to the flags under Residential Flag\n",
    "tbl_PLD_original_SRPT['DAS_SRPT'] = tbl_PLD_original_SRPT.apply(lambda row: \n",
    "                                                            DAS_SurePost if (row['DAS_CATEGORY_UPS'] == 'DASUPS') else \n",
    "                                                           (DAS_Extended_SurePost if (row['DAS_CATEGORY_UPS'] == 'DASEUPS') else \n",
    "                                                           (Remote_Area if (row['DAS_CATEGORY_UPS'] == 'RAUPS') else \n",
    "                                                           (Remote_Area_Alaska if (row['DAS_CATEGORY_UPS'] == 'AKUPS') else\n",
    "                                                           (Remote_Area_Hawaii if (row['DAS_CATEGORY_UPS'] == 'HIUPS') else 0)))), axis=1)\n",
    "\n",
    "\n",
    "#add fuel surcharge % column to PLD\n",
    "tbl_PLD_original_SRPT['SHIP_DATE'] = pd.to_datetime(tbl_PLD_original_SRPT['SHIP_DATE']).dt.date\n",
    "# get unique values from column 'SHIP_DATE' and create a new dataframe\n",
    "new_df = pd.DataFrame({'SHIP_DATE': tbl_PLD_original_SRPT['SHIP_DATE'].unique()})\n",
    "# use this new dataframe to merge to FSC\n",
    "df_merge = new_df.merge(UPS_FSC, how='cross')\n",
    "df_merge['SHIP_DATE'] = pd.to_datetime(df_merge['SHIP_DATE']).dt.date\n",
    "#slice the merged dataframe to make sure ship_date is between the ship_data_start and end. Now we have a FSC file with Ship_date col to merge back to pld so that the pld can get FSC.\n",
    "df_merge = df_merge.query('SHIP_DATE >= SHIP_DATE_START and SHIP_DATE <= SHIP_DATE_END')\n",
    "df_merge = df_merge.drop_duplicates()\n",
    "# Get Ground column as FSC. \n",
    "tbl_PLD_original_SRPT = tbl_PLD_original_SRPT.merge(df_merge, on=['SHIP_DATE'], how='left').drop(['SHIP_DATE_START', 'SHIP_DATE_END', 'Domestic Air'], axis=1)\n",
    "tbl_PLD_original_SRPT.rename(columns={'Ground': 'FSC%_SRPT'}, inplace=True)\n",
    "\n",
    "\n",
    "\n",
    "#add AHS column to PLD\n",
    "tbl_PLD_original_SRPT['ZONE_UPS'] = pd.to_numeric(tbl_PLD_original_SRPT['ZONE_UPS'], errors='coerce').fillna(0).astype(int)\n",
    "\n",
    "tbl_PLD_original_SRPT['AHS_1'] = tbl_PLD_original_SRPT.apply(lambda row: \n",
    "                                                           AH_Girth_2_UPS if (row['GIRTH_AND_L'] > 105 and row['ZONE_UPS'] == 2) else \n",
    "                                                           (AH_Girth_3_4_UPS if (row['GIRTH_AND_L'] > 105 and row['ZONE_UPS'] > 2 and row['ZONE_UPS'] < 5) else\n",
    "                                                           (AH_Girth_5_up_UPS if (row['GIRTH_AND_L'] > 105 and row['ZONE_UPS'] > 4) else 0)), axis=1)\n",
    "                                                    \n",
    "tbl_PLD_original_SRPT['AHS_2'] = tbl_PLD_original_SRPT.apply(lambda row: \n",
    "                                                           AH_L_2_UPS if (row['L'] > 48 and row['ZONE_UPS'] == 2) else \n",
    "                                                           (AH_L_3_4_UPS if (row['L'] > 48 and row['ZONE_UPS'] > 2 and row['ZONE_UPS'] < 5) else \n",
    "                                                           (AH_L_5_up_UPS if (row['L'] > 48 and row['ZONE_UPS'] > 4) else 0)), axis=1)\n",
    "\n",
    "tbl_PLD_original_SRPT['AHS_3'] = tbl_PLD_original_SRPT.apply(lambda row:\n",
    "                                                           AH_W_2_UPS if (row['W'] > 30 and row['ZONE_UPS'] == 2) else \n",
    "                                                           (AH_W_3_4_UPS if (row['W'] > 30 and row['ZONE_UPS'] > 2 and row['ZONE_UPS'] < 5) else \n",
    "                                                           (AH_W_5_up_UPS if (row['W'] > 30 and row['ZONE_UPS'] > 4) else 0)), axis=1)\n",
    "                                        \n",
    "tbl_PLD_original_SRPT['AHS_4'] = tbl_PLD_original_SRPT.apply(lambda row:\n",
    "                                                           AH_WGT_2_UPS if (row['ACTUAL_WEIGHT'] > 50 and row['ZONE_UPS'] == 2) else \n",
    "                                                           (AH_WGT_3_4_UPS if (row['ACTUAL_WEIGHT'] > 50 and row['ZONE_UPS'] > 2 and row['ZONE_UPS'] < 5) else \n",
    "                                                           (AH_WGT_5_up_UPS if (row['ACTUAL_WEIGHT'] > 50 and row['ZONE_UPS'] > 4) else 0)), axis=1)\n",
    "\n",
    "tbl_PLD_original_SRPT['AHS_SRPT'] = tbl_PLD_original_SRPT['AHS_1'] + tbl_PLD_original_SRPT['AHS_2'] + tbl_PLD_original_SRPT['AHS_3'] + tbl_PLD_original_SRPT['AHS_4']\n",
    "tbl_PLD_original_SRPT = tbl_PLD_original_SRPT.drop(['AHS_1','AHS_2','AHS_3','AHS_4'], axis=1)\n",
    "\n",
    "#Get Total rate. Set total rate to null if the freight rate is null and/or some package parameters are out of carrier handling capability.  \n",
    "tbl_PLD_original_SRPT['TOTAL_SRPT'] = round(tbl_PLD_original_SRPT.apply(lambda row: \n",
    "                                                                  (1 + row['FSC%_SRPT']*(1-BarrettDisc_Fsc_UPS)) * (row['FRT_SRPT'] + row['RES_SRPT'] + row['DAS_SRPT'] + row['AHS_SRPT']) if not pd.isna(row['FRT_SRPT']) and row['FRT_SRPT'] != '' else np.nan, axis=1),2)\n",
    "#tbl_PLD_original_SRPT.to_csv('test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3031471-2d1a-42e0-a299-5c956853bd34",
   "metadata": {},
   "source": [
    "# SPST (FedEx Deferred Ground)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 518,
   "id": "00fe5b83-7869-4cb1-92aa-6f606087a692",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get the right client rate and the corresponding PLD\n",
    "tbl_clientRates_10 = tbl_clientRates[(tbl_clientRates['SERVICE_CODE'] == 'SPST')]\n",
    "tbl_PLD_original_SPST = tbl_BP.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 519,
   "id": "3ca68635-808e-47b9-a78e-930681b58a80",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Rerate start\n",
    "#add billed weight column to PLD\n",
    "tbl_PLD_original_SPST['BILLED_WEIGHT_LB'] = tbl_PLD_original_SPST.apply(lambda row: max(row['ACTUAL_WEIGHT_LB'], np.ceil(row['CUBIC_INCH']/dim_factor_fedex_spst)) if (row['CUBIC_INCH'] < 3456) else \n",
    "                                                                                    max(row['ACTUAL_WEIGHT_LB'], np.ceil(row['CUBIC_INCH']/dim_factor_fedex)), axis=1)\n",
    "\n",
    "#add Freight column to PLD\n",
    "tbl_PLD_original_SPST['ZONE_FedEx'] = tbl_PLD_original_SPST['ZONE_FedEx'].astype(str)\n",
    "tbl_PLD_original_SPST = tbl_PLD_original_SPST.merge(tbl_clientRates_10[[\"WEIGHT_LB\", \"ZONE\", \"CLIENT_RATES\"]], left_on=['BILLED_WEIGHT_LB', 'ZONE_FedEx'], right_on=['WEIGHT_LB', 'ZONE'], how='left')\n",
    "tbl_PLD_original_SPST = tbl_PLD_original_SPST.drop(['WEIGHT_LB', 'ZONE'], axis=1)\n",
    "tbl_PLD_original_SPST = tbl_PLD_original_SPST.rename(columns={'CLIENT_RATES': 'FRT_SPST'})\n",
    "\n",
    "\n",
    "#add Residential Surcharge column to PLD\n",
    "tbl_PLD_original_SPST['RES_SPST'] = tbl_PLD_original_SPST.apply(lambda row: 0 if row['RESIDENTIAL_FLAG'] is True else 0, axis=1)\n",
    "\n",
    "\n",
    "#add DAS column to PLD, Pay attention to the flags under Residential Flag\n",
    "tbl_PLD_original_SPST['DAS_SPST'] = tbl_PLD_original_SPST.apply(lambda row: \n",
    "                                                            DAS_Comm_Ground_FedEx if (row['DAS_CATEGORY_FedEx'] == 'DASFDX' and row['RESIDENTIAL_FLAG'] is False) else \n",
    "                                                           (DAS_Comm_Extended_Ground_FedEx if (row['DAS_CATEGORY_FedEx'] == 'DASEFDX' and row['RESIDENTIAL_FLAG'] is False) else\n",
    "                                                           (DAS_Resi_Ground_FedEx if (row['DAS_CATEGORY_FedEx'] == 'DASFDX' and row['RESIDENTIAL_FLAG'] is True) else \n",
    "                                                           (DAS_Resi_Extended_Ground_FedEx if (row['DAS_CATEGORY_FedEx'] == 'DASEFDX' and row['RESIDENTIAL_FLAG'] is True) else \n",
    "                                                           (Remote_Area_FedEx if (row['DAS_CATEGORY_FedEx'] == 'RAFDX') else \n",
    "                                                           (Remote_Area_Alaska_FedEx if (row['DAS_CATEGORY_FedEx'] == 'AKFDX') else\n",
    "                                                           (Remote_Area_Hawaii_FedEx if (row['DAS_CATEGORY_FedEx'] == 'HIFDX') else 0)))))), axis=1)\n",
    "\n",
    "\n",
    "#add fuel surcharge % column to PLD\n",
    "tbl_PLD_original_SPST['SHIP_DATE'] = pd.to_datetime(tbl_PLD_original_SPST['SHIP_DATE']).dt.date\n",
    "# get unique values from column 'SHIP_DATE' and create a new dataframe\n",
    "new_df = pd.DataFrame({'SHIP_DATE': tbl_PLD_original_SPST['SHIP_DATE'].unique()})\n",
    "# use this new dataframe to merge to FSC\n",
    "df_merge = new_df.merge(FedEx_FSC, how='cross')\n",
    "\n",
    "\n",
    "#slice the merged dataframe to make sure ship_date is between the ship_data_start and end. Now we have a FSC file with Ship_date col to merge back to pld so that the pld can get FSC.\n",
    "df_merge['SHIP_DATE'] = pd.to_datetime(df_merge['SHIP_DATE']).dt.date\n",
    "df_merge = df_merge.query('SHIP_DATE >= SHIP_DATE_START and SHIP_DATE <= SHIP_DATE_END')\n",
    "df_merge = df_merge.drop_duplicates()\n",
    "# Get Domestic Air column as FSC. \n",
    "tbl_PLD_original_SPST = tbl_PLD_original_SPST.merge(df_merge, on=['SHIP_DATE'], how='left').drop(['SHIP_DATE_START', 'SHIP_DATE_END', 'Domestic Air FedEx'], axis=1)\n",
    "tbl_PLD_original_SPST.rename(columns={'Ground FedEx': 'FSC%_SPST'}, inplace=True)\n",
    "\n",
    "\n",
    "#add AHS column to PLD\n",
    "# Define the conditions and their corresponding values\n",
    "def calculate_ahs1(row):\n",
    "    if any([row['GIRTH_AND_L'] > 105, row['L'] > 48, row['W'] > 30]):\n",
    "        if row['ZONE_FedEx'] == '2':\n",
    "            return AH_Dim_2_FedEx\n",
    "        elif row['ZONE_FedEx'] in ('3','4'):\n",
    "            return AH_Dim_3_4_FedEx\n",
    "        elif row['ZONE_FedEx'] in ('5','6'):\n",
    "            return AH_Dim_5_6_FedEx\n",
    "        elif row['ZONE_FedEx'] in ('7','8','9','10','11','12','17','26','9','10','17','99'):\n",
    "            return AH_Dim_7_up_FedEx\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def calculate_ahs2(row):\n",
    "    if row['ACTUAL_WEIGHT'] > 50:\n",
    "        if row['ZONE_FedEx'] == '2':\n",
    "            return AH_WGT_2_FedEx\n",
    "        elif row['ZONE_FedEx'] in ('3','4'):\n",
    "            return AH_WGT_3_4_FedEx\n",
    "        elif row['ZONE_FedEx'] in ('5','6'):\n",
    "            return AH_WGT_5_6_FedEx\n",
    "        elif row['ZONE_FedEx'] in ('7','8','9','10','11','12','17','26','9','10','17','99'):\n",
    "            return AH_WGT_7_up_FedEx\n",
    "\n",
    "    else:\n",
    "        return 0\n",
    "        \n",
    "# Apply the function to the DataFrame\n",
    "tbl_PLD_original_SPST['AHS_SPST_1'] = tbl_PLD_original_SPST.apply(calculate_ahs1, axis=1)\n",
    "tbl_PLD_original_SPST['AHS_SPST_2'] = tbl_PLD_original_SPST.apply(calculate_ahs2, axis=1)\n",
    "tbl_PLD_original_SPST['AHS_SPST'] = tbl_PLD_original_SPST['AHS_SPST_1'] + tbl_PLD_original_SPST['AHS_SPST_2']\n",
    "tbl_PLD_original_SPST = tbl_PLD_original_SPST.drop(['AHS_SPST_1','AHS_SPST_2'], axis=1)\n",
    "\n",
    "\n",
    "#Get Total rate. Set total rate to null if the freight rate is null and/or some package parameters are out of carrier handling capability.  \n",
    "tbl_PLD_original_SPST['TOTAL_SPST'] = round(tbl_PLD_original_SPST.apply(lambda row: \n",
    "                                                                  (1 + row['FSC%_SPST']*(1-BarrettDisc_Fsc_FedEx)) * (row['FRT_SPST'] + row['RES_SPST'] + row['DAS_SPST'] + row['AHS_SPST']) if not pd.isna(row['FRT_SPST']) and row['FRT_SPST'] != '' else np.nan, axis=1),2)\n",
    "#tbl_PLD_original_SPST.to_csv('test.csv')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "arbitrary-petersburg",
   "metadata": {},
   "source": [
    "# DHLG (DHL SmartMail Parcel Plus Ground)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58aa8d45",
   "metadata": {},
   "source": [
    "5114358: Franklin (FRA) -> 02038_Outbound  \n",
    "5122444: Somerset, no DHLEM (JER) -> 08873_Outbound  \n",
    "5122723: 8150 Nail Olive Branch (MS2) -> 38654_Outbound  \n",
    "5122739: Nail rd Olive branch (MS3) -> 38654_Outbound  \n",
    "5122855: Garland TX (TX1) -> 75041_Outbound  \n",
    "5122890: Oofos -> is/will be in Byhalia, MS)  \n",
    "5122893: Memphis (TN2, TN3) -> 38141_Outbound  \n",
    "5123280: (MAN)  \n",
    "5123282: Bridgewater (MA5)  \n",
    "5123283: Curtis Bay (GBM) -> 21226_Outbound  \n",
    "5123556: Montebello -> 90640\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d56e1e3",
   "metadata": {},
   "source": [
    "# DHLG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 384,
   "id": "8b0416f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#select the rate to use, #Decide the part of PLD to rerate\n",
    "tbl_clientRates_11 = tbl_clientRates[(tbl_clientRates['SERVICE_CODE'] == 'DHLG_5122893')]\n",
    "tbl_PLD_original_DHLG = tbl_BP.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 385,
   "id": "48414e56",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Start to rerate\n",
    "#add billed weight column to PLD\n",
    "tbl_PLD_original_DHLG['BILLED_WEIGHT_LB'] = tbl_PLD_original_DHLG.apply(lambda row: max(row['ACTUAL_WEIGHT_LB'], np.ceil(row['CUBIC_INCH']/dim_factor_dhl)) if (row['CUBIC_INCH'] >= 1728 and row['ACTUAL_WEIGHT'] >= 1) else\n",
    "                                                                          (row['ACTUAL_WEIGHT_LB']), axis=1)\n",
    "\n",
    "#add Freight column to PLD\n",
    "tbl_PLD_original_DHLG['ZONE'] = tbl_PLD_original_DHLG['ZONE'].astype(str)\n",
    "tbl_PLD_original_DHLG = tbl_PLD_original_DHLG.merge(tbl_clientRates_11[[\"WEIGHT_LB\", \"ZONE\", \"CLIENT_RATES\"]], left_on=['BILLED_WEIGHT_LB', 'ZONE'], right_on=['WEIGHT_LB', 'ZONE'], how='left')\n",
    "tbl_PLD_original_DHLG = tbl_PLD_original_DHLG.drop('WEIGHT_LB', axis=1)\n",
    "tbl_PLD_original_DHLG = tbl_PLD_original_DHLG.rename(columns={'CLIENT_RATES': 'FRT_DHLG'})\n",
    "\n",
    "\n",
    "#add fuel surcharge % column to PLD\n",
    "tbl_PLD_original_DHLG['SHIP_DATE'] = pd.to_datetime(tbl_PLD_original_DHLG['SHIP_DATE']).dt.date\n",
    "# get unique values from column 'SHIP_DATE' and create a new dataframe\n",
    "new_df = pd.DataFrame({'SHIP_DATE': tbl_PLD_original_DHLG['SHIP_DATE'].unique()})\n",
    "# use this new dataframe to merge to FSC\n",
    "df_merge = new_df.merge(DHL_FSC, how='cross')\n",
    "df_merge['SHIP_DATE'] = pd.to_datetime(df_merge['SHIP_DATE']).dt.date\n",
    "#slice the merged dataframe to make sure ship_date is between the ship_data_start and end. Now we have a FSC file with Ship_date col to merge back to pld so that the pld can get FSC.\n",
    "df_merge = df_merge.query('SHIP_DATE >= SHIP_DATE_START and SHIP_DATE <= SHIP_DATE_END')\n",
    "df_merge = df_merge.drop_duplicates()\n",
    "\n",
    "\n",
    "\n",
    "# Get Domestic Air column as FSC. \n",
    "tbl_PLD_original_DHLG = tbl_PLD_original_DHLG.merge(df_merge, on=['SHIP_DATE'], how='left').drop(['SHIP_DATE_START', 'SHIP_DATE_END'], axis=1)\n",
    "tbl_PLD_original_DHLG.rename(columns={'FSC': 'FSC%_DHLG'}, inplace=True)\n",
    "tbl_PLD_original_DHLG['FSC_DHLG'] = tbl_PLD_original_DHLG.apply(lambda row: row['ACTUAL_WEIGHT']*row['FSC%_DHLG'] if (row['ACTUAL_WEIGHT'] <= 1) else\n",
    "                                                                          (row['BILLED_WEIGHT_LB']*row['FSC%_DHLG']), axis=1)\n",
    "\n",
    "\n",
    "#add NQD Surcharge column to PLD\n",
    "tbl_PLD_original_DHLG['SHIP_DATE'] = pd.to_datetime(tbl_PLD_original_DHLG['SHIP_DATE'])\n",
    "\n",
    "# billed weight calculation situations: \n",
    "# situation 1: actual weight <= 1 lb, use np.ceil(row['BILLED_WEIGHT_OZ']/16) or below, use 1 lb\n",
    "# situation 2: actual weight > 1 lb, and cubic inch < 1728, use round up (actual weight)\n",
    "# situation 3: actual weight > 1 lb, and cubic inch >= 1728, use billed weight\n",
    "\n",
    "#dim\n",
    "tbl_PLD_original_DHLG['NQD_DHLG_1'] = tbl_PLD_original_DHLG.apply(lambda row: \n",
    "                                                            2.5 * row['BILLED_WEIGHT_LB'] if (row['ACTUAL_WEIGHT'] >= 1 and row['CUBIC_INCH'] >= 1728 and row['SHIP_DATE'] >= nqd_dhl_date_3 and row['SHIP_DATE'] <= nqd_dhl_date_4) else \n",
    "                                                           (2 * row['BILLED_WEIGHT_LB'] if (row['ACTUAL_WEIGHT'] >= 1 and row['CUBIC_INCH'] >= 1728 and row['SHIP_DATE'] >= nqd_dhl_date_1 and row['SHIP_DATE'] <= nqd_dhl_date_2) else \n",
    "                                                           0), axis=1)\n",
    "\n",
    "#girth + L(As > 84 is unaccepted, needs a special character to mark it, however, we cannot add up this)\n",
    "tbl_PLD_original_DHLG['NQD_DHLG_2'] = tbl_PLD_original_DHLG.apply(lambda row:\n",
    "                                                           np.nan if (row['GIRTH_AND_L'] > 84) else (0), axis=1)\n",
    "\n",
    "\n",
    "#longest side_1\n",
    "tbl_PLD_original_DHLG['NQD_DHLG_3'] = tbl_PLD_original_DHLG.apply(lambda row: \n",
    "                                                            2.5 * row['BILLED_WEIGHT_LB'] if (max(row['L'], row['W'], row['H']) > 27 and row['SHIP_DATE'] >= nqd_dhl_date_3 and row['SHIP_DATE'] <= nqd_dhl_date_4) else \n",
    "                                                           (2 * row['BILLED_WEIGHT_LB'] if (max(row['L'], row['W'], row['H']) > 27 and row['SHIP_DATE'] >= nqd_dhl_date_1 and row['SHIP_DATE'] <= nqd_dhl_date_2) else\n",
    "                                                           0), axis=1)\n",
    "\n",
    "#longest side_2\n",
    "tbl_PLD_original_DHLG['NQD_DHLG_4'] = tbl_PLD_original_DHLG.apply(lambda row:\n",
    "                                                           4.5 if (max(row['L'], row['W'], row['H']) > 22 and max(row['L'], row['W'], row['H']) <= 30) else\n",
    "                                                           (15.5 if (max(row['L'], row['W'], row['H']) > 30) else \n",
    "                                                           0), axis=1)\n",
    "#volume\n",
    "tbl_PLD_original_DHLG['NQD_DHLG_5'] = tbl_PLD_original_DHLG.apply(lambda row: \n",
    "                                                            15.5 if (row['CUBIC_INCH'] > 3456) else \n",
    "                                                           (0), axis=1)\n",
    "\n",
    "\n",
    "tbl_PLD_original_DHLG['NQD_DHLG_OTHER'] = tbl_PLD_original_DHLG['NQD_DHLG_1'] + tbl_PLD_original_DHLG['NQD_DHLG_3'] + tbl_PLD_original_DHLG['NQD_DHLG_4'] + tbl_PLD_original_DHLG['NQD_DHLG_5']\n",
    "tbl_PLD_original_DHLG['NQD_DHLG_GIRTH_AND_L'] = tbl_PLD_original_DHLG['NQD_DHLG_2']\n",
    "tbl_PLD_original_DHLG = tbl_PLD_original_DHLG.drop(['NQD_DHLG_1','NQD_DHLG_2','NQD_DHLG_3','NQD_DHLG_4','NQD_DHLG_5'], axis=1)\n",
    "\n",
    "\n",
    "#Get Total\n",
    "tbl_PLD_original_DHLG['TOTAL_DHLG'] = round(tbl_PLD_original_DHLG.apply(lambda row: \n",
    "                                                                  np.nan if ((row['FRT_DHLG'] == '') | (pd.isna(row['FRT_DHLG'])) | (row['NQD_DHLG_GIRTH_AND_L'] == '') | (pd.isna(row['NQD_DHLG_GIRTH_AND_L'] == '')))\n",
    "                                                                  else\n",
    "                                                                  (row['FRT_DHLG'] + (row['FSC_DHLG'] + row['NQD_DHLG_OTHER'] + row['NQD_DHLG_GIRTH_AND_L'])/(1-Margin_on_DHL)), axis=1),2)\n",
    "\n",
    "\n",
    "#tbl_PLD_original_DHLG.to_csv('test.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aging-anchor",
   "metadata": {},
   "source": [
    "# DHLG<1 (DHL SmartMail Parcel Ground)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a9cd1ae",
   "metadata": {},
   "source": [
    "5114358: Franklin (FRA) -> 02038_Outbound  \n",
    "5122444: Somerset, no DHLEM (JER) -> 08873_Outbound  \n",
    "5122723: 8150 Nail Olive Branch (MS2) -> 38654_Outbound  \n",
    "5122739: Nail rd Olive branch (MS3) -> 38654_Outbound  \n",
    "5122855: Garland TX (TX1) -> 75041_Outbound  \n",
    "5122890: Oofos -> is/will be in Byhalia, MS)  \n",
    "5122893: Memphis (TN2, TN3) -> 38141_Outbound  \n",
    "5123280: (MAN)  \n",
    "5123282: Bridgewater (MA5)  \n",
    "5123283: Curtis Bay (GBM) -> 21226_Outbound"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9da1e82",
   "metadata": {},
   "source": [
    "# DHLG<1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 460,
   "id": "bd8910ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "#select the rate to use, #Decide the part of PLD to rerate\n",
    "tbl_clientRates_12 = tbl_clientRates[(tbl_clientRates['SERVICE_CODE'] == 'DHLG<1_5122739')]\n",
    "tbl_PLD_original_DHLG1 = tbl_BP[(tbl_BP['ACTUAL_WEIGHT'] < 1)].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 461,
   "id": "detailed-kazakhstan",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Start to rerate\n",
    "#add billed weight column to PLD\n",
    "tbl_PLD_original_DHLG1['BILLED_WEIGHT_OZ'] = tbl_PLD_original_DHLG1.apply(lambda row: max(row['ACTUAL_WEIGHT_LB'], np.ceil(row['CUBIC_INCH']/dim_factor_dhl))*16 if (row['CUBIC_INCH'] >= 1728 and row['ACTUAL_WEIGHT'] >= 1) else\n",
    "                                                                          (row['ACTUAL_WEIGHT_OZ']), axis=1)\n",
    "\n",
    "\n",
    "#add Freight column to PLD\n",
    "tbl_PLD_original_DHLG1['ZONE'] = tbl_PLD_original_DHLG1['ZONE'].astype(str)\n",
    "tbl_PLD_original_DHLG1 = tbl_PLD_original_DHLG1.merge(tbl_clientRates_12[[\"WEIGHT_OZ\", \"ZONE\", \"CLIENT_RATES\"]], left_on=['BILLED_WEIGHT_OZ', 'ZONE'], right_on=['WEIGHT_OZ', 'ZONE'], how='left')\n",
    "tbl_PLD_original_DHLG1 = tbl_PLD_original_DHLG1.drop('WEIGHT_OZ', axis=1)\n",
    "tbl_PLD_original_DHLG1 = tbl_PLD_original_DHLG1.rename(columns={'CLIENT_RATES': 'FRT_DHLG1'})\n",
    "\n",
    "#add fuel surcharge % column to PLD\n",
    "tbl_PLD_original_DHLG1['SHIP_DATE'] = pd.to_datetime(tbl_PLD_original_DHLG1['SHIP_DATE']).dt.date\n",
    "# get unique values from column 'SHIP_DATE' and create a new dataframe\n",
    "# when actual weight < 1, use actual weight non round up to get FSC, otherwise, use billed weight to get FSC\n",
    "new_df = pd.DataFrame({'SHIP_DATE': tbl_PLD_original_DHLG1['SHIP_DATE'].unique()})\n",
    "# use this new dataframe to merge to FSC\n",
    "df_merge = new_df.merge(DHL_FSC, how='cross')\n",
    "df_merge['SHIP_DATE'] = pd.to_datetime(df_merge['SHIP_DATE']).dt.date\n",
    "#slice the merged dataframe to make sure ship_date is between the ship_data_start and end. Now we have a FSC file with Ship_date col to merge back to pld so that the pld can get FSC.\n",
    "df_merge = df_merge.query('SHIP_DATE >= SHIP_DATE_START and SHIP_DATE <= SHIP_DATE_END')\n",
    "df_merge = df_merge.drop_duplicates()\n",
    "# Get Domestic Air column as FSC. \n",
    "tbl_PLD_original_DHLG1 = tbl_PLD_original_DHLG1.merge(df_merge, on=['SHIP_DATE'], how='left').drop(['SHIP_DATE_START', 'SHIP_DATE_END'], axis=1)\n",
    "tbl_PLD_original_DHLG1.rename(columns={'FSC': 'FSC%_DHLG1'}, inplace=True)\n",
    "tbl_PLD_original_DHLG1['FSC_DHLG1'] = tbl_PLD_original_DHLG1.apply(lambda row: row['ACTUAL_WEIGHT']*row['FSC%_DHLG1'], axis=1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#add NQD Surcharge column to PLD\n",
    "tbl_PLD_original_DHLG1['SHIP_DATE'] = pd.to_datetime(tbl_PLD_original_DHLG1['SHIP_DATE'])\n",
    "\n",
    "# billed weight calculation situations: \n",
    "# situation 1: actual weight <= 1 lb, use np.ceil(row['BILLED_WEIGHT_OZ']/16) or below, use 1 lb\n",
    "# situation 2: actual weight > 1 lb, and cubic inch < 1728, use round up (actual weight)\n",
    "# situation 3: actual weight > 1 lb, and cubic inch >= 1728, use billed weight\n",
    "\n",
    "#dim, billed weight is used to determine NQD, there is no <1 lb here\n",
    "tbl_PLD_original_DHLG1['NQD_DHLG1_1'] = tbl_PLD_original_DHLG1.apply(lambda row: \n",
    "                                                            2.5 * max(row['ACTUAL_WEIGHT_LB'], np.ceil(row['CUBIC_INCH']/dim_factor_dhl)) if (row['ACTUAL_WEIGHT'] >= 1 and row['CUBIC_INCH'] >= 1728 and row['SHIP_DATE'] >= nqd_dhl_date_3 and row['SHIP_DATE'] <= nqd_dhl_date_4) else \n",
    "                                                           (2 * max(row['ACTUAL_WEIGHT_LB'], np.ceil(row['CUBIC_INCH']/dim_factor_dhl)) if (row['ACTUAL_WEIGHT'] >= 1 and row['CUBIC_INCH'] >= 1728 and row['SHIP_DATE'] >= nqd_dhl_date_1 and row['SHIP_DATE'] <= nqd_dhl_date_2) else \n",
    "                                                           0), axis=1)\n",
    "\n",
    "\n",
    "#girth + L(As > 84 is unaccepted, needs a special character to mark it, however, we cannot add up this, so we make it null)\n",
    "#1 over 1lbs, under lbs\n",
    "#and those < 1 lb will use lb to determine NQD\n",
    "# situation 1: actual weight <= 1 lb, use np.ceil(row['BILLED_WEIGHT_OZ']/16)\n",
    "# situation 2: actual weight > 1 lb, and cubic inch < 1728, use actual weight\n",
    "# situation 3: actual weight > 1 lb, and cubic inch >= 1728, use billed weight\n",
    "\n",
    "tbl_PLD_original_DHLG1['NQD_DHLG1_2'] = tbl_PLD_original_DHLG1.apply(lambda row:\n",
    "                                                            np.nan if (row['GIRTH_AND_L'] > 84) else (0), axis=1)\n",
    "\n",
    "\n",
    "#longest side_1\n",
    "tbl_PLD_original_DHLG1['NQD_DHLG1_3'] = tbl_PLD_original_DHLG1.apply(lambda row: \n",
    "                                                            2.5 * row['ACTUAL_WEIGHT_LB'] if (max(row['L'], row['W'], row['H']) > 27 and row['SHIP_DATE'] >= nqd_dhl_date_3 and row['SHIP_DATE'] <= nqd_dhl_date_4) else \n",
    "                                                           (2 * row['ACTUAL_WEIGHT_LB'] if (max(row['L'], row['W'], row['H']) > 27 and row['SHIP_DATE'] >= nqd_dhl_date_1 and row['SHIP_DATE'] <= nqd_dhl_date_2) else\n",
    "                                                           0), axis=1)\n",
    "\n",
    "#longest side_2\n",
    "tbl_PLD_original_DHLG1['NQD_DHLG1_4'] = tbl_PLD_original_DHLG1.apply(lambda row:\n",
    "                                                           4.5 if (max(row['L'], row['W'], row['H']) > 22 and max(row['L'], row['W'], row['H']) <= 30) else\n",
    "                                                           (15.5 if (max(row['L'], row['W'], row['H']) > 30) else \n",
    "                                                           0), axis=1)\n",
    "#volume\n",
    "tbl_PLD_original_DHLG1['NQD_DHLG1_5'] = tbl_PLD_original_DHLG1.apply(lambda row: \n",
    "                                                            15.5 if (row['CUBIC_INCH'] > 3456) else \n",
    "                                                           (0), axis=1)\n",
    "\n",
    "\n",
    "tbl_PLD_original_DHLG1['NQD_DHLG1_OTHER'] = tbl_PLD_original_DHLG1['NQD_DHLG1_1'] + tbl_PLD_original_DHLG1['NQD_DHLG1_3'] + tbl_PLD_original_DHLG1['NQD_DHLG1_4'] + tbl_PLD_original_DHLG1['NQD_DHLG1_5']\n",
    "tbl_PLD_original_DHLG1['NQD_DHLG1_GIRTH_AND_L'] = tbl_PLD_original_DHLG1['NQD_DHLG1_2']\n",
    "tbl_PLD_original_DHLG1 = tbl_PLD_original_DHLG1.drop(['NQD_DHLG1_1','NQD_DHLG1_2','NQD_DHLG1_3','NQD_DHLG1_4','NQD_DHLG1_5'], axis=1)\n",
    "\n",
    "\n",
    "#Get Total\n",
    "tbl_PLD_original_DHLG1['TOTAL_DHLG1'] = round(tbl_PLD_original_DHLG1.apply(lambda row: \n",
    "                                                                  np.nan if ((row['FRT_DHLG1'] == '') | (pd.isna(row['FRT_DHLG1'])) | (row['NQD_DHLG1_GIRTH_AND_L'] == '') | (pd.isna(row['NQD_DHLG1_GIRTH_AND_L'] == '')))\n",
    "                                                                  else\n",
    "                                                                  (row['FRT_DHLG1'] + (row['FSC_DHLG1'] + row['NQD_DHLG1_OTHER'] + row['NQD_DHLG1_GIRTH_AND_L'])/(1-Margin_on_DHL)), axis=1),2)\n",
    "\n",
    "#tbl_PLD_original_DHLG1.to_csv('test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "coastal-discipline",
   "metadata": {},
   "source": [
    "# DHLE (DHL SmartMail Parcel Plus Expedited)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcdefa52",
   "metadata": {},
   "source": [
    "5114358: Franklin (FRA) -> 02038_Outbound  \n",
    "5122444: Somerset, no DHLEM (JER) -> 08873_Outbound  \n",
    "5122723: 8150 Nail Olive Branch (MS2) -> 38654_Outbound  \n",
    "5122739: Nail rd Olive branch (MS3) -> 38654_Outbound  \n",
    "5122855: Garland TX (TX1) -> 75041_Outbound  \n",
    "5122890: Oofos -> is/will be in Byhalia, MS)  \n",
    "5122893: Memphis (TN2, TN3) -> 38141_Outbound  \n",
    "5123280: (MAN)  \n",
    "5123282: Bridgewater (MA5)  \n",
    "5123283: Curtis Bay (GBM) -> 21226_Outbound"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "71f5d336",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Select the rate to use, #Decide the part of PLD to rerate\n",
    "#get client rate and a copy of PLD\n",
    "tbl_clientRates_13 = tbl_clientRates[(tbl_clientRates['SERVICE_CODE'] == 'DHLE_5122893')]\n",
    "tbl_PLD_original_DHLE = tbl_BP.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "antique-opera",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Start to rerate\n",
    "#add billed weight column to PLD\n",
    "tbl_PLD_original_DHLE['BILLED_WEIGHT_LB'] = tbl_PLD_original_DHLE.apply(lambda row: max(row['ACTUAL_WEIGHT_LB'], np.ceil(row['CUBIC_INCH']/dim_factor_dhl)) if (row['CUBIC_INCH'] >= 1728 and row['ACTUAL_WEIGHT'] >= 1) else\n",
    "                                                                          (row['ACTUAL_WEIGHT_LB']), axis=1)\n",
    "\n",
    "#add Freight column to PLD\n",
    "tbl_PLD_original_DHLE['ZONE'] = tbl_PLD_original_DHLE['ZONE'].astype(str)\n",
    "tbl_PLD_original_DHLE = tbl_PLD_original_DHLE.merge(tbl_clientRates_13[[\"WEIGHT_LB\", \"ZONE\", \"CLIENT_RATES\"]], left_on=['BILLED_WEIGHT_LB', 'ZONE'], right_on=['WEIGHT_LB', 'ZONE'], how='left')\n",
    "tbl_PLD_original_DHLE = tbl_PLD_original_DHLE.drop('WEIGHT_LB', axis=1)\n",
    "tbl_PLD_original_DHLE = tbl_PLD_original_DHLE.rename(columns={'CLIENT_RATES': 'FRT_DHLE'})\n",
    "\n",
    "\n",
    "\n",
    "#add fuel surcharge % column to PLD\n",
    "tbl_PLD_original_DHLE['SHIP_DATE'] = pd.to_datetime(tbl_PLD_original_DHLE['SHIP_DATE']).dt.date\n",
    "# get unique values from column 'SHIP_DATE' and create a new dataframe\n",
    "new_df = pd.DataFrame({'SHIP_DATE': tbl_PLD_original_DHLE['SHIP_DATE'].unique()})\n",
    "# use this new dataframe to merge to FSC\n",
    "df_merge = new_df.merge(DHL_FSC, how='cross')\n",
    "df_merge['SHIP_DATE'] = pd.to_datetime(df_merge['SHIP_DATE']).dt.date\n",
    "#slice the merged dataframe to make sure ship_date is between the ship_data_start and end. Now we have a FSC file with Ship_date col to merge back to pld so that the pld can get FSC.\n",
    "df_merge = df_merge.query('SHIP_DATE >= SHIP_DATE_START and SHIP_DATE <= SHIP_DATE_END')\n",
    "df_merge = df_merge.drop_duplicates()\n",
    "# Get Domestic Air column as FSC. \n",
    "tbl_PLD_original_DHLE = tbl_PLD_original_DHLE.merge(df_merge, on=['SHIP_DATE'], how='left').drop(['SHIP_DATE_START', 'SHIP_DATE_END'], axis=1)\n",
    "tbl_PLD_original_DHLE.rename(columns={'FSC': 'FSC%_DHLE'}, inplace=True)\n",
    "tbl_PLD_original_DHLE['FSC_DHLE'] = tbl_PLD_original_DHLE.apply(lambda row: row['ACTUAL_WEIGHT']*row['FSC%_DHLE'] if (row['ACTUAL_WEIGHT'] <= 1) else\n",
    "                                                                          (row['BILLED_WEIGHT_LB']*row['FSC%_DHLE']), axis=1)\n",
    "\n",
    "\n",
    "#add NQD Surcharge column to PLD\n",
    "tbl_PLD_original_DHLE['SHIP_DATE'] = pd.to_datetime(tbl_PLD_original_DHLE['SHIP_DATE'])\n",
    "\n",
    "# billed weight calculation situations: \n",
    "# situation 1: actual weight <= 1 lb, use np.ceil(row['BILLED_WEIGHT_OZ']/16) or below, use 1 lb\n",
    "# situation 2: actual weight > 1 lb, and cubic inch < 1728, use round up (actual weight)\n",
    "# situation 3: actual weight > 1 lb, and cubic inch >= 1728, use billed weight\n",
    "\n",
    "#dim\n",
    "tbl_PLD_original_DHLE['NQD_DHLE_1'] = tbl_PLD_original_DHLE.apply(lambda row: \n",
    "                                                            2.5 * row['BILLED_WEIGHT_LB'] if (row['ACTUAL_WEIGHT'] >= 1 and row['CUBIC_INCH'] >= 1728 and row['SHIP_DATE'] >= nqd_dhl_date_3 and row['SHIP_DATE'] <= nqd_dhl_date_4) else \n",
    "                                                           (2 * row['BILLED_WEIGHT_LB'] if (row['ACTUAL_WEIGHT'] >= 1 and row['CUBIC_INCH'] >= 1728 and row['SHIP_DATE'] >= nqd_dhl_date_1 and row['SHIP_DATE'] <= nqd_dhl_date_2) else \n",
    "                                                           0), axis=1)\n",
    "\n",
    "\n",
    "#girth + L(As > 84 is unaccepted, needs a special character to mark it, however, we cannot add up this)\n",
    "tbl_PLD_original_DHLE['NQD_DHLE_2'] = tbl_PLD_original_DHLE.apply(lambda row:\n",
    "                                                            np.nan if (row['GIRTH_AND_L'] > 84) else (0), axis=1)\n",
    "\n",
    "\n",
    "#longest side_1\n",
    "tbl_PLD_original_DHLE['NQD_DHLE_3'] = tbl_PLD_original_DHLE.apply(lambda row: \n",
    "                                                            2.5 * row['BILLED_WEIGHT_LB'] if (max(row['L'], row['W'], row['H']) > 27 and row['SHIP_DATE'] >= nqd_dhl_date_3 and row['SHIP_DATE'] <= nqd_dhl_date_4) else \n",
    "                                                           (2 * row['BILLED_WEIGHT_LB'] if (max(row['L'], row['W'], row['H']) > 27 and row['SHIP_DATE'] >= nqd_dhl_date_1 and row['SHIP_DATE'] <= nqd_dhl_date_2) else\n",
    "                                                           0), axis=1)\n",
    "\n",
    "#longest side_2\n",
    "tbl_PLD_original_DHLE['NQD_DHLE_4'] = tbl_PLD_original_DHLE.apply(lambda row:\n",
    "                                                           4.5 if (max(row['L'], row['W'], row['H']) > 22 and max(row['L'], row['W'], row['H']) <= 30) else\n",
    "                                                           (15.5 if (max(row['L'], row['W'], row['H']) > 30) else \n",
    "                                                           0), axis=1)\n",
    "#volume\n",
    "tbl_PLD_original_DHLE['NQD_DHLE_5'] = tbl_PLD_original_DHLE.apply(lambda row: \n",
    "                                                            15.5 if (row['CUBIC_INCH'] > 3456) else \n",
    "                                                           (0), axis=1)\n",
    "\n",
    "\n",
    "tbl_PLD_original_DHLE['NQD_DHLE_OTHER'] = tbl_PLD_original_DHLE['NQD_DHLE_1'] + tbl_PLD_original_DHLE['NQD_DHLE_3'] + tbl_PLD_original_DHLE['NQD_DHLE_4'] + tbl_PLD_original_DHLE['NQD_DHLE_5']\n",
    "tbl_PLD_original_DHLE['NQD_DHLE_GIRTH_AND_L'] = tbl_PLD_original_DHLE['NQD_DHLE_2']\n",
    "tbl_PLD_original_DHLE = tbl_PLD_original_DHLE.drop(['NQD_DHLE_1','NQD_DHLE_2','NQD_DHLE_3','NQD_DHLE_4','NQD_DHLE_5'], axis=1)\n",
    "\n",
    "\n",
    "#Get Total\n",
    "# note that for those with non-round up actual weight, if it is < 1, we still assign it the freight rate. \n",
    "\n",
    "tbl_PLD_original_DHLE['TOTAL_DHLE'] = round(tbl_PLD_original_DHLE.apply(lambda row: \n",
    "                                                                  np.nan if ((row['FRT_DHLE'] == '') | (pd.isna(row['FRT_DHLE'])) | (row['NQD_DHLE_GIRTH_AND_L'] == '') | (pd.isna(row['NQD_DHLE_GIRTH_AND_L'] == '')))\n",
    "                                                                  else\n",
    "                                                                  (row['FRT_DHLE'] + (row['FSC_DHLE'] + row['NQD_DHLE_OTHER'] + row['NQD_DHLE_GIRTH_AND_L'])/(1-Margin_on_DHL)), axis=1),2)\n",
    "\n",
    "#tbl_PLD_original_DHLE.to_csv('test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "5cdd78d3-703a-49a9-b29f-b5f2e88c5f52",
   "metadata": {},
   "outputs": [],
   "source": [
    "tbl_PLD_original_DHLE.to_excel('test.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cordless-yukon",
   "metadata": {},
   "source": [
    "# DHLE<1 (DHL SmartMail Parcel Expedited)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bac591ed",
   "metadata": {},
   "source": [
    "5114358: Franklin (FRA) -> 02038_Outbound  \n",
    "5122444: Somerset, no DHLEM (JER) -> 08873_Outbound  \n",
    "5122723: 8150 Nail Olive Branch (MS2) -> 38654_Outbound  \n",
    "5122739: Nail rd Olive branch (MS3) -> 38654_Outbound  \n",
    "5122855: Garland TX (TX1) -> 75041_Outbound  \n",
    "5122890: Oofos -> is/will be in Byhalia, MS)  \n",
    "5122893: Memphis (TN2, TN3) -> 38141_Outbound  \n",
    "5123280: (MAN)  \n",
    "5123282: Bridgewater (MA5)  \n",
    "5123283: Curtis Bay (GBM) -> 21226_Outbound"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "735d9575",
   "metadata": {},
   "source": [
    "# DHLE<1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "a7ace654",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Select the rate to use, #Decide the part of PLD to rerate\n",
    "#get client rate and a copy of PLD\n",
    "tbl_clientRates_14 = tbl_clientRates[(tbl_clientRates['SERVICE_CODE'] == 'DHLE<1_5122893')]\n",
    "tbl_PLD_original_DHLE1 = tbl_BP[(tbl_BP['ACTUAL_WEIGHT'] < 1)].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "corresponding-monroe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Start to rerate\n",
    "#add billed weight column to PLD\n",
    "tbl_PLD_original_DHLE1['BILLED_WEIGHT_OZ'] = tbl_PLD_original_DHLE1.apply(lambda row: max(row['ACTUAL_WEIGHT_LB'], np.ceil(row['CUBIC_INCH']/dim_factor_dhl))*16 if (row['CUBIC_INCH'] >= 1728 and row['ACTUAL_WEIGHT'] >= 1) else\n",
    "                                                                          (row['ACTUAL_WEIGHT_OZ']), axis=1)\n",
    "\n",
    "\n",
    "#add Freight column to PLD\n",
    "tbl_PLD_original_DHLE1['ZONE'] = tbl_PLD_original_DHLE1['ZONE'].astype(str)\n",
    "tbl_PLD_original_DHLE1 = tbl_PLD_original_DHLE1.merge(tbl_clientRates_14[[\"WEIGHT_OZ\", \"ZONE\", \"CLIENT_RATES\"]], left_on=['BILLED_WEIGHT_OZ', 'ZONE'], right_on=['WEIGHT_OZ', 'ZONE'], how='left')\n",
    "tbl_PLD_original_DHLE1 = tbl_PLD_original_DHLE1.drop('WEIGHT_OZ', axis=1)\n",
    "tbl_PLD_original_DHLE1 = tbl_PLD_original_DHLE1.rename(columns={'CLIENT_RATES': 'FRT_DHLE1'})\n",
    "\n",
    "\n",
    "#add fuel surcharge % column to PLD\n",
    "tbl_PLD_original_DHLE1['SHIP_DATE'] = pd.to_datetime(tbl_PLD_original_DHLE1['SHIP_DATE']).dt.date\n",
    "# get unique values from column 'SHIP_DATE' and create a new dataframe\n",
    "# when actual weight < 1, use actual weight non round up to get FSC, otherwise, use billed weight to get FSC\n",
    "new_df = pd.DataFrame({'SHIP_DATE': tbl_PLD_original_DHLE1['SHIP_DATE'].unique()})\n",
    "# use this new dataframe to merge to FSC\n",
    "df_merge = new_df.merge(DHL_FSC, how='cross')\n",
    "df_merge['SHIP_DATE'] = pd.to_datetime(df_merge['SHIP_DATE']).dt.date\n",
    "#slice the merged dataframe to make sure ship_date is between the ship_data_start and end. Now we have a FSC file with Ship_date col to merge back to pld so that the pld can get FSC.\n",
    "df_merge = df_merge.query('SHIP_DATE >= SHIP_DATE_START and SHIP_DATE <= SHIP_DATE_END')\n",
    "df_merge = df_merge.drop_duplicates()\n",
    "# Get Domestic Air column as FSC. \n",
    "tbl_PLD_original_DHLE1 = tbl_PLD_original_DHLE1.merge(df_merge, on=['SHIP_DATE'], how='left').drop(['SHIP_DATE_START', 'SHIP_DATE_END'], axis=1)\n",
    "tbl_PLD_original_DHLE1.rename(columns={'FSC': 'FSC%_DHLE1'}, inplace=True)\n",
    "tbl_PLD_original_DHLE1['FSC_DHLE1'] = tbl_PLD_original_DHLE1.apply(lambda row: row['ACTUAL_WEIGHT']*row['FSC%_DHLE1'], axis=1)\n",
    "\n",
    "\n",
    "#add NQD Surcharge column to PLD\n",
    "tbl_PLD_original_DHLE1['SHIP_DATE'] = pd.to_datetime(tbl_PLD_original_DHLE1['SHIP_DATE'])\n",
    "\n",
    "# billed weight calculation situations: \n",
    "# situation 1: actual weight <= 1 lb, use np.ceil(row['BILLED_WEIGHT_OZ']/16) or below, use 1 lb\n",
    "# situation 2: actual weight > 1 lb, and cubic inch < 1728, use round up (actual weight)\n",
    "# situation 3: actual weight > 1 lb, and cubic inch >= 1728, use billed weight\n",
    "\n",
    "#dim, billed weight is used to determine NQD, there is no <1 lb here\n",
    "tbl_PLD_original_DHLE1['NQD_DHLE1_1'] = tbl_PLD_original_DHLE1.apply(lambda row: \n",
    "                                                            2.5 * max(row['ACTUAL_WEIGHT_LB'], np.ceil(row['CUBIC_INCH']/dim_factor_dhl)) if (row['ACTUAL_WEIGHT'] >= 1 and row['CUBIC_INCH'] >= 1728 and row['SHIP_DATE'] >= nqd_dhl_date_3 and row['SHIP_DATE'] <= nqd_dhl_date_4) else \n",
    "                                                           (2 * max(row['ACTUAL_WEIGHT_LB'], np.ceil(row['CUBIC_INCH']/dim_factor_dhl)) if (row['ACTUAL_WEIGHT'] >= 1 and row['CUBIC_INCH'] >= 1728 and row['SHIP_DATE'] >= nqd_dhl_date_1 and row['SHIP_DATE'] <= nqd_dhl_date_2) else \n",
    "                                                           0), axis=1)\n",
    "\n",
    "\n",
    "#girth + L(As > 84 is unaccepted, needs a special character to mark it, however, we cannot add up this, so we make it null)\n",
    "#1 over 1lbs, under lbs\n",
    "#and those < 1 lb will use lb to determine NQD\n",
    "# situation 1: actual weight <= 1 lb, use np.ceil(row['BILLED_WEIGHT_OZ']/16)\n",
    "# situation 2: actual weight > 1 lb, and cubic inch < 1728, use actual weight\n",
    "# situation 3: actual weight > 1 lb, and cubic inch >= 1728, use billed weight\n",
    "\n",
    "tbl_PLD_original_DHLE1['NQD_DHLE1_2'] = tbl_PLD_original_DHLE1.apply(lambda row:\n",
    "                                                            np.nan if (row['GIRTH_AND_L'] > 84) else (0), axis=1)\n",
    "\n",
    "#longest side_1\n",
    "tbl_PLD_original_DHLE1['NQD_DHLE1_3'] = tbl_PLD_original_DHLE1.apply(lambda row: \n",
    "                                                            2.5 * row['ACTUAL_WEIGHT_LB'] if (max(row['L'], row['W'], row['H']) > 27 and row['SHIP_DATE'] >= nqd_dhl_date_3 and row['SHIP_DATE'] <= nqd_dhl_date_4) else \n",
    "                                                           (2 * row['ACTUAL_WEIGHT_LB'] if (max(row['L'], row['W'], row['H']) > 27 and row['SHIP_DATE'] >= nqd_dhl_date_1 and row['SHIP_DATE'] <= nqd_dhl_date_2) else\n",
    "                                                           0), axis=1)\n",
    "\n",
    "#longest side_2\n",
    "tbl_PLD_original_DHLE1['NQD_DHLE1_4'] = tbl_PLD_original_DHLE1.apply(lambda row:\n",
    "                                                           4.5 if (max(row['L'], row['W'], row['H']) > 22 and max(row['L'], row['W'], row['H']) <= 30) else\n",
    "                                                           (15.5 if (max(row['L'], row['W'], row['H']) > 30) else \n",
    "                                                           0), axis=1)\n",
    "#volume\n",
    "tbl_PLD_original_DHLE1['NQD_DHLE1_5'] = tbl_PLD_original_DHLE1.apply(lambda row: \n",
    "                                                            15.5 if (row['CUBIC_INCH'] > 3456) else \n",
    "                                                           (0), axis=1)\n",
    "\n",
    "\n",
    "tbl_PLD_original_DHLE1['NQD_DHLE1_OTHER'] = tbl_PLD_original_DHLE1['NQD_DHLE1_1'] + tbl_PLD_original_DHLE1['NQD_DHLE1_3'] + tbl_PLD_original_DHLE1['NQD_DHLE1_4'] + tbl_PLD_original_DHLE1['NQD_DHLE1_5']\n",
    "tbl_PLD_original_DHLE1['NQD_DHLE1_GIRTH_AND_L'] = tbl_PLD_original_DHLE1['NQD_DHLE1_2']\n",
    "tbl_PLD_original_DHLE1 = tbl_PLD_original_DHLE1.drop(['NQD_DHLE1_1','NQD_DHLE1_2','NQD_DHLE1_3','NQD_DHLE1_4','NQD_DHLE1_5'], axis=1)\n",
    "\n",
    "\n",
    "#Get Total\n",
    "tbl_PLD_original_DHLE1['TOTAL_DHLE1'] = round(tbl_PLD_original_DHLE1.apply(lambda row: \n",
    "                                                                  np.nan if ((row['FRT_DHLE1'] == '') | (pd.isna(row['FRT_DHLE1'])) | (row['NQD_DHLE1_GIRTH_AND_L'] == '') | (pd.isna(row['NQD_DHLE1_GIRTH_AND_L'] == '')))\n",
    "                                                                  else\n",
    "                                                                  (row['FRT_DHLE1'] + (row['FSC_DHLE1'] + row['NQD_DHLE1_OTHER'] + row['NQD_DHLE1_GIRTH_AND_L'])/(1-Margin_on_DHL)), axis=1),2)\n",
    "\n",
    "#tbl_PLD_original_DHLE1.to_csv('test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "0c14efc1-abec-404a-81b6-20eb456bbdaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "tbl_PLD_original_DHLE1.to_excel('test.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "numeric-radical",
   "metadata": {},
   "source": [
    "# DHLEM (DHL SmartMail Parcel Expedited Max)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad7deb40",
   "metadata": {},
   "source": [
    "5114358: Franklin (FRA) -> 02038_Outbound  \n",
    "5122444: Somerset, no DHLEM (JER) -> 08873_Outbound  \n",
    "5122723: 8150 Nail Olive Branch (MS2) -> 38654_Outbound  \n",
    "5122739: Nail rd Olive branch (MS3) -> 38654_Outbound  \n",
    "5122855: Garland TX (TX1) -> 75041_Outbound  \n",
    "5122890: Oofos -> is/will be in Byhalia, MS)  \n",
    "5122893: Memphis (TN2, TN3) -> 38141_Outbound  \n",
    "5123280: (MAN)  \n",
    "5123282: Bridgewater (MA5)  \n",
    "5123283: Curtis Bay (GBM) -> 21226_Outbound"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b56c6cac",
   "metadata": {},
   "source": [
    "# DHLEM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "02ef5014",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Select the rate to use, #Decide the part of PLD to rerate\n",
    "#get client rate and a copy of PLD\n",
    "tbl_clientRates_15 = tbl_clientRates[(tbl_clientRates['SERVICE_CODE'] == 'DHLEM_5122893')]\n",
    "tbl_PLD_original_DHLEM1 = tbl_BP[(tbl_BP['ACTUAL_WEIGHT'] <= 1)].copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "employed-silly",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Start to rerate\n",
    "#add billed weight column to PLD\n",
    "tbl_PLD_original_DHLEM1['BILLED_WEIGHT_OZ'] = tbl_PLD_original_DHLEM1.apply(lambda row: max(row['ACTUAL_WEIGHT_LB'], np.ceil(row['CUBIC_INCH']/dim_factor_dhl))*16 if (row['CUBIC_INCH'] >= 1728 and row['ACTUAL_WEIGHT'] >= 1) else\n",
    "                                                                          (row['ACTUAL_WEIGHT_OZ']), axis=1)\n",
    "\n",
    "#add Freight column to PLD\n",
    "tbl_PLD_original_DHLEM1['ZONE'] = tbl_PLD_original_DHLEM1['ZONE'].astype(str)\n",
    "tbl_PLD_original_DHLEM1 = tbl_PLD_original_DHLEM1.merge(tbl_clientRates_15[[\"WEIGHT_OZ\", \"ZONE\", \"CLIENT_RATES\"]], left_on=['BILLED_WEIGHT_OZ', 'ZONE'], right_on=['WEIGHT_OZ', 'ZONE'], how='left')\n",
    "tbl_PLD_original_DHLEM1 = tbl_PLD_original_DHLEM1.drop('WEIGHT_OZ', axis=1)\n",
    "tbl_PLD_original_DHLEM1 = tbl_PLD_original_DHLEM1.rename(columns={'CLIENT_RATES': 'FRT_DHLEM'})\n",
    "\n",
    "\n",
    "#add fuel surcharge % column to PLD\n",
    "tbl_PLD_original_DHLEM1['SHIP_DATE'] = pd.to_datetime(tbl_PLD_original_DHLEM1['SHIP_DATE']).dt.date\n",
    "# get unique values from column 'SHIP_DATE' and create a new dataframe\n",
    "# when actual weight < 1, use actual weight non round up to get FSC, otherwise, use billed weight to get FSC\n",
    "new_df = pd.DataFrame({'SHIP_DATE': tbl_PLD_original_DHLEM1['SHIP_DATE'].unique()})\n",
    "# use this new dataframe to merge to FSC\n",
    "df_merge = new_df.merge(DHL_FSC, how='cross')\n",
    "df_merge['SHIP_DATE'] = pd.to_datetime(df_merge['SHIP_DATE']).dt.date\n",
    "#slice the merged dataframe to make sure ship_date is between the ship_data_start and end. Now we have a FSC file with Ship_date col to merge back to pld so that the pld can get FSC.\n",
    "df_merge = df_merge.query('SHIP_DATE >= SHIP_DATE_START and SHIP_DATE <= SHIP_DATE_END')\n",
    "df_merge = df_merge.drop_duplicates()\n",
    "# Get Domestic Air column as FSC. \n",
    "tbl_PLD_original_DHLEM1 = tbl_PLD_original_DHLEM1.merge(df_merge, on=['SHIP_DATE'], how='left').drop(['SHIP_DATE_START', 'SHIP_DATE_END'], axis=1)\n",
    "tbl_PLD_original_DHLEM1.rename(columns={'FSC': 'FSC%_DHLEM'}, inplace=True)\n",
    "tbl_PLD_original_DHLEM1['FSC_DHLEM'] = tbl_PLD_original_DHLEM1.apply(lambda row: row['ACTUAL_WEIGHT']*row['FSC%_DHLEM'], axis=1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#add NQD Surcharge column to PLD\n",
    "tbl_PLD_original_DHLEM1['SHIP_DATE'] = pd.to_datetime(tbl_PLD_original_DHLEM1['SHIP_DATE'])\n",
    "\n",
    "# billed weight calculation situations: \n",
    "# situation 1: actual weight <= 1 lb, use np.ceil(row['BILLED_WEIGHT_OZ']/16) or below, use 1 lb\n",
    "# situation 2: actual weight > 1 lb, and cubic inch < 1728, use round up (actual weight)\n",
    "# situation 3: actual weight > 1 lb, and cubic inch >= 1728, use billed weight\n",
    "\n",
    "#dim, billed weight is used to determine NQD, there is no <1 lb here\n",
    "tbl_PLD_original_DHLEM1['NQD_DHLEM_1'] = tbl_PLD_original_DHLEM1.apply(lambda row: \n",
    "                                                            2.5 * max(row['ACTUAL_WEIGHT_LB'], np.ceil(row['CUBIC_INCH']/dim_factor_dhl)) if (row['ACTUAL_WEIGHT'] >= 1 and row['CUBIC_INCH'] >= 1728 and row['SHIP_DATE'] >= nqd_dhl_date_3 and row['SHIP_DATE'] <= nqd_dhl_date_4) else \n",
    "                                                           (2 * max(row['ACTUAL_WEIGHT_LB'], np.ceil(row['CUBIC_INCH']/dim_factor_dhl)) if (row['ACTUAL_WEIGHT'] >= 1 and row['CUBIC_INCH'] >= 1728 and row['SHIP_DATE'] >= nqd_dhl_date_1 and row['SHIP_DATE'] <= nqd_dhl_date_2) else \n",
    "                                                           0), axis=1)\n",
    "\n",
    "\n",
    "#girth + L(As > 84 is unaccepted, needs a special character to mark it, however, we cannot add up this, so we make it null)\n",
    "#1 over 1lbs, under lbs\n",
    "#and those < 1 lb will use lb to determine NQD\n",
    "# situation 1: actual weight <= 1 lb, use np.ceil(row['BILLED_WEIGHT_OZ']/16)\n",
    "# situation 2: actual weight > 1 lb, and cubic inch < 1728, use actual weight\n",
    "# situation 3: actual weight > 1 lb, and cubic inch >= 1728, use billed weight\n",
    "\n",
    "tbl_PLD_original_DHLEM1['NQD_DHLEM_2'] = tbl_PLD_original_DHLEM1.apply(lambda row:\n",
    "                                                            np.nan if (row['GIRTH_AND_L'] > 84) else (0), axis=1)\n",
    "\n",
    "\n",
    "#longest side_1\n",
    "tbl_PLD_original_DHLEM1['NQD_DHLEM_3'] = tbl_PLD_original_DHLEM1.apply(lambda row: \n",
    "                                                            2.5 * row['ACTUAL_WEIGHT_LB'] if (max(row['L'], row['W'], row['H']) > 27 and row['SHIP_DATE'] >= nqd_dhl_date_3 and row['SHIP_DATE'] <= nqd_dhl_date_4) else \n",
    "                                                           (2 * row['ACTUAL_WEIGHT_LB'] if (max(row['L'], row['W'], row['H']) > 27 and row['SHIP_DATE'] >= nqd_dhl_date_1 and row['SHIP_DATE'] <= nqd_dhl_date_2) else\n",
    "                                                           0), axis=1)\n",
    "\n",
    "#longest side_2\n",
    "tbl_PLD_original_DHLEM1['NQD_DHLEM_4'] = tbl_PLD_original_DHLEM1.apply(lambda row:\n",
    "                                                           4.5 if (max(row['L'], row['W'], row['H']) > 22 and max(row['L'], row['W'], row['H']) <= 30) else\n",
    "                                                           (15.5 if (max(row['L'], row['W'], row['H']) > 30) else \n",
    "                                                           0), axis=1)\n",
    "#volume\n",
    "tbl_PLD_original_DHLEM1['NQD_DHLEM_5'] = tbl_PLD_original_DHLEM1.apply(lambda row: \n",
    "                                                            15.5 if (row['CUBIC_INCH'] > 3456) else \n",
    "                                                           (0), axis=1)\n",
    "\n",
    "\n",
    "tbl_PLD_original_DHLEM1['NQD_DHLEM_OTHER'] = tbl_PLD_original_DHLEM1['NQD_DHLEM_1'] + tbl_PLD_original_DHLEM1['NQD_DHLEM_3'] + tbl_PLD_original_DHLEM1['NQD_DHLEM_4'] + tbl_PLD_original_DHLEM1['NQD_DHLEM_5']\n",
    "tbl_PLD_original_DHLEM1['NQD_DHLEM_GIRTH_AND_L'] = tbl_PLD_original_DHLEM1['NQD_DHLEM_2']\n",
    "tbl_PLD_original_DHLEM1 = tbl_PLD_original_DHLEM1.drop(['NQD_DHLEM_1','NQD_DHLEM_2','NQD_DHLEM_3','NQD_DHLEM_4','NQD_DHLEM_5'], axis=1)\n",
    "\n",
    "\n",
    "#Get Total\n",
    "tbl_PLD_original_DHLEM1['TOTAL_DHLEM'] = round(tbl_PLD_original_DHLEM1.apply(lambda row: \n",
    "                                                                  np.nan if ((row['FRT_DHLEM'] == '') | (pd.isna(row['FRT_DHLEM'])) | (row['NQD_DHLEM_GIRTH_AND_L'] == '') | (pd.isna(row['NQD_DHLEM_GIRTH_AND_L'] == '')))\n",
    "                                                                  else\n",
    "                                                                  (row['FRT_DHLEM'] + (row['FSC_DHLEM'] + row['NQD_DHLEM_OTHER'] + row['NQD_DHLEM_GIRTH_AND_L'])/(1-Margin_on_DHL)), axis=1),2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "314e4025",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Select the rate to use, #Decide the part of PLD to rerate\n",
    "#get client rate and a copy of PLD\n",
    "tbl_clientRates_16 = tbl_clientRates[(tbl_clientRates['SERVICE_CODE'] == 'DHLEM_5122893')]\n",
    "tbl_PLD_original_DHLEM = tbl_BP[(tbl_BP['ACTUAL_WEIGHT'] > 1)].copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "fifteen-hopkins",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Start to rerate\n",
    "#add billed weight column to PLD\n",
    "tbl_PLD_original_DHLEM['BILLED_WEIGHT_LB'] = tbl_PLD_original_DHLEM.apply(lambda row: max(row['ACTUAL_WEIGHT_LB'], np.ceil(row['CUBIC_INCH']/dim_factor_dhl)) if (row['CUBIC_INCH'] >= 1728 and row['ACTUAL_WEIGHT'] >= 1) else\n",
    "                                                                          (row['ACTUAL_WEIGHT_LB']), axis=1)\n",
    "\n",
    "#add Freight column to PLD\n",
    "tbl_PLD_original_DHLEM['ZONE'] = tbl_PLD_original_DHLEM['ZONE'].astype(str)\n",
    "tbl_PLD_original_DHLEM = tbl_PLD_original_DHLEM.merge(tbl_clientRates_16[[\"WEIGHT_LB\", \"ZONE\", \"CLIENT_RATES\"]], left_on=['BILLED_WEIGHT_LB', 'ZONE'], right_on=['WEIGHT_LB', 'ZONE'], how='left')\n",
    "tbl_PLD_original_DHLEM = tbl_PLD_original_DHLEM.drop('WEIGHT_LB', axis=1)\n",
    "tbl_PLD_original_DHLEM = tbl_PLD_original_DHLEM.rename(columns={'CLIENT_RATES': 'FRT_DHLEM'})\n",
    "\n",
    "\n",
    "\n",
    "#add fuel surcharge % column to PLD\n",
    "tbl_PLD_original_DHLEM['SHIP_DATE'] = pd.to_datetime(tbl_PLD_original_DHLEM['SHIP_DATE']).dt.date\n",
    "# get unique values from column 'SHIP_DATE' and create a new dataframe\n",
    "new_df = pd.DataFrame({'SHIP_DATE': tbl_PLD_original_DHLEM['SHIP_DATE'].unique()})\n",
    "# use this new dataframe to merge to FSC\n",
    "df_merge = new_df.merge(DHL_FSC, how='cross')\n",
    "df_merge['SHIP_DATE'] = pd.to_datetime(df_merge['SHIP_DATE']).dt.date\n",
    "#slice the merged dataframe to make sure ship_date is between the ship_data_start and end. Now we have a FSC file with Ship_date col to merge back to pld so that the pld can get FSC.\n",
    "df_merge = df_merge.query('SHIP_DATE >= SHIP_DATE_START and SHIP_DATE <= SHIP_DATE_END')\n",
    "df_merge = df_merge.drop_duplicates()\n",
    "# Get Domestic Air column as FSC. \n",
    "tbl_PLD_original_DHLEM = tbl_PLD_original_DHLEM.merge(df_merge, on=['SHIP_DATE'], how='left').drop(['SHIP_DATE_START', 'SHIP_DATE_END'], axis=1)\n",
    "tbl_PLD_original_DHLEM.rename(columns={'FSC': 'FSC%_DHLEM'}, inplace=True)\n",
    "tbl_PLD_original_DHLEM['FSC_DHLEM'] = tbl_PLD_original_DHLEM.apply(lambda row: row['ACTUAL_WEIGHT']*row['FSC%_DHLEM'] if (row['ACTUAL_WEIGHT'] <= 1) else\n",
    "                                                                          (row['BILLED_WEIGHT_LB']*row['FSC%_DHLEM']), axis=1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#add NQD Surcharge column to PLD\n",
    "tbl_PLD_original_DHLEM['SHIP_DATE'] = pd.to_datetime(tbl_PLD_original_DHLEM['SHIP_DATE'])\n",
    "\n",
    "# billed weight calculation situations: \n",
    "# situation 1: actual weight <= 1 lb, use np.ceil(row['BILLED_WEIGHT_OZ']/16) or below, use 1 lb\n",
    "# situation 2: actual weight > 1 lb, and cubic inch < 1728, use round up (actual weight)\n",
    "# situation 3: actual weight > 1 lb, and cubic inch >= 1728, use billed weight\n",
    "\n",
    "#dim\n",
    "tbl_PLD_original_DHLEM['NQD_DHLEM_1'] = tbl_PLD_original_DHLEM.apply(lambda row: \n",
    "                                                            2.5 * row['BILLED_WEIGHT_LB'] if (row['ACTUAL_WEIGHT'] >= 1 and row['CUBIC_INCH'] >= 1728 and row['SHIP_DATE'] >= nqd_dhl_date_3 and row['SHIP_DATE'] <= nqd_dhl_date_4) else \n",
    "                                                           (2 * row['BILLED_WEIGHT_LB'] if (row['ACTUAL_WEIGHT'] >= 1 and row['CUBIC_INCH'] >= 1728 and row['SHIP_DATE'] >= nqd_dhl_date_1 and row['SHIP_DATE'] <= nqd_dhl_date_2) else \n",
    "                                                           0), axis=1)\n",
    "\n",
    "\n",
    "#girth + L(As > 84 is unaccepted, needs a special character to mark it, however, we cannot add up this)\n",
    "\n",
    "tbl_PLD_original_DHLEM['NQD_DHLEM_2'] = tbl_PLD_original_DHLEM.apply(lambda row:\n",
    "                                                           np.nan if (row['GIRTH_AND_L'] > 84) else (0), axis=1)\n",
    "\n",
    "#longest side_1\n",
    "tbl_PLD_original_DHLEM['NQD_DHLEM_3'] = tbl_PLD_original_DHLEM.apply(lambda row: \n",
    "                                                            2.5 * row['BILLED_WEIGHT_LB'] if (max(row['L'], row['W'], row['H']) > 27 and row['SHIP_DATE'] >= nqd_dhl_date_3 and row['SHIP_DATE'] <= nqd_dhl_date_4) else \n",
    "                                                           (2 * row['BILLED_WEIGHT_LB'] if (max(row['L'], row['W'], row['H']) > 27 and row['SHIP_DATE'] >= nqd_dhl_date_1 and row['SHIP_DATE'] <= nqd_dhl_date_2) else\n",
    "                                                           0), axis=1)\n",
    "\n",
    "#longest side_2\n",
    "tbl_PLD_original_DHLEM['NQD_DHLEM_4'] = tbl_PLD_original_DHLEM.apply(lambda row:\n",
    "                                                           4.5 if (max(row['L'], row['W'], row['H']) > 22 and max(row['L'], row['W'], row['H']) <= 30) else\n",
    "                                                           (15.5 if (max(row['L'], row['W'], row['H']) > 30) else \n",
    "                                                           0), axis=1)\n",
    "#volume\n",
    "tbl_PLD_original_DHLEM['NQD_DHLEM_5'] = tbl_PLD_original_DHLEM.apply(lambda row: \n",
    "                                                            15.5 if (row['CUBIC_INCH'] > 3456) else \n",
    "                                                           (0), axis=1)\n",
    "\n",
    "\n",
    "tbl_PLD_original_DHLEM['NQD_DHLEM_OTHER'] = tbl_PLD_original_DHLEM['NQD_DHLEM_1'] + tbl_PLD_original_DHLEM['NQD_DHLEM_3'] + tbl_PLD_original_DHLEM['NQD_DHLEM_4'] + tbl_PLD_original_DHLEM['NQD_DHLEM_5']\n",
    "tbl_PLD_original_DHLEM['NQD_DHLEM_GIRTH_AND_L'] = tbl_PLD_original_DHLEM['NQD_DHLEM_2']\n",
    "tbl_PLD_original_DHLEM = tbl_PLD_original_DHLEM.drop(['NQD_DHLEM_1','NQD_DHLEM_2','NQD_DHLEM_3','NQD_DHLEM_4','NQD_DHLEM_5'], axis=1)\n",
    "\n",
    "\n",
    "#Get Total\n",
    "# note that for those with non-round up actual weight, if it is < 1, we still assign it the freight rate. \n",
    "\n",
    "tbl_PLD_original_DHLEM['TOTAL_DHLEM'] = round(tbl_PLD_original_DHLEM.apply(lambda row: \n",
    "                                                                  np.nan if ((row['FRT_DHLEM'] == '') | (pd.isna(row['FRT_DHLEM'])) | (row['NQD_DHLEM_GIRTH_AND_L'] == '') | (pd.isna(row['NQD_DHLEM_GIRTH_AND_L'] == '')))\n",
    "                                                                  else\n",
    "                                                                  (row['FRT_DHLEM'] + (row['FSC_DHLEM'] + row['NQD_DHLEM_OTHER'] + row['NQD_DHLEM_GIRTH_AND_L'])/(1-Margin_on_DHL)), axis=1),2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "rolled-latter",
   "metadata": {},
   "outputs": [],
   "source": [
    "tbl_PLD_original_DHLEM = pd.concat([tbl_PLD_original_DHLEM1, tbl_PLD_original_DHLEM], axis=0)\n",
    "#tbl_PLD_original_DHLEM.to_csv('test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "fdb65c5f-74e3-4006-85c0-a392a2bf033d",
   "metadata": {},
   "outputs": [],
   "source": [
    "tbl_PLD_original_DHLEM.to_excel('test.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "engaged-register",
   "metadata": {},
   "source": [
    "# USPSAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "id": "489d3def",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get the right client rate and the corresponding PLD\n",
    "#get client rate and a copy of PLD\n",
    "tbl_clientRates_17 = tbl_clientRates[(tbl_clientRates['SERVICE_CODE'] == 'USPSAG')]\n",
    "tbl_PLD_original_USPSAG = tbl_BP.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "id": "9a6934f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Rerate start\n",
    "#add billed weight column to PLD\n",
    "tbl_PLD_original_USPSAG['BILLED_WEIGHT_LB'] = tbl_PLD_original_USPSAG.apply(lambda row: max(row['ACTUAL_WEIGHT_LB'], np.ceil(row['CUBIC_INCH']/dim_factor_usps)) if (row['CUBIC_INCH'] > 1728) else\n",
    "                                                                          (row['ACTUAL_WEIGHT_LB']), axis=1)\n",
    "\n",
    "tbl_PLD_original_USPSAG['BILLED_WEIGHT_OZ'] = tbl_PLD_original_USPSAG.apply(lambda row: max(row['ACTUAL_WEIGHT_LB'], np.ceil(row['CUBIC_INCH']/dim_factor_usps))*16 if (row['CUBIC_INCH'] > 1728) else\n",
    "                                                                          (row['ACTUAL_WEIGHT_OZ']), axis=1)\n",
    "\n",
    "tbl_PLD_original_USPSAG['BILLED_WEIGHT_LB'] = tbl_PLD_original_USPSAG.apply(lambda row:\n",
    "                                                            row['BILLED_WEIGHT_OZ']/16 if (row['BILLED_WEIGHT_OZ'] <= 16) else \n",
    "                                                           (row['BILLED_WEIGHT_LB']), axis=1)\n",
    "\n",
    "#usps will only rate for those not exceeding 70 lbs\n",
    "tbl_PLD_original_USPSAG = tbl_PLD_original_USPSAG[(tbl_PLD_original_USPSAG['BILLED_WEIGHT_LB'] <= 70)]\n",
    "\n",
    "\n",
    "#add Freight column to PLD\n",
    "tbl_PLD_original_USPSAG_3 = tbl_PLD_original_USPSAG[(tbl_PLD_original_USPSAG['GIRTH_AND_L'] > 108) & (tbl_PLD_original_USPSAG['GIRTH_AND_L'] <= 130)]\n",
    "tbl_PLD_original_USPSAG_none3 = tbl_PLD_original_USPSAG[(tbl_PLD_original_USPSAG['GIRTH_AND_L'] <= 108)]\n",
    "\n",
    "#main case\n",
    "tbl_PLD_original_USPSAG_none3['ZONE'] = tbl_PLD_original_USPSAG_none3['ZONE'].astype(str)\n",
    "tbl_PLD_original_USPSAG_none3 = tbl_PLD_original_USPSAG_none3.merge(tbl_clientRates_17[[\"WEIGHT_LB\", \"ZONE\", \"CLIENT_RATES\"]], left_on=['BILLED_WEIGHT_LB', 'ZONE'], right_on=['WEIGHT_LB', 'ZONE'], how='left')\n",
    "tbl_PLD_original_USPSAG_none3 = tbl_PLD_original_USPSAG_none3.drop('WEIGHT_LB', axis=1)\n",
    "tbl_PLD_original_USPSAG_none3 = tbl_PLD_original_USPSAG_none3.rename(columns={'CLIENT_RATES': 'FRT_USPSAG'})\n",
    "\n",
    "#3. for parcels that measure in combined length and girth more than 108 inches but not more than 130 inches, use oversized prices, \n",
    "#regardless of weight, based on the applicable zone.\n",
    "\n",
    "tbl_oversize = tbl_clientRates_17[(tbl_clientRates_17['WEIGHT_LB'] == 9999)]\n",
    "tbl_PLD_original_USPSAG_3['ZONE'] = tbl_PLD_original_USPSAG_3['ZONE'].astype(str)\n",
    "tbl_PLD_original_USPSAG_3 = tbl_PLD_original_USPSAG_3.merge(tbl_oversize[[\"ZONE\", \"CLIENT_RATES\"]], left_on=['ZONE'], right_on=['ZONE'], how='left')\n",
    "tbl_PLD_original_USPSAG_3 = tbl_PLD_original_USPSAG_3.rename(columns={'CLIENT_RATES': 'FRT_USPSAG'})\n",
    "\n",
    "#combine again\n",
    "tbl_PLD_original_USPSAG = pd.concat([tbl_PLD_original_USPSAG_3, tbl_PLD_original_USPSAG_none3], axis=0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "473e2662",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'L_CI'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\lib\\site-packages\\pandas\\core\\indexes\\base.py:3802\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   3801\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 3802\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3803\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\lib\\site-packages\\pandas\\_libs\\index.pyx:138\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\lib\\site-packages\\pandas\\_libs\\index.pyx:165\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi:5745\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi:5753\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'L_CI'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[162], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# USPSAG_CI\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m# Regular Package\u001b[39;00m\n\u001b[0;32m      3\u001b[0m tbl_clientRates_21 \u001b[38;5;241m=\u001b[39m tbl_clientRates[(tbl_clientRates[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSERVICE_CODE\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mUSPSAG_CI\u001b[39m\u001b[38;5;124m'\u001b[39m)]\n\u001b[1;32m----> 4\u001b[0m tbl_PLD_original_USPSAG_RP \u001b[38;5;241m=\u001b[39m tbl_BP[(tbl_BP[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mACTUAL_WEIGHT\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m20\u001b[39m) \u001b[38;5;241m&\u001b[39m (\u001b[43mtbl_BP\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mL_CI\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m18\u001b[39m) \u001b[38;5;241m&\u001b[39m (tbl_BP[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCUBIC_INCH\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m1728\u001b[39m)]\u001b[38;5;241m.\u001b[39mcopy()\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\lib\\site-packages\\pandas\\core\\frame.py:3807\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3805\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m   3806\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[1;32m-> 3807\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3808\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[0;32m   3809\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\lib\\site-packages\\pandas\\core\\indexes\\base.py:3804\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   3802\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine\u001b[38;5;241m.\u001b[39mget_loc(casted_key)\n\u001b[0;32m   3803\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m-> 3804\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[0;32m   3805\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m   3806\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[0;32m   3807\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[0;32m   3808\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[0;32m   3809\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[1;31mKeyError\u001b[0m: 'L_CI'"
     ]
    }
   ],
   "source": [
    "# USPSAG_CI\n",
    "# Regular Package\n",
    "tbl_clientRates_21 = tbl_clientRates[(tbl_clientRates['SERVICE_CODE'] == 'USPSAG_CI')]\n",
    "tbl_PLD_original_USPSAG_RP = tbl_BP[(tbl_BP['ACTUAL_WEIGHT'] <= 20) & (tbl_BP['L_CI'] <= 18) & (tbl_BP['CUBIC_INCH'] < 1728)].copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e08767d",
   "metadata": {},
   "outputs": [],
   "source": [
    "tbl_PLD_original_USPSAG_RP['ZONE'] = tbl_PLD_original_USPSAG_RP['ZONE'].astype(str)\n",
    "#This is slow code, but clearer than cross join. Can try cross join if later too slow. \n",
    "def get_rate(zone, cubic_inch_ci):\n",
    "    # Filter df1 for the given zone\n",
    "    filtered_tbl_clientRates_21 = tbl_clientRates_21[tbl_clientRates_21['ZONE'] == zone]\n",
    "\n",
    "    # Sort and iterate to find the correct 'cubic inch' range\n",
    "    for index, row in filtered_tbl_clientRates_21.sort_values('CUBIC_FT').iterrows():\n",
    "        if cubic_inch_ci <= row['CUBIC_FT']:\n",
    "            return row['CLIENT_RATES']\n",
    "    return None  # or some default value if no range is found\n",
    "\n",
    "# Apply the function to df2 to create the new 'rate' column\n",
    "tbl_PLD_original_USPSAG_RP['FRT_USPSAG_RP'] = tbl_PLD_original_USPSAG_RP.apply(lambda row: get_rate(row['ZONE'], row['CUBIC_FT_CI']), axis=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b5587040",
   "metadata": {},
   "outputs": [],
   "source": [
    "# USPSAG_CI\n",
    "# soft Package\n",
    "tbl_clientRates_22 = tbl_clientRates[(tbl_clientRates['SERVICE_CODE'] == 'USPSAG_CI')]\n",
    "tbl_PLD_original_USPSAG_SP = tbl_BP[(tbl_BP['ACTUAL_WEIGHT'] <= 20) & (tbl_BP['L_CI'] <= 18) & (tbl_BP['CUBIC_INCH'] < 1728) & (tbl_BP['L_CI'] + tbl_BP['W_CI'] <= 36)].copy()\n",
    "tbl_PLD_original_USPSAG_SP['ZONE'] = tbl_PLD_original_USPSAG_SP['ZONE'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "8e27940a",
   "metadata": {},
   "outputs": [],
   "source": [
    "ground_softpack_tier = pd.DataFrame({\n",
    "    'Tier': ['Tier1', 'Tier2', 'Tier3', 'Tier4', 'Tier5', 'Tier6', 'Tier7', 'Tier8', 'Tier9', 'Tier10'],\n",
    "    'Min': [0, 16, 21, 24, 26, 28, 30, 32, 34, 35],\n",
    "    'Max': [16, 21, 24, 26, 28, 30, 32, 34, 35, 36] \n",
    "})\n",
    "\n",
    "tbl_PLD_original_USPSAG_SP['Tier'] = np.nan\n",
    "\n",
    "# Vectorized assignment\n",
    "for _, row in ground_softpack_tier.iterrows():\n",
    "    mask = (tbl_PLD_original_USPSAG_SP['L_AND_W_CI'] > row['Min']) & (tbl_PLD_original_USPSAG_SP['L_AND_W_CI'] <= row['Max'])\n",
    "    tbl_PLD_original_USPSAG_SP.loc[mask, 'Tier'] = row['Tier']\n",
    "\n",
    "tbl_PLD_original_USPSAG_SP = tbl_PLD_original_USPSAG_SP.merge(tbl_clientRates_22[[\"ZONE\", \"TIER\", \"CLIENT_RATES\"]], left_on=['ZONE','Tier'], right_on=['ZONE','TIER'], how='left').rename(columns={'CLIENT_RATES': 'FRT_USPSAG_SP'})\n",
    "\n",
    "tbl_PLD_original_USPSAG_SP = tbl_PLD_original_USPSAG_SP.drop(['Tier', 'TIER'], axis=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "4f7df65e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tbl_PLD_original_USPSAG = tbl_PLD_original_USPSAG.merge(tbl_PLD_original_USPSAG_RP[[\"TRACKING_NUMBER\", \"FRT_USPSAG_RP\"]], left_on=['TRACKING_NUMBER'], right_on=['TRACKING_NUMBER'], how='left').merge(tbl_PLD_original_USPSAG_SP[[\"TRACKING_NUMBER\", \"FRT_USPSAG_SP\"]], left_on=['TRACKING_NUMBER'], right_on=['TRACKING_NUMBER'], how='left')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "5816a24a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Calculate the minimum rate for each row\n",
    "tbl_PLD_original_USPSAG['FRT_USPSAG_final'] = tbl_PLD_original_USPSAG[['FRT_USPSAG', 'FRT_USPSAG_RP', 'FRT_USPSAG_SP']].min(axis=1)\n",
    "\n",
    "# Drop the original rate columns\n",
    "#tbl_PLD_original_USPSAG.drop(['FRT_USPSAG_RP', 'FRT_USPSAG_SP'], axis=1, inplace=True)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "id": "226c98eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#6. Parcels that exceed 22 inches but not greater than 30 inches in length, add $4 per package. \n",
    "#7. parcels that exceed 30 inches in length, add $7 per package. \n",
    "\n",
    "tbl_PLD_original_USPSAG['EXTRA_USPSAG_1'] = tbl_PLD_original_USPSAG.apply(lambda row:\n",
    "                                                            4 if (row['L'] > 22 and row['L'] <= 30) else \n",
    "                                                           (18 if (row['L'] > 30) else\n",
    "                                                           (0)), axis=1)\n",
    "\n",
    "\n",
    "#8. parcels that exceed 2 cubic feet (3456 cubic inches), add $15 per package. \n",
    "tbl_PLD_original_USPSAG['EXTRA_USPSAG_2'] = tbl_PLD_original_USPSAG.apply(lambda row:\n",
    "                                                            30 if (row['CUBIC_INCH'] > 3456) else (0), axis=1)\n",
    "\n",
    "#Get Total\n",
    "# note that for those with non-round up actual weight, if it is < 1, we still assign it the freight rate. \n",
    "\n",
    "#Get Total, revise the logic that if the freight rate is 0 or some rate is 0, the total will be 0. ups can handle everything. \n",
    "tbl_PLD_original_USPSAG['TOTAL_USPSAG'] = round(tbl_PLD_original_USPSAG.apply(lambda row: \n",
    "                                                                  (row['FRT_USPSAG'] + (row['EXTRA_USPSAG_1'] + row['EXTRA_USPSAG_2'])/(1-Margin_on_USPS)) if row['FRT_USPSAG'] != '' else '', axis=1),2)                         \n",
    "#tbl_PLD_original_USPSAG.to_csv('test.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vocal-astronomy",
   "metadata": {},
   "source": [
    "# USPSAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "id": "fc9dca26",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get the right client rate and the corresponding PLD\n",
    "#get client rate and a copy of PLD\n",
    "tbl_clientRates_18 = tbl_clientRates[(tbl_clientRates['SERVICE_CODE'] == 'USPSAP')]\n",
    "tbl_PLD_original_USPSAP = tbl_BP.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "id": "educated-possible",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Rerate start\n",
    "#add billed weight column to PLD\n",
    "tbl_PLD_original_USPSAP['BILLED_WEIGHT_LB'] = tbl_PLD_original_USPSAP.apply(lambda row: max(row['ACTUAL_WEIGHT_LB'], np.ceil(row['CUBIC_INCH']/dim_factor_usps)) if (row['CUBIC_INCH'] > 1728) else\n",
    "                                                                          (row['ACTUAL_WEIGHT_LB']), axis=1)\n",
    "\n",
    "tbl_PLD_original_USPSAP['BILLED_WEIGHT_OZ'] = tbl_PLD_original_USPSAP.apply(lambda row: max(row['ACTUAL_WEIGHT_LB'], np.ceil(row['CUBIC_INCH']/dim_factor_usps))*16 if (row['CUBIC_INCH'] > 1728) else\n",
    "                                                                          (row['ACTUAL_WEIGHT_OZ']), axis=1)\n",
    "\n",
    "tbl_PLD_original_USPSAP['BILLED_WEIGHT_LB'] = tbl_PLD_original_USPSAP.apply(lambda row:\n",
    "                                                            0.5 if (row['BILLED_WEIGHT_OZ'] <= 8) else\n",
    "                                                            (1 if (row['BILLED_WEIGHT_OZ'] > 8 and row['BILLED_WEIGHT_OZ'] <= 16) else             \n",
    "                                                            (row['BILLED_WEIGHT_LB'])), axis=1)\n",
    "\n",
    "#add Freight column to PLD\n",
    "tbl_PLD_original_USPSAP_3 = tbl_PLD_original_USPSAP[(tbl_PLD_original_USPSAP['GIRTH_AND_L'] > 108) & (tbl_PLD_original_USPSAP['GIRTH_AND_L'] <= 130)]\n",
    "tbl_PLD_original_USPSAP_none3 = tbl_PLD_original_USPSAP[(tbl_PLD_original_USPSAP['GIRTH_AND_L'] <= 108)]\n",
    "\n",
    "#main case\n",
    "tbl_PLD_original_USPSAP_none3['ZONE'] = tbl_PLD_original_USPSAP_none3['ZONE'].astype(str)\n",
    "tbl_PLD_original_USPSAP_none3 = tbl_PLD_original_USPSAP_none3.merge(tbl_clientRates_18[[\"WEIGHT_LB\", \"ZONE\", \"CLIENT_RATES\"]], left_on=['BILLED_WEIGHT_LB', 'ZONE'], right_on=['WEIGHT_LB', 'ZONE'], how='left')\n",
    "tbl_PLD_original_USPSAP_none3 = tbl_PLD_original_USPSAP_none3.drop('WEIGHT_LB', axis=1)\n",
    "tbl_PLD_original_USPSAP_none3 = tbl_PLD_original_USPSAP_none3.rename(columns={'CLIENT_RATES': 'FRT_USPSAP'})\n",
    "\n",
    "#3. for parcels that measure in combined length and girth more than 108 inches but not more than 130 inches, use oversized prices, \n",
    "#regardless of weight, based on the applicable zone.\n",
    "\n",
    "tbl_oversize = tbl_clientRates_18[(tbl_clientRates_18['WEIGHT_LB'] == 9999)]\n",
    "tbl_PLD_original_USPSAP_3['ZONE'] = tbl_PLD_original_USPSAP_3['ZONE'].astype(str)\n",
    "tbl_PLD_original_USPSAP_3 = tbl_PLD_original_USPSAP_3.merge(tbl_oversize[[\"ZONE\", \"CLIENT_RATES\"]], left_on=['ZONE'], right_on=['ZONE'], how='left')\n",
    "tbl_PLD_original_USPSAP_3 = tbl_PLD_original_USPSAP_3.rename(columns={'CLIENT_RATES': 'FRT_USPSAP'})\n",
    "\n",
    "#combine again\n",
    "tbl_PLD_original_USPSAP = pd.concat([tbl_PLD_original_USPSAP_3, tbl_PLD_original_USPSAP_none3], axis=0)\n",
    "\n",
    "#6. Parcels that exceed 22 inches but not greater than 30 inches in length, add $4 per package. \n",
    "#7. parcels that exceed 30 inches in length, add $7 per package. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "89f4255e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# USPSAP_CI\n",
    "# Regular Package\n",
    "tbl_clientRates_23 = tbl_clientRates[(tbl_clientRates['SERVICE_CODE'] == 'USPSAP_CI')]\n",
    "tbl_PLD_original_USPSAP_RP = tbl_BP[(tbl_BP['ACTUAL_WEIGHT'] <= 20) & (tbl_BP['L_CI'] <= 18) & (tbl_BP['CUBIC_INCH'] < 1728)].copy()\n",
    "tbl_PLD_original_USPSAP_RP['ZONE'] = tbl_PLD_original_USPSAP_RP['ZONE'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "7156d70c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This is slow code, but clearer than cross join. Can try cross join if later too slow. \n",
    "def get_rate(zone, cubic_inch_ci):\n",
    "    # Filter df1 for the given zone\n",
    "    filtered_tbl_clientRates_23 = tbl_clientRates_23[tbl_clientRates_23['ZONE'] == zone]\n",
    "\n",
    "    # Sort and iterate to find the correct 'cubic inch' range\n",
    "    for index, row in filtered_tbl_clientRates_23.sort_values('CUBIC_FT').iterrows():\n",
    "        if cubic_inch_ci <= row['CUBIC_FT']:\n",
    "            return row['CLIENT_RATES']\n",
    "    return None  # or some default value if no range is found\n",
    "\n",
    "# Apply the function to df2 to create the new 'rate' column\n",
    "tbl_PLD_original_USPSAP_RP['FRT_USPSAP_RP'] = tbl_PLD_original_USPSAP_RP.apply(lambda row: get_rate(row['ZONE'], row['CUBIC_FT_CI']), axis=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "aae0d00a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# USPSAG_CI\n",
    "# soft Package\n",
    "tbl_clientRates_24 = tbl_clientRates[(tbl_clientRates['SERVICE_CODE'] == 'USPSAP_CI')]\n",
    "tbl_PLD_original_USPSAP_SP = tbl_BP[(tbl_BP['ACTUAL_WEIGHT'] <= 20) & (tbl_BP['L_CI'] <= 18) & (tbl_BP['CUBIC_INCH'] < 1728) & (tbl_BP['L_CI'] + tbl_BP['W_CI'] <= 36)].copy()\n",
    "tbl_PLD_original_USPSAP_SP['ZONE'] = tbl_PLD_original_USPSAP_SP['ZONE'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "bc5355ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "priority_softpack_tier = pd.DataFrame({\n",
    "    'Tier': ['Tier1', 'Tier2', 'Tier3', 'Tier4', 'Tier5'],\n",
    "    'Min': [0, 21, 27, 31, 34],\n",
    "    'Max': [21, 27, 31, 34, 36] \n",
    "})\n",
    "\n",
    "\n",
    "tbl_PLD_original_USPSAP_SP['Tier'] = np.nan\n",
    "\n",
    "# Vectorized assignment\n",
    "for _, row in priority_softpack_tier.iterrows():\n",
    "    mask = (tbl_PLD_original_USPSAP_SP['L_AND_W_CI'] > row['Min']) & (tbl_PLD_original_USPSAP_SP['L_AND_W_CI'] <= row['Max'])\n",
    "    tbl_PLD_original_USPSAP_SP.loc[mask, 'Tier'] = row['Tier']\n",
    "\n",
    "tbl_PLD_original_USPSAP_SP = tbl_PLD_original_USPSAP_SP.merge(tbl_clientRates_24[[\"ZONE\", \"TIER\", \"CLIENT_RATES\"]], left_on=['ZONE','Tier'], right_on=['ZONE','TIER'], how='left').rename(columns={'CLIENT_RATES': 'FRT_USPSAP_SP'})\n",
    "\n",
    "tbl_PLD_original_USPSAP_SP = tbl_PLD_original_USPSAP_SP.drop(['Tier', 'TIER'], axis=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "b36c6736",
   "metadata": {},
   "outputs": [],
   "source": [
    "tbl_PLD_original_USPSAP = tbl_PLD_original_USPSAP.merge(tbl_PLD_original_USPSAP_RP[[\"TRACKING_NUMBER\", \"FRT_USPSAP_RP\"]], left_on=['TRACKING_NUMBER'], right_on=['TRACKING_NUMBER'], how='left').merge(tbl_PLD_original_USPSAP_SP[[\"TRACKING_NUMBER\", \"FRT_USPSAP_SP\"]], left_on=['TRACKING_NUMBER'], right_on=['TRACKING_NUMBER'], how='left')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "1d41d1e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Calculate the minimum rate for each row\n",
    "tbl_PLD_original_USPSAP['FRT_USPSAP_final'] = tbl_PLD_original_USPSAP[['FRT_USPSAP', 'FRT_USPSAP_RP', 'FRT_USPSAP_SP']].min(axis=1)\n",
    "\n",
    "# Drop the original rate columns\n",
    "#tbl_PLD_original_USPSAP.drop(['FRT_USPSAP_RP', 'FRT_USPSAP_SP'], axis=1, inplace=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "id": "9c8fd31e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tbl_PLD_original_USPSAP['EXTRA_USPSAP_1'] = tbl_PLD_original_USPSAP.apply(lambda row:\n",
    "                                                            4 if (row['L'] > 22 and row['L'] <= 30) else \n",
    "                                                           (18 if (row['L'] > 30) else\n",
    "                                                            (0)), axis=1)\n",
    "\n",
    "\n",
    "#8. parcels that exceed 2 cubic feet (3456 cubic inches), add $15 per package. \n",
    "tbl_PLD_original_USPSAP['EXTRA_USPSAP_2'] = tbl_PLD_original_USPSAP.apply(lambda row:\n",
    "                                                            30 if (row['CUBIC_INCH'] > 3456) else (0), axis=1)\n",
    "\n",
    "#Get Total\n",
    "# note that for those with non-round up actual weight, if it is < 1, we still assign it the freight rate. \n",
    "\n",
    "#Get Total, revise the logic that if the freight rate is 0 or some rate is 0, the total will be 0. ups can handle everything. \n",
    "tbl_PLD_original_USPSAP['TOTAL_USPSAP'] = round(tbl_PLD_original_USPSAP.apply(lambda row: \n",
    "                                                                  (row['FRT_USPSAP'] + (row['EXTRA_USPSAP_1'] + row['EXTRA_USPSAP_2'])/(1-Margin_on_USPS)) if row['FRT_USPSAP'] != '' else '', axis=1),2)                         \n",
    "#tbl_PLD_original_USPSAP.to_csv('test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ff4beff",
   "metadata": {},
   "source": [
    "# MI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "391514a5",
   "metadata": {},
   "source": [
    "#MIPLE (MI Parcel Select Lightweight Expedited): commercial & residential, oz\n",
    "\n",
    "#MIPH (MI Parcel Select Heavyweight): commercial & residential, lb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db5813bb",
   "metadata": {},
   "source": [
    "# MIPLE (MI Parcel Select Lightweight Expedited)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "c6b2671a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#select the rate to use, #Decide the part of PLD to rerate\n",
    "tbl_clientRates_19 = tbl_clientRates[(tbl_clientRates['SERVICE_CODE'] == 'MIPLE')]\n",
    "tbl_PLD_original_MIPLE = tbl_BP[(tbl_BP['ACTUAL_WEIGHT'] < 1) & (tbl_BP['L'] <= 26) & (tbl_BP['W'] <= 26) & (tbl_BP['H'] <= 26)].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "e3fedb27",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Start to rerate\n",
    "#add billed weight column to PLD\n",
    "tbl_PLD_original_MIPLE['BILLED_WEIGHT_OZ'] = tbl_PLD_original_MIPLE.apply(lambda row: max(row['ACTUAL_WEIGHT_LB'], np.ceil(row['CUBIC_INCH']/dim_factor_mi))*16 if (row['CUBIC_INCH'] > 1728 and row['ACTUAL_WEIGHT'] >= 1) else\n",
    "                                                                          (row['ACTUAL_WEIGHT_OZ']), axis=1)\n",
    "\n",
    "\n",
    "#add Freight column to PLD\n",
    "tbl_PLD_original_MIPLE['ZONE'] = tbl_PLD_original_MIPLE['ZONE'].astype(str)\n",
    "tbl_PLD_original_MIPLE = tbl_PLD_original_MIPLE.merge(tbl_clientRates_19[[\"WEIGHT_OZ\", \"ZONE\", \"CLIENT_RATES\"]], left_on=['BILLED_WEIGHT_OZ', 'ZONE'], right_on=['WEIGHT_OZ', 'ZONE'], how='left')\n",
    "tbl_PLD_original_MIPLE = tbl_PLD_original_MIPLE.drop('WEIGHT_OZ', axis=1)\n",
    "tbl_PLD_original_MIPLE = tbl_PLD_original_MIPLE.rename(columns={'CLIENT_RATES': 'FRT_MIPLE'})\n",
    "\n",
    "#add fuel surcharge column to PLD\n",
    "tbl_PLD_original_MIPLE['FSC_MIPLE'] = tbl_PLD_original_MIPLE.apply(lambda row: row['FRT_MIPLE']*0.065, axis=1)\n",
    "\n",
    "#add NQD Surcharge column to PLD\n",
    "\n",
    "# A Non-standard piece fee of $4.00 will apply to any piece with a single side over 22\" or with total dimensions over 2 ft.\n",
    "tbl_PLD_original_MIPLE['NQD_MIPLE'] = tbl_PLD_original_MIPLE.apply(lambda row:\n",
    "                                                            4 if any([row['L'] > 22, row['W'] > 22, row['H'] > 22, row['CUBIC_INCH'] > 3456]) else\n",
    "                                                           (0), axis=1)\n",
    "\n",
    "#Get Total\n",
    "tbl_PLD_original_MIPLE['TOTAL_MIPLE'] = round(tbl_PLD_original_MIPLE.apply(lambda row: \n",
    "                                                                  np.nan if ((row['FRT_MIPLE'] == '') | (pd.isna(row['FRT_MIPLE'])))\n",
    "                                                                  else\n",
    "                                                                  ((row['FRT_MIPLE'] + row['FSC_MIPLE']) + (row['NQD_MIPLE'])/(1-Margin_on_MI)), axis=1),2)\n",
    "\n",
    "#tbl_PLD_original_MIPLE.to_csv('test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "c71c5810-70b1-4ec3-abae-74ed8dadc27e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tbl_PLD_original_MIPLE.to_excel('test.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa199e87",
   "metadata": {},
   "source": [
    "# MIPH (MI Parcel Select Heavyweight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "5dc36d74",
   "metadata": {},
   "outputs": [],
   "source": [
    "#select the rate to use, #Decide the part of PLD to rerate\n",
    "tbl_clientRates_20 = tbl_clientRates[(tbl_clientRates['SERVICE_CODE'] == 'MIPH')]\n",
    "tbl_PLD_original_MIPH = tbl_BP[(tbl_BP['L'] <= 26) & (tbl_BP['W'] <= 26) & (tbl_BP['H'] <= 26)].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "391126f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Start to rerate\n",
    "#add billed weight column to PLD\n",
    "tbl_PLD_original_MIPH['BILLED_WEIGHT_LB'] = tbl_PLD_original_MIPH.apply(lambda row: max(row['ACTUAL_WEIGHT_LB'], np.ceil(row['CUBIC_INCH']/dim_factor_mi)) if (row['CUBIC_INCH'] > 1728 and row['ACTUAL_WEIGHT'] >= 1) else\n",
    "                                                                          (row['ACTUAL_WEIGHT_LB']), axis=1)\n",
    "\n",
    "\n",
    "\n",
    "#add Freight column to PLD\n",
    "tbl_PLD_original_MIPH['ZONE'] = tbl_PLD_original_MIPH['ZONE'].astype(str)\n",
    "tbl_PLD_original_MIPH = tbl_PLD_original_MIPH.merge(tbl_clientRates_20[[\"WEIGHT_LB\", \"ZONE\", \"CLIENT_RATES\"]], left_on=['BILLED_WEIGHT_LB', 'ZONE'], right_on=['WEIGHT_LB', 'ZONE'], how='left')\n",
    "tbl_PLD_original_MIPH = tbl_PLD_original_MIPH.drop('WEIGHT_LB', axis=1)\n",
    "tbl_PLD_original_MIPH = tbl_PLD_original_MIPH.rename(columns={'CLIENT_RATES': 'FRT_MIPH'})\n",
    "\n",
    "#add fuel surcharge column to PLD\n",
    "tbl_PLD_original_MIPH['FSC_MIPH'] = tbl_PLD_original_MIPH.apply(lambda row: row['FRT_MIPH']*0.065, axis=1)\n",
    "\n",
    "\n",
    "#add NQD Surcharge column to PLD\n",
    "\n",
    "# A Non-standard piece fee of $4.00 will apply to any piece with a single side over 22\" or with total dimensions over 2 ft.\n",
    "tbl_PLD_original_MIPH['NQD_MIPH'] = tbl_PLD_original_MIPH.apply(lambda row:\n",
    "                                                            4 if any([row['L'] > 22, row['W'] > 22, row['H'] > 22, row['CUBIC_INCH'] > 3456]) else\n",
    "                                                           (0), axis=1)\n",
    "\n",
    "#Get Total\n",
    "tbl_PLD_original_MIPH['TOTAL_MIPH'] = round(tbl_PLD_original_MIPH.apply(lambda row: \n",
    "                                                                  np.nan if ((row['FRT_MIPH'] == '') | (pd.isna(row['FRT_MIPH'])))\n",
    "                                                                  else\n",
    "                                                                  ((row['FRT_MIPH'] + row['FSC_MIPH']) + (row['NQD_MIPH'])/(1-Margin_on_MI)), axis=1),2)\n",
    "\n",
    "#tbl_PLD_original_MIPH.to_csv('test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "c9147369-774b-4d9d-9c62-ce7e108f9061",
   "metadata": {},
   "outputs": [],
   "source": [
    "tbl_PLD_original_MIPH.to_excel('test.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "smooth-return",
   "metadata": {},
   "source": [
    "# 1.  Model Output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b332202d",
   "metadata": {},
   "source": [
    "# Specify the columns to keep in the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "progressive-battery",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['CustomerID', 'CustomerName', 'Facility', 'BarrettOrderNumber',\n",
       "       'Reference', 'PoNumber', 'TrackingNumber', 'ShipDate', 'Service',\n",
       "       'Shipper', 'ShipToName', 'ShipToContact', 'ShipToCity', 'ShipToState',\n",
       "       'ZipCode', 'ShipToCountry', 'Zone', 'Quantity', 'Dimensions',\n",
       "       'ActualWeight', 'ResidentialFlag', 'JoChannel', 'TRACKING_NUMBER',\n",
       "       'SHIP_DATE', 'OLD_SERVICE', 'ZIP_CODE', 'ZONE', 'DIMENSIONS',\n",
       "       'ACTUAL_WEIGHT', 'RESIDENTIAL_FLAG', 'L', 'W', 'H', 'GIRTH_AND_L',\n",
       "       'CUBIC_INCH', 'ACTUAL_WEIGHT_LB', 'ACTUAL_WEIGHT_OZ'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tbl_BP.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "advance-cabinet",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a copy of the DataFrame with only the specified columns\n",
    "tbl_PLD_original_rerated = tbl_BP.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f6645fb",
   "metadata": {},
   "source": [
    "# Specify the prioritized Services"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 484,
   "id": "6d2c87ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do not run below code during the 1st try (usually due to fsc data duplications) \n",
    "tbl_PLD_original_REDE = tbl_PLD_original_REDE.drop_duplicates()\n",
    "tbl_PLD_original_RED = tbl_PLD_original_RED.drop_duplicates()\n",
    "tbl_PLD_original_REDS = tbl_PLD_original_REDS.drop_duplicates()\n",
    "tbl_PLD_original_2DAM = tbl_PLD_original_2DAM.drop_duplicates()\n",
    "tbl_PLD_original_BLUE = tbl_PLD_original_BLUE.drop_duplicates()\n",
    "tbl_PLD_original_ORNG = tbl_PLD_original_ORNG.drop_duplicates()\n",
    "tbl_PLD_original_GRND = tbl_PLD_original_GRND.drop_duplicates()\n",
    "tbl_PLD_original_GRES = tbl_PLD_original_GRES.drop_duplicates()\n",
    "tbl_PLD_original_SRPT1 = tbl_PLD_original_SRPT1.drop_duplicates()\n",
    "tbl_PLD_original_SRPT = tbl_PLD_original_SRPT.drop_duplicates()\n",
    "\n",
    "tbl_PLD_original_DHLG = tbl_PLD_original_DHLG.drop_duplicates()\n",
    "tbl_PLD_original_DHLG1 = tbl_PLD_original_DHLG1.drop_duplicates()\n",
    "tbl_PLD_original_DHLE = tbl_PLD_original_DHLE.drop_duplicates()\n",
    "tbl_PLD_original_DHLE1 = tbl_PLD_original_DHLE1.drop_duplicates()\n",
    "tbl_PLD_original_DHLEM = tbl_PLD_original_DHLEM.drop_duplicates()\n",
    "\n",
    "tbl_PLD_original_USPSAG = tbl_PLD_original_USPSAG.drop_duplicates()\n",
    "tbl_PLD_original_USPSAP = tbl_PLD_original_USPSAP.drop_duplicates()\n",
    "\n",
    "tbl_PLD_original_MIPH = tbl_PLD_original_MIPH.drop_duplicates()\n",
    "tbl_PLD_original_MIPLE = tbl_PLD_original_MIPLE.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "a4d9ec12-a2a2-42f0-a80c-a148e5ea6084",
   "metadata": {},
   "outputs": [],
   "source": [
    "tbl_PLD_original_DHLE = tbl_PLD_original_DHLE.drop_duplicates()\n",
    "tbl_PLD_original_DHLE1 = tbl_PLD_original_DHLE1.drop_duplicates()\n",
    "tbl_PLD_original_DHLEM = tbl_PLD_original_DHLEM.drop_duplicates()\n",
    "\n",
    "tbl_PLD_original_MIPH = tbl_PLD_original_MIPH.drop_duplicates()\n",
    "tbl_PLD_original_MIPLE = tbl_PLD_original_MIPLE.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "c1ce348c",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tbl_PLD_original_REDE' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[82], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# List only the total rates of all carrier services. \u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m tbl_PLD_original_rerated[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTOTAL_REDE\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m tbl_PLD_original_rerated[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTRACKING_NUMBER\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mmap(tbl_PLD_original_REDE\u001b[38;5;241m.\u001b[39mset_index(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTRACKING_NUMBER\u001b[39m\u001b[38;5;124m'\u001b[39m)[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTOTAL_REDE\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m      3\u001b[0m tbl_PLD_original_rerated[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTOTAL_RED\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m tbl_PLD_original_rerated[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTRACKING_NUMBER\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mmap(tbl_PLD_original_RED\u001b[38;5;241m.\u001b[39mset_index(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTRACKING_NUMBER\u001b[39m\u001b[38;5;124m'\u001b[39m)[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTOTAL_RED\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m      4\u001b[0m tbl_PLD_original_rerated[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTOTAL_REDS\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m tbl_PLD_original_rerated[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTRACKING_NUMBER\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mmap(tbl_PLD_original_REDS\u001b[38;5;241m.\u001b[39mset_index(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTRACKING_NUMBER\u001b[39m\u001b[38;5;124m'\u001b[39m)[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTOTAL_REDS\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "\u001b[1;31mNameError\u001b[0m: name 'tbl_PLD_original_REDE' is not defined"
     ]
    }
   ],
   "source": [
    "# List only the total rates of all carrier services. \n",
    "tbl_PLD_original_rerated['TOTAL_REDE'] = tbl_PLD_original_rerated['TRACKING_NUMBER'].map(tbl_PLD_original_REDE.set_index('TRACKING_NUMBER')['TOTAL_REDE'])\n",
    "tbl_PLD_original_rerated['TOTAL_RED'] = tbl_PLD_original_rerated['TRACKING_NUMBER'].map(tbl_PLD_original_RED.set_index('TRACKING_NUMBER')['TOTAL_RED'])\n",
    "tbl_PLD_original_rerated['TOTAL_REDS'] = tbl_PLD_original_rerated['TRACKING_NUMBER'].map(tbl_PLD_original_REDS.set_index('TRACKING_NUMBER')['TOTAL_REDS'])\n",
    "tbl_PLD_original_rerated['TOTAL_2DAM'] = tbl_PLD_original_rerated['TRACKING_NUMBER'].map(tbl_PLD_original_2DAM.set_index('TRACKING_NUMBER')['TOTAL_2DAM'])\n",
    "tbl_PLD_original_rerated['TOTAL_BLUE'] = tbl_PLD_original_rerated['TRACKING_NUMBER'].map(tbl_PLD_original_BLUE.set_index('TRACKING_NUMBER')['TOTAL_BLUE'])\n",
    "tbl_PLD_original_rerated['TOTAL_ORNG'] = tbl_PLD_original_rerated['TRACKING_NUMBER'].map(tbl_PLD_original_ORNG.set_index('TRACKING_NUMBER')['TOTAL_ORNG'])\n",
    "tbl_PLD_original_rerated['TOTAL_GRND'] = tbl_PLD_original_rerated['TRACKING_NUMBER'].map(tbl_PLD_original_GRND.set_index('TRACKING_NUMBER')['TOTAL_GRND'])\n",
    "tbl_PLD_original_rerated['TOTAL_GRES'] = tbl_PLD_original_rerated['TRACKING_NUMBER'].map(tbl_PLD_original_GRES.set_index('TRACKING_NUMBER')['TOTAL_GRES'])\n",
    "tbl_PLD_original_rerated['TOTAL_SRPT1'] = tbl_PLD_original_rerated['TRACKING_NUMBER'].map(tbl_PLD_original_SRPT1.set_index('TRACKING_NUMBER')['TOTAL_SRPT1'])\n",
    "tbl_PLD_original_rerated['TOTAL_SRPT'] = tbl_PLD_original_rerated['TRACKING_NUMBER'].map(tbl_PLD_original_SRPT.set_index('TRACKING_NUMBER')['TOTAL_SRPT'])\n",
    "\n",
    "tbl_PLD_original_rerated['TOTAL_1STO'] = tbl_PLD_original_rerated['TRACKING_NUMBER'].map(tbl_PLD_original_1STO.set_index('TRACKING_NUMBER')['TOTAL_1STO'])\n",
    "tbl_PLD_original_rerated['TOTAL_PROV'] = tbl_PLD_original_rerated['TRACKING_NUMBER'].map(tbl_PLD_original_PROV.set_index('TRACKING_NUMBER')['TOTAL_PROV'])\n",
    "tbl_PLD_original_rerated['TOTAL_STOV'] = tbl_PLD_original_rerated['TRACKING_NUMBER'].map(tbl_PLD_original_STOV.set_index('TRACKING_NUMBER')['TOTAL_STOV'])\n",
    "tbl_PLD_original_rerated['TOTAL_2AM'] = tbl_PLD_original_rerated['TRACKING_NUMBER'].map(tbl_PLD_original_2AM.set_index('TRACKING_NUMBER')['TOTAL_2AM'])\n",
    "tbl_PLD_original_rerated['TOTAL_2DAY'] = tbl_PLD_original_rerated['TRACKING_NUMBER'].map(tbl_PLD_original_2DAY.set_index('TRACKING_NUMBER')['TOTAL_2DAY'])\n",
    "tbl_PLD_original_rerated['TOTAL_EXSA'] = tbl_PLD_original_rerated['TRACKING_NUMBER'].map(tbl_PLD_original_EXSA.set_index('TRACKING_NUMBER')['TOTAL_EXSA'])\n",
    "tbl_PLD_original_rerated['TOTAL_GRNDF'] = tbl_PLD_original_rerated['TRACKING_NUMBER'].map(tbl_PLD_original_GRNDF.set_index('TRACKING_NUMBER')['TOTAL_GRNDF'])\n",
    "tbl_PLD_original_rerated['TOTAL_HOME'] = tbl_PLD_original_rerated['TRACKING_NUMBER'].map(tbl_PLD_original_HOME.set_index('TRACKING_NUMBER')['TOTAL_HOME'])\n",
    "tbl_PLD_original_rerated['TOTAL_SPST1'] = tbl_PLD_original_rerated['TRACKING_NUMBER'].map(tbl_PLD_original_SPST1.set_index('TRACKING_NUMBER')['TOTAL_SPST1'])\n",
    "tbl_PLD_original_rerated['TOTAL_SPST'] = tbl_PLD_original_rerated['TRACKING_NUMBER'].map(tbl_PLD_original_SPST.set_index('TRACKING_NUMBER')['TOTAL_SPST'])\n",
    "\n",
    "\n",
    "tbl_PLD_original_rerated['TOTAL_DHLG'] = tbl_PLD_original_rerated['TRACKING_NUMBER'].map(tbl_PLD_original_DHLG.set_index('TRACKING_NUMBER')['TOTAL_DHLG'])\n",
    "tbl_PLD_original_rerated['TOTAL_DHLG1'] = tbl_PLD_original_rerated['TRACKING_NUMBER'].map(tbl_PLD_original_DHLG1.set_index('TRACKING_NUMBER')['TOTAL_DHLG1'])\n",
    "tbl_PLD_original_rerated['TOTAL_DHLE'] = tbl_PLD_original_rerated['TRACKING_NUMBER'].map(tbl_PLD_original_DHLE.set_index('TRACKING_NUMBER')['TOTAL_DHLE'])\n",
    "tbl_PLD_original_rerated['TOTAL_DHLE1'] = tbl_PLD_original_rerated['TRACKING_NUMBER'].map(tbl_PLD_original_DHLE1.set_index('TRACKING_NUMBER')['TOTAL_DHLE1'])\n",
    "tbl_PLD_original_rerated['TOTAL_DHLEM'] = tbl_PLD_original_rerated['TRACKING_NUMBER'].map(tbl_PLD_original_DHLEM.set_index('TRACKING_NUMBER')['TOTAL_DHLEM'])\n",
    "\n",
    "tbl_PLD_original_rerated['TOTAL_USPSAG'] = tbl_PLD_original_rerated['TRACKING_NUMBER'].map(tbl_PLD_original_USPSAG.set_index('TRACKING_NUMBER')['TOTAL_USPSAG'])\n",
    "tbl_PLD_original_rerated['TOTAL_USPSAP'] = tbl_PLD_original_rerated['TRACKING_NUMBER'].map(tbl_PLD_original_USPSAP.set_index('TRACKING_NUMBER')['TOTAL_USPSAP'])\n",
    "\n",
    "tbl_PLD_original_rerated['TOTAL_MIPLE'] = tbl_PLD_original_rerated['TRACKING_NUMBER'].map(tbl_PLD_original_MIPLE.set_index('TRACKING_NUMBER')['TOTAL_MIPLE'])\n",
    "tbl_PLD_original_rerated['TOTAL_MIPH'] = tbl_PLD_original_rerated['TRACKING_NUMBER'].map(tbl_PLD_original_MIPH.set_index('TRACKING_NUMBER')['TOTAL_MIPH'])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "de84a799-3c89-4373-b6e4-89573031008b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tbl_PLD_original_rerated['TOTAL_DHLE'] = tbl_PLD_original_rerated['TRACKING_NUMBER'].map(tbl_PLD_original_DHLE.set_index('TRACKING_NUMBER')['TOTAL_DHLE'])\n",
    "tbl_PLD_original_rerated['TOTAL_DHLE1'] = tbl_PLD_original_rerated['TRACKING_NUMBER'].map(tbl_PLD_original_DHLE1.set_index('TRACKING_NUMBER')['TOTAL_DHLE1'])\n",
    "tbl_PLD_original_rerated['TOTAL_DHLEM'] = tbl_PLD_original_rerated['TRACKING_NUMBER'].map(tbl_PLD_original_DHLEM.set_index('TRACKING_NUMBER')['TOTAL_DHLEM'])\n",
    "\n",
    "tbl_PLD_original_rerated['TOTAL_MIPLE'] = tbl_PLD_original_rerated['TRACKING_NUMBER'].map(tbl_PLD_original_MIPLE.set_index('TRACKING_NUMBER')['TOTAL_MIPLE'])\n",
    "tbl_PLD_original_rerated['TOTAL_MIPH'] = tbl_PLD_original_rerated['TRACKING_NUMBER'].map(tbl_PLD_original_MIPH.set_index('TRACKING_NUMBER')['TOTAL_MIPH'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb53050f",
   "metadata": {},
   "source": [
    "# Base Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "permanent-reminder",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Base model output. Make sure to check the obnormal rates due to obnormal dim, etc. \n",
    "tbl_PLD_original_rerated.to_excel('Barrett_DHL&MI.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd080035",
   "metadata": {},
   "source": [
    "# Best Way Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 815,
   "id": "northern-discretion",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_keep_1 = [\n",
    "'TRACKING_NUMBER', 'SHIP_DATE', 'OLD_SERVICE', 'ZIP_CODE', 'ZONE_old',\n",
    "       'L', 'W', 'H', 'ACTUAL_WEIGHT', 'RESIDENTIAL_FLAG', 'State',\n",
    "       'GIRTH_AND_L', 'CUBIC_INCH', 'ACTUAL_WEIGHT_LB', 'ACTUAL_WEIGHT_OZ',\n",
    "       'FROM_ZIP', 'ZONE', 'DAS_CATEGORY']\n",
    "\n",
    "columns_to_keep_2 = ['TOTAL_REDE',\n",
    "     'TOTAL_RED', 'TOTAL_REDS', 'TOTAL_2DAM', \n",
    "    'TOTAL_BLUE', 'TOTAL_ORNG',\n",
    "       'TOTAL_GRND', 'TOTAL_GRES', \n",
    "               'TOTAL_SRPT1', 'TOTAL_SRPT', \n",
    "                    'TOTAL_DHLG','TOTAL_DHLG1', \n",
    "                     'TOTAL_DHLE', 'TOTAL_DHLE1', \n",
    "       'TOTAL_DHLEM',\n",
    "        'TOTAL_USPSAG',  'TOTAL_USPSAP', \n",
    "    'TOTAL_MIPLE', 'TOTAL_MIPH',\n",
    "   'TOTAL_1STO', 'TOTAL_PROV', 'TOTAL_STOV', 'TOTAL_2AM', 'TOTAL_2DAY', \n",
    "   'TOTAL_EXSA', 'TOTAL_GRNDF', 'TOTAL_HOME', 'TOTAL_SPST1', 'TOTAL_SPST'\n",
    "                     \n",
    "                    ]\n",
    "\n",
    "# Make a copy of the DataFrame with only the specified columns\n",
    "tbl_PLD_original_rerated_1 = tbl_PLD_original_rerated[columns_to_keep_1].copy()\n",
    "tbl_PLD_original_rerated_2 = tbl_PLD_original_rerated[columns_to_keep_2].copy()\n",
    "\n",
    "# Select min value from each row\n",
    "tbl_PLD_original_rerated_2 = tbl_PLD_original_rerated_2.fillna(float('inf'))\n",
    "selected_values = tbl_PLD_original_rerated_2.apply(lambda row: row.loc[row.idxmin()], axis=1)\n",
    "\n",
    "# Create a new column based on the corresponding column name\n",
    "tbl_PLD_original_rerated_2['SELECTED_SERVICE'] = tbl_PLD_original_rerated_2.idxmin(axis=1)\n",
    "\n",
    "# Add the selected values as a new column\n",
    "tbl_PLD_original_rerated_2['CARRIER_COST_MIN'] = selected_values\n",
    "\n",
    "tbl_PLD_original_rerated_bestWay = pd.concat([tbl_PLD_original_rerated_1, tbl_PLD_original_rerated_2], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 816,
   "id": "aging-string",
   "metadata": {},
   "outputs": [],
   "source": [
    "tbl_REDE = tbl_PLD_original_rerated_bestWay[(tbl_PLD_original_rerated_bestWay['SELECTED_SERVICE'] == 'TOTAL_REDE')].copy()\n",
    "tbl_RED = tbl_PLD_original_rerated_bestWay[(tbl_PLD_original_rerated_bestWay['SELECTED_SERVICE'] == 'TOTAL_RED')].copy()\n",
    "tbl_REDS = tbl_PLD_original_rerated_bestWay[(tbl_PLD_original_rerated_bestWay['SELECTED_SERVICE'] == 'TOTAL_REDS')].copy()\n",
    "tbl_2DAM = tbl_PLD_original_rerated_bestWay[(tbl_PLD_original_rerated_bestWay['SELECTED_SERVICE'] == 'TOTAL_2DAM')].copy()\n",
    "tbl_BLUE = tbl_PLD_original_rerated_bestWay[(tbl_PLD_original_rerated_bestWay['SELECTED_SERVICE'] == 'TOTAL_BLUE')].copy()\n",
    "\n",
    "tbl_ORNG = tbl_PLD_original_rerated_bestWay[(tbl_PLD_original_rerated_bestWay['SELECTED_SERVICE'] == 'TOTAL_ORNG')].copy()\n",
    "tbl_GRND = tbl_PLD_original_rerated_bestWay[(tbl_PLD_original_rerated_bestWay['SELECTED_SERVICE'] == 'TOTAL_GRND')].copy()\n",
    "tbl_GRES = tbl_PLD_original_rerated_bestWay[(tbl_PLD_original_rerated_bestWay['SELECTED_SERVICE'] == 'TOTAL_GRES')].copy()\n",
    "tbl_SRPT1 = tbl_PLD_original_rerated_bestWay[(tbl_PLD_original_rerated_bestWay['SELECTED_SERVICE'] == 'TOTAL_SRPT1')].copy()\n",
    "tbl_SRPT = tbl_PLD_original_rerated_bestWay[(tbl_PLD_original_rerated_bestWay['SELECTED_SERVICE'] == 'TOTAL_SRPT')].copy()\n",
    "\n",
    "tbl_1STO = tbl_PLD_original_rerated_bestWay[(tbl_PLD_original_rerated_bestWay['SELECTED_SERVICE'] == 'TOTAL_1STO')].copy()\n",
    "tbl_PROV = tbl_PLD_original_rerated_bestWay[(tbl_PLD_original_rerated_bestWay['SELECTED_SERVICE'] == 'TOTAL_PROV')].copy()\n",
    "tbl_STOV = tbl_PLD_original_rerated_bestWay[(tbl_PLD_original_rerated_bestWay['SELECTED_SERVICE'] == 'TOTAL_STOV')].copy()\n",
    "tbl_2AM = tbl_PLD_original_rerated_bestWay[(tbl_PLD_original_rerated_bestWay['SELECTED_SERVICE'] == 'TOTAL_2AM')].copy()\n",
    "tbl_2DAY = tbl_PLD_original_rerated_bestWay[(tbl_PLD_original_rerated_bestWay['SELECTED_SERVICE'] == 'TOTAL_2DAY')].copy()\n",
    "\n",
    "tbl_EXSA = tbl_PLD_original_rerated_bestWay[(tbl_PLD_original_rerated_bestWay['SELECTED_SERVICE'] == 'TOTAL_EXSA')].copy()\n",
    "tbl_GRNDF = tbl_PLD_original_rerated_bestWay[(tbl_PLD_original_rerated_bestWay['SELECTED_SERVICE'] == 'TOTAL_GRNDF')].copy()\n",
    "tbl_HOME = tbl_PLD_original_rerated_bestWay[(tbl_PLD_original_rerated_bestWay['SELECTED_SERVICE'] == 'TOTAL_HOME')].copy()\n",
    "tbl_SPST1 = tbl_PLD_original_rerated_bestWay[(tbl_PLD_original_rerated_bestWay['SELECTED_SERVICE'] == 'TOTAL_SPST1')].copy()\n",
    "tbl_SPST = tbl_PLD_original_rerated_bestWay[(tbl_PLD_original_rerated_bestWay['SELECTED_SERVICE'] == 'TOTAL_SPST')].copy()\n",
    "\n",
    "\n",
    "tbl_DHLG = tbl_PLD_original_rerated_bestWay[(tbl_PLD_original_rerated_bestWay['SELECTED_SERVICE'] == 'TOTAL_DHLG')].copy()\n",
    "tbl_DHLG1 = tbl_PLD_original_rerated_bestWay[(tbl_PLD_original_rerated_bestWay['SELECTED_SERVICE'] == 'TOTAL_DHLG1')].copy()\n",
    "tbl_DHLE = tbl_PLD_original_rerated_bestWay[(tbl_PLD_original_rerated_bestWay['SELECTED_SERVICE'] == 'TOTAL_DHLE')].copy()\n",
    "tbl_DHLE1 = tbl_PLD_original_rerated_bestWay[(tbl_PLD_original_rerated_bestWay['SELECTED_SERVICE'] == 'TOTAL_DHLE1')].copy()\n",
    "tbl_DHLEM = tbl_PLD_original_rerated_bestWay[(tbl_PLD_original_rerated_bestWay['SELECTED_SERVICE'] == 'TOTAL_DHLEM')].copy()\n",
    "\n",
    "tbl_USPSAG = tbl_PLD_original_rerated_bestWay[(tbl_PLD_original_rerated_bestWay['SELECTED_SERVICE'] == 'TOTAL_USPSAG')].copy()\n",
    "tbl_USPSAP = tbl_PLD_original_rerated_bestWay[(tbl_PLD_original_rerated_bestWay['SELECTED_SERVICE'] == 'TOTAL_USPSAP')].copy()\n",
    "\n",
    "tbl_MIPLE = tbl_PLD_original_rerated_bestWay[(tbl_PLD_original_rerated_bestWay['SELECTED_SERVICE'] == 'TOTAL_MIPLE')].copy()\n",
    "tbl_MIPH = tbl_PLD_original_rerated_bestWay[(tbl_PLD_original_rerated_bestWay['SELECTED_SERVICE'] == 'TOTAL_MIPH')].copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 817,
   "id": "detailed-empty",
   "metadata": {},
   "outputs": [],
   "source": [
    "tbl_REDE['FRT_UPS'] = tbl_REDE['TRACKING_NUMBER'].map(tbl_PLD_original_REDE.set_index('TRACKING_NUMBER')['FRT_REDE'])\n",
    "tbl_REDE['FSC%_UPS'] = tbl_REDE['TRACKING_NUMBER'].map(tbl_PLD_original_REDE.set_index('TRACKING_NUMBER')['FSC%_REDE'])\n",
    "tbl_REDE['RES_UPS'] = tbl_REDE['TRACKING_NUMBER'].map(tbl_PLD_original_REDE.set_index('TRACKING_NUMBER')['RES_REDE'])\n",
    "tbl_REDE['DAS_UPS'] = tbl_REDE['TRACKING_NUMBER'].map(tbl_PLD_original_REDE.set_index('TRACKING_NUMBER')['DAS_REDE'])\n",
    "tbl_REDE['AHS_UPS'] = tbl_REDE['TRACKING_NUMBER'].map(tbl_PLD_original_REDE.set_index('TRACKING_NUMBER')['AHS_REDE'])\n",
    "\n",
    "tbl_RED['FRT_UPS'] = tbl_RED['TRACKING_NUMBER'].map(tbl_PLD_original_RED.set_index('TRACKING_NUMBER')['FRT_RED'])\n",
    "tbl_RED['FSC%_UPS'] = tbl_RED['TRACKING_NUMBER'].map(tbl_PLD_original_RED.set_index('TRACKING_NUMBER')['FSC%_RED'])\n",
    "tbl_RED['RES_UPS'] = tbl_RED['TRACKING_NUMBER'].map(tbl_PLD_original_RED.set_index('TRACKING_NUMBER')['RES_RED'])\n",
    "tbl_RED['DAS_UPS'] = tbl_RED['TRACKING_NUMBER'].map(tbl_PLD_original_RED.set_index('TRACKING_NUMBER')['DAS_RED'])\n",
    "tbl_RED['AHS_UPS'] = tbl_RED['TRACKING_NUMBER'].map(tbl_PLD_original_RED.set_index('TRACKING_NUMBER')['AHS_RED'])\n",
    "\n",
    "tbl_REDS['FRT_UPS'] = tbl_REDS['TRACKING_NUMBER'].map(tbl_PLD_original_REDS.set_index('TRACKING_NUMBER')['FRT_REDS'])\n",
    "tbl_REDS['FSC%_UPS'] = tbl_REDS['TRACKING_NUMBER'].map(tbl_PLD_original_REDS.set_index('TRACKING_NUMBER')['FSC%_REDS'])\n",
    "tbl_REDS['RES_UPS'] = tbl_REDS['TRACKING_NUMBER'].map(tbl_PLD_original_REDS.set_index('TRACKING_NUMBER')['RES_REDS'])\n",
    "tbl_REDS['DAS_UPS'] = tbl_REDS['TRACKING_NUMBER'].map(tbl_PLD_original_REDS.set_index('TRACKING_NUMBER')['DAS_REDS'])\n",
    "tbl_REDS['AHS_UPS'] = tbl_REDS['TRACKING_NUMBER'].map(tbl_PLD_original_REDS.set_index('TRACKING_NUMBER')['AHS_REDS'])\n",
    "\n",
    "tbl_2DAM['FRT_UPS'] = tbl_2DAM['TRACKING_NUMBER'].map(tbl_PLD_original_2DAM.set_index('TRACKING_NUMBER')['FRT_2DAM'])\n",
    "tbl_2DAM['FSC%_UPS'] = tbl_2DAM['TRACKING_NUMBER'].map(tbl_PLD_original_2DAM.set_index('TRACKING_NUMBER')['FSC%_2DAM'])\n",
    "tbl_2DAM['RES_UPS'] = tbl_2DAM['TRACKING_NUMBER'].map(tbl_PLD_original_2DAM.set_index('TRACKING_NUMBER')['RES_2DAM'])\n",
    "tbl_2DAM['DAS_UPS'] = tbl_2DAM['TRACKING_NUMBER'].map(tbl_PLD_original_2DAM.set_index('TRACKING_NUMBER')['DAS_2DAM'])\n",
    "tbl_2DAM['AHS_UPS'] = tbl_2DAM['TRACKING_NUMBER'].map(tbl_PLD_original_2DAM.set_index('TRACKING_NUMBER')['AHS_2DAM'])\n",
    "\n",
    "tbl_BLUE['FRT_UPS'] = tbl_BLUE['TRACKING_NUMBER'].map(tbl_PLD_original_BLUE.set_index('TRACKING_NUMBER')['FRT_BLUE'])\n",
    "tbl_BLUE['FSC%_UPS'] = tbl_BLUE['TRACKING_NUMBER'].map(tbl_PLD_original_BLUE.set_index('TRACKING_NUMBER')['FSC%_BLUE'])\n",
    "tbl_BLUE['RES_UPS'] = tbl_BLUE['TRACKING_NUMBER'].map(tbl_PLD_original_BLUE.set_index('TRACKING_NUMBER')['RES_BLUE'])\n",
    "tbl_BLUE['DAS_UPS'] = tbl_BLUE['TRACKING_NUMBER'].map(tbl_PLD_original_BLUE.set_index('TRACKING_NUMBER')['DAS_BLUE'])\n",
    "tbl_BLUE['AHS_UPS'] = tbl_BLUE['TRACKING_NUMBER'].map(tbl_PLD_original_BLUE.set_index('TRACKING_NUMBER')['AHS_BLUE'])\n",
    "\n",
    "\n",
    "tbl_ORNG['FRT_UPS'] = tbl_ORNG['TRACKING_NUMBER'].map(tbl_PLD_original_ORNG.set_index('TRACKING_NUMBER')['FRT_ORNG'])\n",
    "tbl_ORNG['FSC%_UPS'] = tbl_ORNG['TRACKING_NUMBER'].map(tbl_PLD_original_ORNG.set_index('TRACKING_NUMBER')['FSC%_ORNG'])\n",
    "tbl_ORNG['RES_UPS'] = tbl_ORNG['TRACKING_NUMBER'].map(tbl_PLD_original_ORNG.set_index('TRACKING_NUMBER')['RES_ORNG'])\n",
    "tbl_ORNG['DAS_UPS'] = tbl_ORNG['TRACKING_NUMBER'].map(tbl_PLD_original_ORNG.set_index('TRACKING_NUMBER')['DAS_ORNG'])\n",
    "tbl_ORNG['AHS_UPS'] = tbl_ORNG['TRACKING_NUMBER'].map(tbl_PLD_original_ORNG.set_index('TRACKING_NUMBER')['AHS_ORNG'])\n",
    "\n",
    "tbl_GRND['FRT_UPS'] = tbl_GRND['TRACKING_NUMBER'].map(tbl_PLD_original_GRND.set_index('TRACKING_NUMBER')['FRT_GRND'])\n",
    "tbl_GRND['FSC%_UPS'] = tbl_GRND['TRACKING_NUMBER'].map(tbl_PLD_original_GRND.set_index('TRACKING_NUMBER')['FSC%_GRND'])\n",
    "tbl_GRND['RES_UPS'] = tbl_GRND['TRACKING_NUMBER'].map(tbl_PLD_original_GRND.set_index('TRACKING_NUMBER')['RES_GRND'])\n",
    "tbl_GRND['DAS_UPS'] = tbl_GRND['TRACKING_NUMBER'].map(tbl_PLD_original_GRND.set_index('TRACKING_NUMBER')['DAS_GRND'])\n",
    "tbl_GRND['AHS_UPS'] = tbl_GRND['TRACKING_NUMBER'].map(tbl_PLD_original_GRND.set_index('TRACKING_NUMBER')['AHS_GRND'])\n",
    "\n",
    "tbl_GRES['FRT_UPS'] = tbl_GRES['TRACKING_NUMBER'].map(tbl_PLD_original_GRES.set_index('TRACKING_NUMBER')['FRT_GRES'])\n",
    "tbl_GRES['FSC%_UPS'] = tbl_GRES['TRACKING_NUMBER'].map(tbl_PLD_original_GRES.set_index('TRACKING_NUMBER')['FSC%_GRES'])\n",
    "tbl_GRES['RES_UPS'] = tbl_GRES['TRACKING_NUMBER'].map(tbl_PLD_original_GRES.set_index('TRACKING_NUMBER')['RES_GRES'])\n",
    "tbl_GRES['DAS_UPS'] = tbl_GRES['TRACKING_NUMBER'].map(tbl_PLD_original_GRES.set_index('TRACKING_NUMBER')['DAS_GRES'])\n",
    "tbl_GRES['AHS_UPS'] = tbl_GRES['TRACKING_NUMBER'].map(tbl_PLD_original_GRES.set_index('TRACKING_NUMBER')['AHS_GRES'])\n",
    "\n",
    "tbl_SRPT1['FRT_UPS'] = tbl_SRPT1['TRACKING_NUMBER'].map(tbl_PLD_original_SRPT1.set_index('TRACKING_NUMBER')['FRT_SRPT1'])\n",
    "tbl_SRPT1['FSC%_UPS'] = tbl_SRPT1['TRACKING_NUMBER'].map(tbl_PLD_original_SRPT1.set_index('TRACKING_NUMBER')['FSC%_SRPT1'])\n",
    "tbl_SRPT1['RES_UPS'] = tbl_SRPT1['TRACKING_NUMBER'].map(tbl_PLD_original_SRPT1.set_index('TRACKING_NUMBER')['RES_SRPT1'])\n",
    "tbl_SRPT1['DAS_UPS'] = tbl_SRPT1['TRACKING_NUMBER'].map(tbl_PLD_original_SRPT1.set_index('TRACKING_NUMBER')['DAS_SRPT1'])\n",
    "tbl_SRPT1['AHS_UPS'] = tbl_SRPT1['TRACKING_NUMBER'].map(tbl_PLD_original_SRPT1.set_index('TRACKING_NUMBER')['AHS_SRPT1'])\n",
    "\n",
    "tbl_SRPT['FRT_UPS'] = tbl_SRPT['TRACKING_NUMBER'].map(tbl_PLD_original_SRPT.set_index('TRACKING_NUMBER')['FRT_SRPT'])\n",
    "tbl_SRPT['FSC%_UPS'] = tbl_SRPT['TRACKING_NUMBER'].map(tbl_PLD_original_SRPT.set_index('TRACKING_NUMBER')['FSC%_SRPT'])\n",
    "tbl_SRPT['RES_UPS'] = tbl_SRPT['TRACKING_NUMBER'].map(tbl_PLD_original_SRPT.set_index('TRACKING_NUMBER')['RES_SRPT'])\n",
    "tbl_SRPT['DAS_UPS'] = tbl_SRPT['TRACKING_NUMBER'].map(tbl_PLD_original_SRPT.set_index('TRACKING_NUMBER')['DAS_SRPT'])\n",
    "tbl_SRPT['AHS_UPS'] = tbl_SRPT['TRACKING_NUMBER'].map(tbl_PLD_original_SRPT.set_index('TRACKING_NUMBER')['AHS_SRPT'])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "tbl_1STO['FRT_FedEx'] = tbl_1STO['TRACKING_NUMBER'].map(tbl_PLD_original_1STO.set_index('TRACKING_NUMBER')['FRT_1STO'])\n",
    "tbl_1STO['FSC%_FDX'] = tbl_1STO['TRACKING_NUMBER'].map(tbl_PLD_original_1STO.set_index('TRACKING_NUMBER')['FSC%_1STO'])\n",
    "tbl_1STO['RES_FDX'] = tbl_1STO['TRACKING_NUMBER'].map(tbl_PLD_original_1STO.set_index('TRACKING_NUMBER')['RES_1STO'])\n",
    "tbl_1STO['DAS_FDX'] = tbl_1STO['TRACKING_NUMBER'].map(tbl_PLD_original_1STO.set_index('TRACKING_NUMBER')['DAS_1STO'])\n",
    "tbl_1STO['AHS_FDX'] = tbl_1STO['TRACKING_NUMBER'].map(tbl_PLD_original_1STO.set_index('TRACKING_NUMBER')['AHS_1STO'])\n",
    "\n",
    "tbl_PROV['FRT_FDX'] = tbl_PROV['TRACKING_NUMBER'].map(tbl_PLD_original_PROV.set_index('TRACKING_NUMBER')['FRT_PROV'])\n",
    "tbl_PROV['FSC%_FDX'] = tbl_PROV['TRACKING_NUMBER'].map(tbl_PLD_original_PROV.set_index('TRACKING_NUMBER')['FSC%_PROV'])\n",
    "tbl_PROV['RES_FDX'] = tbl_PROV['TRACKING_NUMBER'].map(tbl_PLD_original_PROV.set_index('TRACKING_NUMBER')['RES_PROV'])\n",
    "tbl_PROV['DAS_FDX'] = tbl_PROV['TRACKING_NUMBER'].map(tbl_PLD_original_PROV.set_index('TRACKING_NUMBER')['DAS_PROV'])\n",
    "tbl_PROV['AHS_FDX'] = tbl_PROV['TRACKING_NUMBER'].map(tbl_PLD_original_PROV.set_index('TRACKING_NUMBER')['AHS_PROV'])\n",
    "\n",
    "tbl_STOV['FRT_FDX'] = tbl_STOV['TRACKING_NUMBER'].map(tbl_PLD_original_STOV.set_index('TRACKING_NUMBER')['FRT_STOV'])\n",
    "tbl_STOV['FSC%_FDX'] = tbl_STOV['TRACKING_NUMBER'].map(tbl_PLD_original_STOV.set_index('TRACKING_NUMBER')['FSC%_STOV'])\n",
    "tbl_STOV['RES_FDX'] = tbl_STOV['TRACKING_NUMBER'].map(tbl_PLD_original_STOV.set_index('TRACKING_NUMBER')['RES_STOV'])\n",
    "tbl_STOV['DAS_FDX'] = tbl_STOV['TRACKING_NUMBER'].map(tbl_PLD_original_STOV.set_index('TRACKING_NUMBER')['DAS_STOV'])\n",
    "tbl_STOV['AHS_FDX'] = tbl_STOV['TRACKING_NUMBER'].map(tbl_PLD_original_STOV.set_index('TRACKING_NUMBER')['AHS_STOV'])\n",
    "\n",
    "\n",
    "\n",
    "tbl_2AM['FRT_FDX'] = tbl_2AM['TRACKING_NUMBER'].map(tbl_PLD_original_2AM.set_index('TRACKING_NUMBER')['FRT_2AM'])\n",
    "tbl_2AM['FSC%_FDX'] = tbl_2AM['TRACKING_NUMBER'].map(tbl_PLD_original_2AM.set_index('TRACKING_NUMBER')['FSC%_2AM'])\n",
    "tbl_2AM['RES_FDX'] = tbl_2AM['TRACKING_NUMBER'].map(tbl_PLD_original_2AM.set_index('TRACKING_NUMBER')['RES_2AM'])\n",
    "tbl_2AM['DAS_FDX'] = tbl_2AM['TRACKING_NUMBER'].map(tbl_PLD_original_2AM.set_index('TRACKING_NUMBER')['DAS_2AM'])\n",
    "tbl_2AM['AHS_FDX'] = tbl_2AM['TRACKING_NUMBER'].map(tbl_PLD_original_2AM.set_index('TRACKING_NUMBER')['AHS_2AM'])\n",
    "\n",
    "tbl_2DAY['FRT_FDX'] = tbl_2DAY['TRACKING_NUMBER'].map(tbl_PLD_original_2DAY.set_index('TRACKING_NUMBER')['FRT_2DAY'])\n",
    "tbl_2DAY['FSC%_FDX'] = tbl_2DAY['TRACKING_NUMBER'].map(tbl_PLD_original_2DAY.set_index('TRACKING_NUMBER')['FSC%_2DAY'])\n",
    "tbl_2DAY['RES_FDX'] = tbl_2DAY['TRACKING_NUMBER'].map(tbl_PLD_original_2DAY.set_index('TRACKING_NUMBER')['RES_2DAY'])\n",
    "tbl_2DAY['DAS_FDX'] = tbl_2DAY['TRACKING_NUMBER'].map(tbl_PLD_original_2DAY.set_index('TRACKING_NUMBER')['DAS_2DAY'])\n",
    "tbl_2DAY['AHS_FDX'] = tbl_2DAY['TRACKING_NUMBER'].map(tbl_PLD_original_2DAY.set_index('TRACKING_NUMBER')['AHS_2DAY'])\n",
    "\n",
    "\n",
    "tbl_EXSA['FRT_FDX'] = tbl_EXSA['TRACKING_NUMBER'].map(tbl_PLD_original_EXSA.set_index('TRACKING_NUMBER')['FRT_EXSA'])\n",
    "tbl_EXSA['FSC%_FDX'] = tbl_EXSA['TRACKING_NUMBER'].map(tbl_PLD_original_EXSA.set_index('TRACKING_NUMBER')['FSC%_EXSA'])\n",
    "tbl_EXSA['RES_FDX'] = tbl_EXSA['TRACKING_NUMBER'].map(tbl_PLD_original_EXSA.set_index('TRACKING_NUMBER')['RES_EXSA'])\n",
    "tbl_EXSA['DAS_FDX'] = tbl_EXSA['TRACKING_NUMBER'].map(tbl_PLD_original_EXSA.set_index('TRACKING_NUMBER')['DAS_EXSA'])\n",
    "tbl_EXSA['AHS_FDX'] = tbl_EXSA['TRACKING_NUMBER'].map(tbl_PLD_original_EXSA.set_index('TRACKING_NUMBER')['AHS_EXSA'])\n",
    "\n",
    "\n",
    "tbl_GRNDF['FRT_FDX'] = tbl_GRNDF['TRACKING_NUMBER'].map(tbl_PLD_original_GRNDF.set_index('TRACKING_NUMBER')['FRT_GRNDF'])\n",
    "tbl_GRNDF['FSC%_FDX'] = tbl_GRNDF['TRACKING_NUMBER'].map(tbl_PLD_original_GRNDF.set_index('TRACKING_NUMBER')['FSC%_GRNDF'])\n",
    "tbl_GRNDF['RES_FDX'] = tbl_GRNDF['TRACKING_NUMBER'].map(tbl_PLD_original_GRNDF.set_index('TRACKING_NUMBER')['RES_GRNDF'])\n",
    "tbl_GRNDF['DAS_FDX'] = tbl_GRNDF['TRACKING_NUMBER'].map(tbl_PLD_original_GRNDF.set_index('TRACKING_NUMBER')['DAS_GRNDF'])\n",
    "tbl_GRNDF['AHS_FDX'] = tbl_GRNDF['TRACKING_NUMBER'].map(tbl_PLD_original_GRNDF.set_index('TRACKING_NUMBER')['AHS_GRNDF'])\n",
    "\n",
    "\n",
    "\n",
    "tbl_HOME['FRT_FDX'] = tbl_HOME['TRACKING_NUMBER'].map(tbl_PLD_original_HOME.set_index('TRACKING_NUMBER')['FRT_HOME'])\n",
    "tbl_HOME['FSC%_FDX'] = tbl_HOME['TRACKING_NUMBER'].map(tbl_PLD_original_HOME.set_index('TRACKING_NUMBER')['FSC%_HOME'])\n",
    "tbl_HOME['RES_FDX'] = tbl_HOME['TRACKING_NUMBER'].map(tbl_PLD_original_HOME.set_index('TRACKING_NUMBER')['RES_HOME'])\n",
    "tbl_HOME['DAS_FDX'] = tbl_HOME['TRACKING_NUMBER'].map(tbl_PLD_original_HOME.set_index('TRACKING_NUMBER')['DAS_HOME'])\n",
    "tbl_HOME['AHS_FDX'] = tbl_HOME['TRACKING_NUMBER'].map(tbl_PLD_original_HOME.set_index('TRACKING_NUMBER')['AHS_HOME'])\n",
    "\n",
    "\n",
    "tbl_SPST1['FRT_FDX'] = tbl_SPST1['TRACKING_NUMBER'].map(tbl_PLD_original_SPST1.set_index('TRACKING_NUMBER')['FRT_SPST1'])\n",
    "tbl_SPST1['FSC%_FDX'] = tbl_SPST1['TRACKING_NUMBER'].map(tbl_PLD_original_SPST1.set_index('TRACKING_NUMBER')['FSC%_SPST1'])\n",
    "tbl_SPST1['RES_FDX'] = tbl_SPST1['TRACKING_NUMBER'].map(tbl_PLD_original_SPST1.set_index('TRACKING_NUMBER')['RES_SPST1'])\n",
    "tbl_SPST1['DAS_FDX'] = tbl_SPST1['TRACKING_NUMBER'].map(tbl_PLD_original_SPST1.set_index('TRACKING_NUMBER')['DAS_SPST1'])\n",
    "tbl_SPST1['AHS_FDX'] = tbl_SPST1['TRACKING_NUMBER'].map(tbl_PLD_original_SPST1.set_index('TRACKING_NUMBER')['AHS_SPST1'])\n",
    "\n",
    "\n",
    "tbl_SPST['FRT_FDX'] = tbl_SPST['TRACKING_NUMBER'].map(tbl_PLD_original_SPST.set_index('TRACKING_NUMBER')['FRT_SPST'])\n",
    "tbl_SPST['FSC%_FDX'] = tbl_SPST['TRACKING_NUMBER'].map(tbl_PLD_original_SPST.set_index('TRACKING_NUMBER')['FSC%_SPST'])\n",
    "tbl_SPST['RES_FDX'] = tbl_SPST['TRACKING_NUMBER'].map(tbl_PLD_original_SPST.set_index('TRACKING_NUMBER')['RES_SPST'])\n",
    "tbl_SPST['DAS_FDX'] = tbl_SPST['TRACKING_NUMBER'].map(tbl_PLD_original_SPST.set_index('TRACKING_NUMBER')['DAS_SPST'])\n",
    "tbl_SPST['AHS_FDX'] = tbl_SPST['TRACKING_NUMBER'].map(tbl_PLD_original_SPST.set_index('TRACKING_NUMBER')['AHS_SPST'])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "tbl_DHLG['FRT_DHL'] = tbl_DHLG['TRACKING_NUMBER'].map(tbl_PLD_original_DHLG.set_index('TRACKING_NUMBER')['FRT_DHLG'])\n",
    "tbl_DHLG['FSC%_DHL'] = tbl_DHLG['TRACKING_NUMBER'].map(tbl_PLD_original_DHLG.set_index('TRACKING_NUMBER')['FSC%_DHLG'])\n",
    "tbl_DHLG['FSC_DHL'] = tbl_DHLG['TRACKING_NUMBER'].map(tbl_PLD_original_DHLG.set_index('TRACKING_NUMBER')['FSC_DHLG'])\n",
    "tbl_DHLG['NQD_GIRTH_AND_L_DHL'] = tbl_DHLG['TRACKING_NUMBER'].map(tbl_PLD_original_DHLG.set_index('TRACKING_NUMBER')['NQD_DHLG_GIRTH_AND_L'])\n",
    "tbl_DHLG['NQD_OTHER_DHL'] = tbl_DHLG['TRACKING_NUMBER'].map(tbl_PLD_original_DHLG.set_index('TRACKING_NUMBER')['NQD_DHLG_OTHER'])\n",
    "\n",
    "tbl_DHLG1['FRT_DHL'] = tbl_DHLG1['TRACKING_NUMBER'].map(tbl_PLD_original_DHLG1.set_index('TRACKING_NUMBER')['FRT_DHLG1'])\n",
    "tbl_DHLG1['FSC%_DHL'] = tbl_DHLG1['TRACKING_NUMBER'].map(tbl_PLD_original_DHLG1.set_index('TRACKING_NUMBER')['FSC%_DHLG1'])\n",
    "tbl_DHLG1['FSC_DHL'] = tbl_DHLG1['TRACKING_NUMBER'].map(tbl_PLD_original_DHLG1.set_index('TRACKING_NUMBER')['FSC_DHLG1'])\n",
    "tbl_DHLG1['NQD_GIRTH_AND_L_DHL'] = tbl_DHLG1['TRACKING_NUMBER'].map(tbl_PLD_original_DHLG1.set_index('TRACKING_NUMBER')['NQD_DHLG1_GIRTH_AND_L'])\n",
    "tbl_DHLG1['NQD_OTHER_DHL'] = tbl_DHLG1['TRACKING_NUMBER'].map(tbl_PLD_original_DHLG1.set_index('TRACKING_NUMBER')['NQD_DHLG1_OTHER'])\n",
    "\n",
    "tbl_DHLE['FRT_DHL'] = tbl_DHLE['TRACKING_NUMBER'].map(tbl_PLD_original_DHLE.set_index('TRACKING_NUMBER')['FRT_DHLE'])\n",
    "tbl_DHLE['FSC%_DHL'] = tbl_DHLE['TRACKING_NUMBER'].map(tbl_PLD_original_DHLE.set_index('TRACKING_NUMBER')['FSC%_DHLE'])\n",
    "tbl_DHLE['FSC_DHL'] = tbl_DHLE['TRACKING_NUMBER'].map(tbl_PLD_original_DHLE.set_index('TRACKING_NUMBER')['FSC_DHLE'])\n",
    "tbl_DHLE['NQD_GIRTH_AND_L_DHL'] = tbl_DHLE['TRACKING_NUMBER'].map(tbl_PLD_original_DHLE.set_index('TRACKING_NUMBER')['NQD_DHLE_GIRTH_AND_L'])\n",
    "tbl_DHLE['NQD_OTHER_DHL'] = tbl_DHLE['TRACKING_NUMBER'].map(tbl_PLD_original_DHLE.set_index('TRACKING_NUMBER')['NQD_DHLE_OTHER'])\n",
    "\n",
    "tbl_DHLE1['FRT_DHL'] = tbl_DHLE1['TRACKING_NUMBER'].map(tbl_PLD_original_DHLE1.set_index('TRACKING_NUMBER')['FRT_DHLE1'])\n",
    "tbl_DHLE1['FSC%_DHL'] = tbl_DHLE1['TRACKING_NUMBER'].map(tbl_PLD_original_DHLE1.set_index('TRACKING_NUMBER')['FSC%_DHLE1'])\n",
    "tbl_DHLE1['FSC_DHL'] = tbl_DHLE1['TRACKING_NUMBER'].map(tbl_PLD_original_DHLE1.set_index('TRACKING_NUMBER')['FSC_DHLE1'])\n",
    "tbl_DHLE1['NQD_GIRTH_AND_L_DHL'] = tbl_DHLE1['TRACKING_NUMBER'].map(tbl_PLD_original_DHLE1.set_index('TRACKING_NUMBER')['NQD_DHLE1_GIRTH_AND_L'])\n",
    "tbl_DHLE1['NQD_OTHER_DHL'] = tbl_DHLE1['TRACKING_NUMBER'].map(tbl_PLD_original_DHLE1.set_index('TRACKING_NUMBER')['NQD_DHLE1_OTHER'])\n",
    "\n",
    "tbl_DHLEM['FRT_DHL'] = tbl_DHLEM['TRACKING_NUMBER'].map(tbl_PLD_original_DHLEM.set_index('TRACKING_NUMBER')['FRT_DHLEM'])\n",
    "tbl_DHLEM['FSC%_DHL'] = tbl_DHLEM['TRACKING_NUMBER'].map(tbl_PLD_original_DHLEM.set_index('TRACKING_NUMBER')['FSC%_DHLEM'])\n",
    "tbl_DHLEM['FSC_DHL'] = tbl_DHLEM['TRACKING_NUMBER'].map(tbl_PLD_original_DHLEM.set_index('TRACKING_NUMBER')['FSC_DHLEM'])\n",
    "tbl_DHLEM['NQD_GIRTH_AND_L_DHL'] = tbl_DHLEM['TRACKING_NUMBER'].map(tbl_PLD_original_DHLEM.set_index('TRACKING_NUMBER')['NQD_DHLEM_GIRTH_AND_L'])\n",
    "tbl_DHLEM['NQD_OTHER_DHL'] = tbl_DHLEM['TRACKING_NUMBER'].map(tbl_PLD_original_DHLEM.set_index('TRACKING_NUMBER')['NQD_DHLEM_OTHER'])\n",
    "\n",
    "\n",
    "tbl_USPSAG['FRT_USPS'] = tbl_USPSAG['TRACKING_NUMBER'].map(tbl_PLD_original_USPSAG.set_index('TRACKING_NUMBER')['FRT_USPSAG'])\n",
    "tbl_USPSAG['EXTRA_USPS_1'] = tbl_USPSAG['TRACKING_NUMBER'].map(tbl_PLD_original_USPSAG.set_index('TRACKING_NUMBER')['EXTRA_USPSAG_1'])\n",
    "tbl_USPSAG['EXTRA_USPS_2'] = tbl_USPSAG['TRACKING_NUMBER'].map(tbl_PLD_original_USPSAG.set_index('TRACKING_NUMBER')['EXTRA_USPSAG_2'])\n",
    "\n",
    "tbl_USPSAP['FRT_USPS'] = tbl_USPSAP['TRACKING_NUMBER'].map(tbl_PLD_original_USPSAP.set_index('TRACKING_NUMBER')['FRT_USPSAP'])\n",
    "tbl_USPSAP['EXTRA_USPS_1'] = tbl_USPSAP['TRACKING_NUMBER'].map(tbl_PLD_original_USPSAP.set_index('TRACKING_NUMBER')['EXTRA_USPSAP_1'])\n",
    "tbl_USPSAP['EXTRA_USPS_2'] = tbl_USPSAP['TRACKING_NUMBER'].map(tbl_PLD_original_USPSAP.set_index('TRACKING_NUMBER')['EXTRA_USPSAP_2'])\n",
    "\n",
    "\n",
    "tbl_MIPLE['FRT_MIPLE'] = tbl_MIPLE['TRACKING_NUMBER'].map(tbl_PLD_original_MIPLE.set_index('TRACKING_NUMBER')['FRT_MIPLE'])\n",
    "tbl_MIPLE['FSC_MIPLE'] = tbl_MIPLE['TRACKING_NUMBER'].map(tbl_PLD_original_MIPLE.set_index('TRACKING_NUMBER')['FSC_MIPLE'])\n",
    "tbl_MIPLE['NQD_MIPLE'] = tbl_MIPLE['TRACKING_NUMBER'].map(tbl_PLD_original_MIPLE.set_index('TRACKING_NUMBER')['NQD_MIPLE'])\n",
    "\n",
    "tbl_MIPH['FRT_MIPH'] = tbl_MIPH['TRACKING_NUMBER'].map(tbl_PLD_original_MIPH.set_index('TRACKING_NUMBER')['FRT_MIPH'])\n",
    "tbl_MIPH['FSC_MIPH'] = tbl_MIPH['TRACKING_NUMBER'].map(tbl_PLD_original_MIPH.set_index('TRACKING_NUMBER')['FSC_MIPH'])\n",
    "tbl_MIPH['NQD_MIPH'] = tbl_MIPH['TRACKING_NUMBER'].map(tbl_PLD_original_MIPH.set_index('TRACKING_NUMBER')['NQD_MIPH'])\n",
    "\n",
    "\n",
    "\n",
    "#tbl_PLD_original_REDE, tbl_PLD_original_RED, tbl_PLD_original_REDS, tbl_PLD_original_2DAM, tbl_PLD_original_BLUE\n",
    "#tbl_PLD_original_ORNG, tbl_PLD_original_GRND, tbl_PLD_original_GRES, tbl_PLD_original_SRPT1, tbl_PLD_original_SRPT\n",
    "#tbl_PLD_original_DHLG, tbl_PLD_original_DHLG1, tbl_PLD_original_DHLE, tbl_PLD_original_DHLE1, tbl_PLD_original_DHLEM\n",
    "#tbl_PLD_original_USPSAG, tbl_PLD_original_USPSAP, tbl_PLD_original_MIPLE, tbl_PLD_original_MIPH\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 818,
   "id": "sharing-liverpool",
   "metadata": {},
   "outputs": [],
   "source": [
    "tbl_PLD_rerated_bestWay = pd.concat([\n",
    "   tbl_REDE, tbl_RED, tbl_REDS, tbl_2DAM, \n",
    "    tbl_BLUE, \n",
    "    tbl_ORNG, tbl_GRND, tbl_GRES, \n",
    "   tbl_SRPT1, tbl_SRPT, tbl_1STO, tbl_PROV, tbl_STOV, tbl_2AM, \n",
    "    tbl_2DAY, \n",
    "    tbl_EXSA, tbl_GRNDF, tbl_HOME, \n",
    "   tbl_SPST1, tbl_SPST,\n",
    "    tbl_DHLG, tbl_DHLG1, \n",
    "    tbl_DHLE, tbl_DHLE1, tbl_DHLEM, \n",
    "    tbl_USPSAG, \n",
    "   tbl_USPSAP, \n",
    "   tbl_MIPLE, tbl_MIPH\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 819,
   "id": "illegal-lesbian",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace 'inf' with an empty string in selected columns\n",
    "columns_to_replace = [\n",
    "   'TOTAL_REDE', 'TOTAL_RED', 'TOTAL_REDS', 'TOTAL_2DAM', \n",
    "    'TOTAL_BLUE', \n",
    "'TOTAL_ORNG', 'TOTAL_GRND', 'TOTAL_GRES', \n",
    " 'TOTAL_SRPT1','TOTAL_SRPT', \n",
    "    'TOTAL_1STO', 'TOTAL_PROV', 'TOTAL_STOV', 'TOTAL_2AM', \n",
    "    'TOTAL_2DAY', \n",
    "'TOTAL_EXSA', 'TOTAL_GRNDF', 'TOTAL_HOME', \n",
    " 'TOTAL_SPST1','TOTAL_SPST', \n",
    "    'TOTAL_DHLG', 'TOTAL_DHLG1', \n",
    "    'TOTAL_DHLE', 'TOTAL_DHLE1',\n",
    "    'TOTAL_DHLEM', \n",
    "'TOTAL_USPSAG', \n",
    "    'TOTAL_USPSAP', \n",
    "    'TOTAL_MIPLE', 'TOTAL_MIPH',\n",
    "    'CARRIER_COST_MIN']\n",
    "tbl_PLD_rerated_bestWay[columns_to_replace] = tbl_PLD_rerated_bestWay[columns_to_replace].replace(np.inf, '')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 820,
   "id": "2e845d1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update the values\n",
    "tbl_PLD_rerated_bestWay.loc[tbl_PLD_rerated_bestWay['SELECTED_SERVICE'] == 'TOTAL_REDE', 'SELECTED_SERVICE'] = 'UPS NDA Early'\n",
    "tbl_PLD_rerated_bestWay.loc[tbl_PLD_rerated_bestWay['SELECTED_SERVICE'] == 'TOTAL_RED', 'SELECTED_SERVICE'] = 'UPS NDA'\n",
    "tbl_PLD_rerated_bestWay.loc[tbl_PLD_rerated_bestWay['SELECTED_SERVICE'] == 'TOTAL_REDS', 'SELECTED_SERVICE'] = 'UPS NDA Saver'\n",
    "tbl_PLD_rerated_bestWay.loc[tbl_PLD_rerated_bestWay['SELECTED_SERVICE'] == 'TOTAL_2DAM', 'SELECTED_SERVICE'] = 'UPS 2DA A.M.'\n",
    "tbl_PLD_rerated_bestWay.loc[tbl_PLD_rerated_bestWay['SELECTED_SERVICE'] == 'TOTAL_BLUE', 'SELECTED_SERVICE'] = 'UPS 2DA'\n",
    "\n",
    "tbl_PLD_rerated_bestWay.loc[tbl_PLD_rerated_bestWay['SELECTED_SERVICE'] == 'TOTAL_ORNG', 'SELECTED_SERVICE'] = 'UPS 3DA'\n",
    "tbl_PLD_rerated_bestWay.loc[tbl_PLD_rerated_bestWay['SELECTED_SERVICE'] == 'TOTAL_GRND', 'SELECTED_SERVICE'] = 'UPS Ground Commercial'\n",
    "tbl_PLD_rerated_bestWay.loc[tbl_PLD_rerated_bestWay['SELECTED_SERVICE'] == 'TOTAL_GRES', 'SELECTED_SERVICE'] = 'UPS Ground Residential'\n",
    "tbl_PLD_rerated_bestWay.loc[tbl_PLD_rerated_bestWay['SELECTED_SERVICE'] == 'TOTAL_SRPT1', 'SELECTED_SERVICE'] = 'UPS Surepost 1#>'\n",
    "tbl_PLD_rerated_bestWay.loc[tbl_PLD_rerated_bestWay['SELECTED_SERVICE'] == 'TOTAL_SRPT', 'SELECTED_SERVICE'] = 'UPS Surepost'\n",
    "\n",
    "tbl_PLD_rerated_bestWay.loc[tbl_PLD_rerated_bestWay['SELECTED_SERVICE'] == 'TOTAL_1STO', 'SELECTED_SERVICE'] = 'FedEx First Overnight'\n",
    "tbl_PLD_rerated_bestWay.loc[tbl_PLD_rerated_bestWay['SELECTED_SERVICE'] == 'TOTAL_PROV', 'SELECTED_SERVICE'] = 'FedEx Priority Overnight'\n",
    "tbl_PLD_rerated_bestWay.loc[tbl_PLD_rerated_bestWay['SELECTED_SERVICE'] == 'TOTAL_STOV', 'SELECTED_SERVICE'] = 'FedEx Standard Overnight'\n",
    "tbl_PLD_rerated_bestWay.loc[tbl_PLD_rerated_bestWay['SELECTED_SERVICE'] == 'TOTAL_2AM', 'SELECTED_SERVICE'] = 'FedEx 2nd Day AM'\n",
    "tbl_PLD_rerated_bestWay.loc[tbl_PLD_rerated_bestWay['SELECTED_SERVICE'] == 'TOTAL_2DAY', 'SELECTED_SERVICE'] = 'FedEx 2Day'\n",
    "\n",
    "tbl_PLD_rerated_bestWay.loc[tbl_PLD_rerated_bestWay['SELECTED_SERVICE'] == 'TOTAL_EXSA', 'SELECTED_SERVICE'] = 'FedEx Express Saver'\n",
    "tbl_PLD_rerated_bestWay.loc[tbl_PLD_rerated_bestWay['SELECTED_SERVICE'] == 'TOTAL_GRNDF', 'SELECTED_SERVICE'] = 'FedEx Ground'\n",
    "tbl_PLD_rerated_bestWay.loc[tbl_PLD_rerated_bestWay['SELECTED_SERVICE'] == 'TOTAL_HOME', 'SELECTED_SERVICE'] = 'FedEx Home Delivery'\n",
    "tbl_PLD_rerated_bestWay.loc[tbl_PLD_rerated_bestWay['SELECTED_SERVICE'] == 'TOTAL_SPST1', 'SELECTED_SERVICE'] = 'FedEx SmartPost 1#>'\n",
    "tbl_PLD_rerated_bestWay.loc[tbl_PLD_rerated_bestWay['SELECTED_SERVICE'] == 'TOTAL_SPST', 'SELECTED_SERVICE'] = 'FedEx SmartPost'\n",
    "\n",
    "\n",
    "tbl_PLD_rerated_bestWay.loc[tbl_PLD_rerated_bestWay['SELECTED_SERVICE'] == 'TOTAL_DHLG', 'SELECTED_SERVICE'] = 'DHL SmartMail Parcel Plus Ground 1-25'\n",
    "tbl_PLD_rerated_bestWay.loc[tbl_PLD_rerated_bestWay['SELECTED_SERVICE'] == 'TOTAL_DHLG1', 'SELECTED_SERVICE'] = 'DHL SmartMail Parcel Ground < 1lb'\n",
    "tbl_PLD_rerated_bestWay.loc[tbl_PLD_rerated_bestWay['SELECTED_SERVICE'] == 'TOTAL_DHLE', 'SELECTED_SERVICE'] = 'DHL SmartMail Parcel Plus Expedited 1-25'\n",
    "tbl_PLD_rerated_bestWay.loc[tbl_PLD_rerated_bestWay['SELECTED_SERVICE'] == 'TOTAL_DHLE1', 'SELECTED_SERVICE'] = 'DHL SmartMail Parcel Expedited  < 1lb'\n",
    "tbl_PLD_rerated_bestWay.loc[tbl_PLD_rerated_bestWay['SELECTED_SERVICE'] == 'TOTAL_DHLEM', 'SELECTED_SERVICE'] = 'DHL SmartMail Parcel Expedited Max'\n",
    "\n",
    "tbl_PLD_rerated_bestWay.loc[tbl_PLD_rerated_bestWay['SELECTED_SERVICE'] == 'TOTAL_USPSAG', 'SELECTED_SERVICE'] = 'USPS Auctane GRND'\n",
    "tbl_PLD_rerated_bestWay.loc[tbl_PLD_rerated_bestWay['SELECTED_SERVICE'] == 'TOTAL_USPSAP', 'SELECTED_SERVICE'] = 'USPS Auctane PM'\n",
    "\n",
    "tbl_PLD_rerated_bestWay.loc[tbl_PLD_rerated_bestWay['SELECTED_SERVICE'] == 'TOTAL_MIPLE', 'SELECTED_SERVICE'] = 'MI Parcel Select Lightweight Expedited'\n",
    "tbl_PLD_rerated_bestWay.loc[tbl_PLD_rerated_bestWay['SELECTED_SERVICE'] == 'TOTAL_MIPH', 'SELECTED_SERVICE'] = 'MI Parcel Select Heavyweight'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 821,
   "id": "aggressive-connection",
   "metadata": {},
   "outputs": [],
   "source": [
    "tbl_PLD_rerated_bestWay.to_csv('YEC_20%Other_9%CPP_BW.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 822,
   "id": "cf5d9c35",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(95509, 58)"
      ]
     },
     "execution_count": 822,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tbl_PLD_rerated_bestWay.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac201fc0",
   "metadata": {},
   "source": [
    "# ******End******"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "128a21e4",
   "metadata": {},
   "source": [
    "# Backup Materials"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec2fe1b8",
   "metadata": {},
   "source": [
    "\n",
    "| Carrier | Service | Residential | WeightType | RateType   |\n",
    "|:--------|:---------|:-------------|:------------|:------------|\n",
    "| UPS     | REDE (UPS NDA Early) / RED (UPS NDA) / REDS (UPS NDA Saver) / 2DAM (UPS 2DA A.M.) / BLUE (UPS 2DA) / ORNG (UPS 3DA) / SRPT (UPS Surepost) | both         | lb         | Published   |\n",
    "| UPS     | GRND (UPS Ground Commercial)                                                                                                              | commercial   | lb         | Published   |\n",
    "| UPS     | GRES (UPS Ground Residential)                                                                                                             | residential  | lb         | Published   |\n",
    "| UPS     | SRPT<1 (UPS Surepost 1#>)                                                                                                                 | both         | oz         | Published   |\n",
    "| DHL     | DHLG (DHL SmartMail Parcel Plus Ground 1-25)/DHLE (DHL SmartMail Parcel Plus Expedited 1-25) | both | lb | Barrett |\n",
    "| DHL     | DHLG<1 (DHL SmartMail Parcel Ground < 1lb)/DHLE<1 (DHL SmartMail Parcel Expedited  < 1lb)/DHLEM (DHL SmartMail Parcel Expedited Max) | both | lb | Barrett |\n",
    "| USPS    | USPSAG (USPS Auctane GRND) | both | lb | Barrett |\n",
    "| USPS    | USPSAP (USPS Auctane PM)| both | lb | Barrett |\n",
    "| MI      | MIPLE (MI Parcel Select Lightweight Expedited)|  both | oz | Barrett |\n",
    "| MI      | MIPH (MI Parcel Select Heavyweight)|  both | lb | Barrett |\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fa58b1d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "# Read carrierRateCard_2023 and remake the tables showing below columns.\n",
    "# SERVICE_CODE, SERVICE_PLAINTEXT, ZONE, WEIGHT_OZ, WEIGHT_LB, PUBLISHED_RATES, VERSION, START_DATE, END_DATE\n",
    "xls = pd.ExcelFile('carrierRateCard_2024.xlsx')\n",
    "# Load UPS rate card\n",
    "df1 = pd.read_excel(xls, 'REDE')\n",
    "df2 = pd.read_excel(xls, 'RED')\n",
    "df3 = pd.read_excel(xls, 'REDS')\n",
    "df4 = pd.read_excel(xls, '2DAM')\n",
    "df5 = pd.read_excel(xls, 'BLUE')\n",
    "df6 = pd.read_excel(xls, 'ORNG')\n",
    "df7 = pd.read_excel(xls, 'GRND')\n",
    "df8 = pd.read_excel(xls, 'GRES')\n",
    "df9 = pd.read_excel(xls, 'SRPT<1')\n",
    "df10 = pd.read_excel(xls, 'SRPT')\n",
    "\n",
    "# Load USPS rate card\n",
    "df16 = pd.read_excel(xls, 'USPSAG_CPP')\n",
    "df17 = pd.read_excel(xls, 'USPSAP_CPP')\n",
    "df20 = pd.read_excel(xls, 'USPSAG_CPPCI')\n",
    "df21 = pd.read_excel(xls, 'USPSAP_CPPCI')\n",
    "df22 = pd.read_excel(xls, 'USPSAG_AUCT')\n",
    "df23 = pd.read_excel(xls, 'USPSAP_AUCT')\n",
    "df24 = pd.read_excel(xls, 'USPSAG_AUCTCI')\n",
    "df25 = pd.read_excel(xls, 'USPSAP_AUCTCI')\n",
    "\n",
    "# Load MI rate card\n",
    "df18 = pd.read_excel(xls, 'MIPLE')\n",
    "df19 = pd.read_excel(xls, 'MIPH')\n",
    "\n",
    "# Load DHL rate card\n",
    "# 5114358_dhl_sm_parcel_Franklin\n",
    "df11_1 = pd.read_excel(xls, 'DHLG_5114358')\n",
    "df12_2 = pd.read_excel(xls, 'DHLG<1_5114358')\n",
    "df13_3 = pd.read_excel(xls, 'DHLE_5114358')\n",
    "df14_4 = pd.read_excel(xls, 'DHLE<1_5114358')\n",
    "df15_5 = pd.read_excel(xls, 'DHLEM_5114358')\n",
    "\n",
    "# 5122444_dhl_sm_parcel_Somerset\n",
    "df11_6 = pd.read_excel(xls, 'DHLG_5122444')\n",
    "df12_7 = pd.read_excel(xls, 'DHLG<1_5122444')\n",
    "df13_8 = pd.read_excel(xls, 'DHLE_5122444')\n",
    "df14_9 = pd.read_excel(xls, 'DHLE<1_5122444')\n",
    "df15_10 = pd.read_excel(xls, 'DHLEM_5122444')\n",
    "\n",
    "# 5122723_dhl_sm_parcel_8150 Nail Olive Branch\n",
    "df11_11 = pd.read_excel(xls, 'DHLG_5122723')\n",
    "df12_12 = pd.read_excel(xls, 'DHLG<1_5122723')\n",
    "df13_13 = pd.read_excel(xls, 'DHLE_5122723')\n",
    "df14_14 = pd.read_excel(xls, 'DHLE<1_5122723')\n",
    "df15_15 = pd.read_excel(xls, 'DHLEM_5122723')\n",
    "\n",
    "# 5122739_dhl_sm_parcel_Nail rd Olive branch\n",
    "df11_16 = pd.read_excel(xls, 'DHLG_5122739')\n",
    "df12_17 = pd.read_excel(xls, 'DHLG<1_5122739')\n",
    "df13_18 = pd.read_excel(xls, 'DHLE_5122739')\n",
    "df14_19 = pd.read_excel(xls, 'DHLE<1_5122739')\n",
    "df15_20 = pd.read_excel(xls, 'DHLEM_5122739')\n",
    "\n",
    "# 5122855_dhl_sm_parcel_Garland TX\n",
    "df11_21 = pd.read_excel(xls, 'DHLG_5122855')\n",
    "df12_22 = pd.read_excel(xls, 'DHLG<1_5122855')\n",
    "df13_23 = pd.read_excel(xls, 'DHLE_5122855')\n",
    "df14_24 = pd.read_excel(xls, 'DHLE<1_5122855')\n",
    "df15_25 = pd.read_excel(xls, 'DHLEM_5122855')\n",
    "\n",
    "# 5122890_dhl_sm_parcel_Oofos\n",
    "df11_26 = pd.read_excel(xls, 'DHLG_5122890')\n",
    "df12_27 = pd.read_excel(xls, 'DHLG<1_5122890')\n",
    "df13_28 = pd.read_excel(xls, 'DHLE_5122890')\n",
    "df14_29 = pd.read_excel(xls, 'DHLE<1_5122890')\n",
    "df15_30 = pd.read_excel(xls, 'DHLEM_5122890')\n",
    "\n",
    "# 5122893_dhl_sm_parcel Memphis\n",
    "df11_31 = pd.read_excel(xls, 'DHLG_5122893')\n",
    "df12_32 = pd.read_excel(xls, 'DHLG<1_5122893')\n",
    "df13_33 = pd.read_excel(xls, 'DHLE_5122893')\n",
    "df14_34 = pd.read_excel(xls, 'DHLE<1_5122893')\n",
    "df15_35 = pd.read_excel(xls, 'DHLEM_5122893')\n",
    "\n",
    "# 5123280_dhl_sm_parcel_MAN\n",
    "df11_36 = pd.read_excel(xls, 'DHLG_5123280')\n",
    "df12_37 = pd.read_excel(xls, 'DHLG<1_5123280')\n",
    "df13_38 = pd.read_excel(xls, 'DHLE_5123280')\n",
    "df14_39 = pd.read_excel(xls, 'DHLE<1_5123280')\n",
    "df15_40 = pd.read_excel(xls, 'DHLEM_5123280')\n",
    "\n",
    "# 5123282_dhl_sm_parcel Bridgewater\n",
    "df11_41 = pd.read_excel(xls, 'DHLG_5123282')\n",
    "df12_42 = pd.read_excel(xls, 'DHLG<1_5123282')\n",
    "df13_43 = pd.read_excel(xls, 'DHLE_5123282')\n",
    "df14_44 = pd.read_excel(xls, 'DHLE<1_5123282')\n",
    "df15_45 = pd.read_excel(xls, 'DHLEM_5123282')\n",
    "\n",
    "# 5123283_dhl_sm_parcel_Curtis Bay\n",
    "df11_46 = pd.read_excel(xls, 'DHLG_5123283')\n",
    "df12_47 = pd.read_excel(xls, 'DHLG<1_5123283')\n",
    "df13_48 = pd.read_excel(xls, 'DHLE_5123283')\n",
    "df14_49 = pd.read_excel(xls, 'DHLE<1_5123283')\n",
    "df15_50 = pd.read_excel(xls, 'DHLEM_5123283')\n",
    "\n",
    "# 5123283_dhl_sm_parcel_Curtis Bay\n",
    "df11_51 = pd.read_excel(xls, 'DHLG_5123556')\n",
    "df12_52 = pd.read_excel(xls, 'DHLG<1_5123556')\n",
    "df13_53 = pd.read_excel(xls, 'DHLE_5123556')\n",
    "df14_54 = pd.read_excel(xls, 'DHLE<1_5123556')\n",
    "df15_55 = pd.read_excel(xls, 'DHLEM_5123556')\n",
    "\n",
    "\n",
    "df26 = pd.read_excel(xls, '1STO')\n",
    "df27 = pd.read_excel(xls, 'PROV')\n",
    "df28 = pd.read_excel(xls, 'STOV')\n",
    "df29 = pd.read_excel(xls, '2AM')\n",
    "df30 = pd.read_excel(xls, '2DAY')\n",
    "df31 = pd.read_excel(xls, 'EXSA')\n",
    "df32 = pd.read_excel(xls, 'GRNDF')\n",
    "df33 = pd.read_excel(xls, 'HOME')\n",
    "df34 = pd.read_excel(xls, 'SPST<1')\n",
    "df35 = pd.read_excel(xls, 'SPST')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "158ea7a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-arrange the tables\n",
    "# REDE, Barrett Rates = Published Rates. \n",
    "# made sure the tables are completely loaded into the dataframe by checking df1.shape\n",
    "\n",
    "df1 = df1.set_index('Zones').unstack().rename_axis(('ZONE','WEIGHT_LB')).reset_index(name='PUBLISHED_RATES')\n",
    "df2 = df2.set_index('Zones').unstack().rename_axis(('ZONE','WEIGHT_LB')).reset_index(name='PUBLISHED_RATES')\n",
    "df3 = df3.set_index('Zones').unstack().rename_axis(('ZONE','WEIGHT_LB')).reset_index(name='PUBLISHED_RATES')\n",
    "df4 = df4.set_index('Zones').unstack().rename_axis(('ZONE','WEIGHT_LB')).reset_index(name='PUBLISHED_RATES')\n",
    "df5 = df5.set_index('Zones').unstack().rename_axis(('ZONE','WEIGHT_LB')).reset_index(name='PUBLISHED_RATES')\n",
    "df6 = df6.set_index('Zones').unstack().rename_axis(('ZONE','WEIGHT_LB')).reset_index(name='PUBLISHED_RATES')\n",
    "df7 = df7.set_index('Zones').unstack().rename_axis(('ZONE','WEIGHT_LB')).reset_index(name='PUBLISHED_RATES')\n",
    "df8 = df8.set_index('Zones').unstack().rename_axis(('ZONE','WEIGHT_LB')).reset_index(name='PUBLISHED_RATES')\n",
    "df9 = df9.set_index('Zones').unstack().rename_axis(('ZONE','WEIGHT_OZ')).reset_index(name='PUBLISHED_RATES')\n",
    "df10 = df10.set_index('Zones').unstack().rename_axis(('ZONE','WEIGHT_LB')).reset_index(name='PUBLISHED_RATES')\n",
    "\n",
    "\n",
    "# put cpp price as Barrett Rate to ease the calculation\n",
    "df16 = df16.set_index('Zones').unstack().rename_axis(('ZONE','WEIGHT_LB')).reset_index(name='BARRETT_RATES')\n",
    "df17 = df17.set_index('Zones').unstack().rename_axis(('ZONE','WEIGHT_LB')).reset_index(name='BARRETT_RATES')\n",
    "df20 = df20.set_index('Zones').unstack().rename_axis(('ZONE','CUBIC_FT')).reset_index(name='BARRETT_RATES')\n",
    "df21 = df21.set_index('Zones').unstack().rename_axis(('ZONE','CUBIC_FT')).reset_index(name='BARRETT_RATES')\n",
    "df22 = df22.set_index('Zones').unstack().rename_axis(('ZONE','WEIGHT_LB')).reset_index(name='BARRETT_RATES')\n",
    "df23 = df23.set_index('Zones').unstack().rename_axis(('ZONE','WEIGHT_LB')).reset_index(name='BARRETT_RATES')\n",
    "df24 = df24.set_index('Zones').unstack().rename_axis(('ZONE','CUBIC_FT')).reset_index(name='BARRETT_RATES')\n",
    "df25 = df25.set_index('Zones').unstack().rename_axis(('ZONE','CUBIC_FT')).reset_index(name='BARRETT_RATES')\n",
    "\n",
    "df18 = df18.set_index('Zones').unstack().rename_axis(('ZONE','WEIGHT_OZ')).reset_index(name='BARRETT_RATES')\n",
    "df19 = df19.set_index('Zones').unstack().rename_axis(('ZONE','WEIGHT_LB')).reset_index(name='BARRETT_RATES')\n",
    "\n",
    "df11_1 = df11_1.set_index('Zones').unstack().rename_axis(('ZONE','WEIGHT_LB')).reset_index(name='BARRETT_RATES')\n",
    "df12_2 = df12_2.set_index('Zones').unstack().rename_axis(('ZONE','WEIGHT_OZ')).reset_index(name='BARRETT_RATES')\n",
    "df13_3 = df13_3.set_index('Zones').unstack().rename_axis(('ZONE','WEIGHT_LB')).reset_index(name='BARRETT_RATES')\n",
    "df14_4 = df14_4.set_index('Zones').unstack().rename_axis(('ZONE','WEIGHT_OZ')).reset_index(name='BARRETT_RATES')\n",
    "df15_5 = df15_5.set_index('Zones').unstack().rename_axis(('ZONE','WEIGHT_OZ')).reset_index(name='BARRETT_RATES')\n",
    "\n",
    "df11_6 = df11_6.set_index('Zones').unstack().rename_axis(('ZONE','WEIGHT_LB')).reset_index(name='BARRETT_RATES')\n",
    "df12_7 = df12_7.set_index('Zones').unstack().rename_axis(('ZONE','WEIGHT_OZ')).reset_index(name='BARRETT_RATES')\n",
    "df13_8 = df13_8.set_index('Zones').unstack().rename_axis(('ZONE','WEIGHT_LB')).reset_index(name='BARRETT_RATES')\n",
    "df14_9 = df14_9.set_index('Zones').unstack().rename_axis(('ZONE','WEIGHT_OZ')).reset_index(name='BARRETT_RATES')\n",
    "df15_10 = df15_10.set_index('Zones').unstack().rename_axis(('ZONE','WEIGHT_OZ')).reset_index(name='BARRETT_RATES')\n",
    "\n",
    "df11_11 = df11_11.set_index('Zones').unstack().rename_axis(('ZONE','WEIGHT_LB')).reset_index(name='BARRETT_RATES')\n",
    "df12_12 = df12_12.set_index('Zones').unstack().rename_axis(('ZONE','WEIGHT_OZ')).reset_index(name='BARRETT_RATES')\n",
    "df13_13 = df13_13.set_index('Zones').unstack().rename_axis(('ZONE','WEIGHT_LB')).reset_index(name='BARRETT_RATES')\n",
    "df14_14 = df14_14.set_index('Zones').unstack().rename_axis(('ZONE','WEIGHT_OZ')).reset_index(name='BARRETT_RATES')\n",
    "df15_15 = df15_15.set_index('Zones').unstack().rename_axis(('ZONE','WEIGHT_OZ')).reset_index(name='BARRETT_RATES')\n",
    "\n",
    "df11_16 = df11_16.set_index('Zones').unstack().rename_axis(('ZONE','WEIGHT_LB')).reset_index(name='BARRETT_RATES')\n",
    "df12_17 = df12_17.set_index('Zones').unstack().rename_axis(('ZONE','WEIGHT_OZ')).reset_index(name='BARRETT_RATES')\n",
    "df13_18 = df13_18.set_index('Zones').unstack().rename_axis(('ZONE','WEIGHT_LB')).reset_index(name='BARRETT_RATES')\n",
    "df14_19 = df14_19.set_index('Zones').unstack().rename_axis(('ZONE','WEIGHT_OZ')).reset_index(name='BARRETT_RATES')\n",
    "df15_20 = df15_20.set_index('Zones').unstack().rename_axis(('ZONE','WEIGHT_OZ')).reset_index(name='BARRETT_RATES')\n",
    "\n",
    "df11_21 = df11_21.set_index('Zones').unstack().rename_axis(('ZONE','WEIGHT_LB')).reset_index(name='BARRETT_RATES')\n",
    "df12_22 = df12_22.set_index('Zones').unstack().rename_axis(('ZONE','WEIGHT_OZ')).reset_index(name='BARRETT_RATES')\n",
    "df13_23 = df13_23.set_index('Zones').unstack().rename_axis(('ZONE','WEIGHT_LB')).reset_index(name='BARRETT_RATES')\n",
    "df14_24 = df14_24.set_index('Zones').unstack().rename_axis(('ZONE','WEIGHT_OZ')).reset_index(name='BARRETT_RATES')\n",
    "df15_25 = df15_25.set_index('Zones').unstack().rename_axis(('ZONE','WEIGHT_OZ')).reset_index(name='BARRETT_RATES')\n",
    "\n",
    "df11_26 = df11_26.set_index('Zones').unstack().rename_axis(('ZONE','WEIGHT_LB')).reset_index(name='BARRETT_RATES')\n",
    "df12_27 = df12_27.set_index('Zones').unstack().rename_axis(('ZONE','WEIGHT_OZ')).reset_index(name='BARRETT_RATES')\n",
    "df13_28 = df13_28.set_index('Zones').unstack().rename_axis(('ZONE','WEIGHT_LB')).reset_index(name='BARRETT_RATES')\n",
    "df14_29 = df14_29.set_index('Zones').unstack().rename_axis(('ZONE','WEIGHT_OZ')).reset_index(name='BARRETT_RATES')\n",
    "df15_30 = df15_30.set_index('Zones').unstack().rename_axis(('ZONE','WEIGHT_OZ')).reset_index(name='BARRETT_RATES')\n",
    "\n",
    "df11_31 = df11_31.set_index('Zones').unstack().rename_axis(('ZONE','WEIGHT_LB')).reset_index(name='BARRETT_RATES')\n",
    "df12_32 = df12_32.set_index('Zones').unstack().rename_axis(('ZONE','WEIGHT_OZ')).reset_index(name='BARRETT_RATES')\n",
    "df13_33 = df13_33.set_index('Zones').unstack().rename_axis(('ZONE','WEIGHT_LB')).reset_index(name='BARRETT_RATES')\n",
    "df14_34 = df14_34.set_index('Zones').unstack().rename_axis(('ZONE','WEIGHT_OZ')).reset_index(name='BARRETT_RATES')\n",
    "df15_35 = df15_35.set_index('Zones').unstack().rename_axis(('ZONE','WEIGHT_OZ')).reset_index(name='BARRETT_RATES')\n",
    "\n",
    "df11_36 = df11_36.set_index('Zones').unstack().rename_axis(('ZONE','WEIGHT_LB')).reset_index(name='BARRETT_RATES')\n",
    "df12_37 = df12_37.set_index('Zones').unstack().rename_axis(('ZONE','WEIGHT_OZ')).reset_index(name='BARRETT_RATES')\n",
    "df13_38 = df13_38.set_index('Zones').unstack().rename_axis(('ZONE','WEIGHT_LB')).reset_index(name='BARRETT_RATES')\n",
    "df14_39 = df14_39.set_index('Zones').unstack().rename_axis(('ZONE','WEIGHT_OZ')).reset_index(name='BARRETT_RATES')\n",
    "df15_40 = df15_40.set_index('Zones').unstack().rename_axis(('ZONE','WEIGHT_OZ')).reset_index(name='BARRETT_RATES')\n",
    "\n",
    "df11_41 = df11_41.set_index('Zones').unstack().rename_axis(('ZONE','WEIGHT_LB')).reset_index(name='BARRETT_RATES')\n",
    "df12_42 = df12_42.set_index('Zones').unstack().rename_axis(('ZONE','WEIGHT_OZ')).reset_index(name='BARRETT_RATES')\n",
    "df13_43 = df13_43.set_index('Zones').unstack().rename_axis(('ZONE','WEIGHT_LB')).reset_index(name='BARRETT_RATES')\n",
    "df14_44 = df14_44.set_index('Zones').unstack().rename_axis(('ZONE','WEIGHT_OZ')).reset_index(name='BARRETT_RATES')\n",
    "df15_45 = df15_45.set_index('Zones').unstack().rename_axis(('ZONE','WEIGHT_OZ')).reset_index(name='BARRETT_RATES')\n",
    "\n",
    "df11_46 = df11_46.set_index('Zones').unstack().rename_axis(('ZONE','WEIGHT_LB')).reset_index(name='BARRETT_RATES')\n",
    "df12_47 = df12_47.set_index('Zones').unstack().rename_axis(('ZONE','WEIGHT_OZ')).reset_index(name='BARRETT_RATES')\n",
    "df13_48 = df13_48.set_index('Zones').unstack().rename_axis(('ZONE','WEIGHT_LB')).reset_index(name='BARRETT_RATES')\n",
    "df14_49 = df14_49.set_index('Zones').unstack().rename_axis(('ZONE','WEIGHT_OZ')).reset_index(name='BARRETT_RATES')\n",
    "df15_50 = df15_50.set_index('Zones').unstack().rename_axis(('ZONE','WEIGHT_OZ')).reset_index(name='BARRETT_RATES')\n",
    "\n",
    "df11_51 = df11_51.set_index('Zones').unstack().rename_axis(('ZONE','WEIGHT_LB')).reset_index(name='BARRETT_RATES')\n",
    "df12_52 = df12_52.set_index('Zones').unstack().rename_axis(('ZONE','WEIGHT_OZ')).reset_index(name='BARRETT_RATES')\n",
    "df13_53 = df13_53.set_index('Zones').unstack().rename_axis(('ZONE','WEIGHT_LB')).reset_index(name='BARRETT_RATES')\n",
    "df14_54 = df14_54.set_index('Zones').unstack().rename_axis(('ZONE','WEIGHT_OZ')).reset_index(name='BARRETT_RATES')\n",
    "df15_55 = df15_55.set_index('Zones').unstack().rename_axis(('ZONE','WEIGHT_OZ')).reset_index(name='BARRETT_RATES')\n",
    "\n",
    "df26 = df26.set_index('Zones').unstack().rename_axis(('ZONE','WEIGHT_LB')).reset_index(name='PUBLISHED_RATES')\n",
    "df27 = df27.set_index('Zones').unstack().rename_axis(('ZONE','WEIGHT_LB')).reset_index(name='PUBLISHED_RATES')\n",
    "df28 = df28.set_index('Zones').unstack().rename_axis(('ZONE','WEIGHT_LB')).reset_index(name='PUBLISHED_RATES')\n",
    "df29 = df29.set_index('Zones').unstack().rename_axis(('ZONE','WEIGHT_LB')).reset_index(name='PUBLISHED_RATES')\n",
    "df30 = df30.set_index('Zones').unstack().rename_axis(('ZONE','WEIGHT_LB')).reset_index(name='PUBLISHED_RATES')\n",
    "df31 = df31.set_index('Zones').unstack().rename_axis(('ZONE','WEIGHT_LB')).reset_index(name='PUBLISHED_RATES')\n",
    "df32 = df32.set_index('Zones').unstack().rename_axis(('ZONE','WEIGHT_LB')).reset_index(name='PUBLISHED_RATES')\n",
    "df33 = df33.set_index('Zones').unstack().rename_axis(('ZONE','WEIGHT_LB')).reset_index(name='PUBLISHED_RATES')\n",
    "df34 = df34.set_index('Zones').unstack().rename_axis(('ZONE','WEIGHT_OZ')).reset_index(name='PUBLISHED_RATES')\n",
    "df35 = df35.set_index('Zones').unstack().rename_axis(('ZONE','WEIGHT_LB')).reset_index(name='PUBLISHED_RATES')\n",
    "\n",
    "\n",
    "# Make sure the dataframes have both weight in lb and oz columns\n",
    "df1['WEIGHT_OZ'] = df1['WEIGHT_LB']*16\n",
    "df2['WEIGHT_OZ'] = df2['WEIGHT_LB']*16\n",
    "df3['WEIGHT_OZ'] = df3['WEIGHT_LB']*16\n",
    "df4['WEIGHT_OZ'] = df4['WEIGHT_LB']*16\n",
    "df5['WEIGHT_OZ'] = df5['WEIGHT_LB']*16\n",
    "df6['WEIGHT_OZ'] = df6['WEIGHT_LB']*16\n",
    "df7['WEIGHT_OZ'] = df7['WEIGHT_LB']*16\n",
    "df8['WEIGHT_OZ'] = df8['WEIGHT_LB']*16\n",
    "df9['WEIGHT_LB'] = df9['WEIGHT_OZ']/16\n",
    "df10['WEIGHT_OZ'] = df10['WEIGHT_LB']*16\n",
    "\n",
    "df16['WEIGHT_OZ'] = df16['WEIGHT_LB']*16\n",
    "df17['WEIGHT_OZ'] = df17['WEIGHT_LB']*16\n",
    "\n",
    "df22['WEIGHT_OZ'] = df22['WEIGHT_LB']*16\n",
    "df23['WEIGHT_OZ'] = df23['WEIGHT_LB']*16\n",
    "\n",
    "df18['WEIGHT_LB'] = df18['WEIGHT_OZ']/16\n",
    "df19['WEIGHT_OZ'] = df19['WEIGHT_LB']*16\n",
    "\n",
    "df11_1['WEIGHT_OZ'] = df11_1['WEIGHT_LB']*16\n",
    "df12_2['WEIGHT_LB'] = df12_2['WEIGHT_OZ']/16\n",
    "df13_3['WEIGHT_OZ'] = df13_3['WEIGHT_LB']*16\n",
    "df14_4['WEIGHT_LB'] = df14_4['WEIGHT_OZ']/16\n",
    "df15_5['WEIGHT_LB'] = df15_5['WEIGHT_OZ']/16\n",
    "\n",
    "df11_6['WEIGHT_OZ'] = df11_6['WEIGHT_LB']*16\n",
    "df12_7['WEIGHT_LB'] = df12_7['WEIGHT_OZ']/16\n",
    "df13_8['WEIGHT_OZ'] = df13_8['WEIGHT_LB']*16\n",
    "df14_9['WEIGHT_LB'] = df14_9['WEIGHT_OZ']/16\n",
    "df15_10['WEIGHT_LB'] = df15_10['WEIGHT_OZ']/16\n",
    "\n",
    "df11_11['WEIGHT_OZ'] = df11_11['WEIGHT_LB']*16\n",
    "df12_12['WEIGHT_LB'] = df12_12['WEIGHT_OZ']/16\n",
    "df13_13['WEIGHT_OZ'] = df13_13['WEIGHT_LB']*16\n",
    "df14_14['WEIGHT_LB'] = df14_14['WEIGHT_OZ']/16\n",
    "df15_15['WEIGHT_LB'] = df15_15['WEIGHT_OZ']/16\n",
    "\n",
    "df11_16['WEIGHT_OZ'] = df11_16['WEIGHT_LB']*16\n",
    "df12_17['WEIGHT_LB'] = df12_17['WEIGHT_OZ']/16\n",
    "df13_18['WEIGHT_OZ'] = df13_18['WEIGHT_LB']*16\n",
    "df14_19['WEIGHT_LB'] = df14_19['WEIGHT_OZ']/16\n",
    "df15_20['WEIGHT_LB'] = df15_20['WEIGHT_OZ']/16\n",
    "\n",
    "df11_21['WEIGHT_OZ'] = df11_21['WEIGHT_LB']*16\n",
    "df12_22['WEIGHT_LB'] = df12_22['WEIGHT_OZ']/16\n",
    "df13_23['WEIGHT_OZ'] = df13_23['WEIGHT_LB']*16\n",
    "df14_24['WEIGHT_LB'] = df14_24['WEIGHT_OZ']/16\n",
    "df15_25['WEIGHT_LB'] = df15_25['WEIGHT_OZ']/16\n",
    "\n",
    "df11_26['WEIGHT_OZ'] = df11_26['WEIGHT_LB']*16\n",
    "df12_27['WEIGHT_LB'] = df12_27['WEIGHT_OZ']/16\n",
    "df13_28['WEIGHT_OZ'] = df13_28['WEIGHT_LB']*16\n",
    "df14_29['WEIGHT_LB'] = df14_29['WEIGHT_OZ']/16\n",
    "df15_30['WEIGHT_LB'] = df15_30['WEIGHT_OZ']/16\n",
    "\n",
    "df11_31['WEIGHT_OZ'] = df11_31['WEIGHT_LB']*16\n",
    "df12_32['WEIGHT_LB'] = df12_32['WEIGHT_OZ']/16\n",
    "df13_33['WEIGHT_OZ'] = df13_33['WEIGHT_LB']*16\n",
    "df14_34['WEIGHT_LB'] = df14_34['WEIGHT_OZ']/16\n",
    "df15_35['WEIGHT_LB'] = df15_35['WEIGHT_OZ']/16\n",
    "\n",
    "df11_36['WEIGHT_OZ'] = df11_36['WEIGHT_LB']*16\n",
    "df12_37['WEIGHT_LB'] = df12_37['WEIGHT_OZ']/16\n",
    "df13_38['WEIGHT_OZ'] = df13_38['WEIGHT_LB']*16\n",
    "df14_39['WEIGHT_LB'] = df14_39['WEIGHT_OZ']/16\n",
    "df15_40['WEIGHT_LB'] = df15_40['WEIGHT_OZ']/16\n",
    "\n",
    "df11_41['WEIGHT_OZ'] = df11_41['WEIGHT_LB']*16\n",
    "df12_42['WEIGHT_LB'] = df12_42['WEIGHT_OZ']/16\n",
    "df13_43['WEIGHT_OZ'] = df13_43['WEIGHT_LB']*16\n",
    "df14_44['WEIGHT_LB'] = df14_44['WEIGHT_OZ']/16\n",
    "df15_45['WEIGHT_LB'] = df15_45['WEIGHT_OZ']/16\n",
    "\n",
    "df11_46['WEIGHT_OZ'] = df11_46['WEIGHT_LB']*16\n",
    "df12_47['WEIGHT_LB'] = df12_47['WEIGHT_OZ']/16\n",
    "df13_48['WEIGHT_OZ'] = df13_48['WEIGHT_LB']*16\n",
    "df14_49['WEIGHT_LB'] = df14_49['WEIGHT_OZ']/16\n",
    "df15_50['WEIGHT_LB'] = df15_50['WEIGHT_OZ']/16\n",
    "\n",
    "\n",
    "df11_51['WEIGHT_OZ'] = df11_51['WEIGHT_LB']*16\n",
    "df12_52['WEIGHT_LB'] = df12_52['WEIGHT_OZ']/16\n",
    "df13_53['WEIGHT_OZ'] = df13_53['WEIGHT_LB']*16\n",
    "df14_54['WEIGHT_LB'] = df14_54['WEIGHT_OZ']/16\n",
    "df15_55['WEIGHT_LB'] = df15_55['WEIGHT_OZ']/16\n",
    "\n",
    "df26['WEIGHT_OZ'] = df26['WEIGHT_LB']*16\n",
    "df27['WEIGHT_OZ'] = df27['WEIGHT_LB']*16\n",
    "df28['WEIGHT_OZ'] = df28['WEIGHT_LB']*16\n",
    "df29['WEIGHT_OZ'] = df29['WEIGHT_LB']*16\n",
    "df30['WEIGHT_OZ'] = df30['WEIGHT_LB']*16\n",
    "df31['WEIGHT_OZ'] = df31['WEIGHT_LB']*16\n",
    "df32['WEIGHT_OZ'] = df32['WEIGHT_LB']*16\n",
    "df33['WEIGHT_OZ'] = df33['WEIGHT_LB']*16\n",
    "df34['WEIGHT_LB'] = df34['WEIGHT_OZ']/16\n",
    "df35['WEIGHT_OZ'] = df35['WEIGHT_LB']*16\n",
    "\n",
    "\n",
    "# use below if the charts have empty spaces. No need to use here as the source tables are already clean.\n",
    "# df1 = df1.dropna(axis = 0, how = 'all')\n",
    "# df1 = df1.dropna(axis = 1, how = 'all')\n",
    "# df1 = df1.dropna()\n",
    "\n",
    "\n",
    "# Add Service code and Service columns\n",
    "df1['SERVICE_PLAINTEXT']='UPS NDA Early'\n",
    "df2['SERVICE_PLAINTEXT']='UPS NDA'\n",
    "df3['SERVICE_PLAINTEXT']='UPS NDA Saver'\n",
    "df4['SERVICE_PLAINTEXT']='UPS 2DA A.M.'\n",
    "df5['SERVICE_PLAINTEXT']='UPS 2DA'\n",
    "df6['SERVICE_PLAINTEXT']='UPS 3DA'\n",
    "df7['SERVICE_PLAINTEXT']='UPS Ground Commercial'\n",
    "df8['SERVICE_PLAINTEXT']='UPS Ground Residential'\n",
    "df9['SERVICE_PLAINTEXT']='UPS Surepost 1#>'\n",
    "df10['SERVICE_PLAINTEXT']='UPS Surepost'\n",
    "\n",
    "df16['SERVICE_PLAINTEXT']='USPS Ground Cpp'\n",
    "df17['SERVICE_PLAINTEXT']='USPS Priority Cpp'\n",
    "\n",
    "df20['SERVICE_PLAINTEXT']='USPS Ground Cpp Cubic Inch'\n",
    "df21['SERVICE_PLAINTEXT']='USPS Priority Cpp Cubic Inch'\n",
    "\n",
    "df22['SERVICE_PLAINTEXT']='USPS Ground Auctane'\n",
    "df23['SERVICE_PLAINTEXT']='USPS Priority Auctane'\n",
    "\n",
    "df24['SERVICE_PLAINTEXT']='USPS Ground Auctane Cubic Inch'\n",
    "df25['SERVICE_PLAINTEXT']='USPS Priority Auctane Cubic Inch'\n",
    "\n",
    "df18['SERVICE_PLAINTEXT']='MI Parcel Select Lightweight Expedited'\n",
    "df19['SERVICE_PLAINTEXT']='MI Parcel Select Heavyweight'\n",
    "\n",
    "# Load DHL rate card\n",
    "# 5114358_dhl_sm_parcel_Franklin\n",
    "# 5122444_dhl_sm_parcel_Somerset, no DHLEM\n",
    "# 5122723_dhl_sm_parcel_8150 Nail Olive Branch\n",
    "# 5122739_dhl_sm_parcel_Nail rd Olive branch\n",
    "# 5122855_dhl_sm_parcel_Garland TX\n",
    "# 5122890_dhl_sm_parcel_Oofos\n",
    "# 5122893_dhl_sm_parcell Memphis\n",
    "# 5123280_dhl_sm_parcel_MAN\n",
    "# 5123282_dhl_sm_parcel Bridgewater\n",
    "# 5123283_dhl_sm_parcel_Curtis Bay\n",
    "\n",
    "df11_1['SERVICE_PLAINTEXT']='DHL SmartMail Parcel Plus Ground 1-25 5114358'\n",
    "df12_2['SERVICE_PLAINTEXT']='DHL SmartMail Parcel Ground < 1lb 5114358'\n",
    "df13_3['SERVICE_PLAINTEXT']='DHL SmartMail Parcel Plus Expedited 1-25 5114358'\n",
    "df14_4['SERVICE_PLAINTEXT']='DHL SmartMail Parcel Expedited < 1lb 5114358'\n",
    "df15_5['SERVICE_PLAINTEXT']='DHL SmartMail Parcel Expedited Max 5114358'\n",
    "\n",
    "df11_6['SERVICE_PLAINTEXT']='DHL SmartMail Parcel Plus Ground 1-25 5122444'\n",
    "df12_7['SERVICE_PLAINTEXT']='DHL SmartMail Parcel Ground < 1lb 5122444'\n",
    "df13_8['SERVICE_PLAINTEXT']='DHL SmartMail Parcel Plus Expedited 1-25 5122444'\n",
    "df14_9['SERVICE_PLAINTEXT']='DHL SmartMail Parcel Expedited < 1lb 5122444'\n",
    "df15_10['SERVICE_PLAINTEXT']='DHL SmartMail Parcel Expedited Max 5122444'\n",
    "\n",
    "df11_11['SERVICE_PLAINTEXT']='DHL SmartMail Parcel Plus Ground 1-25 5122723'\n",
    "df12_12['SERVICE_PLAINTEXT']='DHL SmartMail Parcel Ground < 1lb 5122723'\n",
    "df13_13['SERVICE_PLAINTEXT']='DHL SmartMail Parcel Plus Expedited 1-25 5122723'\n",
    "df14_14['SERVICE_PLAINTEXT']='DHL SmartMail Parcel Expedited < 1lb 5122723'\n",
    "df15_15['SERVICE_PLAINTEXT']='DHL SmartMail Parcel Expedited Max 5122723'\n",
    "\n",
    "df11_16['SERVICE_PLAINTEXT']='DHL SmartMail Parcel Plus Ground 1-25 5122739'\n",
    "df12_17['SERVICE_PLAINTEXT']='DHL SmartMail Parcel Ground < 1lb 5122739'\n",
    "df13_18['SERVICE_PLAINTEXT']='DHL SmartMail Parcel Plus Expedited 1-25 5122739'\n",
    "df14_19['SERVICE_PLAINTEXT']='DHL SmartMail Parcel Expedited < 1lb 5122739'\n",
    "df15_20['SERVICE_PLAINTEXT']='DHL SmartMail Parcel Expedited Max 5122739'\n",
    "\n",
    "df11_21['SERVICE_PLAINTEXT']='DHL SmartMail Parcel Plus Ground 1-25 5122855'\n",
    "df12_22['SERVICE_PLAINTEXT']='DHL SmartMail Parcel Ground < 1lb 5122855'\n",
    "df13_23['SERVICE_PLAINTEXT']='DHL SmartMail Parcel Plus Expedited 1-25 5122855'\n",
    "df14_24['SERVICE_PLAINTEXT']='DHL SmartMail Parcel Expedited < 1lb 5122855'\n",
    "df15_25['SERVICE_PLAINTEXT']='DHL SmartMail Parcel Expedited Max 5122855'\n",
    "\n",
    "df11_26['SERVICE_PLAINTEXT']='DHL SmartMail Parcel Plus Ground 1-25 5122890'\n",
    "df12_27['SERVICE_PLAINTEXT']='DHL SmartMail Parcel Ground < 1lb 5122890'\n",
    "df13_28['SERVICE_PLAINTEXT']='DHL SmartMail Parcel Plus Expedited 1-25 5122890'\n",
    "df14_29['SERVICE_PLAINTEXT']='DHL SmartMail Parcel Expedited < 1lb 5122890'\n",
    "df15_30['SERVICE_PLAINTEXT']='DHL SmartMail Parcel Expedited Max 5122890'\n",
    "\n",
    "df11_31['SERVICE_PLAINTEXT']='DHL SmartMail Parcel Plus Ground 1-25 5122893'\n",
    "df12_32['SERVICE_PLAINTEXT']='DHL SmartMail Parcel Ground < 1lb 5122893'\n",
    "df13_33['SERVICE_PLAINTEXT']='DHL SmartMail Parcel Plus Expedited 1-25 5122893'\n",
    "df14_34['SERVICE_PLAINTEXT']='DHL SmartMail Parcel Expedited < 1lb 5122893'\n",
    "df15_35['SERVICE_PLAINTEXT']='DHL SmartMail Parcel Expedited Max 5122893'\n",
    "\n",
    "df11_36['SERVICE_PLAINTEXT']='DHL SmartMail Parcel Plus Ground 1-25 5123280'\n",
    "df12_37['SERVICE_PLAINTEXT']='DHL SmartMail Parcel Ground < 1lb 5123280'\n",
    "df13_38['SERVICE_PLAINTEXT']='DHL SmartMail Parcel Plus Expedited 1-25 5123280'\n",
    "df14_39['SERVICE_PLAINTEXT']='DHL SmartMail Parcel Expedited < 1lb 5123280'\n",
    "df15_40['SERVICE_PLAINTEXT']='DHL SmartMail Parcel Expedited Max 5123280'\n",
    "\n",
    "df11_41['SERVICE_PLAINTEXT']='DHL SmartMail Parcel Plus Ground 1-25 5123282'\n",
    "df12_42['SERVICE_PLAINTEXT']='DHL SmartMail Parcel Ground < 1lb 5123282'\n",
    "df13_43['SERVICE_PLAINTEXT']='DHL SmartMail Parcel Plus Expedited 1-25 5123282'\n",
    "df14_44['SERVICE_PLAINTEXT']='DHL SmartMail Parcel Expedited < 1lb 5123282'\n",
    "df15_45['SERVICE_PLAINTEXT']='DHL SmartMail Parcel Expedited Max 5123282'\n",
    "\n",
    "df11_46['SERVICE_PLAINTEXT']='DHL SmartMail Parcel Plus Ground 1-25 5123283'\n",
    "df12_47['SERVICE_PLAINTEXT']='DHL SmartMail Parcel Ground < 1lb 5123283'\n",
    "df13_48['SERVICE_PLAINTEXT']='DHL SmartMail Parcel Plus Expedited 1-25 5123283'\n",
    "df14_49['SERVICE_PLAINTEXT']='DHL SmartMail Parcel Expedited < 1lb 5123283'\n",
    "df15_50['SERVICE_PLAINTEXT']='DHL SmartMail Parcel Expedited Max 5123283'\n",
    "\n",
    "df11_51['SERVICE_PLAINTEXT']='DHL SmartMail Parcel Plus Ground 1-25 5123556'\n",
    "df12_52['SERVICE_PLAINTEXT']='DHL SmartMail Parcel Ground < 1lb 5123556'\n",
    "df13_53['SERVICE_PLAINTEXT']='DHL SmartMail Parcel Plus Expedited 1-25 5123556'\n",
    "df14_54['SERVICE_PLAINTEXT']='DHL SmartMail Parcel Expedited < 1lb 5123556'\n",
    "df15_55['SERVICE_PLAINTEXT']='DHL SmartMail Parcel Expedited Max 5123556'\n",
    "\n",
    "df26['SERVICE_PLAINTEXT']='FedEx First Overnight'\n",
    "df27['SERVICE_PLAINTEXT']='FedEx Priority Overnight'\n",
    "df28['SERVICE_PLAINTEXT']='FedEx Standard Overnight'\n",
    "df29['SERVICE_PLAINTEXT']='FedEx 2nd Day AM'\n",
    "df30['SERVICE_PLAINTEXT']='FedEx 2Day'\n",
    "df31['SERVICE_PLAINTEXT']='FedEx Express Saver'\n",
    "df32['SERVICE_PLAINTEXT']='FedEx Ground'\n",
    "df33['SERVICE_PLAINTEXT']='FedEx Home Delivery'\n",
    "df34['SERVICE_PLAINTEXT']='FedEx SmartPost 1#>'\n",
    "df35['SERVICE_PLAINTEXT']='FedEx SmartPost'\n",
    "\n",
    "\n",
    "\n",
    "df1['SERVICE_CODE']='REDE'\n",
    "df2['SERVICE_CODE']='RED'\n",
    "df3['SERVICE_CODE']='REDS'\n",
    "df4['SERVICE_CODE']='2DAM'\n",
    "df5['SERVICE_CODE']='BLUE'\n",
    "df6['SERVICE_CODE']='ORNG'\n",
    "df7['SERVICE_CODE']='GRND'\n",
    "df8['SERVICE_CODE']='GRES'\n",
    "df9['SERVICE_CODE']='SRPT<1'\n",
    "df10['SERVICE_CODE']='SRPT'                               \n",
    "                             \n",
    "df16['SERVICE_CODE']='USPSAG_CPP'\n",
    "df17['SERVICE_CODE']='USPSAP_CPP'\n",
    "\n",
    "df20['SERVICE_CODE']='USPSAG_CPPCI'\n",
    "df21['SERVICE_CODE']='USPSAP_CPPCI'\n",
    "\n",
    "df22['SERVICE_CODE']='USPSAG_AUCT'\n",
    "df23['SERVICE_CODE']='USPSAP_AUCT'\n",
    "\n",
    "df24['SERVICE_CODE']='USPSAG_AUCTCI'\n",
    "df25['SERVICE_CODE']='USPSAP_AUCTCI'\n",
    "\n",
    "\n",
    "df18['SERVICE_CODE']='MIPLE'\n",
    "df19['SERVICE_CODE']='MIPH'\n",
    "\n",
    "df11_1['SERVICE_CODE']='DHLG_5114358'\n",
    "df12_2['SERVICE_CODE']='DHLG<1_5114358'\n",
    "df13_3['SERVICE_CODE']='DHLE_5114358'\n",
    "df14_4['SERVICE_CODE']='DHLE<1_5114358'\n",
    "df15_5['SERVICE_CODE']='DHLEM_5114358'\n",
    "\n",
    "df11_6['SERVICE_CODE']='DHLG_5122444'\n",
    "df12_7['SERVICE_CODE']='DHLG<1_5122444'\n",
    "df13_8['SERVICE_CODE']='DHLE_5122444'\n",
    "df14_9['SERVICE_CODE']='DHLE<1_5122444'\n",
    "df15_10['SERVICE_CODE']='DHLEM_5122444'\n",
    "\n",
    "df11_11['SERVICE_CODE']='DHLG_5122723'\n",
    "df12_12['SERVICE_CODE']='DHLG<1_5122723'\n",
    "df13_13['SERVICE_CODE']='DHLE_5122723'\n",
    "df14_14['SERVICE_CODE']='DHLE<1_5122723'\n",
    "df15_15['SERVICE_CODE']='DHLEM_5122723'\n",
    "\n",
    "df11_16['SERVICE_CODE']='DHLG_5122739'\n",
    "df12_17['SERVICE_CODE']='DHLG<1_5122739'\n",
    "df13_18['SERVICE_CODE']='DHLE_5122739'\n",
    "df14_19['SERVICE_CODE']='DHLE<1_5122739'\n",
    "df15_20['SERVICE_CODE']='DHLEM_5122739'\n",
    "\n",
    "df11_21['SERVICE_CODE']='DHLG_5122855'\n",
    "df12_22['SERVICE_CODE']='DHLG<1_5122855'\n",
    "df13_23['SERVICE_CODE']='DHLE_5122855'\n",
    "df14_24['SERVICE_CODE']='DHLE<1_5122855'\n",
    "df15_25['SERVICE_CODE']='DHLEM_5122855'\n",
    "\n",
    "df11_26['SERVICE_CODE']='DHLG_5122890'\n",
    "df12_27['SERVICE_CODE']='DHLG<1_5122890'\n",
    "df13_28['SERVICE_CODE']='DHLE_5122890'\n",
    "df14_29['SERVICE_CODE']='DHLE<1_5122890'\n",
    "df15_30['SERVICE_CODE']='DHLEM_5122890'\n",
    "\n",
    "df11_31['SERVICE_CODE']='DHLG_5122893'\n",
    "df12_32['SERVICE_CODE']='DHLG<1_5122893'\n",
    "df13_33['SERVICE_CODE']='DHLE_5122893'\n",
    "df14_34['SERVICE_CODE']='DHLE<1_5122893'\n",
    "df15_35['SERVICE_CODE']='DHLEM_5122893'\n",
    "\n",
    "df11_36['SERVICE_CODE']='DHLG_5123280'\n",
    "df12_37['SERVICE_CODE']='DHLG<1_5123280'\n",
    "df13_38['SERVICE_CODE']='DHLE_5123280'\n",
    "df14_39['SERVICE_CODE']='DHLE<1_5123280'\n",
    "df15_40['SERVICE_CODE']='DHLEM_5123280'\n",
    "\n",
    "df11_41['SERVICE_CODE']='DHLG_5123282'\n",
    "df12_42['SERVICE_CODE']='DHLG<1_5123282'\n",
    "df13_43['SERVICE_CODE']='DHLE_5123282'\n",
    "df14_44['SERVICE_CODE']='DHLE<1_5123282'\n",
    "df15_45['SERVICE_CODE']='DHLEM_5123282'\n",
    "\n",
    "df11_46['SERVICE_CODE']='DHLG_5123283'\n",
    "df12_47['SERVICE_CODE']='DHLG<1_5123283'\n",
    "df13_48['SERVICE_CODE']='DHLE_5123283'\n",
    "df14_49['SERVICE_CODE']='DHLE<1_5123283'\n",
    "df15_50['SERVICE_CODE']='DHLEM_5123283'\n",
    "\n",
    "df11_51['SERVICE_CODE']='DHLG_5123556'\n",
    "df12_52['SERVICE_CODE']='DHLG<1_5123556'\n",
    "df13_53['SERVICE_CODE']='DHLE_5123556'\n",
    "df14_54['SERVICE_CODE']='DHLE<1_5123556'\n",
    "df15_55['SERVICE_CODE']='DHLEM_5123556'\n",
    "\n",
    "\n",
    "df26['SERVICE_CODE']='1STO'\n",
    "df27['SERVICE_CODE']='PROV'\n",
    "df28['SERVICE_CODE']='STOV'\n",
    "df29['SERVICE_CODE']='2AM'\n",
    "df30['SERVICE_CODE']='2DAY'\n",
    "df31['SERVICE_CODE']='EXSA'\n",
    "df32['SERVICE_CODE']='GRNDF'\n",
    "df33['SERVICE_CODE']='HOME'\n",
    "df34['SERVICE_CODE']='SPST<1'\n",
    "df35['SERVICE_CODE']='SPST'  \n",
    "\n",
    "\n",
    "#Concatenate all together after rearranging the columns\n",
    "df1 = df1.reindex(columns=['SERVICE_CODE', 'SERVICE_PLAINTEXT', 'ZONE', 'WEIGHT_OZ', 'WEIGHT_LB', 'PUBLISHED_RATES'])\n",
    "df2 = df2.reindex(columns=['SERVICE_CODE', 'SERVICE_PLAINTEXT', 'ZONE', 'WEIGHT_OZ', 'WEIGHT_LB', 'PUBLISHED_RATES'])\n",
    "df3 = df3.reindex(columns=['SERVICE_CODE', 'SERVICE_PLAINTEXT', 'ZONE', 'WEIGHT_OZ', 'WEIGHT_LB', 'PUBLISHED_RATES'])\n",
    "df4 = df4.reindex(columns=['SERVICE_CODE', 'SERVICE_PLAINTEXT', 'ZONE', 'WEIGHT_OZ', 'WEIGHT_LB', 'PUBLISHED_RATES'])\n",
    "df5 = df5.reindex(columns=['SERVICE_CODE', 'SERVICE_PLAINTEXT', 'ZONE', 'WEIGHT_OZ', 'WEIGHT_LB', 'PUBLISHED_RATES'])\n",
    "df6 = df6.reindex(columns=['SERVICE_CODE', 'SERVICE_PLAINTEXT', 'ZONE', 'WEIGHT_OZ', 'WEIGHT_LB', 'PUBLISHED_RATES'])\n",
    "df7 = df7.reindex(columns=['SERVICE_CODE', 'SERVICE_PLAINTEXT', 'ZONE', 'WEIGHT_OZ', 'WEIGHT_LB', 'PUBLISHED_RATES'])\n",
    "df8 = df8.reindex(columns=['SERVICE_CODE', 'SERVICE_PLAINTEXT', 'ZONE', 'WEIGHT_OZ', 'WEIGHT_LB', 'PUBLISHED_RATES'])\n",
    "df9 = df9.reindex(columns=['SERVICE_CODE', 'SERVICE_PLAINTEXT', 'ZONE', 'WEIGHT_OZ', 'WEIGHT_LB', 'PUBLISHED_RATES'])\n",
    "df10 = df10.reindex(columns=['SERVICE_CODE', 'SERVICE_PLAINTEXT', 'ZONE', 'WEIGHT_OZ', 'WEIGHT_LB', 'PUBLISHED_RATES'])\n",
    "\n",
    "df16 = df16.reindex(columns=['SERVICE_CODE', 'SERVICE_PLAINTEXT', 'ZONE', 'WEIGHT_OZ', 'WEIGHT_LB', 'BARRETT_RATES'])\n",
    "df17 = df17.reindex(columns=['SERVICE_CODE', 'SERVICE_PLAINTEXT', 'ZONE', 'WEIGHT_OZ', 'WEIGHT_LB', 'BARRETT_RATES'])\n",
    "\n",
    "df20 = df20.reindex(columns=['SERVICE_CODE', 'SERVICE_PLAINTEXT', 'ZONE', 'CUBIC_FT', 'BARRETT_RATES'])\n",
    "df21 = df21.reindex(columns=['SERVICE_CODE', 'SERVICE_PLAINTEXT', 'ZONE', 'CUBIC_FT', 'BARRETT_RATES'])\n",
    "\n",
    "df22 = df22.reindex(columns=['SERVICE_CODE', 'SERVICE_PLAINTEXT', 'ZONE', 'WEIGHT_OZ', 'WEIGHT_LB', 'BARRETT_RATES'])\n",
    "df23 = df23.reindex(columns=['SERVICE_CODE', 'SERVICE_PLAINTEXT', 'ZONE', 'WEIGHT_OZ', 'WEIGHT_LB', 'BARRETT_RATES'])\n",
    "\n",
    "df24 = df24.reindex(columns=['SERVICE_CODE', 'SERVICE_PLAINTEXT', 'ZONE', 'CUBIC_FT', 'BARRETT_RATES'])\n",
    "df25 = df25.reindex(columns=['SERVICE_CODE', 'SERVICE_PLAINTEXT', 'ZONE', 'CUBIC_FT', 'BARRETT_RATES'])\n",
    "\n",
    "\n",
    "df18 = df18.reindex(columns=['SERVICE_CODE', 'SERVICE_PLAINTEXT', 'ZONE', 'WEIGHT_OZ', 'WEIGHT_LB', 'BARRETT_RATES'])\n",
    "df19 = df19.reindex(columns=['SERVICE_CODE', 'SERVICE_PLAINTEXT', 'ZONE', 'WEIGHT_OZ', 'WEIGHT_LB', 'BARRETT_RATES'])\n",
    "\n",
    "df11_1 = df11_1.reindex(columns=['SERVICE_CODE', 'SERVICE_PLAINTEXT', 'ZONE', 'WEIGHT_OZ', 'WEIGHT_LB', 'BARRETT_RATES'])\n",
    "df12_2 = df12_2.reindex(columns=['SERVICE_CODE', 'SERVICE_PLAINTEXT', 'ZONE', 'WEIGHT_OZ', 'WEIGHT_LB', 'BARRETT_RATES'])\n",
    "df13_3 = df13_3.reindex(columns=['SERVICE_CODE', 'SERVICE_PLAINTEXT', 'ZONE', 'WEIGHT_OZ', 'WEIGHT_LB', 'BARRETT_RATES'])\n",
    "df14_4 = df14_4.reindex(columns=['SERVICE_CODE', 'SERVICE_PLAINTEXT', 'ZONE', 'WEIGHT_OZ', 'WEIGHT_LB', 'BARRETT_RATES'])\n",
    "df15_5 = df15_5.reindex(columns=['SERVICE_CODE', 'SERVICE_PLAINTEXT', 'ZONE', 'WEIGHT_OZ', 'WEIGHT_LB', 'BARRETT_RATES'])\n",
    "\n",
    "df11_6 = df11_6.reindex(columns=['SERVICE_CODE', 'SERVICE_PLAINTEXT', 'ZONE', 'WEIGHT_OZ', 'WEIGHT_LB', 'BARRETT_RATES'])\n",
    "df12_7 = df12_7.reindex(columns=['SERVICE_CODE', 'SERVICE_PLAINTEXT', 'ZONE', 'WEIGHT_OZ', 'WEIGHT_LB', 'BARRETT_RATES'])\n",
    "df13_8 = df13_8.reindex(columns=['SERVICE_CODE', 'SERVICE_PLAINTEXT', 'ZONE', 'WEIGHT_OZ', 'WEIGHT_LB', 'BARRETT_RATES'])\n",
    "df14_9 = df14_9.reindex(columns=['SERVICE_CODE', 'SERVICE_PLAINTEXT', 'ZONE', 'WEIGHT_OZ', 'WEIGHT_LB', 'BARRETT_RATES'])\n",
    "df15_10 = df15_10.reindex(columns=['SERVICE_CODE', 'SERVICE_PLAINTEXT', 'ZONE', 'WEIGHT_OZ', 'WEIGHT_LB', 'BARRETT_RATES'])\n",
    "\n",
    "df11_11 = df11_11.reindex(columns=['SERVICE_CODE', 'SERVICE_PLAINTEXT', 'ZONE', 'WEIGHT_OZ', 'WEIGHT_LB', 'BARRETT_RATES'])\n",
    "df12_12 = df12_12.reindex(columns=['SERVICE_CODE', 'SERVICE_PLAINTEXT', 'ZONE', 'WEIGHT_OZ', 'WEIGHT_LB', 'BARRETT_RATES'])\n",
    "df13_13 = df13_13.reindex(columns=['SERVICE_CODE', 'SERVICE_PLAINTEXT', 'ZONE', 'WEIGHT_OZ', 'WEIGHT_LB', 'BARRETT_RATES'])\n",
    "df14_14 = df14_14.reindex(columns=['SERVICE_CODE', 'SERVICE_PLAINTEXT', 'ZONE', 'WEIGHT_OZ', 'WEIGHT_LB', 'BARRETT_RATES'])\n",
    "df15_15 = df15_15.reindex(columns=['SERVICE_CODE', 'SERVICE_PLAINTEXT', 'ZONE', 'WEIGHT_OZ', 'WEIGHT_LB', 'BARRETT_RATES'])\n",
    "\n",
    "df11_16 = df11_16.reindex(columns=['SERVICE_CODE', 'SERVICE_PLAINTEXT', 'ZONE', 'WEIGHT_OZ', 'WEIGHT_LB', 'BARRETT_RATES'])\n",
    "df12_17 = df12_17.reindex(columns=['SERVICE_CODE', 'SERVICE_PLAINTEXT', 'ZONE', 'WEIGHT_OZ', 'WEIGHT_LB', 'BARRETT_RATES'])\n",
    "df13_18 = df13_18.reindex(columns=['SERVICE_CODE', 'SERVICE_PLAINTEXT', 'ZONE', 'WEIGHT_OZ', 'WEIGHT_LB', 'BARRETT_RATES'])\n",
    "df14_19 = df14_19.reindex(columns=['SERVICE_CODE', 'SERVICE_PLAINTEXT', 'ZONE', 'WEIGHT_OZ', 'WEIGHT_LB', 'BARRETT_RATES'])\n",
    "df15_20 = df15_20.reindex(columns=['SERVICE_CODE', 'SERVICE_PLAINTEXT', 'ZONE', 'WEIGHT_OZ', 'WEIGHT_LB', 'BARRETT_RATES'])\n",
    "\n",
    "df11_21 = df11_21.reindex(columns=['SERVICE_CODE', 'SERVICE_PLAINTEXT', 'ZONE', 'WEIGHT_OZ', 'WEIGHT_LB', 'BARRETT_RATES'])\n",
    "df12_22 = df12_22.reindex(columns=['SERVICE_CODE', 'SERVICE_PLAINTEXT', 'ZONE', 'WEIGHT_OZ', 'WEIGHT_LB', 'BARRETT_RATES'])\n",
    "df13_23 = df13_23.reindex(columns=['SERVICE_CODE', 'SERVICE_PLAINTEXT', 'ZONE', 'WEIGHT_OZ', 'WEIGHT_LB', 'BARRETT_RATES'])\n",
    "df14_24 = df14_24.reindex(columns=['SERVICE_CODE', 'SERVICE_PLAINTEXT', 'ZONE', 'WEIGHT_OZ', 'WEIGHT_LB', 'BARRETT_RATES'])\n",
    "df15_25 = df15_25.reindex(columns=['SERVICE_CODE', 'SERVICE_PLAINTEXT', 'ZONE', 'WEIGHT_OZ', 'WEIGHT_LB', 'BARRETT_RATES'])\n",
    "\n",
    "df11_26 = df11_26.reindex(columns=['SERVICE_CODE', 'SERVICE_PLAINTEXT', 'ZONE', 'WEIGHT_OZ', 'WEIGHT_LB', 'BARRETT_RATES'])\n",
    "df12_27 = df12_27.reindex(columns=['SERVICE_CODE', 'SERVICE_PLAINTEXT', 'ZONE', 'WEIGHT_OZ', 'WEIGHT_LB', 'BARRETT_RATES'])\n",
    "df13_28 = df13_28.reindex(columns=['SERVICE_CODE', 'SERVICE_PLAINTEXT', 'ZONE', 'WEIGHT_OZ', 'WEIGHT_LB', 'BARRETT_RATES'])\n",
    "df14_29 = df14_29.reindex(columns=['SERVICE_CODE', 'SERVICE_PLAINTEXT', 'ZONE', 'WEIGHT_OZ', 'WEIGHT_LB', 'BARRETT_RATES'])\n",
    "df15_30 = df15_30.reindex(columns=['SERVICE_CODE', 'SERVICE_PLAINTEXT', 'ZONE', 'WEIGHT_OZ', 'WEIGHT_LB', 'BARRETT_RATES'])\n",
    "\n",
    "df11_31 = df11_31.reindex(columns=['SERVICE_CODE', 'SERVICE_PLAINTEXT', 'ZONE', 'WEIGHT_OZ', 'WEIGHT_LB', 'BARRETT_RATES'])\n",
    "df12_32 = df12_32.reindex(columns=['SERVICE_CODE', 'SERVICE_PLAINTEXT', 'ZONE', 'WEIGHT_OZ', 'WEIGHT_LB', 'BARRETT_RATES'])\n",
    "df13_33 = df13_33.reindex(columns=['SERVICE_CODE', 'SERVICE_PLAINTEXT', 'ZONE', 'WEIGHT_OZ', 'WEIGHT_LB', 'BARRETT_RATES'])\n",
    "df14_34 = df14_34.reindex(columns=['SERVICE_CODE', 'SERVICE_PLAINTEXT', 'ZONE', 'WEIGHT_OZ', 'WEIGHT_LB', 'BARRETT_RATES'])\n",
    "df15_35 = df15_35.reindex(columns=['SERVICE_CODE', 'SERVICE_PLAINTEXT', 'ZONE', 'WEIGHT_OZ', 'WEIGHT_LB', 'BARRETT_RATES'])\n",
    "\n",
    "df11_36 = df11_36.reindex(columns=['SERVICE_CODE', 'SERVICE_PLAINTEXT', 'ZONE', 'WEIGHT_OZ', 'WEIGHT_LB', 'BARRETT_RATES'])\n",
    "df12_37 = df12_37.reindex(columns=['SERVICE_CODE', 'SERVICE_PLAINTEXT', 'ZONE', 'WEIGHT_OZ', 'WEIGHT_LB', 'BARRETT_RATES'])\n",
    "df13_38 = df13_38.reindex(columns=['SERVICE_CODE', 'SERVICE_PLAINTEXT', 'ZONE', 'WEIGHT_OZ', 'WEIGHT_LB', 'BARRETT_RATES'])\n",
    "df14_39 = df14_39.reindex(columns=['SERVICE_CODE', 'SERVICE_PLAINTEXT', 'ZONE', 'WEIGHT_OZ', 'WEIGHT_LB', 'BARRETT_RATES'])\n",
    "df15_40 = df15_40.reindex(columns=['SERVICE_CODE', 'SERVICE_PLAINTEXT', 'ZONE', 'WEIGHT_OZ', 'WEIGHT_LB', 'BARRETT_RATES'])\n",
    "\n",
    "df11_41 = df11_41.reindex(columns=['SERVICE_CODE', 'SERVICE_PLAINTEXT', 'ZONE', 'WEIGHT_OZ', 'WEIGHT_LB', 'BARRETT_RATES'])\n",
    "df12_42 = df12_42.reindex(columns=['SERVICE_CODE', 'SERVICE_PLAINTEXT', 'ZONE', 'WEIGHT_OZ', 'WEIGHT_LB', 'BARRETT_RATES'])\n",
    "df13_43 = df13_43.reindex(columns=['SERVICE_CODE', 'SERVICE_PLAINTEXT', 'ZONE', 'WEIGHT_OZ', 'WEIGHT_LB', 'BARRETT_RATES'])\n",
    "df14_44 = df14_44.reindex(columns=['SERVICE_CODE', 'SERVICE_PLAINTEXT', 'ZONE', 'WEIGHT_OZ', 'WEIGHT_LB', 'BARRETT_RATES'])\n",
    "df15_45 = df15_45.reindex(columns=['SERVICE_CODE', 'SERVICE_PLAINTEXT', 'ZONE', 'WEIGHT_OZ', 'WEIGHT_LB', 'BARRETT_RATES'])\n",
    "\n",
    "df11_46 = df11_46.reindex(columns=['SERVICE_CODE', 'SERVICE_PLAINTEXT', 'ZONE', 'WEIGHT_OZ', 'WEIGHT_LB', 'BARRETT_RATES'])\n",
    "df12_47 = df12_47.reindex(columns=['SERVICE_CODE', 'SERVICE_PLAINTEXT', 'ZONE', 'WEIGHT_OZ', 'WEIGHT_LB', 'BARRETT_RATES'])\n",
    "df13_48 = df13_48.reindex(columns=['SERVICE_CODE', 'SERVICE_PLAINTEXT', 'ZONE', 'WEIGHT_OZ', 'WEIGHT_LB', 'BARRETT_RATES'])\n",
    "df14_49 = df14_49.reindex(columns=['SERVICE_CODE', 'SERVICE_PLAINTEXT', 'ZONE', 'WEIGHT_OZ', 'WEIGHT_LB', 'BARRETT_RATES'])\n",
    "df15_50 = df15_50.reindex(columns=['SERVICE_CODE', 'SERVICE_PLAINTEXT', 'ZONE', 'WEIGHT_OZ', 'WEIGHT_LB', 'BARRETT_RATES'])\n",
    "\n",
    "df11_51 = df11_51.reindex(columns=['SERVICE_CODE', 'SERVICE_PLAINTEXT', 'ZONE', 'WEIGHT_OZ', 'WEIGHT_LB', 'BARRETT_RATES'])\n",
    "df12_52 = df12_52.reindex(columns=['SERVICE_CODE', 'SERVICE_PLAINTEXT', 'ZONE', 'WEIGHT_OZ', 'WEIGHT_LB', 'BARRETT_RATES'])\n",
    "df13_53 = df13_53.reindex(columns=['SERVICE_CODE', 'SERVICE_PLAINTEXT', 'ZONE', 'WEIGHT_OZ', 'WEIGHT_LB', 'BARRETT_RATES'])\n",
    "df14_54 = df14_54.reindex(columns=['SERVICE_CODE', 'SERVICE_PLAINTEXT', 'ZONE', 'WEIGHT_OZ', 'WEIGHT_LB', 'BARRETT_RATES'])\n",
    "df15_55 = df15_55.reindex(columns=['SERVICE_CODE', 'SERVICE_PLAINTEXT', 'ZONE', 'WEIGHT_OZ', 'WEIGHT_LB', 'BARRETT_RATES'])\n",
    "\n",
    "df26 = df26.reindex(columns=['SERVICE_CODE', 'SERVICE_PLAINTEXT', 'ZONE', 'WEIGHT_OZ', 'WEIGHT_LB', 'PUBLISHED_RATES'])\n",
    "df27 = df27.reindex(columns=['SERVICE_CODE', 'SERVICE_PLAINTEXT', 'ZONE', 'WEIGHT_OZ', 'WEIGHT_LB', 'PUBLISHED_RATES'])\n",
    "df28 = df28.reindex(columns=['SERVICE_CODE', 'SERVICE_PLAINTEXT', 'ZONE', 'WEIGHT_OZ', 'WEIGHT_LB', 'PUBLISHED_RATES'])\n",
    "df29 = df29.reindex(columns=['SERVICE_CODE', 'SERVICE_PLAINTEXT', 'ZONE', 'WEIGHT_OZ', 'WEIGHT_LB', 'PUBLISHED_RATES'])\n",
    "df30 = df30.reindex(columns=['SERVICE_CODE', 'SERVICE_PLAINTEXT', 'ZONE', 'WEIGHT_OZ', 'WEIGHT_LB', 'PUBLISHED_RATES'])\n",
    "df31 = df31.reindex(columns=['SERVICE_CODE', 'SERVICE_PLAINTEXT', 'ZONE', 'WEIGHT_OZ', 'WEIGHT_LB', 'PUBLISHED_RATES'])\n",
    "df32 = df32.reindex(columns=['SERVICE_CODE', 'SERVICE_PLAINTEXT', 'ZONE', 'WEIGHT_OZ', 'WEIGHT_LB', 'PUBLISHED_RATES'])\n",
    "df33 = df33.reindex(columns=['SERVICE_CODE', 'SERVICE_PLAINTEXT', 'ZONE', 'WEIGHT_OZ', 'WEIGHT_LB', 'PUBLISHED_RATES'])\n",
    "df34 = df34.reindex(columns=['SERVICE_CODE', 'SERVICE_PLAINTEXT', 'ZONE', 'WEIGHT_OZ', 'WEIGHT_LB', 'PUBLISHED_RATES'])\n",
    "df35 = df35.reindex(columns=['SERVICE_CODE', 'SERVICE_PLAINTEXT', 'ZONE', 'WEIGHT_OZ', 'WEIGHT_LB', 'PUBLISHED_RATES'])\n",
    "\n",
    "\n",
    "\n",
    "tbl_publishedRates = pd.concat([df1, df2, df3, df4, df5, df6, df7, df8, df9, df10, df16, df17, df20, df21, df22, df23, df24, df25, df18, df19,\n",
    "                               df11_1, df12_2, df13_3, df14_4, df15_5, df11_6, df12_7, df13_8, df14_9, \n",
    "                               df11_11, df12_12, df13_13, df14_14, df15_15, df11_16, df12_17, df13_18, df14_19, df15_20, \n",
    "                               df11_21, df12_22, df13_23, df14_24, df15_25, df11_26, df12_27, df13_28, df14_29, df15_30, \n",
    "                               df11_31, df12_32, df13_33, df14_34, df15_35, df11_36, df12_37, df13_38, df14_39, df15_40, \n",
    "                               df11_41, df12_42, df13_43, df14_44, df15_45, df11_46, df12_47, df13_48, df14_49, df15_50,\n",
    "                               df11_51, df12_52, df13_53, df14_54, df15_55, df26, df27, df28, df29, df30, df31, df32, df33, df34, df35\n",
    "                               ], axis=0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4bfdeab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the version column\n",
    "tbl_publishedRates['VERSION'] ='2024'\n",
    "tbl_publishedRates['START_DATE'] ='2024/01/01'\n",
    "tbl_publishedRates['END_DATE'] ='2024/12/31'\n",
    "\n",
    "# Unify the Zone values\n",
    "def get_portion(value):\n",
    "    if isinstance(value, int):\n",
    "        return str(value)  # convert integer to string\n",
    "    if 'Zone' in value:\n",
    "        return value[5:]\n",
    "    elif 'US' in value:\n",
    "        return value[3:]\n",
    "    \n",
    "tbl_publishedRates['ZONE_new'] = tbl_publishedRates['ZONE'].apply(get_portion)\n",
    "tbl_publishedRates['ZONE_new'] = tbl_publishedRates['ZONE_new'].str.replace('20', '')\n",
    "tbl_publishedRates['ZONE_new'] = tbl_publishedRates['ZONE_new'].str.replace('30', '')\n",
    "tbl_publishedRates['ZONE_new'] = tbl_publishedRates['ZONE_new'].str.replace('102', '2')\n",
    "tbl_publishedRates['ZONE_new'] = tbl_publishedRates['ZONE_new'].str.replace('103', '3')\n",
    "tbl_publishedRates['ZONE_new'] = tbl_publishedRates['ZONE_new'].str.replace('104', '4')\n",
    "tbl_publishedRates['ZONE_new'] = tbl_publishedRates['ZONE_new'].str.replace('105', '5')\n",
    "tbl_publishedRates['ZONE_new'] = tbl_publishedRates['ZONE_new'].str.replace('106', '6')\n",
    "tbl_publishedRates['ZONE_new'] = tbl_publishedRates['ZONE_new'].str.replace('107', '7')\n",
    "tbl_publishedRates['ZONE_new'] = tbl_publishedRates['ZONE_new'].str.replace('108', '8')\n",
    "tbl_publishedRates['ZONE_new'] = tbl_publishedRates['ZONE_new'].str.replace('124', '9')\n",
    "tbl_publishedRates['ZONE_new'] = tbl_publishedRates['ZONE_new'].str.replace('125', '10')\n",
    "tbl_publishedRates['ZONE_new'] = tbl_publishedRates['ZONE_new'].str.replace('126', '11')\n",
    "tbl_publishedRates['ZONE_new'] = tbl_publishedRates['ZONE_new'].str.replace('224', '9')\n",
    "tbl_publishedRates['ZONE_new'] = tbl_publishedRates['ZONE_new'].str.replace('225', '10')\n",
    "tbl_publishedRates['ZONE_new'] = tbl_publishedRates['ZONE_new'].str.replace('226', '11')\n",
    "tbl_publishedRates['ZONE_new'] = tbl_publishedRates['ZONE_new'].str.replace('44', '9')\n",
    "tbl_publishedRates['ZONE_new'] = tbl_publishedRates['ZONE_new'].str.replace('45', '10')\n",
    "tbl_publishedRates['ZONE_new'] = tbl_publishedRates['ZONE_new'].str.replace('46', '11')\n",
    "tbl_publishedRates['ZONE_new'] = tbl_publishedRates['ZONE_new'].str.replace('03', '3')\n",
    "tbl_publishedRates['ZONE_new'] = tbl_publishedRates['ZONE_new'].str.replace('04', '4')\n",
    "tbl_publishedRates['ZONE_new'] = tbl_publishedRates['ZONE_new'].str.replace('05', '5')\n",
    "tbl_publishedRates['ZONE_new'] = tbl_publishedRates['ZONE_new'].str.replace('06', '6')\n",
    "tbl_publishedRates['ZONE_new'] = tbl_publishedRates['ZONE_new'].str.replace('07', '7')\n",
    "tbl_publishedRates['ZONE_new'] = tbl_publishedRates['ZONE_new'].str.replace('08', '8')   \n",
    "tbl_publishedRates['ZONE_new'] = tbl_publishedRates['ZONE_new'].str.replace('09', '9')  \n",
    "\n",
    "#delete the original Zone column and use the new one\n",
    "tbl_publishedRates['ZONE'] = tbl_publishedRates['ZONE_new']\n",
    "tbl_publishedRates = tbl_publishedRates.drop('ZONE_new', axis=1)\n",
    "\n",
    "#adjust the index and finish creating this table.\n",
    "tbl_publishedRates.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# tbl_publishedRates.head()\n",
    "# tbl_publishedRates.to_csv('test.csv')\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "300760de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep rows where 'ServiceCode' is not in ['cpp1', 'cpp2', 'cpp3']\n",
    "tbl_publishedRates = tbl_publishedRates[~tbl_publishedRates['SERVICE_CODE'].isin(['USPSAG_AUCT', 'USPSAP_AUCT', 'USPSAG_AUCTCI', 'USPSAP_AUCTCI'])]\n",
    "         \n",
    "# 2. Replacing values\n",
    "# Define a mapping of old values to new values\n",
    "replacement_dict = {\n",
    "    'USPSAG_CPP': 'USPSAG',\n",
    "    'USPSAP_CPP': 'USPSAP',\n",
    "    'USPSAG_CPPCI': 'USPSAG_CI',\n",
    "    'USPSAP_CPPCI': 'USPSAP_CI'\n",
    "    \n",
    "}\n",
    "\n",
    "# Replace the values in 'ServiceCode' column\n",
    "tbl_publishedRates['SERVICE_CODE'] = tbl_publishedRates['SERVICE_CODE'].replace(replacement_dict)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "010fcd12",
   "metadata": {},
   "source": [
    "# Step 1 - b. Create Freight Rate Card (tbl_BarrettRates)\n",
    "*Add ups BarrettRates to tbl_publishedRates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "dd5c2c60",
   "metadata": {},
   "outputs": [],
   "source": [
    "#The result is BarrettRate when margin = 0.\n",
    "#For UPS client freight & accessorial discounts, write \"YES\"/\"NO\" for rounding/not rounding to 2 decimal places.\n",
    "Round_Discount = \"NO\"\n",
    "\n",
    "#Set UPS margin for the client\n",
    "Margin_on_Min_GRND_BR_UPS = 0\n",
    "Margin_on_Min_AIR_BR_UPS = 0\n",
    "\n",
    "Margin_on_UPS_Parcel_Freight_GRND_BR = 0\n",
    "Margin_on_UPS_Parcel_Freight_AIR_BR = 0\n",
    "\n",
    "Margin_on_Value_Add_GRND_BR_UPS = 0\n",
    "Margin_on_Value_Add_AIR_BR_UPS = 0\n",
    "\n",
    "\n",
    "# UPSAccessorial_2023 has 2 tabs, UPS_Rates and UPS_Accessorial. UPS_Rates is used to create Barrett Rates\n",
    "xls = pd.ExcelFile('Accessorial_2024.xlsx')\n",
    "UPS_Rates = pd.read_excel(xls, 'UPS_Rates')\n",
    "\n",
    "# This is the value of the left side of \"Rates\" tab from \"Customer Rate Contract Cards 2023\" doc\n",
    "# UPS_Rates\n",
    "\n",
    "\n",
    "#add 2 columns to this table with min and max weight\n",
    "def f(col):\n",
    "    if col['WEIGHT_BREAK'] == '1-5lb':\n",
    "        return 5\n",
    "    elif col['WEIGHT_BREAK'] == '6-10lb':\n",
    "        return 10\n",
    "    elif col['WEIGHT_BREAK'] == '11-20lb':\n",
    "        return 20\n",
    "    elif col['WEIGHT_BREAK'] == '21-30lb':\n",
    "        return 30\n",
    "    elif (col['WEIGHT_BREAK'] == '31+lb'):\n",
    "        return 999\n",
    "    elif (col['SERVICE_LEVEL'] == 'SurePost - Less than 1lb') & (col['WEIGHT_BREAK'] == '1-16oz'):\n",
    "        return 16\n",
    "    elif (col['SERVICE_LEVEL'] == 'SurePost - Greater than 1lb') & (col['WEIGHT_BREAK'] == '1-3lb'):\n",
    "        return 3\n",
    "    elif (col['SERVICE_LEVEL'] == 'SurePost - Greater than 1lb') & (col['WEIGHT_BREAK'] == '4-6lb'):\n",
    "        return 6\n",
    "    elif (col['SERVICE_LEVEL'] == 'SurePost - Greater than 1lb') & (col['WEIGHT_BREAK'] == '7-8lb'):\n",
    "        return 8\n",
    "    elif (col['SERVICE_LEVEL'] == 'SurePost - Greater than 1lb') & (col['WEIGHT_BREAK'] == '9lb'):\n",
    "        return 9  \n",
    "\n",
    "def g(col):\n",
    "    if col['WEIGHT_BREAK'] == '1-5lb':\n",
    "        return 1\n",
    "    elif col['WEIGHT_BREAK'] == '6-10lb':\n",
    "        return 6\n",
    "    elif col['WEIGHT_BREAK'] == '11-20lb':\n",
    "        return 11\n",
    "    elif col['WEIGHT_BREAK'] == '21-30lb':\n",
    "        return 21\n",
    "    elif col['WEIGHT_BREAK'] == '31+lb':\n",
    "        return 31\n",
    "    elif (col['SERVICE_LEVEL'] == 'SurePost - Less than 1lb') & (col['WEIGHT_BREAK'] == '1-16oz'):\n",
    "        return 1\n",
    "    elif (col['SERVICE_LEVEL'] == 'SurePost - Greater than 1lb') & (col['WEIGHT_BREAK'] == '1-3lb'):\n",
    "        return 1\n",
    "    elif (col['SERVICE_LEVEL'] == 'SurePost - Greater than 1lb') & (col['WEIGHT_BREAK'] == '4-6lb'):\n",
    "        return 4\n",
    "    elif (col['SERVICE_LEVEL'] == 'SurePost - Greater than 1lb') & (col['WEIGHT_BREAK'] == '7-8lb'):\n",
    "        return 7\n",
    "    elif (col['SERVICE_LEVEL'] == 'SurePost - Greater than 1lb') & (col['WEIGHT_BREAK'] == '9lb'):\n",
    "        return 9 \n",
    "\n",
    "UPS_Rates['WEIGHT_LOWER'] = UPS_Rates.apply(g, axis=1)\n",
    "UPS_Rates['WEIGHT_UPPER'] = UPS_Rates.apply(f, axis=1)\n",
    "\n",
    "\n",
    "# add client margin mins column\n",
    "def h(col):   \n",
    "    if col['SERVICE_LEVEL'] == 'Next Day Air Early AM':\n",
    "        return col['MIN_WITH_REBATE']\n",
    "    \n",
    "    elif (col['SERVICE_LEVEL'] == 'Next Day Air') | (col['SERVICE_LEVEL'] == 'Next Day Air Saver') | (col['SERVICE_LEVEL'] == '2nd A.M. Day Air') | (col['SERVICE_LEVEL'] == '2nd Day Air') | (col['SERVICE_LEVEL'] == '3 Day Select'): \n",
    "        try: \n",
    "            return min(col['MIN_WITH_REBATE'] + 4, col['MIN_WITH_REBATE'] / (1-Margin_on_Min_AIR_BR_UPS))\n",
    "        except ValueError:\n",
    "            return 0\n",
    "\n",
    "    elif (col['SERVICE_LEVEL'] == 'Ground Commercial') | (col['SERVICE_LEVEL'] == 'Ground Residential'): \n",
    "        try: \n",
    "            return min(10.7, col['MIN_WITH_REBATE'] / (1-Margin_on_Min_GRND_BR_UPS))\n",
    "        except ValueError:\n",
    "            return 0\n",
    "\n",
    "    elif (col['SERVICE_LEVEL'] == 'SurePost - Greater than 1lb'): \n",
    "        try: \n",
    "            return min(9.1, col['MIN_WITH_REBATE'] / (1-Margin_on_Min_GRND_BR_UPS))\n",
    "        except ValueError:\n",
    "            return 0\n",
    "    \n",
    "    elif (col['SERVICE_LEVEL'] == 'SurePost - Less than 1lb'): \n",
    "        try: \n",
    "            return min(8.86, col['MIN_WITH_REBATE'] / (1-Margin_on_Min_GRND_BR_UPS))\n",
    "        except ValueError:\n",
    "            return 0\n",
    "  \n",
    "    elif col['SERVICE_LEVEL'] == 'Ground CWT': \n",
    "        try: \n",
    "            return min(80, col['MIN_WITH_REBATE'] / (1-Margin_on_Min_GRND_BR_UPS))\n",
    "        except ValueError:\n",
    "            return 0\n",
    "  \n",
    "    elif (col['SERVICE_LEVEL'] == 'Standard to Canada') | (col['SERVICE_LEVEL'] == 'Standard to Mexico'):\n",
    "        try: \n",
    "            return min(col['MIN_WITH_REBATE'] + 4, col['MIN_WITH_REBATE'] / (1-Margin_on_Min_GRND_BR_UPS))\n",
    "        except ValueError:\n",
    "            return 0\n",
    "\n",
    "UPS_Rates['CLIENT_MARGIN_MINS'] = UPS_Rates.apply(h, axis=1)\n",
    "\n",
    "# add ADD_MARGIN_(COL_M/DECIMAL) column\n",
    "UPS_Rates['DISCOUNT_RECIPROCAL_(BDC_PAY_ON_THE_DOLLAR)'] = 1 - UPS_Rates['NET_CONTRACT_DISC']\n",
    "UPS_Rates.loc[UPS_Rates['SERVICE_LEVEL'] == 'Next Day Air Early AM', 'DISCOUNT_RECIPROCAL_(BDC_PAY_ON_THE_DOLLAR)'] = None\n",
    "\n",
    "def i(col):   \n",
    "    if (col['SERVICE_LEVEL'] == 'SurePost - Greater than 1lb') | (col['WEIGHT_BREAK'] == '9lb'):\n",
    "        return col['DISCOUNT_RECIPROCAL_(BDC_PAY_ON_THE_DOLLAR)'] / (1-Margin_on_UPS_Parcel_Freight_AIR_BR)\n",
    "    elif col['SERVICE_LEVEL'] == 'Next Day Air Early AM':\n",
    "        return\n",
    "    elif (col['SERVICE_LEVEL'] == 'Ground Commercial') | (col['SERVICE_LEVEL'] == 'Ground Residential') | (col['SERVICE_LEVEL'] == 'SurePost - Less than 1lb') | (col['SERVICE_LEVEL'] == 'SurePost - Greater than 1lb'): \n",
    "        return col['DISCOUNT_RECIPROCAL_(BDC_PAY_ON_THE_DOLLAR)'] / (1-Margin_on_UPS_Parcel_Freight_GRND_BR)\n",
    "    else:\n",
    "        return col['DISCOUNT_RECIPROCAL_(BDC_PAY_ON_THE_DOLLAR)'] / (1-Margin_on_UPS_Parcel_Freight_AIR_BR)\n",
    "\n",
    "UPS_Rates['ADD_MARGIN_(COL_M/DECIMAL)'] = UPS_Rates.apply(i, axis=1) \n",
    "\n",
    "# add DISCOUNT_TO_CLIENT(1-COLN) column \n",
    "def j(col):   \n",
    "    if (col['SERVICE_LEVEL'] == 'Standard to Canada') | (col['SERVICE_LEVEL'] == 'Standard to Mexico'):\n",
    "        return max(0, 1 - col['ADD_MARGIN_(COL_M/DECIMAL)'])\n",
    "    elif col['SERVICE_LEVEL'] == 'Next Day Air Early AM':\n",
    "        return\n",
    "    else:\n",
    "        return 1 - col['ADD_MARGIN_(COL_M/DECIMAL)']\n",
    "\n",
    "UPS_Rates['DISCOUNT_TO_CLIENT(1-COLN)'] = UPS_Rates.apply(j, axis=1)\n",
    "\n",
    "\n",
    "# This completes the left side of \"Rates\" tab from \"Customer Rate Contract Cards 2023\" doc\n",
    "# UPS_Rates\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "58e8e2f1-fd6c-4fdd-9c26-ca60b0de78df",
   "metadata": {},
   "outputs": [],
   "source": [
    "#The result is BarrettRate when margin = 0.\n",
    "#For UPS client freight & accessorial discounts, write \"YES\"/\"NO\" for rounding/not rounding to 2 decimal places.\n",
    "Round_Discount = \"NO\"\n",
    "\n",
    "BarrettDisc_1STO_FedEx = 0.55\n",
    "\n",
    "#Set UPS margin for the client\n",
    "Margin_on_Min_GRND_BR_FedEx = 0\n",
    "Margin_on_Min_AIR_BR_FedEx = 0\n",
    "\n",
    "Margin_on_FedEx_Parcel_Freight_GRND_BR = 0\n",
    "Margin_on_FedEx_Parcel_Freight_AIR_BR = 0\n",
    "\n",
    "Margin_on_Value_Add_GRND_BR_FedEx = 0\n",
    "Margin_on_Value_Add_AIR_BR_FedEx = 0\n",
    "\n",
    "\n",
    "# UPSAccessorial_2023 has 2 tabs, UPS_Rates and UPS_Accessorial. UPS_Rates is used to create Barrett Rates\n",
    "xls = pd.ExcelFile('Accessorial_2024.xlsx')\n",
    "FedEx_Rates = pd.read_excel(xls, 'FedEx_Rates')\n",
    "\n",
    "# This is the value of the left side of \"Rates\" tab from \"Customer Rate Contract Cards 2023\" doc\n",
    "\n",
    "\n",
    "#add 2 columns to this table with min and max weight\n",
    "def f(col):\n",
    "    if col['WEIGHT_BREAK'] == '1-5lb':\n",
    "        return 5\n",
    "    elif col['WEIGHT_BREAK'] == '6-10lb':\n",
    "        return 10\n",
    "    elif col['WEIGHT_BREAK'] == '11-20lb':\n",
    "        return 20\n",
    "    elif col['WEIGHT_BREAK'] == '21-30lb':\n",
    "        return 30\n",
    "    elif (col['WEIGHT_BREAK'] == '31+lb'):\n",
    "        return 999\n",
    "    elif (col['SERVICE_LEVEL'] == 'FedEx SmartPost - Less than 1lb') & (col['WEIGHT_BREAK'] == '1-3oz'):\n",
    "        return 3\n",
    "    elif (col['SERVICE_LEVEL'] == 'FedEx SmartPost - Less than 1lb') & (col['WEIGHT_BREAK'] == '4-16oz'):\n",
    "        return 16\n",
    "    elif (col['SERVICE_LEVEL'] == 'FedEx SmartPost - Greater than 1lb') & (col['WEIGHT_BREAK'] == '1-2lbs'):\n",
    "        return 2\n",
    "    elif (col['SERVICE_LEVEL'] == 'FedEx SmartPost - Greater than 1lb') & (col['WEIGHT_BREAK'] == '3 lbs'):\n",
    "        return 3\n",
    "    elif (col['SERVICE_LEVEL'] == 'FedEx SmartPost - Greater than 1lb') & (col['WEIGHT_BREAK'] == '4-70lbs'):\n",
    "        return 70\n",
    "    elif (col['SERVICE_LEVEL'] == 'FedEx SmartPost - Greater than 1lb') & (col['WEIGHT_BREAK'] == '70+lbs'):\n",
    "        return 999  \n",
    "\n",
    "def g(col):\n",
    "    if col['WEIGHT_BREAK'] == '1-5lb':\n",
    "        return 1\n",
    "    elif col['WEIGHT_BREAK'] == '6-10lb':\n",
    "        return 6\n",
    "    elif col['WEIGHT_BREAK'] == '11-20lb':\n",
    "        return 11\n",
    "    elif col['WEIGHT_BREAK'] == '21-30lb':\n",
    "        return 21\n",
    "    elif col['WEIGHT_BREAK'] == '31+lb':\n",
    "        return 31\n",
    "    elif (col['SERVICE_LEVEL'] == 'FedEx SmartPost - Less than 1lb') & (col['WEIGHT_BREAK'] == '1-3oz'):\n",
    "        return 1\n",
    "    elif (col['SERVICE_LEVEL'] == 'FedEx SmartPost - Less than 1lb') & (col['WEIGHT_BREAK'] == '4-16oz'):\n",
    "        return 4\n",
    "    elif (col['SERVICE_LEVEL'] == 'FedEx SmartPost - Greater than 1lb') & (col['WEIGHT_BREAK'] == '1-2lbs'):\n",
    "        return 1\n",
    "    elif (col['SERVICE_LEVEL'] == 'FedEx SmartPost - Greater than 1lb') & (col['WEIGHT_BREAK'] == '3 lbs'):\n",
    "        return 3\n",
    "    elif (col['SERVICE_LEVEL'] == 'FedEx SmartPost - Greater than 1lb') & (col['WEIGHT_BREAK'] == '4-70lbs'):\n",
    "        return 4\n",
    "    elif (col['SERVICE_LEVEL'] == 'FedEx SmartPost - Greater than 1lb') & (col['WEIGHT_BREAK'] == '70+lbs'):\n",
    "        return 70\n",
    "\n",
    "FedEx_Rates['WEIGHT_LOWER'] = FedEx_Rates.apply(g, axis=1)\n",
    "FedEx_Rates['WEIGHT_UPPER'] = FedEx_Rates.apply(f, axis=1)\n",
    "\n",
    "\n",
    "# add client margin mins column\n",
    "def h(col):   \n",
    "    if col['SERVICE_LEVEL'] == 'FedEx First Overnight':\n",
    "        return col['MIN_WITH_REBATE']\n",
    "    \n",
    "    elif (col['SERVICE_LEVEL'] == 'FedEx Priority Overnight') | (col['SERVICE_LEVEL'] == 'FedEx Standard Overnight') | (col['SERVICE_LEVEL'] == 'FedEx 2nd Day AM') | (col['SERVICE_LEVEL'] == 'FedEx 2Day') | (col['SERVICE_LEVEL'] == 'FedEx Express Saver'): \n",
    "        try: \n",
    "            return min(col['MIN_WITH_REBATE'] + 4, col['MIN_WITH_REBATE'] / (1-Margin_on_Min_AIR_BR_FedEx))\n",
    "        except ValueError:\n",
    "            return 0\n",
    "\n",
    "    elif (col['SERVICE_LEVEL'] == 'FedEx Ground') | (col['SERVICE_LEVEL'] == 'FedEx Home Delivery'): \n",
    "        try: \n",
    "            return min(10.7, col['MIN_WITH_REBATE'] / (1-Margin_on_Min_GRND_BR_FedEx))\n",
    "        except ValueError:\n",
    "            return 0\n",
    "\n",
    "    elif (col['SERVICE_LEVEL'] == 'FedEx SmartPost - Less than 1lb'): \n",
    "        try: \n",
    "            return min(10.7, col['MIN_WITH_REBATE'] / (1-Margin_on_Min_GRND_BR_FedEx))\n",
    "        except ValueError:\n",
    "            return 0\n",
    "    \n",
    "    elif (col['SERVICE_LEVEL'] == 'FedEx SmartPost - Greater than 1lb'): \n",
    "        try: \n",
    "            return min(10.7, col['MIN_WITH_REBATE'] / (1-Margin_on_Min_GRND_BR_FedEx))\n",
    "        except ValueError:\n",
    "            return 0\n",
    "  \n",
    "    elif col['SERVICE_LEVEL'] == 'FedEx MWT': \n",
    "        try: \n",
    "            return min(80, col['MIN_WITH_REBATE'] / (1-Margin_on_Min_GRND_BR_FedEx))\n",
    "        except ValueError:\n",
    "            return 0\n",
    "  \n",
    "    elif (col['SERVICE_LEVEL'] == 'FedEx Ground Canada') | (col['SERVICE_LEVEL'] == 'FedEx Ground Mexico'):\n",
    "        try: \n",
    "            return min(col['MIN_WITH_REBATE'] + 4, col['MIN_WITH_REBATE'] / (1-Margin_on_Min_GRND_BR_FedEx))\n",
    "        except ValueError:\n",
    "            return 0\n",
    "\n",
    "FedEx_Rates['CLIENT_MARGIN_MINS'] = FedEx_Rates.apply(h, axis=1)\n",
    "\n",
    "# add ADD_MARGIN_(COL_M/DECIMAL) column\n",
    "FedEx_Rates['DISCOUNT_RECIPROCAL_(BDC_PAY_ON_THE_DOLLAR)'] = 1 - FedEx_Rates['NET_CONTRACT_DISC']\n",
    "FedEx_Rates.loc[FedEx_Rates['SERVICE_LEVEL'] == 'FedEx First Overnight', 'DISCOUNT_RECIPROCAL_(BDC_PAY_ON_THE_DOLLAR)'] = None\n",
    "\n",
    "def i(col):   \n",
    "    if (col['SERVICE_LEVEL'] == 'FedEx SmartPost - Greater than 1lb') | (col['WEIGHT_BREAK'] == '70lb'):\n",
    "        return col['DISCOUNT_RECIPROCAL_(BDC_PAY_ON_THE_DOLLAR)'] / (1-Margin_on_FedEx_Parcel_Freight_AIR_BR)\n",
    "    elif col['SERVICE_LEVEL'] == 'FedEx First Overnight':\n",
    "        return\n",
    "    elif (col['SERVICE_LEVEL'] == 'FedEx Ground') | (col['SERVICE_LEVEL'] == 'FedEx Home Delivery') | (col['SERVICE_LEVEL'] == 'FedEx SmartPost - Less than 1lb') | (col['SERVICE_LEVEL'] == 'FedEx SmartPost - Greater than 1lb'): \n",
    "        return col['DISCOUNT_RECIPROCAL_(BDC_PAY_ON_THE_DOLLAR)'] / (1-Margin_on_FedEx_Parcel_Freight_GRND_BR)\n",
    "    else:\n",
    "        return col['DISCOUNT_RECIPROCAL_(BDC_PAY_ON_THE_DOLLAR)'] / (1-Margin_on_FedEx_Parcel_Freight_AIR_BR)\n",
    "\n",
    "FedEx_Rates['ADD_MARGIN_(COL_M/DECIMAL)'] = FedEx_Rates.apply(i, axis=1) \n",
    "\n",
    "# add DISCOUNT_TO_CLIENT(1-COLN) column \n",
    "def j(col):   \n",
    "    if (col['SERVICE_LEVEL'] == 'Standard to Canada') | (col['SERVICE_LEVEL'] == 'Standard to Mexico'):\n",
    "        return max(0, 1 - col['ADD_MARGIN_(COL_M/DECIMAL)'])\n",
    "    elif col['SERVICE_LEVEL'] == 'FedEx First Overnight':\n",
    "        return\n",
    "    else:\n",
    "        return 1 - col['ADD_MARGIN_(COL_M/DECIMAL)']\n",
    "\n",
    "FedEx_Rates['DISCOUNT_TO_CLIENT(1-COLN)'] = FedEx_Rates.apply(j, axis=1)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ebca0a96",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\SLiu\\AppData\\Local\\Temp\\ipykernel_7228\\1387918324.py:14: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  tbl_barrettRates_1['BARRETT_RATES_1'] = tbl_barrettRates_1['PUBLISHED_RATES']*(1-DISCOUNT_TO_CLIENT_1)\n",
      "C:\\Users\\SLiu\\AppData\\Local\\Temp\\ipykernel_7228\\1387918324.py:15: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  tbl_barrettRates_1['BARRETT_RATES'] = round(tbl_barrettRates_1['BARRETT_RATES_1'].apply(lambda x: max(x, CLIENT_MARGIN_MINS_1)),2)\n",
      "C:\\Users\\SLiu\\AppData\\Local\\Temp\\ipykernel_7228\\1387918324.py:27: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  tbl_barrettRates_2['BARRETT_RATES_1'] = tbl_barrettRates_2['PUBLISHED_RATES']*(1-DISCOUNT_TO_CLIENT_2)\n",
      "C:\\Users\\SLiu\\AppData\\Local\\Temp\\ipykernel_7228\\1387918324.py:28: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  tbl_barrettRates_2['BARRETT_RATES'] = round(tbl_barrettRates_2['BARRETT_RATES_1'].apply(lambda x: max(x, CLIENT_MARGIN_MINS_2)),2)\n",
      "C:\\Users\\SLiu\\AppData\\Local\\Temp\\ipykernel_7228\\1387918324.py:40: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  tbl_barrettRates_3['BARRETT_RATES_1'] = tbl_barrettRates_3['PUBLISHED_RATES']*(1-DISCOUNT_TO_CLIENT_3)\n",
      "C:\\Users\\SLiu\\AppData\\Local\\Temp\\ipykernel_7228\\1387918324.py:41: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  tbl_barrettRates_3['BARRETT_RATES'] = round(tbl_barrettRates_3['BARRETT_RATES_1'].apply(lambda x: max(x, CLIENT_MARGIN_MINS_3)),2)\n",
      "C:\\Users\\SLiu\\AppData\\Local\\Temp\\ipykernel_7228\\1387918324.py:53: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  tbl_barrettRates_4['BARRETT_RATES_1'] = tbl_barrettRates_4['PUBLISHED_RATES']*(1-DISCOUNT_TO_CLIENT_4)\n",
      "C:\\Users\\SLiu\\AppData\\Local\\Temp\\ipykernel_7228\\1387918324.py:54: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  tbl_barrettRates_4['BARRETT_RATES'] = round(tbl_barrettRates_4['BARRETT_RATES_1'].apply(lambda x: max(x, CLIENT_MARGIN_MINS_4)),2)\n",
      "C:\\Users\\SLiu\\AppData\\Local\\Temp\\ipykernel_7228\\1387918324.py:66: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  tbl_barrettRates_5['BARRETT_RATES_1'] = tbl_barrettRates_5['PUBLISHED_RATES']*(1-DISCOUNT_TO_CLIENT_5)\n",
      "C:\\Users\\SLiu\\AppData\\Local\\Temp\\ipykernel_7228\\1387918324.py:67: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  tbl_barrettRates_5['BARRETT_RATES'] = round(tbl_barrettRates_5['BARRETT_RATES_1'].apply(lambda x: max(x, CLIENT_MARGIN_MINS_5)),2)\n",
      "C:\\Users\\SLiu\\AppData\\Local\\Temp\\ipykernel_7228\\1387918324.py:80: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  tbl_barrettRates_6['BARRETT_RATES_1'] = tbl_barrettRates_6['PUBLISHED_RATES']*(1-DISCOUNT_TO_CLIENT_6)\n",
      "C:\\Users\\SLiu\\AppData\\Local\\Temp\\ipykernel_7228\\1387918324.py:81: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  tbl_barrettRates_6['BARRETT_RATES'] = round(tbl_barrettRates_6['BARRETT_RATES_1'].apply(lambda x: max(x, CLIENT_MARGIN_MINS_6)),2)\n",
      "C:\\Users\\SLiu\\AppData\\Local\\Temp\\ipykernel_7228\\1387918324.py:93: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  tbl_barrettRates_7['BARRETT_RATES_1'] = tbl_barrettRates_7['PUBLISHED_RATES']*(1-DISCOUNT_TO_CLIENT_7)\n",
      "C:\\Users\\SLiu\\AppData\\Local\\Temp\\ipykernel_7228\\1387918324.py:94: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  tbl_barrettRates_7['BARRETT_RATES'] = round(tbl_barrettRates_7['BARRETT_RATES_1'].apply(lambda x: max(x, CLIENT_MARGIN_MINS_7)),2)\n",
      "C:\\Users\\SLiu\\AppData\\Local\\Temp\\ipykernel_7228\\1387918324.py:106: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  tbl_barrettRates_8['BARRETT_RATES_1'] = tbl_barrettRates_8['PUBLISHED_RATES']*(1-DISCOUNT_TO_CLIENT_8)\n",
      "C:\\Users\\SLiu\\AppData\\Local\\Temp\\ipykernel_7228\\1387918324.py:107: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  tbl_barrettRates_8['BARRETT_RATES'] = round(tbl_barrettRates_8['BARRETT_RATES_1'].apply(lambda x: max(x, CLIENT_MARGIN_MINS_8)),2)\n",
      "C:\\Users\\SLiu\\AppData\\Local\\Temp\\ipykernel_7228\\1387918324.py:119: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  tbl_barrettRates_9['BARRETT_RATES_1'] = tbl_barrettRates_9['PUBLISHED_RATES']*(1-DISCOUNT_TO_CLIENT_9)\n",
      "C:\\Users\\SLiu\\AppData\\Local\\Temp\\ipykernel_7228\\1387918324.py:120: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  tbl_barrettRates_9['BARRETT_RATES'] = round(tbl_barrettRates_9['BARRETT_RATES_1'].apply(lambda x: max(x, CLIENT_MARGIN_MINS_9)),2)\n",
      "C:\\Users\\SLiu\\AppData\\Local\\Temp\\ipykernel_7228\\1387918324.py:132: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  tbl_barrettRates_10['BARRETT_RATES_1'] = tbl_barrettRates_10['PUBLISHED_RATES']*(1-DISCOUNT_TO_CLIENT_10)\n",
      "C:\\Users\\SLiu\\AppData\\Local\\Temp\\ipykernel_7228\\1387918324.py:133: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  tbl_barrettRates_10['BARRETT_RATES'] = round(tbl_barrettRates_10['BARRETT_RATES_1'].apply(lambda x: max(x, CLIENT_MARGIN_MINS_10)),2)\n",
      "C:\\Users\\SLiu\\AppData\\Local\\Temp\\ipykernel_7228\\1387918324.py:146: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  tbl_barrettRates_11['BARRETT_RATES_1'] = tbl_barrettRates_11['PUBLISHED_RATES']*(1-DISCOUNT_TO_CLIENT_11)\n",
      "C:\\Users\\SLiu\\AppData\\Local\\Temp\\ipykernel_7228\\1387918324.py:147: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  tbl_barrettRates_11['BARRETT_RATES'] = round(tbl_barrettRates_11['BARRETT_RATES_1'].apply(lambda x: max(x, CLIENT_MARGIN_MINS_11)),2)\n",
      "C:\\Users\\SLiu\\AppData\\Local\\Temp\\ipykernel_7228\\1387918324.py:159: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  tbl_barrettRates_12['BARRETT_RATES_1'] = tbl_barrettRates_12['PUBLISHED_RATES']*(1-DISCOUNT_TO_CLIENT_12)\n",
      "C:\\Users\\SLiu\\AppData\\Local\\Temp\\ipykernel_7228\\1387918324.py:160: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  tbl_barrettRates_12['BARRETT_RATES'] = round(tbl_barrettRates_12['BARRETT_RATES_1'].apply(lambda x: max(x, CLIENT_MARGIN_MINS_12)),2)\n",
      "C:\\Users\\SLiu\\AppData\\Local\\Temp\\ipykernel_7228\\1387918324.py:172: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  tbl_barrettRates_13['BARRETT_RATES_1'] = tbl_barrettRates_13['PUBLISHED_RATES']*(1-DISCOUNT_TO_CLIENT_13)\n",
      "C:\\Users\\SLiu\\AppData\\Local\\Temp\\ipykernel_7228\\1387918324.py:173: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  tbl_barrettRates_13['BARRETT_RATES'] = round(tbl_barrettRates_13['BARRETT_RATES_1'].apply(lambda x: max(x, CLIENT_MARGIN_MINS_13)),2)\n",
      "C:\\Users\\SLiu\\AppData\\Local\\Temp\\ipykernel_7228\\1387918324.py:185: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  tbl_barrettRates_14['BARRETT_RATES_1'] = tbl_barrettRates_14['PUBLISHED_RATES']*(1-DISCOUNT_TO_CLIENT_14)\n",
      "C:\\Users\\SLiu\\AppData\\Local\\Temp\\ipykernel_7228\\1387918324.py:186: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  tbl_barrettRates_14['BARRETT_RATES'] = round(tbl_barrettRates_14['BARRETT_RATES_1'].apply(lambda x: max(x, CLIENT_MARGIN_MINS_14)),2)\n",
      "C:\\Users\\SLiu\\AppData\\Local\\Temp\\ipykernel_7228\\1387918324.py:193: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  tbl_barrettRates_15['BARRETT_RATES'] = tbl_barrettRates_15['PUBLISHED_RATES']\n",
      "C:\\Users\\SLiu\\AppData\\Local\\Temp\\ipykernel_7228\\1387918324.py:204: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  tbl_barrettRates_16['BARRETT_RATES_1'] = tbl_barrettRates_16['PUBLISHED_RATES']*(1-DISCOUNT_TO_CLIENT_16)\n",
      "C:\\Users\\SLiu\\AppData\\Local\\Temp\\ipykernel_7228\\1387918324.py:205: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  tbl_barrettRates_16['BARRETT_RATES'] = round(tbl_barrettRates_16['BARRETT_RATES_1'].apply(lambda x: max(x, CLIENT_MARGIN_MINS_16)),2)\n",
      "C:\\Users\\SLiu\\AppData\\Local\\Temp\\ipykernel_7228\\1387918324.py:213: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  tbl_barrettRates_17['BARRETT_RATES_1'] = tbl_barrettRates_17['PUBLISHED_RATES']*(1-DISCOUNT_TO_CLIENT_17)\n",
      "C:\\Users\\SLiu\\AppData\\Local\\Temp\\ipykernel_7228\\1387918324.py:214: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  tbl_barrettRates_17['BARRETT_RATES'] = round(tbl_barrettRates_17['BARRETT_RATES_1'].apply(lambda x: max(x, CLIENT_MARGIN_MINS_17)),2)\n",
      "C:\\Users\\SLiu\\AppData\\Local\\Temp\\ipykernel_7228\\1387918324.py:222: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  tbl_barrettRates_18['BARRETT_RATES_1'] = tbl_barrettRates_18['PUBLISHED_RATES']*(1-DISCOUNT_TO_CLIENT_18)\n",
      "C:\\Users\\SLiu\\AppData\\Local\\Temp\\ipykernel_7228\\1387918324.py:223: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  tbl_barrettRates_18['BARRETT_RATES'] = round(tbl_barrettRates_18['BARRETT_RATES_1'].apply(lambda x: max(x, CLIENT_MARGIN_MINS_18)),2)\n",
      "C:\\Users\\SLiu\\AppData\\Local\\Temp\\ipykernel_7228\\1387918324.py:235: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  tbl_barrettRates_19['BARRETT_RATES_1'] = tbl_barrettRates_19['PUBLISHED_RATES']*(1-DISCOUNT_TO_CLIENT_19)\n",
      "C:\\Users\\SLiu\\AppData\\Local\\Temp\\ipykernel_7228\\1387918324.py:236: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  tbl_barrettRates_19['BARRETT_RATES'] = round(tbl_barrettRates_19['BARRETT_RATES_1'].apply(lambda x: max(x, CLIENT_MARGIN_MINS_19)),2)\n",
      "C:\\Users\\SLiu\\AppData\\Local\\Temp\\ipykernel_7228\\1387918324.py:248: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  tbl_barrettRates_20['BARRETT_RATES_1'] = tbl_barrettRates_20['PUBLISHED_RATES']*(1-DISCOUNT_TO_CLIENT_20)\n",
      "C:\\Users\\SLiu\\AppData\\Local\\Temp\\ipykernel_7228\\1387918324.py:249: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  tbl_barrettRates_20['BARRETT_RATES'] = round(tbl_barrettRates_20['BARRETT_RATES_1'].apply(lambda x: max(x, CLIENT_MARGIN_MINS_20)),2)\n",
      "C:\\Users\\SLiu\\AppData\\Local\\Temp\\ipykernel_7228\\1387918324.py:261: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  tbl_barrettRates_21['BARRETT_RATES_1'] = tbl_barrettRates_21['PUBLISHED_RATES']*(1-DISCOUNT_TO_CLIENT_21)\n",
      "C:\\Users\\SLiu\\AppData\\Local\\Temp\\ipykernel_7228\\1387918324.py:262: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  tbl_barrettRates_21['BARRETT_RATES'] = round(tbl_barrettRates_21['BARRETT_RATES_1'].apply(lambda x: max(x, CLIENT_MARGIN_MINS_21)),2)\n"
     ]
    }
   ],
   "source": [
    "#GRND ['WEIGHT_BREAK'] == '1-5lb'\n",
    "# slice the published rates\n",
    "tbl_barrettRates_1 = tbl_publishedRates[(tbl_publishedRates['SERVICE_CODE'] == 'GRND') & (tbl_publishedRates['WEIGHT_LB'] <= 5)]\n",
    "\n",
    "# get DISCOUNT_TO_CLIENT(1-COLN) and CLIENT_MARGIN_MINS\n",
    "DISCOUNT_TO_CLIENT_1 = UPS_Rates.loc[(UPS_Rates['SERVICE_CODE'] == 'GRND') & (UPS_Rates['WEIGHT_BREAK'] == '1-5lb'), 'DISCOUNT_TO_CLIENT(1-COLN)'].values[0]\n",
    "if Round_Discount == 'YES':\n",
    "    DISCOUNT_TO_CLIENT_1 = round(DISCOUNT_TO_CLIENT_1,2)\n",
    "else:\n",
    "    DISCOUNT_TO_CLIENT_1 = DISCOUNT_TO_CLIENT_1\n",
    "CLIENT_MARGIN_MINS_1 = UPS_Rates.loc[(UPS_Rates['SERVICE_CODE'] == 'GRND') & (UPS_Rates['WEIGHT_BREAK'] == '1-5lb'), 'CLIENT_MARGIN_MINS'].values[0]\n",
    "\n",
    "# use max(published rates * (1-round(DISCOUNT_TO_CLIENT)), CLIENT_MARGIN_MINS) to get the client rate\n",
    "tbl_barrettRates_1['BARRETT_RATES_1'] = tbl_barrettRates_1['PUBLISHED_RATES']*(1-DISCOUNT_TO_CLIENT_1)\n",
    "tbl_barrettRates_1['BARRETT_RATES'] = round(tbl_barrettRates_1['BARRETT_RATES_1'].apply(lambda x: max(x, CLIENT_MARGIN_MINS_1)),2)\n",
    "tbl_barrettRates_1 = tbl_barrettRates_1.drop('BARRETT_RATES_1', axis=1)\n",
    "\n",
    "\n",
    "#GRND ['WEIGHT_BREAK'] == '6-10lb'\n",
    "tbl_barrettRates_2 = tbl_publishedRates[(tbl_publishedRates['SERVICE_CODE'] == 'GRND') & (tbl_publishedRates['WEIGHT_LB'] <= 10) & (tbl_publishedRates['WEIGHT_LB'] >= 6)]\n",
    "DISCOUNT_TO_CLIENT_2 = UPS_Rates.loc[(UPS_Rates['SERVICE_CODE'] == 'GRND') & (UPS_Rates['WEIGHT_BREAK'] == '6-10lb'), 'DISCOUNT_TO_CLIENT(1-COLN)'].values[0]\n",
    "if Round_Discount == 'YES':\n",
    "    DISCOUNT_TO_CLIENT_2 = round(DISCOUNT_TO_CLIENT_2,2)\n",
    "else:\n",
    "    DISCOUNT_TO_CLIENT_2 = DISCOUNT_TO_CLIENT_2\n",
    "CLIENT_MARGIN_MINS_2 = UPS_Rates.loc[(UPS_Rates['SERVICE_CODE'] == 'GRND') & (UPS_Rates['WEIGHT_BREAK'] == '6-10lb'), 'CLIENT_MARGIN_MINS'].values[0]\n",
    "tbl_barrettRates_2['BARRETT_RATES_1'] = tbl_barrettRates_2['PUBLISHED_RATES']*(1-DISCOUNT_TO_CLIENT_2)\n",
    "tbl_barrettRates_2['BARRETT_RATES'] = round(tbl_barrettRates_2['BARRETT_RATES_1'].apply(lambda x: max(x, CLIENT_MARGIN_MINS_2)),2)\n",
    "tbl_barrettRates_2 = tbl_barrettRates_2.drop('BARRETT_RATES_1', axis=1)\n",
    "\n",
    "\n",
    "#GRND ['WEIGHT_BREAK'] == '11-20lb'\n",
    "tbl_barrettRates_3 = tbl_publishedRates[(tbl_publishedRates['SERVICE_CODE'] == 'GRND') & (tbl_publishedRates['WEIGHT_LB'] <= 20) & (tbl_publishedRates['WEIGHT_LB'] >= 11)]\n",
    "DISCOUNT_TO_CLIENT_3 = UPS_Rates.loc[(UPS_Rates['SERVICE_CODE'] == 'GRND') & (UPS_Rates['WEIGHT_BREAK'] == '11-20lb'), 'DISCOUNT_TO_CLIENT(1-COLN)'].values[0]\n",
    "if Round_Discount == 'YES':\n",
    "    DISCOUNT_TO_CLIENT_3 = round(DISCOUNT_TO_CLIENT_3,2)\n",
    "else:\n",
    "    DISCOUNT_TO_CLIENT_3 = DISCOUNT_TO_CLIENT_3\n",
    "CLIENT_MARGIN_MINS_3 = UPS_Rates.loc[(UPS_Rates['SERVICE_CODE'] == 'GRND') & (UPS_Rates['WEIGHT_BREAK'] == '11-20lb'), 'CLIENT_MARGIN_MINS'].values[0]\n",
    "tbl_barrettRates_3['BARRETT_RATES_1'] = tbl_barrettRates_3['PUBLISHED_RATES']*(1-DISCOUNT_TO_CLIENT_3)\n",
    "tbl_barrettRates_3['BARRETT_RATES'] = round(tbl_barrettRates_3['BARRETT_RATES_1'].apply(lambda x: max(x, CLIENT_MARGIN_MINS_3)),2)\n",
    "tbl_barrettRates_3 = tbl_barrettRates_3.drop('BARRETT_RATES_1', axis=1)\n",
    "\n",
    "\n",
    "#GRND ['WEIGHT_BREAK'] == '21-30lb'\n",
    "tbl_barrettRates_4 = tbl_publishedRates[(tbl_publishedRates['SERVICE_CODE'] == 'GRND') & (tbl_publishedRates['WEIGHT_LB'] <= 30) & (tbl_publishedRates['WEIGHT_LB'] >= 21)]\n",
    "DISCOUNT_TO_CLIENT_4 = UPS_Rates.loc[(UPS_Rates['SERVICE_CODE'] == 'GRND') & (UPS_Rates['WEIGHT_BREAK'] == '21-30lb'), 'DISCOUNT_TO_CLIENT(1-COLN)'].values[0]\n",
    "if Round_Discount == 'YES':\n",
    "    DISCOUNT_TO_CLIENT_4 = round(DISCOUNT_TO_CLIENT_4,2)\n",
    "else:\n",
    "    DISCOUNT_TO_CLIENT_4 = DISCOUNT_TO_CLIENT_4\n",
    "CLIENT_MARGIN_MINS_4 = UPS_Rates.loc[(UPS_Rates['SERVICE_CODE'] == 'GRND') & (UPS_Rates['WEIGHT_BREAK'] == '21-30lb'), 'CLIENT_MARGIN_MINS'].values[0]\n",
    "tbl_barrettRates_4['BARRETT_RATES_1'] = tbl_barrettRates_4['PUBLISHED_RATES']*(1-DISCOUNT_TO_CLIENT_4)\n",
    "tbl_barrettRates_4['BARRETT_RATES'] = round(tbl_barrettRates_4['BARRETT_RATES_1'].apply(lambda x: max(x, CLIENT_MARGIN_MINS_4)),2)\n",
    "tbl_barrettRates_4 = tbl_barrettRates_4.drop('BARRETT_RATES_1', axis=1)\n",
    "\n",
    "\n",
    "#GRND ['WEIGHT_BREAK'] == '31+lb'\n",
    "tbl_barrettRates_5 = tbl_publishedRates[(tbl_publishedRates['SERVICE_CODE'] == 'GRND') & (tbl_publishedRates['WEIGHT_LB'] >= 31)]\n",
    "DISCOUNT_TO_CLIENT_5 = UPS_Rates.loc[(UPS_Rates['SERVICE_CODE'] == 'GRND') & (UPS_Rates['WEIGHT_BREAK'] == '31+lb'), 'DISCOUNT_TO_CLIENT(1-COLN)'].values[0]\n",
    "if Round_Discount == 'YES':\n",
    "    DISCOUNT_TO_CLIENT_5 = round(DISCOUNT_TO_CLIENT_5,2)\n",
    "else:\n",
    "    DISCOUNT_TO_CLIENT_5 = DISCOUNT_TO_CLIENT_5\n",
    "CLIENT_MARGIN_MINS_5 = UPS_Rates.loc[(UPS_Rates['SERVICE_CODE'] == 'GRND') & (UPS_Rates['WEIGHT_BREAK'] == '31+lb'), 'CLIENT_MARGIN_MINS'].values[0]\n",
    "tbl_barrettRates_5['BARRETT_RATES_1'] = tbl_barrettRates_5['PUBLISHED_RATES']*(1-DISCOUNT_TO_CLIENT_5)\n",
    "tbl_barrettRates_5['BARRETT_RATES'] = round(tbl_barrettRates_5['BARRETT_RATES_1'].apply(lambda x: max(x, CLIENT_MARGIN_MINS_5)),2)\n",
    "tbl_barrettRates_5 = tbl_barrettRates_5.drop('BARRETT_RATES_1', axis=1)\n",
    "\n",
    "\n",
    "#GRES \n",
    "#GRES ['WEIGHT_BREAK'] == '1-5lb'\n",
    "tbl_barrettRates_6 = tbl_publishedRates[(tbl_publishedRates['SERVICE_CODE'] == 'GRES') & (tbl_publishedRates['WEIGHT_LB'] <= 5)]\n",
    "DISCOUNT_TO_CLIENT_6 = UPS_Rates.loc[(UPS_Rates['SERVICE_CODE'] == 'GRES') & (UPS_Rates['WEIGHT_BREAK'] == '1-5lb'), 'DISCOUNT_TO_CLIENT(1-COLN)'].values[0]\n",
    "if Round_Discount == 'YES':\n",
    "    DISCOUNT_TO_CLIENT_6 = round(DISCOUNT_TO_CLIENT_6,2)\n",
    "else:\n",
    "    DISCOUNT_TO_CLIENT_6 = DISCOUNT_TO_CLIENT_6\n",
    "CLIENT_MARGIN_MINS_6 = UPS_Rates.loc[(UPS_Rates['SERVICE_CODE'] == 'GRES') & (UPS_Rates['WEIGHT_BREAK'] == '1-5lb'), 'CLIENT_MARGIN_MINS'].values[0]\n",
    "tbl_barrettRates_6['BARRETT_RATES_1'] = tbl_barrettRates_6['PUBLISHED_RATES']*(1-DISCOUNT_TO_CLIENT_6)\n",
    "tbl_barrettRates_6['BARRETT_RATES'] = round(tbl_barrettRates_6['BARRETT_RATES_1'].apply(lambda x: max(x, CLIENT_MARGIN_MINS_6)),2)\n",
    "tbl_barrettRates_6 = tbl_barrettRates_6.drop('BARRETT_RATES_1', axis=1)\n",
    "\n",
    "\n",
    "#GRES ['WEIGHT_BREAK'] == '6-10lb'\n",
    "tbl_barrettRates_7 = tbl_publishedRates[(tbl_publishedRates['SERVICE_CODE'] == 'GRES') & (tbl_publishedRates['WEIGHT_LB'] <= 10) & (tbl_publishedRates['WEIGHT_LB'] >= 6)]\n",
    "DISCOUNT_TO_CLIENT_7 = UPS_Rates.loc[(UPS_Rates['SERVICE_CODE'] == 'GRES') & (UPS_Rates['WEIGHT_BREAK'] == '6-10lb'), 'DISCOUNT_TO_CLIENT(1-COLN)'].values[0]\n",
    "if Round_Discount == 'YES':\n",
    "    DISCOUNT_TO_CLIENT_7 = round(DISCOUNT_TO_CLIENT_7,2)\n",
    "else:\n",
    "    DISCOUNT_TO_CLIENT_7 = DISCOUNT_TO_CLIENT_7\n",
    "CLIENT_MARGIN_MINS_7 = UPS_Rates.loc[(UPS_Rates['SERVICE_CODE'] == 'GRES') & (UPS_Rates['WEIGHT_BREAK'] == '6-10lb'), 'CLIENT_MARGIN_MINS'].values[0]\n",
    "tbl_barrettRates_7['BARRETT_RATES_1'] = tbl_barrettRates_7['PUBLISHED_RATES']*(1-DISCOUNT_TO_CLIENT_7)\n",
    "tbl_barrettRates_7['BARRETT_RATES'] = round(tbl_barrettRates_7['BARRETT_RATES_1'].apply(lambda x: max(x, CLIENT_MARGIN_MINS_7)),2)\n",
    "tbl_barrettRates_7 = tbl_barrettRates_7.drop('BARRETT_RATES_1', axis=1)\n",
    "\n",
    "\n",
    "#GRES ['WEIGHT_BREAK'] == '11-20lb'\n",
    "tbl_barrettRates_8 = tbl_publishedRates[(tbl_publishedRates['SERVICE_CODE'] == 'GRES') & (tbl_publishedRates['WEIGHT_LB'] <= 20) & (tbl_publishedRates['WEIGHT_LB'] >= 11)]\n",
    "DISCOUNT_TO_CLIENT_8 = UPS_Rates.loc[(UPS_Rates['SERVICE_CODE'] == 'GRES') & (UPS_Rates['WEIGHT_BREAK'] == '11-20lb'), 'DISCOUNT_TO_CLIENT(1-COLN)'].values[0]\n",
    "if Round_Discount == 'YES':\n",
    "    DISCOUNT_TO_CLIENT_8 = round(DISCOUNT_TO_CLIENT_8,2)\n",
    "else:\n",
    "    DISCOUNT_TO_CLIENT_8 = DISCOUNT_TO_CLIENT_8\n",
    "CLIENT_MARGIN_MINS_8 = UPS_Rates.loc[(UPS_Rates['SERVICE_CODE'] == 'GRES') & (UPS_Rates['WEIGHT_BREAK'] == '11-20lb'), 'CLIENT_MARGIN_MINS'].values[0]\n",
    "tbl_barrettRates_8['BARRETT_RATES_1'] = tbl_barrettRates_8['PUBLISHED_RATES']*(1-DISCOUNT_TO_CLIENT_8)\n",
    "tbl_barrettRates_8['BARRETT_RATES'] = round(tbl_barrettRates_8['BARRETT_RATES_1'].apply(lambda x: max(x, CLIENT_MARGIN_MINS_8)),2)\n",
    "tbl_barrettRates_8 = tbl_barrettRates_8.drop('BARRETT_RATES_1', axis=1)\n",
    "\n",
    "\n",
    "#GRES ['WEIGHT_BREAK'] == '21-30lb'\n",
    "tbl_barrettRates_9 = tbl_publishedRates[(tbl_publishedRates['SERVICE_CODE'] == 'GRES') & (tbl_publishedRates['WEIGHT_LB'] <= 30) & (tbl_publishedRates['WEIGHT_LB'] >= 21)]\n",
    "DISCOUNT_TO_CLIENT_9 = UPS_Rates.loc[(UPS_Rates['SERVICE_CODE'] == 'GRES') & (UPS_Rates['WEIGHT_BREAK'] == '21-30lb'), 'DISCOUNT_TO_CLIENT(1-COLN)'].values[0]\n",
    "if Round_Discount == 'YES':\n",
    "    DISCOUNT_TO_CLIENT_9 = round(DISCOUNT_TO_CLIENT_9,2)\n",
    "else:\n",
    "    DISCOUNT_TO_CLIENT_9 = DISCOUNT_TO_CLIENT_9\n",
    "CLIENT_MARGIN_MINS_9 = UPS_Rates.loc[(UPS_Rates['SERVICE_CODE'] == 'GRES') & (UPS_Rates['WEIGHT_BREAK'] == '21-30lb'), 'CLIENT_MARGIN_MINS'].values[0]\n",
    "tbl_barrettRates_9['BARRETT_RATES_1'] = tbl_barrettRates_9['PUBLISHED_RATES']*(1-DISCOUNT_TO_CLIENT_9)\n",
    "tbl_barrettRates_9['BARRETT_RATES'] = round(tbl_barrettRates_9['BARRETT_RATES_1'].apply(lambda x: max(x, CLIENT_MARGIN_MINS_9)),2)\n",
    "tbl_barrettRates_9 = tbl_barrettRates_9.drop('BARRETT_RATES_1', axis=1)\n",
    "\n",
    "\n",
    "#GRES ['WEIGHT_BREAK'] == '31+lb'\n",
    "tbl_barrettRates_10 = tbl_publishedRates[(tbl_publishedRates['SERVICE_CODE'] == 'GRES') & (tbl_publishedRates['WEIGHT_LB'] >= 31)]\n",
    "DISCOUNT_TO_CLIENT_10 = UPS_Rates.loc[(UPS_Rates['SERVICE_CODE'] == 'GRES') & (UPS_Rates['WEIGHT_BREAK'] == '31+lb'), 'DISCOUNT_TO_CLIENT(1-COLN)'].values[0]\n",
    "if Round_Discount == 'YES':\n",
    "    DISCOUNT_TO_CLIENT_10 = round(DISCOUNT_TO_CLIENT_10,2)\n",
    "else:\n",
    "    DISCOUNT_TO_CLIENT_10 = DISCOUNT_TO_CLIENT_10\n",
    "CLIENT_MARGIN_MINS_10 = UPS_Rates.loc[(UPS_Rates['SERVICE_CODE'] == 'GRES') & (UPS_Rates['WEIGHT_BREAK'] == '31+lb'), 'CLIENT_MARGIN_MINS'].values[0]\n",
    "tbl_barrettRates_10['BARRETT_RATES_1'] = tbl_barrettRates_10['PUBLISHED_RATES']*(1-DISCOUNT_TO_CLIENT_10)\n",
    "tbl_barrettRates_10['BARRETT_RATES'] = round(tbl_barrettRates_10['BARRETT_RATES_1'].apply(lambda x: max(x, CLIENT_MARGIN_MINS_10)),2)\n",
    "tbl_barrettRates_10 = tbl_barrettRates_10.drop('BARRETT_RATES_1', axis=1)\n",
    "\n",
    "\n",
    "#SRPT\n",
    "#SRPT ['WEIGHT_BREAK'] == '1-3lb'\n",
    "tbl_barrettRates_11 = tbl_publishedRates[(tbl_publishedRates['SERVICE_CODE'] == 'SRPT') & (tbl_publishedRates['WEIGHT_LB'] <= 3)]\n",
    "DISCOUNT_TO_CLIENT_11 = UPS_Rates.loc[(UPS_Rates['SERVICE_CODE'] == 'SRPT') & (UPS_Rates['WEIGHT_BREAK'] == '1-3lb'), 'DISCOUNT_TO_CLIENT(1-COLN)'].values[0]\n",
    "if Round_Discount == 'YES':\n",
    "    DISCOUNT_TO_CLIENT_11 = round(DISCOUNT_TO_CLIENT_11,2)\n",
    "else:\n",
    "    DISCOUNT_TO_CLIENT_11 = DISCOUNT_TO_CLIENT_11\n",
    "CLIENT_MARGIN_MINS_11 = UPS_Rates.loc[(UPS_Rates['SERVICE_CODE'] == 'SRPT') & (UPS_Rates['WEIGHT_BREAK'] == '1-3lb'), 'CLIENT_MARGIN_MINS'].values[0]\n",
    "tbl_barrettRates_11['BARRETT_RATES_1'] = tbl_barrettRates_11['PUBLISHED_RATES']*(1-DISCOUNT_TO_CLIENT_11)\n",
    "tbl_barrettRates_11['BARRETT_RATES'] = round(tbl_barrettRates_11['BARRETT_RATES_1'].apply(lambda x: max(x, CLIENT_MARGIN_MINS_11)),2)\n",
    "tbl_barrettRates_11 = tbl_barrettRates_11.drop('BARRETT_RATES_1', axis=1)\n",
    "\n",
    "\n",
    "#SRPT ['WEIGHT_BREAK'] == '4-6lb'\n",
    "tbl_barrettRates_12 = tbl_publishedRates[(tbl_publishedRates['SERVICE_CODE'] == 'SRPT') & (tbl_publishedRates['WEIGHT_LB'] >= 4) & (tbl_publishedRates['WEIGHT_LB'] <= 6)]\n",
    "DISCOUNT_TO_CLIENT_12 = UPS_Rates.loc[(UPS_Rates['SERVICE_CODE'] == 'SRPT') & (UPS_Rates['WEIGHT_BREAK'] == '4-6lb'), 'DISCOUNT_TO_CLIENT(1-COLN)'].values[0]\n",
    "if Round_Discount == 'YES':\n",
    "    DISCOUNT_TO_CLIENT_12 = round(DISCOUNT_TO_CLIENT_12,2)\n",
    "else:\n",
    "    DISCOUNT_TO_CLIENT_12 = DISCOUNT_TO_CLIENT_12\n",
    "CLIENT_MARGIN_MINS_12 = UPS_Rates.loc[(UPS_Rates['SERVICE_CODE'] == 'SRPT') & (UPS_Rates['WEIGHT_BREAK'] == '4-6lb'), 'CLIENT_MARGIN_MINS'].values[0]\n",
    "tbl_barrettRates_12['BARRETT_RATES_1'] = tbl_barrettRates_12['PUBLISHED_RATES']*(1-DISCOUNT_TO_CLIENT_12)\n",
    "tbl_barrettRates_12['BARRETT_RATES'] = round(tbl_barrettRates_12['BARRETT_RATES_1'].apply(lambda x: max(x, CLIENT_MARGIN_MINS_12)),2)\n",
    "tbl_barrettRates_12 = tbl_barrettRates_12.drop('BARRETT_RATES_1', axis=1)\n",
    "\n",
    "\n",
    "#SRPT ['WEIGHT_BREAK'] == '7-8lb'\n",
    "tbl_barrettRates_13 = tbl_publishedRates[(tbl_publishedRates['SERVICE_CODE'] == 'SRPT') & (tbl_publishedRates['WEIGHT_LB'] >= 7) & (tbl_publishedRates['WEIGHT_LB'] <= 8)]\n",
    "DISCOUNT_TO_CLIENT_13 = UPS_Rates.loc[(UPS_Rates['SERVICE_CODE'] == 'SRPT') & (UPS_Rates['WEIGHT_BREAK'] == '7-8lb'), 'DISCOUNT_TO_CLIENT(1-COLN)'].values[0]\n",
    "if Round_Discount == 'YES':\n",
    "    DISCOUNT_TO_CLIENT_13 = round(DISCOUNT_TO_CLIENT_13,2)\n",
    "else:\n",
    "    DISCOUNT_TO_CLIENT_13 = DISCOUNT_TO_CLIENT_13\n",
    "CLIENT_MARGIN_MINS_13 = UPS_Rates.loc[(UPS_Rates['SERVICE_CODE'] == 'SRPT') & (UPS_Rates['WEIGHT_BREAK'] == '7-8lb'), 'CLIENT_MARGIN_MINS'].values[0]\n",
    "tbl_barrettRates_13['BARRETT_RATES_1'] = tbl_barrettRates_13['PUBLISHED_RATES']*(1-DISCOUNT_TO_CLIENT_13)\n",
    "tbl_barrettRates_13['BARRETT_RATES'] = round(tbl_barrettRates_13['BARRETT_RATES_1'].apply(lambda x: max(x, CLIENT_MARGIN_MINS_13)),2)\n",
    "tbl_barrettRates_13 = tbl_barrettRates_13.drop('BARRETT_RATES_1', axis=1)\n",
    "\n",
    "\n",
    "#SRPT ['WEIGHT_BREAK'] == '9lb'\n",
    "tbl_barrettRates_14 = tbl_publishedRates[(tbl_publishedRates['SERVICE_CODE'] == 'SRPT') & (tbl_publishedRates['WEIGHT_LB'] == 9)]\n",
    "DISCOUNT_TO_CLIENT_14 = UPS_Rates.loc[(UPS_Rates['SERVICE_CODE'] == 'SRPT') & (UPS_Rates['WEIGHT_BREAK'] == '9lb'), 'DISCOUNT_TO_CLIENT(1-COLN)'].values[0]\n",
    "if Round_Discount == 'YES':\n",
    "    DISCOUNT_TO_CLIENT_14 = round(DISCOUNT_TO_CLIENT_14,2)\n",
    "else:\n",
    "    DISCOUNT_TO_CLIENT_14 = DISCOUNT_TO_CLIENT_14\n",
    "CLIENT_MARGIN_MINS_14 = UPS_Rates.loc[(UPS_Rates['SERVICE_CODE'] == 'SRPT') & (UPS_Rates['WEIGHT_BREAK'] == '9lb'), 'CLIENT_MARGIN_MINS'].values[0]\n",
    "tbl_barrettRates_14['BARRETT_RATES_1'] = tbl_barrettRates_14['PUBLISHED_RATES']*(1-DISCOUNT_TO_CLIENT_14)\n",
    "tbl_barrettRates_14['BARRETT_RATES'] = round(tbl_barrettRates_14['BARRETT_RATES_1'].apply(lambda x: max(x, CLIENT_MARGIN_MINS_14)),2)\n",
    "tbl_barrettRates_14 = tbl_barrettRates_14.drop('BARRETT_RATES_1', axis=1)\n",
    "\n",
    "\n",
    "#Others:\n",
    "#UPS REDE, there is no discount, therefore REDE's client rate = published rate\n",
    "tbl_barrettRates_15 = tbl_publishedRates[(tbl_publishedRates['SERVICE_CODE'] == 'REDE')]\n",
    "tbl_barrettRates_15['BARRETT_RATES'] = tbl_barrettRates_15['PUBLISHED_RATES']\n",
    "\n",
    "\n",
    "#UPS RED\n",
    "tbl_barrettRates_16 = tbl_publishedRates[(tbl_publishedRates['SERVICE_CODE'] == 'RED')]\n",
    "DISCOUNT_TO_CLIENT_16 = UPS_Rates.loc[(UPS_Rates['SERVICE_CODE'] == 'RED'), 'DISCOUNT_TO_CLIENT(1-COLN)'].values[0]\n",
    "if Round_Discount == 'YES':\n",
    "    DISCOUNT_TO_CLIENT_16 = round(DISCOUNT_TO_CLIENT_16,2)\n",
    "else:\n",
    "    DISCOUNT_TO_CLIENT_16 = DISCOUNT_TO_CLIENT_16\n",
    "CLIENT_MARGIN_MINS_16 = UPS_Rates.loc[(UPS_Rates['SERVICE_CODE'] == 'RED'), 'CLIENT_MARGIN_MINS'].values[0]\n",
    "tbl_barrettRates_16['BARRETT_RATES_1'] = tbl_barrettRates_16['PUBLISHED_RATES']*(1-DISCOUNT_TO_CLIENT_16)\n",
    "tbl_barrettRates_16['BARRETT_RATES'] = round(tbl_barrettRates_16['BARRETT_RATES_1'].apply(lambda x: max(x, CLIENT_MARGIN_MINS_16)),2)\n",
    "tbl_barrettRates_16 = tbl_barrettRates_16.drop('BARRETT_RATES_1', axis=1)\n",
    "\n",
    "\n",
    "#UPS REDS, this does not have round up option (1/2)\n",
    "tbl_barrettRates_17 = tbl_publishedRates[(tbl_publishedRates['SERVICE_CODE'] == 'REDS')]\n",
    "DISCOUNT_TO_CLIENT_17 = UPS_Rates.loc[(UPS_Rates['SERVICE_CODE'] == 'REDS'), 'DISCOUNT_TO_CLIENT(1-COLN)'].values[0]\n",
    "CLIENT_MARGIN_MINS_17 = UPS_Rates.loc[(UPS_Rates['SERVICE_CODE'] == 'REDS'), 'CLIENT_MARGIN_MINS'].values[0]\n",
    "tbl_barrettRates_17['BARRETT_RATES_1'] = tbl_barrettRates_17['PUBLISHED_RATES']*(1-DISCOUNT_TO_CLIENT_17)\n",
    "tbl_barrettRates_17['BARRETT_RATES'] = round(tbl_barrettRates_17['BARRETT_RATES_1'].apply(lambda x: max(x, CLIENT_MARGIN_MINS_17)),2)\n",
    "tbl_barrettRates_17 = tbl_barrettRates_17.drop('BARRETT_RATES_1', axis=1)\n",
    "\n",
    "\n",
    "#UPS 2DAM, this does not have round up option (2/2)\n",
    "tbl_barrettRates_18 = tbl_publishedRates[(tbl_publishedRates['SERVICE_CODE'] == '2DAM')]\n",
    "DISCOUNT_TO_CLIENT_18 = UPS_Rates.loc[(UPS_Rates['SERVICE_CODE'] == '2DAM'), 'DISCOUNT_TO_CLIENT(1-COLN)'].values[0]\n",
    "CLIENT_MARGIN_MINS_18 = UPS_Rates.loc[(UPS_Rates['SERVICE_CODE'] == '2DAM'), 'CLIENT_MARGIN_MINS'].values[0]\n",
    "tbl_barrettRates_18['BARRETT_RATES_1'] = tbl_barrettRates_18['PUBLISHED_RATES']*(1-DISCOUNT_TO_CLIENT_18)\n",
    "tbl_barrettRates_18['BARRETT_RATES'] = round(tbl_barrettRates_18['BARRETT_RATES_1'].apply(lambda x: max(x, CLIENT_MARGIN_MINS_18)),2)\n",
    "tbl_barrettRates_18 = tbl_barrettRates_18.drop('BARRETT_RATES_1', axis=1)\n",
    "\n",
    "\n",
    "#UPS BLUE\n",
    "tbl_barrettRates_19 = tbl_publishedRates[(tbl_publishedRates['SERVICE_CODE'] == 'BLUE')]\n",
    "DISCOUNT_TO_CLIENT_19 = UPS_Rates.loc[(UPS_Rates['SERVICE_CODE'] == 'BLUE'), 'DISCOUNT_TO_CLIENT(1-COLN)'].values[0]\n",
    "if Round_Discount == 'YES':\n",
    "    DISCOUNT_TO_CLIENT_19 = round(DISCOUNT_TO_CLIENT_19,2)\n",
    "else:\n",
    "    DISCOUNT_TO_CLIENT_19 = DISCOUNT_TO_CLIENT_19\n",
    "CLIENT_MARGIN_MINS_19 = UPS_Rates.loc[(UPS_Rates['SERVICE_CODE'] == 'BLUE') , 'CLIENT_MARGIN_MINS'].values[0]\n",
    "tbl_barrettRates_19['BARRETT_RATES_1'] = tbl_barrettRates_19['PUBLISHED_RATES']*(1-DISCOUNT_TO_CLIENT_19)\n",
    "tbl_barrettRates_19['BARRETT_RATES'] = round(tbl_barrettRates_19['BARRETT_RATES_1'].apply(lambda x: max(x, CLIENT_MARGIN_MINS_19)),2)\n",
    "tbl_barrettRates_19 = tbl_barrettRates_19.drop('BARRETT_RATES_1', axis=1)\n",
    "\n",
    "\n",
    "#UPS ORNG\n",
    "tbl_barrettRates_20 = tbl_publishedRates[(tbl_publishedRates['SERVICE_CODE'] == 'ORNG')]\n",
    "DISCOUNT_TO_CLIENT_20 = UPS_Rates.loc[(UPS_Rates['SERVICE_CODE'] == 'ORNG'), 'DISCOUNT_TO_CLIENT(1-COLN)'].values[0]\n",
    "if Round_Discount == 'YES':\n",
    "    DISCOUNT_TO_CLIENT_20 = round(DISCOUNT_TO_CLIENT_20,2)\n",
    "else:\n",
    "    DISCOUNT_TO_CLIENT_20 = DISCOUNT_TO_CLIENT_20\n",
    "CLIENT_MARGIN_MINS_20 = UPS_Rates.loc[(UPS_Rates['SERVICE_CODE'] == 'ORNG'), 'CLIENT_MARGIN_MINS'].values[0]\n",
    "tbl_barrettRates_20['BARRETT_RATES_1'] = tbl_barrettRates_20['PUBLISHED_RATES']*(1-DISCOUNT_TO_CLIENT_20)\n",
    "tbl_barrettRates_20['BARRETT_RATES'] = round(tbl_barrettRates_20['BARRETT_RATES_1'].apply(lambda x: max(x, CLIENT_MARGIN_MINS_20)),2)\n",
    "tbl_barrettRates_20 = tbl_barrettRates_20.drop('BARRETT_RATES_1', axis=1)\n",
    "\n",
    "\n",
    "#UPS SRPT<1\n",
    "tbl_barrettRates_21 = tbl_publishedRates[(tbl_publishedRates['SERVICE_CODE'] == 'SRPT<1')]\n",
    "DISCOUNT_TO_CLIENT_21 = UPS_Rates.loc[(UPS_Rates['SERVICE_CODE'] == 'SRPT<1'), 'DISCOUNT_TO_CLIENT(1-COLN)'].values[0]\n",
    "if Round_Discount == 'YES':\n",
    "    DISCOUNT_TO_CLIENT_21 = round(DISCOUNT_TO_CLIENT_21,2)\n",
    "else:\n",
    "    DISCOUNT_TO_CLIENT_21 = DISCOUNT_TO_CLIENT_21\n",
    "CLIENT_MARGIN_MINS_21 = UPS_Rates.loc[(UPS_Rates['SERVICE_CODE'] == 'SRPT<1'), 'CLIENT_MARGIN_MINS'].values[0]\n",
    "tbl_barrettRates_21['BARRETT_RATES_1'] = tbl_barrettRates_21['PUBLISHED_RATES']*(1-DISCOUNT_TO_CLIENT_21)\n",
    "tbl_barrettRates_21['BARRETT_RATES'] = round(tbl_barrettRates_21['BARRETT_RATES_1'].apply(lambda x: max(x, CLIENT_MARGIN_MINS_21)),2)\n",
    "tbl_barrettRates_21 = tbl_barrettRates_21.drop('BARRETT_RATES_1', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4ec606bb-6799-4895-af2f-0bf853a69a06",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\SLiu\\AppData\\Local\\Temp\\ipykernel_7228\\3352589481.py:14: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  tbl_barrettRates_23['BARRETT_RATES_1'] = tbl_barrettRates_23['PUBLISHED_RATES']*(1-DISCOUNT_TO_CLIENT_1)\n",
      "C:\\Users\\SLiu\\AppData\\Local\\Temp\\ipykernel_7228\\3352589481.py:15: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  tbl_barrettRates_23['BARRETT_RATES'] = round(tbl_barrettRates_23['BARRETT_RATES_1'].apply(lambda x: max(x, CLIENT_MARGIN_MINS_1)),2)\n",
      "C:\\Users\\SLiu\\AppData\\Local\\Temp\\ipykernel_7228\\3352589481.py:27: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  tbl_barrettRates_24['BARRETT_RATES_1'] = tbl_barrettRates_24['PUBLISHED_RATES']*(1-DISCOUNT_TO_CLIENT_2)\n",
      "C:\\Users\\SLiu\\AppData\\Local\\Temp\\ipykernel_7228\\3352589481.py:28: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  tbl_barrettRates_24['BARRETT_RATES'] = round(tbl_barrettRates_24['BARRETT_RATES_1'].apply(lambda x: max(x, CLIENT_MARGIN_MINS_2)),2)\n",
      "C:\\Users\\SLiu\\AppData\\Local\\Temp\\ipykernel_7228\\3352589481.py:40: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  tbl_barrettRates_25['BARRETT_RATES_1'] = tbl_barrettRates_25['PUBLISHED_RATES']*(1-DISCOUNT_TO_CLIENT_3)\n",
      "C:\\Users\\SLiu\\AppData\\Local\\Temp\\ipykernel_7228\\3352589481.py:41: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  tbl_barrettRates_25['BARRETT_RATES'] = round(tbl_barrettRates_25['BARRETT_RATES_1'].apply(lambda x: max(x, CLIENT_MARGIN_MINS_3)),2)\n",
      "C:\\Users\\SLiu\\AppData\\Local\\Temp\\ipykernel_7228\\3352589481.py:53: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  tbl_barrettRates_26['BARRETT_RATES_1'] = tbl_barrettRates_26['PUBLISHED_RATES']*(1-DISCOUNT_TO_CLIENT_4)\n",
      "C:\\Users\\SLiu\\AppData\\Local\\Temp\\ipykernel_7228\\3352589481.py:54: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  tbl_barrettRates_26['BARRETT_RATES'] = round(tbl_barrettRates_26['BARRETT_RATES_1'].apply(lambda x: max(x, CLIENT_MARGIN_MINS_4)),2)\n",
      "C:\\Users\\SLiu\\AppData\\Local\\Temp\\ipykernel_7228\\3352589481.py:66: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  tbl_barrettRates_27['BARRETT_RATES_1'] = tbl_barrettRates_27['PUBLISHED_RATES']*(1-DISCOUNT_TO_CLIENT_5)\n",
      "C:\\Users\\SLiu\\AppData\\Local\\Temp\\ipykernel_7228\\3352589481.py:67: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  tbl_barrettRates_27['BARRETT_RATES'] = round(tbl_barrettRates_27['BARRETT_RATES_1'].apply(lambda x: max(x, CLIENT_MARGIN_MINS_5)),2)\n",
      "C:\\Users\\SLiu\\AppData\\Local\\Temp\\ipykernel_7228\\3352589481.py:80: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  tbl_barrettRates_28['BARRETT_RATES_1'] = tbl_barrettRates_28['PUBLISHED_RATES']*(1-DISCOUNT_TO_CLIENT_6)\n",
      "C:\\Users\\SLiu\\AppData\\Local\\Temp\\ipykernel_7228\\3352589481.py:81: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  tbl_barrettRates_28['BARRETT_RATES'] = round(tbl_barrettRates_28['BARRETT_RATES_1'].apply(lambda x: max(x, CLIENT_MARGIN_MINS_6)),2)\n",
      "C:\\Users\\SLiu\\AppData\\Local\\Temp\\ipykernel_7228\\3352589481.py:93: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  tbl_barrettRates_29['BARRETT_RATES_1'] = tbl_barrettRates_29['PUBLISHED_RATES']*(1-DISCOUNT_TO_CLIENT_7)\n",
      "C:\\Users\\SLiu\\AppData\\Local\\Temp\\ipykernel_7228\\3352589481.py:94: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  tbl_barrettRates_29['BARRETT_RATES'] = round(tbl_barrettRates_29['BARRETT_RATES_1'].apply(lambda x: max(x, CLIENT_MARGIN_MINS_7)),2)\n",
      "C:\\Users\\SLiu\\AppData\\Local\\Temp\\ipykernel_7228\\3352589481.py:106: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  tbl_barrettRates_30['BARRETT_RATES_1'] = tbl_barrettRates_30['PUBLISHED_RATES']*(1-DISCOUNT_TO_CLIENT_8)\n",
      "C:\\Users\\SLiu\\AppData\\Local\\Temp\\ipykernel_7228\\3352589481.py:107: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  tbl_barrettRates_30['BARRETT_RATES'] = round(tbl_barrettRates_30['BARRETT_RATES_1'].apply(lambda x: max(x, CLIENT_MARGIN_MINS_8)),2)\n",
      "C:\\Users\\SLiu\\AppData\\Local\\Temp\\ipykernel_7228\\3352589481.py:119: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  tbl_barrettRates_31['BARRETT_RATES_1'] = tbl_barrettRates_31['PUBLISHED_RATES']*(1-DISCOUNT_TO_CLIENT_9)\n",
      "C:\\Users\\SLiu\\AppData\\Local\\Temp\\ipykernel_7228\\3352589481.py:120: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  tbl_barrettRates_31['BARRETT_RATES'] = round(tbl_barrettRates_31['BARRETT_RATES_1'].apply(lambda x: max(x, CLIENT_MARGIN_MINS_9)),2)\n",
      "C:\\Users\\SLiu\\AppData\\Local\\Temp\\ipykernel_7228\\3352589481.py:132: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  tbl_barrettRates_32['BARRETT_RATES_1'] = tbl_barrettRates_32['PUBLISHED_RATES']*(1-DISCOUNT_TO_CLIENT_10)\n",
      "C:\\Users\\SLiu\\AppData\\Local\\Temp\\ipykernel_7228\\3352589481.py:133: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  tbl_barrettRates_32['BARRETT_RATES'] = round(tbl_barrettRates_32['BARRETT_RATES_1'].apply(lambda x: max(x, CLIENT_MARGIN_MINS_10)),2)\n",
      "C:\\Users\\SLiu\\AppData\\Local\\Temp\\ipykernel_7228\\3352589481.py:146: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  tbl_barrettRates_33['BARRETT_RATES_1'] = tbl_barrettRates_33['PUBLISHED_RATES']*(1-DISCOUNT_TO_CLIENT_11)\n",
      "C:\\Users\\SLiu\\AppData\\Local\\Temp\\ipykernel_7228\\3352589481.py:147: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  tbl_barrettRates_33['BARRETT_RATES'] = round(tbl_barrettRates_33['BARRETT_RATES_1'].apply(lambda x: max(x, CLIENT_MARGIN_MINS_11)),2)\n",
      "C:\\Users\\SLiu\\AppData\\Local\\Temp\\ipykernel_7228\\3352589481.py:160: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  tbl_barrettRates_34['BARRETT_RATES_1'] = tbl_barrettRates_34['PUBLISHED_RATES']*(1-DISCOUNT_TO_CLIENT_12)\n",
      "C:\\Users\\SLiu\\AppData\\Local\\Temp\\ipykernel_7228\\3352589481.py:161: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  tbl_barrettRates_34['BARRETT_RATES'] = round(tbl_barrettRates_34['BARRETT_RATES_1'].apply(lambda x: max(x, CLIENT_MARGIN_MINS_12)),2)\n",
      "C:\\Users\\SLiu\\AppData\\Local\\Temp\\ipykernel_7228\\3352589481.py:173: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  tbl_barrettRates_35['BARRETT_RATES_1'] = tbl_barrettRates_35['PUBLISHED_RATES']*(1-DISCOUNT_TO_CLIENT_13)\n",
      "C:\\Users\\SLiu\\AppData\\Local\\Temp\\ipykernel_7228\\3352589481.py:174: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  tbl_barrettRates_35['BARRETT_RATES'] = round(tbl_barrettRates_35['BARRETT_RATES_1'].apply(lambda x: max(x, CLIENT_MARGIN_MINS_13)),2)\n",
      "C:\\Users\\SLiu\\AppData\\Local\\Temp\\ipykernel_7228\\3352589481.py:194: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  tbl_barrettRates_37['BARRETT_RATES'] = tbl_barrettRates_37['PUBLISHED_RATES']*(1-BarrettDisc_1STO_FedEx)\n",
      "C:\\Users\\SLiu\\AppData\\Local\\Temp\\ipykernel_7228\\3352589481.py:205: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  tbl_barrettRates_38['BARRETT_RATES_1'] = tbl_barrettRates_38['PUBLISHED_RATES']*(1-DISCOUNT_TO_CLIENT_16)\n",
      "C:\\Users\\SLiu\\AppData\\Local\\Temp\\ipykernel_7228\\3352589481.py:206: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  tbl_barrettRates_38['BARRETT_RATES'] = round(tbl_barrettRates_38['BARRETT_RATES_1'].apply(lambda x: max(x, CLIENT_MARGIN_MINS_16)),2)\n",
      "C:\\Users\\SLiu\\AppData\\Local\\Temp\\ipykernel_7228\\3352589481.py:214: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  tbl_barrettRates_39['BARRETT_RATES_1'] = tbl_barrettRates_39['PUBLISHED_RATES']*(1-DISCOUNT_TO_CLIENT_17)\n",
      "C:\\Users\\SLiu\\AppData\\Local\\Temp\\ipykernel_7228\\3352589481.py:215: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  tbl_barrettRates_39['BARRETT_RATES'] = round(tbl_barrettRates_39['BARRETT_RATES_1'].apply(lambda x: max(x, CLIENT_MARGIN_MINS_17)),2)\n",
      "C:\\Users\\SLiu\\AppData\\Local\\Temp\\ipykernel_7228\\3352589481.py:223: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  tbl_barrettRates_40['BARRETT_RATES_1'] = tbl_barrettRates_40['PUBLISHED_RATES']*(1-DISCOUNT_TO_CLIENT_18)\n",
      "C:\\Users\\SLiu\\AppData\\Local\\Temp\\ipykernel_7228\\3352589481.py:224: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  tbl_barrettRates_40['BARRETT_RATES'] = round(tbl_barrettRates_40['BARRETT_RATES_1'].apply(lambda x: max(x, CLIENT_MARGIN_MINS_18)),2)\n",
      "C:\\Users\\SLiu\\AppData\\Local\\Temp\\ipykernel_7228\\3352589481.py:236: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  tbl_barrettRates_41['BARRETT_RATES_1'] = tbl_barrettRates_41['PUBLISHED_RATES']*(1-DISCOUNT_TO_CLIENT_19)\n",
      "C:\\Users\\SLiu\\AppData\\Local\\Temp\\ipykernel_7228\\3352589481.py:237: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  tbl_barrettRates_41['BARRETT_RATES'] = round(tbl_barrettRates_41['BARRETT_RATES_1'].apply(lambda x: max(x, CLIENT_MARGIN_MINS_19)),2)\n",
      "C:\\Users\\SLiu\\AppData\\Local\\Temp\\ipykernel_7228\\3352589481.py:249: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  tbl_barrettRates_42['BARRETT_RATES_1'] = tbl_barrettRates_42['PUBLISHED_RATES']*(1-DISCOUNT_TO_CLIENT_20)\n",
      "C:\\Users\\SLiu\\AppData\\Local\\Temp\\ipykernel_7228\\3352589481.py:250: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  tbl_barrettRates_42['BARRETT_RATES'] = round(tbl_barrettRates_42['BARRETT_RATES_1'].apply(lambda x: max(x, CLIENT_MARGIN_MINS_20)),2)\n",
      "C:\\Users\\SLiu\\AppData\\Local\\Temp\\ipykernel_7228\\3352589481.py:262: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  tbl_barrettRates_43['BARRETT_RATES_1'] = tbl_barrettRates_43['PUBLISHED_RATES']*(1-DISCOUNT_TO_CLIENT_21)\n",
      "C:\\Users\\SLiu\\AppData\\Local\\Temp\\ipykernel_7228\\3352589481.py:263: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  tbl_barrettRates_43['BARRETT_RATES'] = round(tbl_barrettRates_43['BARRETT_RATES_1'].apply(lambda x: max(x, CLIENT_MARGIN_MINS_21)),2)\n",
      "C:\\Users\\SLiu\\AppData\\Local\\Temp\\ipykernel_7228\\3352589481.py:275: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  tbl_barrettRates_44['BARRETT_RATES_1'] = tbl_barrettRates_44['PUBLISHED_RATES']*(1-DISCOUNT_TO_CLIENT_22)\n",
      "C:\\Users\\SLiu\\AppData\\Local\\Temp\\ipykernel_7228\\3352589481.py:276: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  tbl_barrettRates_44['BARRETT_RATES'] = round(tbl_barrettRates_44['BARRETT_RATES_1'].apply(lambda x: max(x, CLIENT_MARGIN_MINS_22)),2)\n"
     ]
    }
   ],
   "source": [
    "#GRNDF ['WEIGHT_BREAK'] == '1-5lb'\n",
    "# slice the published rates\n",
    "tbl_barrettRates_23 = tbl_publishedRates[(tbl_publishedRates['SERVICE_CODE'] == 'GRNDF') & (tbl_publishedRates['WEIGHT_LB'] <= 5)]\n",
    "\n",
    "# get DISCOUNT_TO_CLIENT(1-COLN) and CLIENT_MARGIN_MINS\n",
    "DISCOUNT_TO_CLIENT_1 = FedEx_Rates.loc[(FedEx_Rates['SERVICE_CODE'] == 'GRNDF') & (FedEx_Rates['WEIGHT_BREAK'] == '1-5lb'), 'DISCOUNT_TO_CLIENT(1-COLN)'].values[0]\n",
    "if Round_Discount == 'YES':\n",
    "    DISCOUNT_TO_CLIENT_1 = round(DISCOUNT_TO_CLIENT_1,2)\n",
    "else:\n",
    "    DISCOUNT_TO_CLIENT_1 = DISCOUNT_TO_CLIENT_1\n",
    "CLIENT_MARGIN_MINS_1 = FedEx_Rates.loc[(FedEx_Rates['SERVICE_CODE'] == 'GRNDF') & (FedEx_Rates['WEIGHT_BREAK'] == '1-5lb'), 'CLIENT_MARGIN_MINS'].values[0]\n",
    "\n",
    "# use max(published rates * (1-round(DISCOUNT_TO_CLIENT)), CLIENT_MARGIN_MINS) to get the client rate\n",
    "tbl_barrettRates_23['BARRETT_RATES_1'] = tbl_barrettRates_23['PUBLISHED_RATES']*(1-DISCOUNT_TO_CLIENT_1)\n",
    "tbl_barrettRates_23['BARRETT_RATES'] = round(tbl_barrettRates_23['BARRETT_RATES_1'].apply(lambda x: max(x, CLIENT_MARGIN_MINS_1)),2)\n",
    "tbl_barrettRates_23 = tbl_barrettRates_23.drop('BARRETT_RATES_1', axis=1)\n",
    "\n",
    "\n",
    "#GRNDF ['WEIGHT_BREAK'] == '6-10lb'\n",
    "tbl_barrettRates_24 = tbl_publishedRates[(tbl_publishedRates['SERVICE_CODE'] == 'GRNDF') & (tbl_publishedRates['WEIGHT_LB'] <= 10) & (tbl_publishedRates['WEIGHT_LB'] >= 6)]\n",
    "DISCOUNT_TO_CLIENT_2 = FedEx_Rates.loc[(FedEx_Rates['SERVICE_CODE'] == 'GRNDF') & (FedEx_Rates['WEIGHT_BREAK'] == '6-10lb'), 'DISCOUNT_TO_CLIENT(1-COLN)'].values[0]\n",
    "if Round_Discount == 'YES':\n",
    "    DISCOUNT_TO_CLIENT_2 = round(DISCOUNT_TO_CLIENT_2,2)\n",
    "else:\n",
    "    DISCOUNT_TO_CLIENT_2 = DISCOUNT_TO_CLIENT_2\n",
    "CLIENT_MARGIN_MINS_2 = FedEx_Rates.loc[(FedEx_Rates['SERVICE_CODE'] == 'GRNDF') & (FedEx_Rates['WEIGHT_BREAK'] == '6-10lb'), 'CLIENT_MARGIN_MINS'].values[0]\n",
    "tbl_barrettRates_24['BARRETT_RATES_1'] = tbl_barrettRates_24['PUBLISHED_RATES']*(1-DISCOUNT_TO_CLIENT_2)\n",
    "tbl_barrettRates_24['BARRETT_RATES'] = round(tbl_barrettRates_24['BARRETT_RATES_1'].apply(lambda x: max(x, CLIENT_MARGIN_MINS_2)),2)\n",
    "tbl_barrettRates_24 = tbl_barrettRates_24.drop('BARRETT_RATES_1', axis=1)\n",
    "\n",
    "\n",
    "#GRNDF ['WEIGHT_BREAK'] == '11-20lb'\n",
    "tbl_barrettRates_25 = tbl_publishedRates[(tbl_publishedRates['SERVICE_CODE'] == 'GRNDF') & (tbl_publishedRates['WEIGHT_LB'] <= 20) & (tbl_publishedRates['WEIGHT_LB'] >= 11)]\n",
    "DISCOUNT_TO_CLIENT_3 = FedEx_Rates.loc[(FedEx_Rates['SERVICE_CODE'] == 'GRNDF') & (FedEx_Rates['WEIGHT_BREAK'] == '11-20lb'), 'DISCOUNT_TO_CLIENT(1-COLN)'].values[0]\n",
    "if Round_Discount == 'YES':\n",
    "    DISCOUNT_TO_CLIENT_3 = round(DISCOUNT_TO_CLIENT_3,2)\n",
    "else:\n",
    "    DISCOUNT_TO_CLIENT_3 = DISCOUNT_TO_CLIENT_3\n",
    "CLIENT_MARGIN_MINS_3 = FedEx_Rates.loc[(FedEx_Rates['SERVICE_CODE'] == 'GRNDF') & (FedEx_Rates['WEIGHT_BREAK'] == '11-20lb'), 'CLIENT_MARGIN_MINS'].values[0]\n",
    "tbl_barrettRates_25['BARRETT_RATES_1'] = tbl_barrettRates_25['PUBLISHED_RATES']*(1-DISCOUNT_TO_CLIENT_3)\n",
    "tbl_barrettRates_25['BARRETT_RATES'] = round(tbl_barrettRates_25['BARRETT_RATES_1'].apply(lambda x: max(x, CLIENT_MARGIN_MINS_3)),2)\n",
    "tbl_barrettRates_25 = tbl_barrettRates_25.drop('BARRETT_RATES_1', axis=1)\n",
    "\n",
    "\n",
    "#GRNDF ['WEIGHT_BREAK'] == '21-30lb'\n",
    "tbl_barrettRates_26 = tbl_publishedRates[(tbl_publishedRates['SERVICE_CODE'] == 'GRNDF') & (tbl_publishedRates['WEIGHT_LB'] <= 30) & (tbl_publishedRates['WEIGHT_LB'] >= 21)]\n",
    "DISCOUNT_TO_CLIENT_4 = FedEx_Rates.loc[(FedEx_Rates['SERVICE_CODE'] == 'GRNDF') & (FedEx_Rates['WEIGHT_BREAK'] == '21-30lb'), 'DISCOUNT_TO_CLIENT(1-COLN)'].values[0]\n",
    "if Round_Discount == 'YES':\n",
    "    DISCOUNT_TO_CLIENT_4 = round(DISCOUNT_TO_CLIENT_4,2)\n",
    "else:\n",
    "    DISCOUNT_TO_CLIENT_4 = DISCOUNT_TO_CLIENT_4\n",
    "CLIENT_MARGIN_MINS_4 = FedEx_Rates.loc[(FedEx_Rates['SERVICE_CODE'] == 'GRNDF') & (FedEx_Rates['WEIGHT_BREAK'] == '21-30lb'), 'CLIENT_MARGIN_MINS'].values[0]\n",
    "tbl_barrettRates_26['BARRETT_RATES_1'] = tbl_barrettRates_26['PUBLISHED_RATES']*(1-DISCOUNT_TO_CLIENT_4)\n",
    "tbl_barrettRates_26['BARRETT_RATES'] = round(tbl_barrettRates_26['BARRETT_RATES_1'].apply(lambda x: max(x, CLIENT_MARGIN_MINS_4)),2)\n",
    "tbl_barrettRates_26 = tbl_barrettRates_26.drop('BARRETT_RATES_1', axis=1)\n",
    "\n",
    "\n",
    "#GRNDF ['WEIGHT_BREAK'] == '31+lb'\n",
    "tbl_barrettRates_27 = tbl_publishedRates[(tbl_publishedRates['SERVICE_CODE'] == 'GRNDF') & (tbl_publishedRates['WEIGHT_LB'] >= 31)]\n",
    "DISCOUNT_TO_CLIENT_5 = FedEx_Rates.loc[(FedEx_Rates['SERVICE_CODE'] == 'GRNDF') & (FedEx_Rates['WEIGHT_BREAK'] == '31+lb'), 'DISCOUNT_TO_CLIENT(1-COLN)'].values[0]\n",
    "if Round_Discount == 'YES':\n",
    "    DISCOUNT_TO_CLIENT_5 = round(DISCOUNT_TO_CLIENT_5,2)\n",
    "else:\n",
    "    DISCOUNT_TO_CLIENT_5 = DISCOUNT_TO_CLIENT_5\n",
    "CLIENT_MARGIN_MINS_5 = FedEx_Rates.loc[(FedEx_Rates['SERVICE_CODE'] == 'GRNDF') & (FedEx_Rates['WEIGHT_BREAK'] == '31+lb'), 'CLIENT_MARGIN_MINS'].values[0]\n",
    "tbl_barrettRates_27['BARRETT_RATES_1'] = tbl_barrettRates_27['PUBLISHED_RATES']*(1-DISCOUNT_TO_CLIENT_5)\n",
    "tbl_barrettRates_27['BARRETT_RATES'] = round(tbl_barrettRates_27['BARRETT_RATES_1'].apply(lambda x: max(x, CLIENT_MARGIN_MINS_5)),2)\n",
    "tbl_barrettRates_27 = tbl_barrettRates_27.drop('BARRETT_RATES_1', axis=1)\n",
    "\n",
    "\n",
    "#HOME \n",
    "#HOME ['WEIGHT_BREAK'] == '1-5lb'\n",
    "tbl_barrettRates_28 = tbl_publishedRates[(tbl_publishedRates['SERVICE_CODE'] == 'HOME') & (tbl_publishedRates['WEIGHT_LB'] <= 5)]\n",
    "DISCOUNT_TO_CLIENT_6 = FedEx_Rates.loc[(FedEx_Rates['SERVICE_CODE'] == 'HOME') & (FedEx_Rates['WEIGHT_BREAK'] == '1-5lb'), 'DISCOUNT_TO_CLIENT(1-COLN)'].values[0]\n",
    "if Round_Discount == 'YES':\n",
    "    DISCOUNT_TO_CLIENT_6 = round(DISCOUNT_TO_CLIENT_6,2)\n",
    "else:\n",
    "    DISCOUNT_TO_CLIENT_6 = DISCOUNT_TO_CLIENT_6\n",
    "CLIENT_MARGIN_MINS_6 = FedEx_Rates.loc[(FedEx_Rates['SERVICE_CODE'] == 'HOME') & (FedEx_Rates['WEIGHT_BREAK'] == '1-5lb'), 'CLIENT_MARGIN_MINS'].values[0]\n",
    "tbl_barrettRates_28['BARRETT_RATES_1'] = tbl_barrettRates_28['PUBLISHED_RATES']*(1-DISCOUNT_TO_CLIENT_6)\n",
    "tbl_barrettRates_28['BARRETT_RATES'] = round(tbl_barrettRates_28['BARRETT_RATES_1'].apply(lambda x: max(x, CLIENT_MARGIN_MINS_6)),2)\n",
    "tbl_barrettRates_28 = tbl_barrettRates_28.drop('BARRETT_RATES_1', axis=1)\n",
    "\n",
    "\n",
    "#HOME ['WEIGHT_BREAK'] == '6-10lb'\n",
    "tbl_barrettRates_29 = tbl_publishedRates[(tbl_publishedRates['SERVICE_CODE'] == 'HOME') & (tbl_publishedRates['WEIGHT_LB'] <= 10) & (tbl_publishedRates['WEIGHT_LB'] >= 6)]\n",
    "DISCOUNT_TO_CLIENT_7 = FedEx_Rates.loc[(FedEx_Rates['SERVICE_CODE'] == 'HOME') & (FedEx_Rates['WEIGHT_BREAK'] == '6-10lb'), 'DISCOUNT_TO_CLIENT(1-COLN)'].values[0]\n",
    "if Round_Discount == 'YES':\n",
    "    DISCOUNT_TO_CLIENT_7 = round(DISCOUNT_TO_CLIENT_7,2)\n",
    "else:\n",
    "    DISCOUNT_TO_CLIENT_7 = DISCOUNT_TO_CLIENT_7\n",
    "CLIENT_MARGIN_MINS_7 = FedEx_Rates.loc[(FedEx_Rates['SERVICE_CODE'] == 'HOME') & (FedEx_Rates['WEIGHT_BREAK'] == '6-10lb'), 'CLIENT_MARGIN_MINS'].values[0]\n",
    "tbl_barrettRates_29['BARRETT_RATES_1'] = tbl_barrettRates_29['PUBLISHED_RATES']*(1-DISCOUNT_TO_CLIENT_7)\n",
    "tbl_barrettRates_29['BARRETT_RATES'] = round(tbl_barrettRates_29['BARRETT_RATES_1'].apply(lambda x: max(x, CLIENT_MARGIN_MINS_7)),2)\n",
    "tbl_barrettRates_29 = tbl_barrettRates_29.drop('BARRETT_RATES_1', axis=1)\n",
    "\n",
    "\n",
    "#HOME ['WEIGHT_BREAK'] == '11-20lb'\n",
    "tbl_barrettRates_30 = tbl_publishedRates[(tbl_publishedRates['SERVICE_CODE'] == 'HOME') & (tbl_publishedRates['WEIGHT_LB'] <= 20) & (tbl_publishedRates['WEIGHT_LB'] >= 11)]\n",
    "DISCOUNT_TO_CLIENT_8 = FedEx_Rates.loc[(FedEx_Rates['SERVICE_CODE'] == 'HOME') & (FedEx_Rates['WEIGHT_BREAK'] == '11-20lb'), 'DISCOUNT_TO_CLIENT(1-COLN)'].values[0]\n",
    "if Round_Discount == 'YES':\n",
    "    DISCOUNT_TO_CLIENT_8 = round(DISCOUNT_TO_CLIENT_8,2)\n",
    "else:\n",
    "    DISCOUNT_TO_CLIENT_8 = DISCOUNT_TO_CLIENT_8\n",
    "CLIENT_MARGIN_MINS_8 = FedEx_Rates.loc[(FedEx_Rates['SERVICE_CODE'] == 'HOME') & (FedEx_Rates['WEIGHT_BREAK'] == '11-20lb'), 'CLIENT_MARGIN_MINS'].values[0]\n",
    "tbl_barrettRates_30['BARRETT_RATES_1'] = tbl_barrettRates_30['PUBLISHED_RATES']*(1-DISCOUNT_TO_CLIENT_8)\n",
    "tbl_barrettRates_30['BARRETT_RATES'] = round(tbl_barrettRates_30['BARRETT_RATES_1'].apply(lambda x: max(x, CLIENT_MARGIN_MINS_8)),2)\n",
    "tbl_barrettRates_30 = tbl_barrettRates_30.drop('BARRETT_RATES_1', axis=1)\n",
    "\n",
    "\n",
    "#HOME ['WEIGHT_BREAK'] == '21-30lb'\n",
    "tbl_barrettRates_31 = tbl_publishedRates[(tbl_publishedRates['SERVICE_CODE'] == 'HOME') & (tbl_publishedRates['WEIGHT_LB'] <= 30) & (tbl_publishedRates['WEIGHT_LB'] >= 21)]\n",
    "DISCOUNT_TO_CLIENT_9 = FedEx_Rates.loc[(FedEx_Rates['SERVICE_CODE'] == 'HOME') & (FedEx_Rates['WEIGHT_BREAK'] == '21-30lb'), 'DISCOUNT_TO_CLIENT(1-COLN)'].values[0]\n",
    "if Round_Discount == 'YES':\n",
    "    DISCOUNT_TO_CLIENT_9 = round(DISCOUNT_TO_CLIENT_9,2)\n",
    "else:\n",
    "    DISCOUNT_TO_CLIENT_9 = DISCOUNT_TO_CLIENT_9\n",
    "CLIENT_MARGIN_MINS_9 = FedEx_Rates.loc[(FedEx_Rates['SERVICE_CODE'] == 'HOME') & (FedEx_Rates['WEIGHT_BREAK'] == '21-30lb'), 'CLIENT_MARGIN_MINS'].values[0]\n",
    "tbl_barrettRates_31['BARRETT_RATES_1'] = tbl_barrettRates_31['PUBLISHED_RATES']*(1-DISCOUNT_TO_CLIENT_9)\n",
    "tbl_barrettRates_31['BARRETT_RATES'] = round(tbl_barrettRates_31['BARRETT_RATES_1'].apply(lambda x: max(x, CLIENT_MARGIN_MINS_9)),2)\n",
    "tbl_barrettRates_31 = tbl_barrettRates_31.drop('BARRETT_RATES_1', axis=1)\n",
    "\n",
    "\n",
    "#HOME ['WEIGHT_BREAK'] == '31+lb'\n",
    "tbl_barrettRates_32 = tbl_publishedRates[(tbl_publishedRates['SERVICE_CODE'] == 'HOME') & (tbl_publishedRates['WEIGHT_LB'] >= 31)]\n",
    "DISCOUNT_TO_CLIENT_10 = FedEx_Rates.loc[(FedEx_Rates['SERVICE_CODE'] == 'HOME') & (FedEx_Rates['WEIGHT_BREAK'] == '31+lb'), 'DISCOUNT_TO_CLIENT(1-COLN)'].values[0]\n",
    "if Round_Discount == 'YES':\n",
    "    DISCOUNT_TO_CLIENT_10 = round(DISCOUNT_TO_CLIENT_10,2)\n",
    "else:\n",
    "    DISCOUNT_TO_CLIENT_10 = DISCOUNT_TO_CLIENT_10\n",
    "CLIENT_MARGIN_MINS_10 = FedEx_Rates.loc[(FedEx_Rates['SERVICE_CODE'] == 'HOME') & (FedEx_Rates['WEIGHT_BREAK'] == '31+lb'), 'CLIENT_MARGIN_MINS'].values[0]\n",
    "tbl_barrettRates_32['BARRETT_RATES_1'] = tbl_barrettRates_32['PUBLISHED_RATES']*(1-DISCOUNT_TO_CLIENT_10)\n",
    "tbl_barrettRates_32['BARRETT_RATES'] = round(tbl_barrettRates_32['BARRETT_RATES_1'].apply(lambda x: max(x, CLIENT_MARGIN_MINS_10)),2)\n",
    "tbl_barrettRates_32 = tbl_barrettRates_32.drop('BARRETT_RATES_1', axis=1)\n",
    "\n",
    "\n",
    "#SPST\n",
    "#SPST ['WEIGHT_BREAK'] == '1-2lb'\n",
    "tbl_barrettRates_33 = tbl_publishedRates[(tbl_publishedRates['SERVICE_CODE'] == 'SPST') & (tbl_publishedRates['WEIGHT_LB'] <= 2)]\n",
    "DISCOUNT_TO_CLIENT_11 = FedEx_Rates.loc[(FedEx_Rates['SERVICE_CODE'] == 'SPST') & (FedEx_Rates['WEIGHT_BREAK'] == '1-2lbs'), 'DISCOUNT_TO_CLIENT(1-COLN)'].values[0]\n",
    "if Round_Discount == 'YES':\n",
    "    DISCOUNT_TO_CLIENT_11 = round(DISCOUNT_TO_CLIENT_11,2)\n",
    "else:\n",
    "    DISCOUNT_TO_CLIENT_11 = DISCOUNT_TO_CLIENT_11\n",
    "CLIENT_MARGIN_MINS_11 = FedEx_Rates.loc[(FedEx_Rates['SERVICE_CODE'] == 'SPST') & (FedEx_Rates['WEIGHT_BREAK'] == '1-2lbs'), 'CLIENT_MARGIN_MINS'].values[0]\n",
    "tbl_barrettRates_33['BARRETT_RATES_1'] = tbl_barrettRates_33['PUBLISHED_RATES']*(1-DISCOUNT_TO_CLIENT_11)\n",
    "tbl_barrettRates_33['BARRETT_RATES'] = round(tbl_barrettRates_33['BARRETT_RATES_1'].apply(lambda x: max(x, CLIENT_MARGIN_MINS_11)),2)\n",
    "tbl_barrettRates_33 = tbl_barrettRates_33.drop('BARRETT_RATES_1', axis=1)\n",
    "\n",
    "\n",
    "\n",
    "#SPST ['WEIGHT_BREAK'] == '3lb'\n",
    "tbl_barrettRates_34 = tbl_publishedRates[(tbl_publishedRates['SERVICE_CODE'] == 'SPST') & (tbl_publishedRates['WEIGHT_LB'] >= 3) & (tbl_publishedRates['WEIGHT_LB'] <= 3)]\n",
    "DISCOUNT_TO_CLIENT_12 = FedEx_Rates.loc[(FedEx_Rates['SERVICE_CODE'] == 'SPST') & (FedEx_Rates['WEIGHT_BREAK'] == '3 lbs'), 'DISCOUNT_TO_CLIENT(1-COLN)'].values[0]\n",
    "if Round_Discount == 'YES':\n",
    "    DISCOUNT_TO_CLIENT_12 = round(DISCOUNT_TO_CLIENT_12,2)\n",
    "else:\n",
    "    DISCOUNT_TO_CLIENT_12 = DISCOUNT_TO_CLIENT_12\n",
    "CLIENT_MARGIN_MINS_12 = FedEx_Rates.loc[(FedEx_Rates['SERVICE_CODE'] == 'SPST') & (FedEx_Rates['WEIGHT_BREAK'] == '3 lbs'), 'CLIENT_MARGIN_MINS'].values[0]\n",
    "tbl_barrettRates_34['BARRETT_RATES_1'] = tbl_barrettRates_34['PUBLISHED_RATES']*(1-DISCOUNT_TO_CLIENT_12)\n",
    "tbl_barrettRates_34['BARRETT_RATES'] = round(tbl_barrettRates_34['BARRETT_RATES_1'].apply(lambda x: max(x, CLIENT_MARGIN_MINS_12)),2)\n",
    "tbl_barrettRates_34 = tbl_barrettRates_34.drop('BARRETT_RATES_1', axis=1)\n",
    "\n",
    "\n",
    "#SPST ['WEIGHT_BREAK'] == '4-70lb'\n",
    "tbl_barrettRates_35 = tbl_publishedRates[(tbl_publishedRates['SERVICE_CODE'] == 'SPST') & (tbl_publishedRates['WEIGHT_LB'] >= 4) & (tbl_publishedRates['WEIGHT_LB'] <= 70)]\n",
    "DISCOUNT_TO_CLIENT_13 = FedEx_Rates.loc[(FedEx_Rates['SERVICE_CODE'] == 'SPST') & (FedEx_Rates['WEIGHT_BREAK'] == '4-70lbs'), 'DISCOUNT_TO_CLIENT(1-COLN)'].values[0]\n",
    "if Round_Discount == 'YES':\n",
    "    DISCOUNT_TO_CLIENT_13 = round(DISCOUNT_TO_CLIENT_13,2)\n",
    "else:\n",
    "    DISCOUNT_TO_CLIENT_13 = DISCOUNT_TO_CLIENT_13\n",
    "CLIENT_MARGIN_MINS_13 = FedEx_Rates.loc[(FedEx_Rates['SERVICE_CODE'] == 'SPST') & (FedEx_Rates['WEIGHT_BREAK'] == '4-70lbs'), 'CLIENT_MARGIN_MINS'].values[0]\n",
    "tbl_barrettRates_35['BARRETT_RATES_1'] = tbl_barrettRates_35['PUBLISHED_RATES']*(1-DISCOUNT_TO_CLIENT_13)\n",
    "tbl_barrettRates_35['BARRETT_RATES'] = round(tbl_barrettRates_35['BARRETT_RATES_1'].apply(lambda x: max(x, CLIENT_MARGIN_MINS_13)),2)\n",
    "tbl_barrettRates_35 = tbl_barrettRates_35.drop('BARRETT_RATES_1', axis=1)\n",
    "\n",
    "\n",
    "#SPST ['WEIGHT_BREAK'] == '70lb+'\n",
    "tbl_barrettRates_36 = tbl_publishedRates[(tbl_publishedRates['SERVICE_CODE'] == 'SPST') & (tbl_publishedRates['WEIGHT_LB'] == 70)]\n",
    "DISCOUNT_TO_CLIENT_14 = FedEx_Rates.loc[(FedEx_Rates['SERVICE_CODE'] == 'SPST') & (FedEx_Rates['WEIGHT_BREAK'] == '70+lbs'), 'DISCOUNT_TO_CLIENT(1-COLN)'].values[0]\n",
    "if Round_Discount == 'YES':\n",
    "    DISCOUNT_TO_CLIENT_14 = round(DISCOUNT_TO_CLIENT_14,2)\n",
    "else:\n",
    "    DISCOUNT_TO_CLIENT_14 = DISCOUNT_TO_CLIENT_14\n",
    "CLIENT_MARGIN_MINS_14 = FedEx_Rates.loc[(FedEx_Rates['SERVICE_CODE'] == 'SPST') & (FedEx_Rates['WEIGHT_BREAK'] == '70+lbs'), 'CLIENT_MARGIN_MINS'].values[0]\n",
    "tbl_barrettRates_36['BARRETT_RATES_1'] = tbl_barrettRates_36['PUBLISHED_RATES']*(1-DISCOUNT_TO_CLIENT_14)\n",
    "tbl_barrettRates_36['BARRETT_RATES'] = round(tbl_barrettRates_36['BARRETT_RATES_1'].apply(lambda x: max(x, CLIENT_MARGIN_MINS_14)),2)\n",
    "tbl_barrettRates_36 = tbl_barrettRates_36.drop('BARRETT_RATES_1', axis=1)\n",
    "\n",
    "\n",
    "#Others:\n",
    "#FedEx 1STO, there is no discount, therefore 1STO's client rate = published rate\n",
    "tbl_barrettRates_37 = tbl_publishedRates[(tbl_publishedRates['SERVICE_CODE'] == '1STO')]\n",
    "tbl_barrettRates_37['BARRETT_RATES'] = tbl_barrettRates_37['PUBLISHED_RATES']*(1-BarrettDisc_1STO_FedEx)\n",
    "\n",
    "\n",
    "#FedEx PROV\n",
    "tbl_barrettRates_38 = tbl_publishedRates[(tbl_publishedRates['SERVICE_CODE'] == 'PROV')]\n",
    "DISCOUNT_TO_CLIENT_16 = FedEx_Rates.loc[(FedEx_Rates['SERVICE_CODE'] == 'PROV'), 'DISCOUNT_TO_CLIENT(1-COLN)'].values[0]\n",
    "if Round_Discount == 'YES':\n",
    "    DISCOUNT_TO_CLIENT_16 = round(DISCOUNT_TO_CLIENT_16,2)\n",
    "else:\n",
    "    DISCOUNT_TO_CLIENT_16 = DISCOUNT_TO_CLIENT_16\n",
    "CLIENT_MARGIN_MINS_16 = FedEx_Rates.loc[(FedEx_Rates['SERVICE_CODE'] == 'PROV'), 'CLIENT_MARGIN_MINS'].values[0]\n",
    "tbl_barrettRates_38['BARRETT_RATES_1'] = tbl_barrettRates_38['PUBLISHED_RATES']*(1-DISCOUNT_TO_CLIENT_16)\n",
    "tbl_barrettRates_38['BARRETT_RATES'] = round(tbl_barrettRates_38['BARRETT_RATES_1'].apply(lambda x: max(x, CLIENT_MARGIN_MINS_16)),2)\n",
    "tbl_barrettRates_38 = tbl_barrettRates_38.drop('BARRETT_RATES_1', axis=1)\n",
    "\n",
    "\n",
    "#FedEx STOV, this does not have round up option (1/2)\n",
    "tbl_barrettRates_39 = tbl_publishedRates[(tbl_publishedRates['SERVICE_CODE'] == 'STOV')]\n",
    "DISCOUNT_TO_CLIENT_17 = FedEx_Rates.loc[(FedEx_Rates['SERVICE_CODE'] == 'STOV'), 'DISCOUNT_TO_CLIENT(1-COLN)'].values[0]\n",
    "CLIENT_MARGIN_MINS_17 = FedEx_Rates.loc[(FedEx_Rates['SERVICE_CODE'] == 'STOV'), 'CLIENT_MARGIN_MINS'].values[0]\n",
    "tbl_barrettRates_39['BARRETT_RATES_1'] = tbl_barrettRates_39['PUBLISHED_RATES']*(1-DISCOUNT_TO_CLIENT_17)\n",
    "tbl_barrettRates_39['BARRETT_RATES'] = round(tbl_barrettRates_39['BARRETT_RATES_1'].apply(lambda x: max(x, CLIENT_MARGIN_MINS_17)),2)\n",
    "tbl_barrettRates_39 = tbl_barrettRates_39.drop('BARRETT_RATES_1', axis=1)\n",
    "\n",
    "\n",
    "#FedEx 2AM, this does not have round up option (2/2)\n",
    "tbl_barrettRates_40 = tbl_publishedRates[(tbl_publishedRates['SERVICE_CODE'] == '2AM')]\n",
    "DISCOUNT_TO_CLIENT_18 = FedEx_Rates.loc[(FedEx_Rates['SERVICE_CODE'] == '2AM'), 'DISCOUNT_TO_CLIENT(1-COLN)'].values[0]\n",
    "CLIENT_MARGIN_MINS_18 = FedEx_Rates.loc[(FedEx_Rates['SERVICE_CODE'] == '2AM'), 'CLIENT_MARGIN_MINS'].values[0]\n",
    "tbl_barrettRates_40['BARRETT_RATES_1'] = tbl_barrettRates_40['PUBLISHED_RATES']*(1-DISCOUNT_TO_CLIENT_18)\n",
    "tbl_barrettRates_40['BARRETT_RATES'] = round(tbl_barrettRates_40['BARRETT_RATES_1'].apply(lambda x: max(x, CLIENT_MARGIN_MINS_18)),2)\n",
    "tbl_barrettRates_40 = tbl_barrettRates_40.drop('BARRETT_RATES_1', axis=1)\n",
    "\n",
    "\n",
    "#FedEx 2DAY\n",
    "tbl_barrettRates_41 = tbl_publishedRates[(tbl_publishedRates['SERVICE_CODE'] == '2DAY')]\n",
    "DISCOUNT_TO_CLIENT_19 = FedEx_Rates.loc[(FedEx_Rates['SERVICE_CODE'] == '2DAY'), 'DISCOUNT_TO_CLIENT(1-COLN)'].values[0]\n",
    "if Round_Discount == 'YES':\n",
    "    DISCOUNT_TO_CLIENT_19 = round(DISCOUNT_TO_CLIENT_19,2)\n",
    "else:\n",
    "    DISCOUNT_TO_CLIENT_19 = DISCOUNT_TO_CLIENT_19\n",
    "CLIENT_MARGIN_MINS_19 = FedEx_Rates.loc[(FedEx_Rates['SERVICE_CODE'] == '2DAY') , 'CLIENT_MARGIN_MINS'].values[0]\n",
    "tbl_barrettRates_41['BARRETT_RATES_1'] = tbl_barrettRates_41['PUBLISHED_RATES']*(1-DISCOUNT_TO_CLIENT_19)\n",
    "tbl_barrettRates_41['BARRETT_RATES'] = round(tbl_barrettRates_41['BARRETT_RATES_1'].apply(lambda x: max(x, CLIENT_MARGIN_MINS_19)),2)\n",
    "tbl_barrettRates_41 = tbl_barrettRates_41.drop('BARRETT_RATES_1', axis=1)\n",
    "\n",
    "\n",
    "#FedEx EXSA\n",
    "tbl_barrettRates_42 = tbl_publishedRates[(tbl_publishedRates['SERVICE_CODE'] == 'EXSA')]\n",
    "DISCOUNT_TO_CLIENT_20 = FedEx_Rates.loc[(FedEx_Rates['SERVICE_CODE'] == 'EXSA'), 'DISCOUNT_TO_CLIENT(1-COLN)'].values[0]\n",
    "if Round_Discount == 'YES':\n",
    "    DISCOUNT_TO_CLIENT_20 = round(DISCOUNT_TO_CLIENT_20,2)\n",
    "else:\n",
    "    DISCOUNT_TO_CLIENT_20 = DISCOUNT_TO_CLIENT_20\n",
    "CLIENT_MARGIN_MINS_20 = FedEx_Rates.loc[(FedEx_Rates['SERVICE_CODE'] == 'EXSA'), 'CLIENT_MARGIN_MINS'].values[0]\n",
    "tbl_barrettRates_42['BARRETT_RATES_1'] = tbl_barrettRates_42['PUBLISHED_RATES']*(1-DISCOUNT_TO_CLIENT_20)\n",
    "tbl_barrettRates_42['BARRETT_RATES'] = round(tbl_barrettRates_42['BARRETT_RATES_1'].apply(lambda x: max(x, CLIENT_MARGIN_MINS_20)),2)\n",
    "tbl_barrettRates_42 = tbl_barrettRates_42.drop('BARRETT_RATES_1', axis=1)\n",
    "\n",
    "\n",
    "#FedEx SPST<1 1-3oz\n",
    "tbl_barrettRates_43 = tbl_publishedRates[(tbl_publishedRates['SERVICE_CODE'] == 'SPST<1') & (tbl_publishedRates['WEIGHT_OZ'] <= 3)]\n",
    "DISCOUNT_TO_CLIENT_21 = FedEx_Rates.loc[(FedEx_Rates['SERVICE_CODE'] == 'SPST<1') & (FedEx_Rates['WEIGHT_BREAK'] == '1-3oz'), 'DISCOUNT_TO_CLIENT(1-COLN)'].values[0]\n",
    "if Round_Discount == 'YES':\n",
    "    DISCOUNT_TO_CLIENT_21 = round(DISCOUNT_TO_CLIENT_21,2)\n",
    "else:\n",
    "    DISCOUNT_TO_CLIENT_21 = DISCOUNT_TO_CLIENT_21\n",
    "CLIENT_MARGIN_MINS_21 = FedEx_Rates.loc[(FedEx_Rates['SERVICE_CODE'] == 'SPST<1') & (FedEx_Rates['WEIGHT_BREAK'] == '1-3oz'), 'CLIENT_MARGIN_MINS'].values[0]\n",
    "tbl_barrettRates_43['BARRETT_RATES_1'] = tbl_barrettRates_43['PUBLISHED_RATES']*(1-DISCOUNT_TO_CLIENT_21)\n",
    "tbl_barrettRates_43['BARRETT_RATES'] = round(tbl_barrettRates_43['BARRETT_RATES_1'].apply(lambda x: max(x, CLIENT_MARGIN_MINS_21)),2)\n",
    "tbl_barrettRates_43 = tbl_barrettRates_43.drop('BARRETT_RATES_1', axis=1)\n",
    "\n",
    "\n",
    "#FedEx SPST<1 4-16oz\n",
    "tbl_barrettRates_44 = tbl_publishedRates[(tbl_publishedRates['SERVICE_CODE'] == 'SPST<1') & (tbl_publishedRates['WEIGHT_OZ'] <= 16) & (tbl_publishedRates['WEIGHT_OZ'] > 3)]\n",
    "DISCOUNT_TO_CLIENT_22 = FedEx_Rates.loc[(FedEx_Rates['SERVICE_CODE'] == 'SPST<1') & (FedEx_Rates['WEIGHT_BREAK'] == '4-16oz'), 'DISCOUNT_TO_CLIENT(1-COLN)'].values[0]\n",
    "if Round_Discount == 'YES':\n",
    "    DISCOUNT_TO_CLIENT_22 = round(DISCOUNT_TO_CLIENT_22,2)\n",
    "else:\n",
    "    DISCOUNT_TO_CLIENT_22 = DISCOUNT_TO_CLIENT_22\n",
    "CLIENT_MARGIN_MINS_22 = FedEx_Rates.loc[(FedEx_Rates['SERVICE_CODE'] == 'SPST<1') & (FedEx_Rates['WEIGHT_BREAK'] == '4-16oz'), 'CLIENT_MARGIN_MINS'].values[0]\n",
    "tbl_barrettRates_44['BARRETT_RATES_1'] = tbl_barrettRates_44['PUBLISHED_RATES']*(1-DISCOUNT_TO_CLIENT_22)\n",
    "tbl_barrettRates_44['BARRETT_RATES'] = round(tbl_barrettRates_44['BARRETT_RATES_1'].apply(lambda x: max(x, CLIENT_MARGIN_MINS_22)),2)\n",
    "tbl_barrettRates_44 = tbl_barrettRates_44.drop('BARRETT_RATES_1', axis=1)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "cafd65e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get dhl, usps, and mi rates\n",
    "tbl_barrettRates_22 = tbl_publishedRates[(tbl_publishedRates['SERVICE_CODE'] == 'USPSAG') | \n",
    "                                         (tbl_publishedRates['SERVICE_CODE'] == 'USPSAP') | \n",
    "                                         (tbl_publishedRates['SERVICE_CODE'] == 'USPSAG_CI') | \n",
    "                                         (tbl_publishedRates['SERVICE_CODE'] == 'USPSAP_CI') | \n",
    "                                         (tbl_publishedRates['SERVICE_CODE'] == 'MIPLE') |\n",
    "                                         (tbl_publishedRates['SERVICE_CODE'] == 'MIPH') |\n",
    "                                         (tbl_publishedRates['SERVICE_CODE'].str.startswith('DHLG_'))   |\n",
    "                                         (tbl_publishedRates['SERVICE_CODE'].str.startswith('DHLG<1_')) |\n",
    "                                         (tbl_publishedRates['SERVICE_CODE'].str.startswith('DHLE_'))   |\n",
    "                                         (tbl_publishedRates['SERVICE_CODE'].str.startswith('DHLE<1_')) |\n",
    "                                         (tbl_publishedRates['SERVICE_CODE'].str.startswith('DHLEM_'))]\n",
    "\n",
    "\n",
    "#combine them to rename as BarrettRates\n",
    "tbl_BarrettRates = pd.concat([tbl_barrettRates_1, tbl_barrettRates_2, tbl_barrettRates_3, tbl_barrettRates_4, \n",
    "                    tbl_barrettRates_5, tbl_barrettRates_6, tbl_barrettRates_7, tbl_barrettRates_8,\n",
    "                    tbl_barrettRates_9, tbl_barrettRates_10, tbl_barrettRates_11, tbl_barrettRates_12,\n",
    "                    tbl_barrettRates_13, tbl_barrettRates_14, tbl_barrettRates_15, tbl_barrettRates_16,\n",
    "                    tbl_barrettRates_17, tbl_barrettRates_18, tbl_barrettRates_19, tbl_barrettRates_20,\n",
    "                    tbl_barrettRates_21, tbl_barrettRates_22, tbl_barrettRates_23, tbl_barrettRates_24,\n",
    "                    tbl_barrettRates_25, tbl_barrettRates_26, tbl_barrettRates_27, tbl_barrettRates_28,\n",
    "                    tbl_barrettRates_29, tbl_barrettRates_30, tbl_barrettRates_31, tbl_barrettRates_32,\n",
    "                    tbl_barrettRates_33, tbl_barrettRates_34, tbl_barrettRates_35, tbl_barrettRates_36,\n",
    "                    tbl_barrettRates_37, tbl_barrettRates_38, tbl_barrettRates_39, tbl_barrettRates_40,\n",
    "                    tbl_barrettRates_41, tbl_barrettRates_42, tbl_barrettRates_43, tbl_barrettRates_44])\n",
    "#Keep this table as it only contains published rates and Barrett rates\n",
    "#tbl_BarrettRates.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "54c2fdd6-7c65-4ee7-8968-d7f0c3d9664a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define the conditions\n",
    "condition1 = (tbl_BarrettRates['SERVICE_CODE'] == 'SPST') & (tbl_BarrettRates['ZONE'].isin(['17', '9', '10', '26', '99'])) & (tbl_BarrettRates['WEIGHT_LB'] <= 2)\n",
    "condition2 = (tbl_BarrettRates['SERVICE_CODE'] == 'SPST') & (tbl_BarrettRates['ZONE'].isin(['17', '9', '10', '26', '99'])) & (tbl_BarrettRates['WEIGHT_LB'] > 2) & (tbl_BarrettRates['WEIGHT_LB'] < 4)\n",
    "condition3 = (tbl_BarrettRates['SERVICE_CODE'] == 'SPST') & (tbl_BarrettRates['ZONE'].isin(['17', '9', '10', '26', '99'])) & (tbl_BarrettRates['WEIGHT_LB'] >= 4) & (tbl_BarrettRates['WEIGHT_LB'] < 70)\n",
    "condition4 = (tbl_BarrettRates['SERVICE_CODE'] == 'SPST') & (tbl_BarrettRates['ZONE'].isin(['17', '9', '10', '26', '99'])) & (tbl_BarrettRates['WEIGHT_LB'] >= 70)\n",
    "\n",
    "# Overwrite values where the condition is met\n",
    "tbl_BarrettRates.loc[condition1, 'BARRETT_RATES'] = tbl_BarrettRates['PUBLISHED_RATES'].apply(lambda x: max(18.28, x * (1 - 0.81)))\n",
    "tbl_BarrettRates.loc[condition2, 'BARRETT_RATES'] = tbl_BarrettRates['PUBLISHED_RATES'].apply(lambda x: max(18.28, x * (1 - 0.58)))\n",
    "tbl_BarrettRates.loc[condition3, 'BARRETT_RATES'] = tbl_BarrettRates['PUBLISHED_RATES'].apply(lambda x: max(18.28, x * (1 - 0.61)))\n",
    "tbl_BarrettRates.loc[condition4, 'BARRETT_RATES'] = tbl_BarrettRates['PUBLISHED_RATES'].apply(lambda x: max(18.28, x * (1 - 0.61)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2d536e32",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define two different mappings\n",
    "mapping_for_ground = {0.1: 'Tier1', 0.2: 'Tier2', 0.3: 'Tier3', 0.4: 'Tier4', 0.5: 'Tier5', 0.6: 'Tier6', 0.7: 'Tier7', \n",
    "                      0.8: 'Tier8', 0.9: 'Tier9', 1: 'Tier10'}\n",
    "\n",
    "mapping_for_priority = {0.1: 'Tier1', 0.2: 'Tier2', 0.3: 'Tier3', 0.4: 'Tier4', 0.5: 'Tier5'}\n",
    "\n",
    "def apply_custom_mapping(row):\n",
    "    if row['SERVICE_CODE'] == 'USPSAG_CI':\n",
    "        return mapping_for_ground.get(row['CUBIC_FT'], None)  # None for default\n",
    "    elif row['SERVICE_CODE'] == 'USPSAP_CI':\n",
    "        return mapping_for_priority.get(row['CUBIC_FT'], None)  # None for default\n",
    "    else:\n",
    "        return None  # Or some default value for other ServiceCodes\n",
    "\n",
    "# Apply the custom function to create a new column\n",
    "tbl_publishedRates['TIER'] = tbl_publishedRates.apply(apply_custom_mapping, axis=1)\n",
    "tbl_BarrettRates['TIER'] = tbl_BarrettRates.apply(apply_custom_mapping, axis=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0d380a3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tbl_BarrettRates.to_csv('tbl_BarrettRates.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c3ae7871",
   "metadata": {},
   "outputs": [],
   "source": [
    "tbl_publishedRates.to_csv('tbl_publishedRates.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8bb22f20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Zone from facility doc\n",
    "# When the distribution is all over the U.S., we use TN Facility. Needs business input. Make sure the facility matches \n",
    "# between dhl rates and Zone finder.\n",
    "xls = pd.ExcelFile('zipOutbound.xlsx')\n",
    "# Load Outbound card\n",
    "OB_MA = pd.read_excel(xls, '02038_Outbound') # 5114358_dhl_sm_parcel_Franklin\n",
    "OB_NJ = pd.read_excel(xls, '08873_Outbound') # 5122444_dhl_sm_parcel_Somerset\n",
    "OB_MD = pd.read_excel(xls, '21226_Outbound') # 5123283_dhl_sm_parcel_Curtis Bay, GBM\n",
    "OB_TN = pd.read_excel(xls, '38141_Outbound') # 5122893_dhl_sm_parcell Memphis\n",
    "OB_MS = pd.read_excel(xls, '38654_Outbound') # 5122723_dhl_sm_parcel_8150 Nail Olive Branch, MS3/5122739_dhl_sm_parcel_Nail rd Olive branch, MS2\n",
    "OB_TX = pd.read_excel(xls, '75041_Outbound') # 5122855_dhl_sm_parcel_Garland TX\n",
    "OB_CA = pd.read_excel(xls, '90640_Outbound') # none\n",
    "\n",
    "OB_02324 = pd.read_excel(xls, '02324_Outbound')\n",
    "OB_06096 = pd.read_excel(xls, '06096_Outbound')\n",
    "OB_18105 = pd.read_excel(xls, '18105_Outbound')\n",
    "OB_21240 = pd.read_excel(xls, '21240_Outbound')\n",
    "OB_43004 = pd.read_excel(xls, '43004_Outbound')\n",
    "\n",
    "\n",
    "OB_MA_1 = OB_MA.copy()\n",
    "OB_MA_1 = OB_MA_1.drop('Origin', axis=1)\n",
    "OB_MA_1['Origin'] = '2048'\n",
    "\n",
    "OB_STATE_UPS = pd.concat([OB_MA, OB_NJ, OB_MD, OB_TN, OB_MS, OB_TX, OB_CA, OB_MA_1, OB_02324, OB_06096, OB_18105, OB_21240, OB_43004])\n",
    "\n",
    "# n/a - 5122890_dhl_sm_parcel_Oofos, Oofos is (or will be in Byhalia, MS)\n",
    "# n/a - 5123280_dhl_sm_parcel_MAN, 02048\n",
    "# n/a - 5123282_dhl_sm_parcel Bridgewater, MA5\n",
    "# 90640 is Montebello, CA (I don't see that here)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "44c8eaab",
   "metadata": {},
   "outputs": [],
   "source": [
    "OB_STATE_UPS.to_csv('OB_STATE_UPS.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0f0555a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Zone from facility doc\n",
    "# When the distribution is all over the U.S., we use TN Facility. Needs business input. Make sure the facility matches \n",
    "# between dhl rates and Zone finder.\n",
    "xls = pd.ExcelFile('6-2.ZipToZoneFedex.xlsx')\n",
    "\n",
    "# Load Outbound card\n",
    "\n",
    "\n",
    "OB_18101_FXG = pd.read_excel(xls, 'TDR29112023_18101_FXG') \n",
    "OB_18101_FHD = pd.read_excel(xls, 'TDR29112023_18101_FHD')\n",
    "\n",
    "OB_90640_FXG = pd.read_excel(xls, 'TDR14122023_90640_FXG') \n",
    "OB_90640_FHD = pd.read_excel(xls, 'TDR14122023_90640_FHD') \n",
    "\n",
    "OB_75041_FXG = pd.read_excel(xls, 'TDR14122023_75041_FXG') # 5123283_dhl_sm_parcel_Curtis Bay, GBM\n",
    "OB_75041_FHD = pd.read_excel(xls, 'TDR14122023_75041_FHD') # 5123283_dhl_sm_parcel_Curtis Bay, GBM\n",
    "\n",
    "OB_38654_FXG = pd.read_excel(xls, 'TDR14122023_38654_FXG') \n",
    "OB_38654_FHD = pd.read_excel(xls, 'TDR14122023_38654_FHD') \n",
    "\n",
    "OB_38141_FXG = pd.read_excel(xls, 'TDR14122023_38141_FXG') # 5122893_dhl_sm_parcell Memphis\n",
    "OB_38141_FHD = pd.read_excel(xls, 'TDR14122023_38141_FHD') # 5122893_dhl_sm_parcell Memphis\n",
    "\n",
    "OB_21226_FXG = pd.read_excel(xls, 'TDR14122023_21226_FXG') # 5122855_dhl_sm_parcel_Garland TX\n",
    "OB_21226_FHD = pd.read_excel(xls, 'TDR14122023_21226_FHD') # 5122855_dhl_sm_parcel_Garland TX\n",
    "\n",
    "OB_08873_FXG = pd.read_excel(xls, 'TDR14122023_08873_FXG') # 5122444_dhl_sm_parcel_Somerset\n",
    "OB_08873_FHD = pd.read_excel(xls, 'TDR14122023_08873_FHD') # 5122444_dhl_sm_parcel_Somerset\n",
    "\n",
    "OB_02038_FXG = pd.read_excel(xls, 'TDR14122023_02038_FXG') # 5114358_dhl_sm_parcel_Franklin\n",
    "OB_02038_FHD = pd.read_excel(xls, 'TDR14122023_02038_FHD') # 5114358_dhl_sm_parcel_Franklin\n",
    "\n",
    "# 5122723_dhl_sm_parcel_8150 Nail Olive Branch, MS3/5122739_dhl_sm_parcel_Nail rd Olive branch, MS2\n",
    "# 5122723_dhl_sm_parcel_8150 Nail Olive Branch, MS3/5122739_dhl_sm_parcel_Nail rd Olive branch, MS2\n",
    "\n",
    "OB_18101_FXG['FileID'] = '18101_FXG'\n",
    "OB_18101_FHD['FileID'] = '18101_FHD'\n",
    "OB_90640_FXG['FileID'] = '90640_FXG'\n",
    "OB_90640_FHD['FileID'] = '90640_FHD'\n",
    "OB_75041_FXG['FileID'] = '75041_FXG'\n",
    "OB_75041_FHD['FileID'] = '75041_FHD'\n",
    "OB_38654_FXG['FileID'] = '38654_FXG'\n",
    "OB_38654_FHD['FileID'] = '38654_FHD'\n",
    "OB_38141_FXG['FileID'] = '38141_FXG'\n",
    "OB_38141_FHD['FileID'] = '38141_FHD'\n",
    "OB_21226_FXG['FileID'] = '21226_FXG'\n",
    "OB_21226_FHD['FileID'] = '21226_FHD'\n",
    "OB_08873_FXG['FileID'] = '08873_FXG'\n",
    "OB_08873_FHD['FileID'] = '08873_FHD'\n",
    "OB_02038_FXG['FileID'] = '02038_FXG'\n",
    "OB_02038_FHD['FileID'] = '02038_FHD'\n",
    "\n",
    "OB_STATE_FedEx = pd.concat([OB_18101_FXG, OB_18101_FHD, OB_90640_FXG, OB_90640_FHD, OB_75041_FXG, OB_75041_FHD, \n",
    "                      OB_38654_FXG, OB_38654_FHD, OB_38141_FXG, OB_38141_FHD, OB_21226_FXG, OB_21226_FHD, \n",
    "                      OB_08873_FXG, OB_08873_FHD, OB_02038_FXG, OB_02038_FHD])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "62620497",
   "metadata": {},
   "outputs": [],
   "source": [
    "OB_STATE_FedEx.to_csv('OB_STATE_FedEx.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c456031-6b0d-47fd-8613-84f56f9c97c8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7415291a-920d-4a49-b7bf-ce788ab3dc2d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3231b5cd-0068-4922-8509-1da4b6889008",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
