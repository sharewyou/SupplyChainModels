{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "european-northern",
   "metadata": {},
   "source": [
    "# Set Client Margin "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1057,
   "id": "gross-category",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "#For UPS client freight & accessorial discounts, write \"YES\"/\"NO\" for rounding/not rounding to 2 decimal places.\n",
    "Round_Discount = \"NO\" #default = no"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1058,
   "id": "5458b457",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<p style=\"font-family: 'Arial'; font-size: 20px; color: red; background-color: yellow;\">\n",
       "    Edit Location 1 of 8\n",
       "</p>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import HTML\n",
    "\n",
    "# Create a styled HTML string\n",
    "styled_html = HTML(\"\"\"\n",
    "<p style=\"font-family: 'Arial'; font-size: 20px; color: red; background-color: yellow;\">\n",
    "    Edit Location 1 of 8\n",
    "</p>\n",
    "\"\"\")\n",
    "\n",
    "# Display the styled HTML in the notebook\n",
    "display(styled_html)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1059,
   "id": "indian-nothing",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set UPS margin for the client\n",
    "Margin_on_Min_GRND = 0.0  #----------------------Edit\n",
    "Margin_on_Min_AIR = 0.0  #-----------------------Edit\n",
    "\n",
    "Margin_on_UPS_Parcel_Freight_GRND = 0.0 #--------Edit\n",
    "Margin_on_UPS_Parcel_Freight_AIR = 0.0 #---------Edit\n",
    "\n",
    "Margin_on_Value_Add_GRND = 0.2 #-----------------Edit\n",
    "Margin_on_Value_Add_AIR = 0.2 #------------------Edit\n",
    "\n",
    "#Set DHL, USPS, and MI margin for the client\n",
    "Margin_on_DHL = 0.0 #----------------------------Edit\n",
    "Margin_on_USPS = 0.0 #---------------------------Edit\n",
    "Margin_on_MI = 0.0 #-----------------------------Edit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "separate-species",
   "metadata": {},
   "source": [
    "# Carrier Dim Factor 2023"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1060,
   "id": "optimum-century",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dim factor 2023\n",
    "dim_factor_ups = 280\n",
    "dim_factor_ups_srpt = 139\n",
    "dim_factor_dhl = 166\n",
    "dim_factor_usps = 166\n",
    "dim_factor_mi = 166"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbbe6deb",
   "metadata": {},
   "source": [
    "# Set the Year based on PLD \n",
    "This quote can only apply to 1 year at one time. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1061,
   "id": "cecb090f",
   "metadata": {},
   "outputs": [],
   "source": [
    "nqd_dhl_date_1 = '2023-01-01'\n",
    "nqd_dhl_date_2 = '2023-09-30'\n",
    "nqd_dhl_date_3 = '2023-10-01'\n",
    "nqd_dhl_date_4 = '2023-12-31'\n",
    "nqd_dhl_date_1 = pd.to_datetime(nqd_dhl_date_1)\n",
    "nqd_dhl_date_2 = pd.to_datetime(nqd_dhl_date_2)\n",
    "nqd_dhl_date_3 = pd.to_datetime(nqd_dhl_date_3)\n",
    "nqd_dhl_date_4 = pd.to_datetime(nqd_dhl_date_4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 947,
   "id": "3f16fd9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#OB_STATE = pd.read_csv('C:\\\\Users\\\\sliu\\\\BD\\\\re-rate_model\\\\OB_STATE.csv')\n",
    "\n",
    "##### Get Zone from facility doc\n",
    "##### When the distribution is all over the U.S., we use TN Facility. Needs business input. Make sure the facility matches \n",
    "##### between dhl rates and Zone finder.\n",
    "\n",
    "#xls = pd.ExcelFile('zipOutbound.xlsx')\n",
    "\n",
    "##### Load Outbound card\n",
    "\n",
    "#OB_MA = pd.read_excel(xls, '02038_Outbound') # 5114358_dhl_sm_parcel_Franklin\n",
    "#OB_NJ = pd.read_excel(xls, '08873_Outbound') # 5122444_dhl_sm_parcel_Somerset\n",
    "#OB_MD = pd.read_excel(xls, '21226_Outbound') # 5123283_dhl_sm_parcel_Curtis Bay, GBM\n",
    "#OB_TN = pd.read_excel(xls, '38141_Outbound') # 5122893_dhl_sm_parcell Memphis\n",
    "#OB_MS = pd.read_excel(xls, '38654_Outbound') # 5122723_dhl_sm_parcel_8150 Nail Olive Branch, MS3/5122739_dhl_sm_parcel_Nail rd Olive branch, MS2\n",
    "#OB_TX = pd.read_excel(xls, '75041_Outbound') # 5122855_dhl_sm_parcel_Garland TX\n",
    "#OB_CA = pd.read_excel(xls, '90640_Outbound') # none\n",
    "\n",
    "#OB_02324 = pd.read_excel(xls, '02324_Outbound')\n",
    "#OB_06096 = pd.read_excel(xls, '06096_Outbound')\n",
    "#OB_18105 = pd.read_excel(xls, '18105_Outbound')\n",
    "#OB_21240 = pd.read_excel(xls, '21240_Outbound')\n",
    "#OB_43004 = pd.read_excel(xls, '43004_Outbound')\n",
    "\n",
    "#OB_MA_1 = OB_MA.copy()\n",
    "#OB_MA_1 = OB_MA_1.drop('Origin', axis=1)\n",
    "#OB_MA_1['Origin'] = '2048'\n",
    "\n",
    "#OB_STATE = pd.concat([OB_MA, OB_NJ, OB_MD, OB_TN, OB_MS, OB_TX, OB_CA, OB_MA_1, OB_02324, OB_06096, OB_18105, OB_21240, OB_43004])\n",
    "\n",
    "##### n/a - 5122890_dhl_sm_parcel_Oofos, Oofos is (or will be in Byhalia, MS)\n",
    "##### n/a - 5123280_dhl_sm_parcel_MAN, 02048\n",
    "##### n/a - 5123282_dhl_sm_parcel Bridgewater, MA5\n",
    "##### 90640 is Montebello, CA (I don't see that here)   \n",
    "\n",
    "#OB_STATE.to_csv('OB_STATE.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vocational-unemployment",
   "metadata": {},
   "source": [
    "# ======Model Structure======"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "collect-nickname",
   "metadata": {},
   "source": [
    "Source tables:\n",
    "1. carrierRateCard_2023: published freight rate card (insert newer version once a year)\n",
    "\n",
    "   1-1. UPSAccessorial_2023: UPS accessorial rate card. (insert newer version once a year)\n",
    "\n",
    "\n",
    "2. areaSurchargeZipsUs_2023: a table to assign the type of DAS Category (insert newer version once a year)\n",
    "\n",
    "3. FSC: fuel surcharge rate (insert a new number once a week before automation)\n",
    "\n",
    "4. zip_Outbound: after decided the origin facility, get zone from destination zip \n",
    "\n",
    "5. ZiptoState: get state from destination zip to decide the origin facility\n",
    "    \n",
    "6. PLD: for rerate\n",
    "\n",
    "Structure:\n",
    "\n",
    "1 - a. Create Freight Rate Card (tbl_PublishedRates)  \n",
    "1 - b. Create Freight Rate Card (tbl_BarrettRates)  \n",
    "1 - c. Create Freight Rate Card (tbl_clientRates)\n",
    "\n",
    "2 - a. Create UPS accessorial rates  \n",
    "2 - b. Prepare all other rates for rerate\n",
    "\n",
    "3 - a. Prepare PLD for rerate: Load, Clean, and Format PLD  \n",
    "3 - b. Rerate Begin  \n",
    "3 - c. Output and analysis\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "| Carrier | Service | Residential | WeightType | RateType   |\n",
    "|:--------|:---------|:-------------|:------------|:------------|\n",
    "| UPS     | REDE (UPS NDA Early) / RED (UPS NDA) / REDS (UPS NDA Saver) / 2DAM (UPS 2DA A.M.) / BLUE (UPS 2DA) / ORNG (UPS 3DA) / SRPT (UPS Surepost) | both         | lb         | Published   |\n",
    "| UPS     | GRND (UPS Ground Commercial)                                                                                                              | commercial   | lb         | Published   |\n",
    "| UPS     | GRES (UPS Ground Residential)                                                                                                             | residential  | lb         | Published   |\n",
    "| UPS     | SRPT<1 (UPS Surepost 1#>)                                                                                                                 | both         | oz         | Published   |\n",
    "| DHL     | DHLG (DHL SmartMail Parcel Plus Ground 1-25)/DHLE (DHL SmartMail Parcel Plus Expedited 1-25) | both | lb | Barrett |\n",
    "| DHL     | DHLG<1 (DHL SmartMail Parcel Ground < 1lb)/DHLE<1 (DHL SmartMail Parcel Expedited  < 1lb)/DHLEM (DHL SmartMail Parcel Expedited Max) | both | lb | Barrett |\n",
    "| USPS    | USPSAG (USPS Auctane GRND) | both | lb | Barrett |\n",
    "| USPS    | USPSAP (USPS Auctane PM)| both | lb | Barrett |\n",
    "| MI      | MIPLE (MI Parcel Select Lightweight Expedited)|  both | oz | Barrett |\n",
    "| MI      | MIPH (MI Parcel Select Heavyweight)|  both | lb | Barrett |\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sitting-testament",
   "metadata": {},
   "source": [
    "# Step 1 - a & 1 - b\n",
    "*rate card: Add UPS published rates, DHL Barrett rates, and USPS Barrett rates. Needs to renew once a year. Can add more carriers later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1062,
   "id": "c5b459fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read carrierRateCard_2023 and remake the tables showing below columns.\n",
    "# SERVICE_CODE, SERVICE_PLAINTEXT, ZONE, WEIGHT_OZ, WEIGHT_LB, PUBLISHED_RATES, VERSION, START_DATE, END_DATE\n",
    "xls = pd.ExcelFile('carrierRateCard_2023.xlsx')\n",
    "# Load UPS rate card\n",
    "df1 = pd.read_excel(xls, 'REDE')\n",
    "df2 = pd.read_excel(xls, 'RED')\n",
    "df3 = pd.read_excel(xls, 'REDS')\n",
    "df4 = pd.read_excel(xls, '2DAM')\n",
    "df5 = pd.read_excel(xls, 'BLUE')\n",
    "df6 = pd.read_excel(xls, 'ORNG')\n",
    "df7 = pd.read_excel(xls, 'GRND')\n",
    "df8 = pd.read_excel(xls, 'GRES')\n",
    "df9 = pd.read_excel(xls, 'SRPT<1')\n",
    "df10 = pd.read_excel(xls, 'SRPT')\n",
    "\n",
    "# Load USPS rate card\n",
    "df16 = pd.read_excel(xls, 'USPSAG_CPP')\n",
    "df17 = pd.read_excel(xls, 'USPSAP_CPP')\n",
    "df20 = pd.read_excel(xls, 'USPSAG_CPPCI')\n",
    "df21 = pd.read_excel(xls, 'USPSAP_CPPCI')\n",
    "df22 = pd.read_excel(xls, 'USPSAG_AUCT')\n",
    "df23 = pd.read_excel(xls, 'USPSAP_AUCT')\n",
    "df24 = pd.read_excel(xls, 'USPSAG_AUCTCI')\n",
    "df25 = pd.read_excel(xls, 'USPSAP_AUCTCI')\n",
    "\n",
    "# Load MI rate card\n",
    "df18 = pd.read_excel(xls, 'MIPLE')\n",
    "df19 = pd.read_excel(xls, 'MIPH')\n",
    "\n",
    "# Load DHL rate card\n",
    "# 5114358_dhl_sm_parcel_Franklin\n",
    "df11_1 = pd.read_excel(xls, 'DHLG_5114358')\n",
    "df12_2 = pd.read_excel(xls, 'DHLG<1_5114358')\n",
    "df13_3 = pd.read_excel(xls, 'DHLE_5114358')\n",
    "df14_4 = pd.read_excel(xls, 'DHLE<1_5114358')\n",
    "df15_5 = pd.read_excel(xls, 'DHLEM_5114358')\n",
    "\n",
    "# 5122444_dhl_sm_parcel_Somerset\n",
    "df11_6 = pd.read_excel(xls, 'DHLG_5122444')\n",
    "df12_7 = pd.read_excel(xls, 'DHLG<1_5122444')\n",
    "df13_8 = pd.read_excel(xls, 'DHLE_5122444')\n",
    "df14_9 = pd.read_excel(xls, 'DHLE<1_5122444')\n",
    "#df15_10 = pd.read_excel(xls, 'DHLEM_5122444')\n",
    "\n",
    "# 5122723_dhl_sm_parcel_8150 Nail Olive Branch\n",
    "df11_11 = pd.read_excel(xls, 'DHLG_5122723')\n",
    "df12_12 = pd.read_excel(xls, 'DHLG<1_5122723')\n",
    "df13_13 = pd.read_excel(xls, 'DHLE_5122723')\n",
    "df14_14 = pd.read_excel(xls, 'DHLE<1_5122723')\n",
    "df15_15 = pd.read_excel(xls, 'DHLEM_5122723')\n",
    "\n",
    "# 5122739_dhl_sm_parcel_Nail rd Olive branch\n",
    "df11_16 = pd.read_excel(xls, 'DHLG_5122739')\n",
    "df12_17 = pd.read_excel(xls, 'DHLG<1_5122739')\n",
    "df13_18 = pd.read_excel(xls, 'DHLE_5122739')\n",
    "df14_19 = pd.read_excel(xls, 'DHLE<1_5122739')\n",
    "df15_20 = pd.read_excel(xls, 'DHLEM_5122739')\n",
    "\n",
    "# 5122855_dhl_sm_parcel_Garland TX\n",
    "df11_21 = pd.read_excel(xls, 'DHLG_5122855')\n",
    "df12_22 = pd.read_excel(xls, 'DHLG<1_5122855')\n",
    "df13_23 = pd.read_excel(xls, 'DHLE_5122855')\n",
    "df14_24 = pd.read_excel(xls, 'DHLE<1_5122855')\n",
    "df15_25 = pd.read_excel(xls, 'DHLEM_5122855')\n",
    "\n",
    "# 5122890_dhl_sm_parcel_Oofos\n",
    "df11_26 = pd.read_excel(xls, 'DHLG_5122890')\n",
    "df12_27 = pd.read_excel(xls, 'DHLG<1_5122890')\n",
    "df13_28 = pd.read_excel(xls, 'DHLE_5122890')\n",
    "df14_29 = pd.read_excel(xls, 'DHLE<1_5122890')\n",
    "df15_30 = pd.read_excel(xls, 'DHLEM_5122890')\n",
    "\n",
    "# 5122893_dhl_sm_parcel Memphis\n",
    "df11_31 = pd.read_excel(xls, 'DHLG_5122893')\n",
    "df12_32 = pd.read_excel(xls, 'DHLG<1_5122893')\n",
    "df13_33 = pd.read_excel(xls, 'DHLE_5122893')\n",
    "df14_34 = pd.read_excel(xls, 'DHLE<1_5122893')\n",
    "df15_35 = pd.read_excel(xls, 'DHLEM_5122893')\n",
    "\n",
    "# 5123280_dhl_sm_parcel_MAN\n",
    "df11_36 = pd.read_excel(xls, 'DHLG_5123280')\n",
    "df12_37 = pd.read_excel(xls, 'DHLG<1_5123280')\n",
    "df13_38 = pd.read_excel(xls, 'DHLE_5123280')\n",
    "df14_39 = pd.read_excel(xls, 'DHLE<1_5123280')\n",
    "df15_40 = pd.read_excel(xls, 'DHLEM_5123280')\n",
    "\n",
    "# 5123282_dhl_sm_parcel Bridgewater\n",
    "df11_41 = pd.read_excel(xls, 'DHLG_5123282')\n",
    "df12_42 = pd.read_excel(xls, 'DHLG<1_5123282')\n",
    "df13_43 = pd.read_excel(xls, 'DHLE_5123282')\n",
    "df14_44 = pd.read_excel(xls, 'DHLE<1_5123282')\n",
    "df15_45 = pd.read_excel(xls, 'DHLEM_5123282')\n",
    "\n",
    "# 5123283_dhl_sm_parcel_Curtis Bay\n",
    "df11_46 = pd.read_excel(xls, 'DHLG_5123283')\n",
    "df12_47 = pd.read_excel(xls, 'DHLG<1_5123283')\n",
    "df13_48 = pd.read_excel(xls, 'DHLE_5123283')\n",
    "df14_49 = pd.read_excel(xls, 'DHLE<1_5123283')\n",
    "df15_50 = pd.read_excel(xls, 'DHLEM_5123283')\n",
    "\n",
    "\n",
    "# 5123283_dhl_sm_parcel_Curtis Bay\n",
    "#df11_51 = pd.read_excel(xls, 'DHLG_5123556')\n",
    "#df12_52 = pd.read_excel(xls, 'DHLG<1_5123556')\n",
    "#df13_53 = pd.read_excel(xls, 'DHLE_5123556')\n",
    "#df14_54 = pd.read_excel(xls, 'DHLE<1_5123556')\n",
    "#df15_55 = pd.read_excel(xls, 'DHLEM_5123556')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1063,
   "id": "0a786035",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-arrange the tables\n",
    "# REDE, Barrett Rates = Published Rates. \n",
    "# made sure the tables are completely loaded into the dataframe by checking df1.shape\n",
    "\n",
    "df1 = df1.set_index('Zones').unstack().rename_axis(('ZONE','WEIGHT_LB')).reset_index(name='PUBLISHED_RATES')\n",
    "df2 = df2.set_index('Zones').unstack().rename_axis(('ZONE','WEIGHT_LB')).reset_index(name='PUBLISHED_RATES')\n",
    "df3 = df3.set_index('Zones').unstack().rename_axis(('ZONE','WEIGHT_LB')).reset_index(name='PUBLISHED_RATES')\n",
    "df4 = df4.set_index('Zones').unstack().rename_axis(('ZONE','WEIGHT_LB')).reset_index(name='PUBLISHED_RATES')\n",
    "df5 = df5.set_index('Zones').unstack().rename_axis(('ZONE','WEIGHT_LB')).reset_index(name='PUBLISHED_RATES')\n",
    "df6 = df6.set_index('Zones').unstack().rename_axis(('ZONE','WEIGHT_LB')).reset_index(name='PUBLISHED_RATES')\n",
    "df7 = df7.set_index('Zones').unstack().rename_axis(('ZONE','WEIGHT_LB')).reset_index(name='PUBLISHED_RATES')\n",
    "df8 = df8.set_index('Zones').unstack().rename_axis(('ZONE','WEIGHT_LB')).reset_index(name='PUBLISHED_RATES')\n",
    "df9 = df9.set_index('Zones').unstack().rename_axis(('ZONE','WEIGHT_OZ')).reset_index(name='PUBLISHED_RATES')\n",
    "df10 = df10.set_index('Zones').unstack().rename_axis(('ZONE','WEIGHT_LB')).reset_index(name='PUBLISHED_RATES')\n",
    "\n",
    "\n",
    "# put cpp price as Barrett Rate to ease the calculation\n",
    "df16 = df16.set_index('Zones').unstack().rename_axis(('ZONE','WEIGHT_LB')).reset_index(name='BARRETT_RATES')\n",
    "df17 = df17.set_index('Zones').unstack().rename_axis(('ZONE','WEIGHT_LB')).reset_index(name='BARRETT_RATES')\n",
    "df20 = df20.set_index('Zones').unstack().rename_axis(('ZONE','CUBIC_FT')).reset_index(name='BARRETT_RATES')\n",
    "df21 = df21.set_index('Zones').unstack().rename_axis(('ZONE','CUBIC_FT')).reset_index(name='BARRETT_RATES')\n",
    "df22 = df22.set_index('Zones').unstack().rename_axis(('ZONE','WEIGHT_LB')).reset_index(name='BARRETT_RATES')\n",
    "df23 = df23.set_index('Zones').unstack().rename_axis(('ZONE','WEIGHT_LB')).reset_index(name='BARRETT_RATES')\n",
    "df24 = df24.set_index('Zones').unstack().rename_axis(('ZONE','CUBIC_FT')).reset_index(name='BARRETT_RATES')\n",
    "df25 = df25.set_index('Zones').unstack().rename_axis(('ZONE','CUBIC_FT')).reset_index(name='BARRETT_RATES')\n",
    "\n",
    "df18 = df18.set_index('Zones').unstack().rename_axis(('ZONE','WEIGHT_OZ')).reset_index(name='BARRETT_RATES')\n",
    "df19 = df19.set_index('Zones').unstack().rename_axis(('ZONE','WEIGHT_LB')).reset_index(name='BARRETT_RATES')\n",
    "\n",
    "df11_1 = df11_1.set_index('Zones').unstack().rename_axis(('ZONE','WEIGHT_LB')).reset_index(name='BARRETT_RATES')\n",
    "df12_2 = df12_2.set_index('Zones').unstack().rename_axis(('ZONE','WEIGHT_OZ')).reset_index(name='BARRETT_RATES')\n",
    "df13_3 = df13_3.set_index('Zones').unstack().rename_axis(('ZONE','WEIGHT_LB')).reset_index(name='BARRETT_RATES')\n",
    "df14_4 = df14_4.set_index('Zones').unstack().rename_axis(('ZONE','WEIGHT_OZ')).reset_index(name='BARRETT_RATES')\n",
    "df15_5 = df15_5.set_index('Zones').unstack().rename_axis(('ZONE','WEIGHT_OZ')).reset_index(name='BARRETT_RATES')\n",
    "\n",
    "df11_6 = df11_6.set_index('Zones').unstack().rename_axis(('ZONE','WEIGHT_LB')).reset_index(name='BARRETT_RATES')\n",
    "df12_7 = df12_7.set_index('Zones').unstack().rename_axis(('ZONE','WEIGHT_OZ')).reset_index(name='BARRETT_RATES')\n",
    "df13_8 = df13_8.set_index('Zones').unstack().rename_axis(('ZONE','WEIGHT_LB')).reset_index(name='BARRETT_RATES')\n",
    "df14_9 = df14_9.set_index('Zones').unstack().rename_axis(('ZONE','WEIGHT_OZ')).reset_index(name='BARRETT_RATES')\n",
    "#df15_10 = df15_10.set_index('Zones').unstack().rename_axis(('ZONE','WEIGHT_OZ')).reset_index(name='BARRETT_RATES')\n",
    "\n",
    "df11_11 = df11_11.set_index('Zones').unstack().rename_axis(('ZONE','WEIGHT_LB')).reset_index(name='BARRETT_RATES')\n",
    "df12_12 = df12_12.set_index('Zones').unstack().rename_axis(('ZONE','WEIGHT_OZ')).reset_index(name='BARRETT_RATES')\n",
    "df13_13 = df13_13.set_index('Zones').unstack().rename_axis(('ZONE','WEIGHT_LB')).reset_index(name='BARRETT_RATES')\n",
    "df14_14 = df14_14.set_index('Zones').unstack().rename_axis(('ZONE','WEIGHT_OZ')).reset_index(name='BARRETT_RATES')\n",
    "df15_15 = df15_15.set_index('Zones').unstack().rename_axis(('ZONE','WEIGHT_OZ')).reset_index(name='BARRETT_RATES')\n",
    "\n",
    "df11_16 = df11_16.set_index('Zones').unstack().rename_axis(('ZONE','WEIGHT_LB')).reset_index(name='BARRETT_RATES')\n",
    "df12_17 = df12_17.set_index('Zones').unstack().rename_axis(('ZONE','WEIGHT_OZ')).reset_index(name='BARRETT_RATES')\n",
    "df13_18 = df13_18.set_index('Zones').unstack().rename_axis(('ZONE','WEIGHT_LB')).reset_index(name='BARRETT_RATES')\n",
    "df14_19 = df14_19.set_index('Zones').unstack().rename_axis(('ZONE','WEIGHT_OZ')).reset_index(name='BARRETT_RATES')\n",
    "df15_20 = df15_20.set_index('Zones').unstack().rename_axis(('ZONE','WEIGHT_OZ')).reset_index(name='BARRETT_RATES')\n",
    "\n",
    "df11_21 = df11_21.set_index('Zones').unstack().rename_axis(('ZONE','WEIGHT_LB')).reset_index(name='BARRETT_RATES')\n",
    "df12_22 = df12_22.set_index('Zones').unstack().rename_axis(('ZONE','WEIGHT_OZ')).reset_index(name='BARRETT_RATES')\n",
    "df13_23 = df13_23.set_index('Zones').unstack().rename_axis(('ZONE','WEIGHT_LB')).reset_index(name='BARRETT_RATES')\n",
    "df14_24 = df14_24.set_index('Zones').unstack().rename_axis(('ZONE','WEIGHT_OZ')).reset_index(name='BARRETT_RATES')\n",
    "df15_25 = df15_25.set_index('Zones').unstack().rename_axis(('ZONE','WEIGHT_OZ')).reset_index(name='BARRETT_RATES')\n",
    "\n",
    "df11_26 = df11_26.set_index('Zones').unstack().rename_axis(('ZONE','WEIGHT_LB')).reset_index(name='BARRETT_RATES')\n",
    "df12_27 = df12_27.set_index('Zones').unstack().rename_axis(('ZONE','WEIGHT_OZ')).reset_index(name='BARRETT_RATES')\n",
    "df13_28 = df13_28.set_index('Zones').unstack().rename_axis(('ZONE','WEIGHT_LB')).reset_index(name='BARRETT_RATES')\n",
    "df14_29 = df14_29.set_index('Zones').unstack().rename_axis(('ZONE','WEIGHT_OZ')).reset_index(name='BARRETT_RATES')\n",
    "df15_30 = df15_30.set_index('Zones').unstack().rename_axis(('ZONE','WEIGHT_OZ')).reset_index(name='BARRETT_RATES')\n",
    "\n",
    "df11_31 = df11_31.set_index('Zones').unstack().rename_axis(('ZONE','WEIGHT_LB')).reset_index(name='BARRETT_RATES')\n",
    "df12_32 = df12_32.set_index('Zones').unstack().rename_axis(('ZONE','WEIGHT_OZ')).reset_index(name='BARRETT_RATES')\n",
    "df13_33 = df13_33.set_index('Zones').unstack().rename_axis(('ZONE','WEIGHT_LB')).reset_index(name='BARRETT_RATES')\n",
    "df14_34 = df14_34.set_index('Zones').unstack().rename_axis(('ZONE','WEIGHT_OZ')).reset_index(name='BARRETT_RATES')\n",
    "df15_35 = df15_35.set_index('Zones').unstack().rename_axis(('ZONE','WEIGHT_OZ')).reset_index(name='BARRETT_RATES')\n",
    "\n",
    "df11_36 = df11_36.set_index('Zones').unstack().rename_axis(('ZONE','WEIGHT_LB')).reset_index(name='BARRETT_RATES')\n",
    "df12_37 = df12_37.set_index('Zones').unstack().rename_axis(('ZONE','WEIGHT_OZ')).reset_index(name='BARRETT_RATES')\n",
    "df13_38 = df13_38.set_index('Zones').unstack().rename_axis(('ZONE','WEIGHT_LB')).reset_index(name='BARRETT_RATES')\n",
    "df14_39 = df14_39.set_index('Zones').unstack().rename_axis(('ZONE','WEIGHT_OZ')).reset_index(name='BARRETT_RATES')\n",
    "df15_40 = df15_40.set_index('Zones').unstack().rename_axis(('ZONE','WEIGHT_OZ')).reset_index(name='BARRETT_RATES')\n",
    "\n",
    "df11_41 = df11_41.set_index('Zones').unstack().rename_axis(('ZONE','WEIGHT_LB')).reset_index(name='BARRETT_RATES')\n",
    "df12_42 = df12_42.set_index('Zones').unstack().rename_axis(('ZONE','WEIGHT_OZ')).reset_index(name='BARRETT_RATES')\n",
    "df13_43 = df13_43.set_index('Zones').unstack().rename_axis(('ZONE','WEIGHT_LB')).reset_index(name='BARRETT_RATES')\n",
    "df14_44 = df14_44.set_index('Zones').unstack().rename_axis(('ZONE','WEIGHT_OZ')).reset_index(name='BARRETT_RATES')\n",
    "df15_45 = df15_45.set_index('Zones').unstack().rename_axis(('ZONE','WEIGHT_OZ')).reset_index(name='BARRETT_RATES')\n",
    "\n",
    "df11_46 = df11_46.set_index('Zones').unstack().rename_axis(('ZONE','WEIGHT_LB')).reset_index(name='BARRETT_RATES')\n",
    "df12_47 = df12_47.set_index('Zones').unstack().rename_axis(('ZONE','WEIGHT_OZ')).reset_index(name='BARRETT_RATES')\n",
    "df13_48 = df13_48.set_index('Zones').unstack().rename_axis(('ZONE','WEIGHT_LB')).reset_index(name='BARRETT_RATES')\n",
    "df14_49 = df14_49.set_index('Zones').unstack().rename_axis(('ZONE','WEIGHT_OZ')).reset_index(name='BARRETT_RATES')\n",
    "df15_50 = df15_50.set_index('Zones').unstack().rename_axis(('ZONE','WEIGHT_OZ')).reset_index(name='BARRETT_RATES')\n",
    "\n",
    "#df11_51 = df11_51.set_index('Zones').unstack().rename_axis(('ZONE','WEIGHT_LB')).reset_index(name='BARRETT_RATES')\n",
    "#df12_52 = df12_52.set_index('Zones').unstack().rename_axis(('ZONE','WEIGHT_OZ')).reset_index(name='BARRETT_RATES')\n",
    "#df13_53 = df13_53.set_index('Zones').unstack().rename_axis(('ZONE','WEIGHT_LB')).reset_index(name='BARRETT_RATES')\n",
    "#df14_54 = df14_54.set_index('Zones').unstack().rename_axis(('ZONE','WEIGHT_OZ')).reset_index(name='BARRETT_RATES')\n",
    "#df15_55 = df15_55.set_index('Zones').unstack().rename_axis(('ZONE','WEIGHT_OZ')).reset_index(name='BARRETT_RATES')\n",
    "\n",
    "# Make sure the dataframes have both weight in lb and oz columns\n",
    "df1['WEIGHT_OZ'] = df1['WEIGHT_LB']*16\n",
    "df2['WEIGHT_OZ'] = df2['WEIGHT_LB']*16\n",
    "df3['WEIGHT_OZ'] = df3['WEIGHT_LB']*16\n",
    "df4['WEIGHT_OZ'] = df4['WEIGHT_LB']*16\n",
    "df5['WEIGHT_OZ'] = df5['WEIGHT_LB']*16\n",
    "df6['WEIGHT_OZ'] = df6['WEIGHT_LB']*16\n",
    "df7['WEIGHT_OZ'] = df7['WEIGHT_LB']*16\n",
    "df8['WEIGHT_OZ'] = df8['WEIGHT_LB']*16\n",
    "df9['WEIGHT_LB'] = df9['WEIGHT_OZ']/16\n",
    "df10['WEIGHT_OZ'] = df10['WEIGHT_LB']*16\n",
    "\n",
    "df16['WEIGHT_OZ'] = df16['WEIGHT_LB']*16\n",
    "df17['WEIGHT_OZ'] = df17['WEIGHT_LB']*16\n",
    "\n",
    "df22['WEIGHT_OZ'] = df22['WEIGHT_LB']*16\n",
    "df23['WEIGHT_OZ'] = df23['WEIGHT_LB']*16\n",
    "\n",
    "df18['WEIGHT_LB'] = df18['WEIGHT_OZ']/16\n",
    "df19['WEIGHT_OZ'] = df19['WEIGHT_LB']*16\n",
    "\n",
    "df11_1['WEIGHT_OZ'] = df11_1['WEIGHT_LB']*16\n",
    "df12_2['WEIGHT_LB'] = df12_2['WEIGHT_OZ']/16\n",
    "df13_3['WEIGHT_OZ'] = df13_3['WEIGHT_LB']*16\n",
    "df14_4['WEIGHT_LB'] = df14_4['WEIGHT_OZ']/16\n",
    "df15_5['WEIGHT_LB'] = df15_5['WEIGHT_OZ']/16\n",
    "\n",
    "df11_6['WEIGHT_OZ'] = df11_6['WEIGHT_LB']*16\n",
    "df12_7['WEIGHT_LB'] = df12_7['WEIGHT_OZ']/16\n",
    "df13_8['WEIGHT_OZ'] = df13_8['WEIGHT_LB']*16\n",
    "df14_9['WEIGHT_LB'] = df14_9['WEIGHT_OZ']/16\n",
    "#df15_10['WEIGHT_LB'] = df15_10['WEIGHT_OZ']/16\n",
    "\n",
    "df11_11['WEIGHT_OZ'] = df11_11['WEIGHT_LB']*16\n",
    "df12_12['WEIGHT_LB'] = df12_12['WEIGHT_OZ']/16\n",
    "df13_13['WEIGHT_OZ'] = df13_13['WEIGHT_LB']*16\n",
    "df14_14['WEIGHT_LB'] = df14_14['WEIGHT_OZ']/16\n",
    "df15_15['WEIGHT_LB'] = df15_15['WEIGHT_OZ']/16\n",
    "\n",
    "df11_16['WEIGHT_OZ'] = df11_16['WEIGHT_LB']*16\n",
    "df12_17['WEIGHT_LB'] = df12_17['WEIGHT_OZ']/16\n",
    "df13_18['WEIGHT_OZ'] = df13_18['WEIGHT_LB']*16\n",
    "df14_19['WEIGHT_LB'] = df14_19['WEIGHT_OZ']/16\n",
    "df15_20['WEIGHT_LB'] = df15_20['WEIGHT_OZ']/16\n",
    "\n",
    "df11_21['WEIGHT_OZ'] = df11_21['WEIGHT_LB']*16\n",
    "df12_22['WEIGHT_LB'] = df12_22['WEIGHT_OZ']/16\n",
    "df13_23['WEIGHT_OZ'] = df13_23['WEIGHT_LB']*16\n",
    "df14_24['WEIGHT_LB'] = df14_24['WEIGHT_OZ']/16\n",
    "df15_25['WEIGHT_LB'] = df15_25['WEIGHT_OZ']/16\n",
    "\n",
    "df11_26['WEIGHT_OZ'] = df11_26['WEIGHT_LB']*16\n",
    "df12_27['WEIGHT_LB'] = df12_27['WEIGHT_OZ']/16\n",
    "df13_28['WEIGHT_OZ'] = df13_28['WEIGHT_LB']*16\n",
    "df14_29['WEIGHT_LB'] = df14_29['WEIGHT_OZ']/16\n",
    "df15_30['WEIGHT_LB'] = df15_30['WEIGHT_OZ']/16\n",
    "\n",
    "df11_31['WEIGHT_OZ'] = df11_31['WEIGHT_LB']*16\n",
    "df12_32['WEIGHT_LB'] = df12_32['WEIGHT_OZ']/16\n",
    "df13_33['WEIGHT_OZ'] = df13_33['WEIGHT_LB']*16\n",
    "df14_34['WEIGHT_LB'] = df14_34['WEIGHT_OZ']/16\n",
    "df15_35['WEIGHT_LB'] = df15_35['WEIGHT_OZ']/16\n",
    "\n",
    "df11_36['WEIGHT_OZ'] = df11_36['WEIGHT_LB']*16\n",
    "df12_37['WEIGHT_LB'] = df12_37['WEIGHT_OZ']/16\n",
    "df13_38['WEIGHT_OZ'] = df13_38['WEIGHT_LB']*16\n",
    "df14_39['WEIGHT_LB'] = df14_39['WEIGHT_OZ']/16\n",
    "df15_40['WEIGHT_LB'] = df15_40['WEIGHT_OZ']/16\n",
    "\n",
    "df11_41['WEIGHT_OZ'] = df11_41['WEIGHT_LB']*16\n",
    "df12_42['WEIGHT_LB'] = df12_42['WEIGHT_OZ']/16\n",
    "df13_43['WEIGHT_OZ'] = df13_43['WEIGHT_LB']*16\n",
    "df14_44['WEIGHT_LB'] = df14_44['WEIGHT_OZ']/16\n",
    "df15_45['WEIGHT_LB'] = df15_45['WEIGHT_OZ']/16\n",
    "\n",
    "df11_46['WEIGHT_OZ'] = df11_46['WEIGHT_LB']*16\n",
    "df12_47['WEIGHT_LB'] = df12_47['WEIGHT_OZ']/16\n",
    "df13_48['WEIGHT_OZ'] = df13_48['WEIGHT_LB']*16\n",
    "df14_49['WEIGHT_LB'] = df14_49['WEIGHT_OZ']/16\n",
    "df15_50['WEIGHT_LB'] = df15_50['WEIGHT_OZ']/16\n",
    "\n",
    "\n",
    "#df11_51['WEIGHT_OZ'] = df11_51['WEIGHT_LB']*16\n",
    "#df12_52['WEIGHT_LB'] = df12_52['WEIGHT_OZ']/16\n",
    "#df13_53['WEIGHT_OZ'] = df13_53['WEIGHT_LB']*16\n",
    "#df14_54['WEIGHT_LB'] = df14_54['WEIGHT_OZ']/16\n",
    "#df15_55['WEIGHT_LB'] = df15_55['WEIGHT_OZ']/16\n",
    "\n",
    "\n",
    "# use below if the charts have empty spaces. No need to use here as the source tables are already clean.\n",
    "# df1 = df1.dropna(axis = 0, how = 'all')\n",
    "# df1 = df1.dropna(axis = 1, how = 'all')\n",
    "# df1 = df1.dropna()\n",
    "\n",
    "# Add Service code and Service columns\n",
    "df1['SERVICE_PLAINTEXT']='UPS NDA Early'\n",
    "df2['SERVICE_PLAINTEXT']='UPS NDA'\n",
    "df3['SERVICE_PLAINTEXT']='UPS NDA Saver'\n",
    "df4['SERVICE_PLAINTEXT']='UPS 2DA A.M.'\n",
    "df5['SERVICE_PLAINTEXT']='UPS 2DA'\n",
    "df6['SERVICE_PLAINTEXT']='UPS 3DA'\n",
    "df7['SERVICE_PLAINTEXT']='UPS Ground Commercial'\n",
    "df8['SERVICE_PLAINTEXT']='UPS Ground Residential'\n",
    "df9['SERVICE_PLAINTEXT']='UPS Surepost 1#>'\n",
    "df10['SERVICE_PLAINTEXT']='UPS Surepost'\n",
    "\n",
    "df16['SERVICE_PLAINTEXT']='USPS Ground Cpp'\n",
    "df17['SERVICE_PLAINTEXT']='USPS Priority Cpp'\n",
    "\n",
    "df20['SERVICE_PLAINTEXT']='USPS Ground Cpp Cubic Inch'\n",
    "df21['SERVICE_PLAINTEXT']='USPS Priority Cpp Cubic Inch'\n",
    "\n",
    "df22['SERVICE_PLAINTEXT']='USPS Ground Auctane'\n",
    "df23['SERVICE_PLAINTEXT']='USPS Priority Auctane'\n",
    "\n",
    "df24['SERVICE_PLAINTEXT']='USPS Ground Auctane Cubic Inch'\n",
    "df25['SERVICE_PLAINTEXT']='USPS Priority Auctane Cubic Inch'\n",
    "\n",
    "df18['SERVICE_PLAINTEXT']='MI Parcel Select Lightweight Expedited'\n",
    "df19['SERVICE_PLAINTEXT']='MI Parcel Select Heavyweight'\n",
    "\n",
    "# Load DHL rate card\n",
    "# 5114358_dhl_sm_parcel_Franklin\n",
    "# 5122444_dhl_sm_parcel_Somerset, no DHLEM\n",
    "# 5122723_dhl_sm_parcel_8150 Nail Olive Branch\n",
    "# 5122739_dhl_sm_parcel_Nail rd Olive branch\n",
    "# 5122855_dhl_sm_parcel_Garland TX\n",
    "# 5122890_dhl_sm_parcel_Oofos\n",
    "# 5122893_dhl_sm_parcell Memphis\n",
    "# 5123280_dhl_sm_parcel_MAN\n",
    "# 5123282_dhl_sm_parcel Bridgewater\n",
    "# 5123283_dhl_sm_parcel_Curtis Bay\n",
    "\n",
    "df11_1['SERVICE_PLAINTEXT']='DHL SmartMail Parcel Plus Ground 1-25 5114358'\n",
    "df12_2['SERVICE_PLAINTEXT']='DHL SmartMail Parcel Ground < 1lb 5114358'\n",
    "df13_3['SERVICE_PLAINTEXT']='DHL SmartMail Parcel Plus Expedited 1-25 5114358'\n",
    "df14_4['SERVICE_PLAINTEXT']='DHL SmartMail Parcel Expedited < 1lb 5114358'\n",
    "df15_5['SERVICE_PLAINTEXT']='DHL SmartMail Parcel Expedited Max 5114358'\n",
    "\n",
    "df11_6['SERVICE_PLAINTEXT']='DHL SmartMail Parcel Plus Ground 1-25 5122444'\n",
    "df12_7['SERVICE_PLAINTEXT']='DHL SmartMail Parcel Ground < 1lb 5122444'\n",
    "df13_8['SERVICE_PLAINTEXT']='DHL SmartMail Parcel Plus Expedited 1-25 5122444'\n",
    "df14_9['SERVICE_PLAINTEXT']='DHL SmartMail Parcel Expedited < 1lb 5122444'\n",
    "#df15_10['SERVICE_PLAINTEXT']='DHL SmartMail Parcel Expedited Max 5122444'\n",
    "\n",
    "df11_11['SERVICE_PLAINTEXT']='DHL SmartMail Parcel Plus Ground 1-25 5122723'\n",
    "df12_12['SERVICE_PLAINTEXT']='DHL SmartMail Parcel Ground < 1lb 5122723'\n",
    "df13_13['SERVICE_PLAINTEXT']='DHL SmartMail Parcel Plus Expedited 1-25 5122723'\n",
    "df14_14['SERVICE_PLAINTEXT']='DHL SmartMail Parcel Expedited < 1lb 5122723'\n",
    "df15_15['SERVICE_PLAINTEXT']='DHL SmartMail Parcel Expedited Max 5122723'\n",
    "\n",
    "df11_16['SERVICE_PLAINTEXT']='DHL SmartMail Parcel Plus Ground 1-25 5122739'\n",
    "df12_17['SERVICE_PLAINTEXT']='DHL SmartMail Parcel Ground < 1lb 5122739'\n",
    "df13_18['SERVICE_PLAINTEXT']='DHL SmartMail Parcel Plus Expedited 1-25 5122739'\n",
    "df14_19['SERVICE_PLAINTEXT']='DHL SmartMail Parcel Expedited < 1lb 5122739'\n",
    "df15_20['SERVICE_PLAINTEXT']='DHL SmartMail Parcel Expedited Max 5122739'\n",
    "\n",
    "df11_21['SERVICE_PLAINTEXT']='DHL SmartMail Parcel Plus Ground 1-25 5122855'\n",
    "df12_22['SERVICE_PLAINTEXT']='DHL SmartMail Parcel Ground < 1lb 5122855'\n",
    "df13_23['SERVICE_PLAINTEXT']='DHL SmartMail Parcel Plus Expedited 1-25 5122855'\n",
    "df14_24['SERVICE_PLAINTEXT']='DHL SmartMail Parcel Expedited < 1lb 5122855'\n",
    "df15_25['SERVICE_PLAINTEXT']='DHL SmartMail Parcel Expedited Max 5122855'\n",
    "\n",
    "df11_26['SERVICE_PLAINTEXT']='DHL SmartMail Parcel Plus Ground 1-25 5122890'\n",
    "df12_27['SERVICE_PLAINTEXT']='DHL SmartMail Parcel Ground < 1lb 5122890'\n",
    "df13_28['SERVICE_PLAINTEXT']='DHL SmartMail Parcel Plus Expedited 1-25 5122890'\n",
    "df14_29['SERVICE_PLAINTEXT']='DHL SmartMail Parcel Expedited < 1lb 5122890'\n",
    "df15_30['SERVICE_PLAINTEXT']='DHL SmartMail Parcel Expedited Max 5122890'\n",
    "\n",
    "df11_31['SERVICE_PLAINTEXT']='DHL SmartMail Parcel Plus Ground 1-25 5122893'\n",
    "df12_32['SERVICE_PLAINTEXT']='DHL SmartMail Parcel Ground < 1lb 5122893'\n",
    "df13_33['SERVICE_PLAINTEXT']='DHL SmartMail Parcel Plus Expedited 1-25 5122893'\n",
    "df14_34['SERVICE_PLAINTEXT']='DHL SmartMail Parcel Expedited < 1lb 5122893'\n",
    "df15_35['SERVICE_PLAINTEXT']='DHL SmartMail Parcel Expedited Max 5122893'\n",
    "\n",
    "df11_36['SERVICE_PLAINTEXT']='DHL SmartMail Parcel Plus Ground 1-25 5123280'\n",
    "df12_37['SERVICE_PLAINTEXT']='DHL SmartMail Parcel Ground < 1lb 5123280'\n",
    "df13_38['SERVICE_PLAINTEXT']='DHL SmartMail Parcel Plus Expedited 1-25 5123280'\n",
    "df14_39['SERVICE_PLAINTEXT']='DHL SmartMail Parcel Expedited < 1lb 5123280'\n",
    "df15_40['SERVICE_PLAINTEXT']='DHL SmartMail Parcel Expedited Max 5123280'\n",
    "\n",
    "df11_41['SERVICE_PLAINTEXT']='DHL SmartMail Parcel Plus Ground 1-25 5123282'\n",
    "df12_42['SERVICE_PLAINTEXT']='DHL SmartMail Parcel Ground < 1lb 5123282'\n",
    "df13_43['SERVICE_PLAINTEXT']='DHL SmartMail Parcel Plus Expedited 1-25 5123282'\n",
    "df14_44['SERVICE_PLAINTEXT']='DHL SmartMail Parcel Expedited < 1lb 5123282'\n",
    "df15_45['SERVICE_PLAINTEXT']='DHL SmartMail Parcel Expedited Max 5123282'\n",
    "\n",
    "df11_46['SERVICE_PLAINTEXT']='DHL SmartMail Parcel Plus Ground 1-25 5123283'\n",
    "df12_47['SERVICE_PLAINTEXT']='DHL SmartMail Parcel Ground < 1lb 5123283'\n",
    "df13_48['SERVICE_PLAINTEXT']='DHL SmartMail Parcel Plus Expedited 1-25 5123283'\n",
    "df14_49['SERVICE_PLAINTEXT']='DHL SmartMail Parcel Expedited < 1lb 5123283'\n",
    "df15_50['SERVICE_PLAINTEXT']='DHL SmartMail Parcel Expedited Max 5123283'\n",
    "\n",
    "#df11_51['SERVICE_PLAINTEXT']='DHL SmartMail Parcel Plus Ground 1-25 5123556'\n",
    "#df12_52['SERVICE_PLAINTEXT']='DHL SmartMail Parcel Ground < 1lb 5123556'\n",
    "#df13_53['SERVICE_PLAINTEXT']='DHL SmartMail Parcel Plus Expedited 1-25 5123556'\n",
    "#df14_54['SERVICE_PLAINTEXT']='DHL SmartMail Parcel Expedited < 1lb 5123556'\n",
    "#df15_55['SERVICE_PLAINTEXT']='DHL SmartMail Parcel Expedited Max 5123556'\n",
    "\n",
    "\n",
    "\n",
    "df1['SERVICE_CODE']='REDE'\n",
    "df2['SERVICE_CODE']='RED'\n",
    "df3['SERVICE_CODE']='REDS'\n",
    "df4['SERVICE_CODE']='2DAM'\n",
    "df5['SERVICE_CODE']='BLUE'\n",
    "df6['SERVICE_CODE']='ORNG'\n",
    "df7['SERVICE_CODE']='GRND'\n",
    "df8['SERVICE_CODE']='GRES'\n",
    "df9['SERVICE_CODE']='SRPT<1'\n",
    "df10['SERVICE_CODE']='SRPT'                               \n",
    "                             \n",
    "df16['SERVICE_CODE']='USPSAG_CPP'\n",
    "df17['SERVICE_CODE']='USPSAP_CPP'\n",
    "\n",
    "df20['SERVICE_CODE']='USPSAG_CPPCI'\n",
    "df21['SERVICE_CODE']='USPSAP_CPPCI'\n",
    "\n",
    "df22['SERVICE_CODE']='USPSAG_AUCT'\n",
    "df23['SERVICE_CODE']='USPSAP_AUCT'\n",
    "\n",
    "df24['SERVICE_CODE']='USPSAG_AUCTCI'\n",
    "df25['SERVICE_CODE']='USPSAP_AUCTCI'\n",
    "\n",
    "\n",
    "df18['SERVICE_CODE']='MIPLE'\n",
    "df19['SERVICE_CODE']='MIPH'\n",
    "\n",
    "df11_1['SERVICE_CODE']='DHLG_5114358'\n",
    "df12_2['SERVICE_CODE']='DHLG<1_5114358'\n",
    "df13_3['SERVICE_CODE']='DHLE_5114358'\n",
    "df14_4['SERVICE_CODE']='DHLE<1_5114358'\n",
    "df15_5['SERVICE_CODE']='DHLEM_5114358'\n",
    "\n",
    "df11_6['SERVICE_CODE']='DHLG_5122444'\n",
    "df12_7['SERVICE_CODE']='DHLG<1_5122444'\n",
    "df13_8['SERVICE_CODE']='DHLE_5122444'\n",
    "df14_9['SERVICE_CODE']='DHLE<1_5122444'\n",
    "#df15_10['SERVICE_CODE']='DHLEM_5122444'\n",
    "\n",
    "df11_11['SERVICE_CODE']='DHLG_5122723'\n",
    "df12_12['SERVICE_CODE']='DHLG<1_5122723'\n",
    "df13_13['SERVICE_CODE']='DHLE_5122723'\n",
    "df14_14['SERVICE_CODE']='DHLE<1_5122723'\n",
    "df15_15['SERVICE_CODE']='DHLEM_5122723'\n",
    "\n",
    "df11_16['SERVICE_CODE']='DHLG_5122739'\n",
    "df12_17['SERVICE_CODE']='DHLG<1_5122739'\n",
    "df13_18['SERVICE_CODE']='DHLE_5122739'\n",
    "df14_19['SERVICE_CODE']='DHLE<1_5122739'\n",
    "df15_20['SERVICE_CODE']='DHLEM_5122739'\n",
    "\n",
    "df11_21['SERVICE_CODE']='DHLG_5122855'\n",
    "df12_22['SERVICE_CODE']='DHLG<1_5122855'\n",
    "df13_23['SERVICE_CODE']='DHLE_5122855'\n",
    "df14_24['SERVICE_CODE']='DHLE<1_5122855'\n",
    "df15_25['SERVICE_CODE']='DHLEM_5122855'\n",
    "\n",
    "df11_26['SERVICE_CODE']='DHLG_5122890'\n",
    "df12_27['SERVICE_CODE']='DHLG<1_5122890'\n",
    "df13_28['SERVICE_CODE']='DHLE_5122890'\n",
    "df14_29['SERVICE_CODE']='DHLE<1_5122890'\n",
    "df15_30['SERVICE_CODE']='DHLEM_5122890'\n",
    "\n",
    "df11_31['SERVICE_CODE']='DHLG_5122893'\n",
    "df12_32['SERVICE_CODE']='DHLG<1_5122893'\n",
    "df13_33['SERVICE_CODE']='DHLE_5122893'\n",
    "df14_34['SERVICE_CODE']='DHLE<1_5122893'\n",
    "df15_35['SERVICE_CODE']='DHLEM_5122893'\n",
    "\n",
    "df11_36['SERVICE_CODE']='DHLG_5123280'\n",
    "df12_37['SERVICE_CODE']='DHLG<1_5123280'\n",
    "df13_38['SERVICE_CODE']='DHLE_5123280'\n",
    "df14_39['SERVICE_CODE']='DHLE<1_5123280'\n",
    "df15_40['SERVICE_CODE']='DHLEM_5123280'\n",
    "\n",
    "df11_41['SERVICE_CODE']='DHLG_5123282'\n",
    "df12_42['SERVICE_CODE']='DHLG<1_5123282'\n",
    "df13_43['SERVICE_CODE']='DHLE_5123282'\n",
    "df14_44['SERVICE_CODE']='DHLE<1_5123282'\n",
    "df15_45['SERVICE_CODE']='DHLEM_5123282'\n",
    "\n",
    "df11_46['SERVICE_CODE']='DHLG_5123283'\n",
    "df12_47['SERVICE_CODE']='DHLG<1_5123283'\n",
    "df13_48['SERVICE_CODE']='DHLE_5123283'\n",
    "df14_49['SERVICE_CODE']='DHLE<1_5123283'\n",
    "df15_50['SERVICE_CODE']='DHLEM_5123283'\n",
    "\n",
    "\n",
    "#df11_51['SERVICE_CODE']='DHLG_5123556'\n",
    "#df12_52['SERVICE_CODE']='DHLG<1_5123556'\n",
    "#df13_53['SERVICE_CODE']='DHLE_5123556'\n",
    "#df14_54['SERVICE_CODE']='DHLE<1_5123556'\n",
    "#df15_55['SERVICE_CODE']='DHLEM_5123556'\n",
    "\n",
    "\n",
    "#Concatenate all together after rearranging the columns\n",
    "df1 = df1.reindex(columns=['SERVICE_CODE', 'SERVICE_PLAINTEXT', 'ZONE', 'WEIGHT_OZ', 'WEIGHT_LB', 'PUBLISHED_RATES'])\n",
    "df2 = df2.reindex(columns=['SERVICE_CODE', 'SERVICE_PLAINTEXT', 'ZONE', 'WEIGHT_OZ', 'WEIGHT_LB', 'PUBLISHED_RATES'])\n",
    "df3 = df3.reindex(columns=['SERVICE_CODE', 'SERVICE_PLAINTEXT', 'ZONE', 'WEIGHT_OZ', 'WEIGHT_LB', 'PUBLISHED_RATES'])\n",
    "df4 = df4.reindex(columns=['SERVICE_CODE', 'SERVICE_PLAINTEXT', 'ZONE', 'WEIGHT_OZ', 'WEIGHT_LB', 'PUBLISHED_RATES'])\n",
    "df5 = df5.reindex(columns=['SERVICE_CODE', 'SERVICE_PLAINTEXT', 'ZONE', 'WEIGHT_OZ', 'WEIGHT_LB', 'PUBLISHED_RATES'])\n",
    "df6 = df6.reindex(columns=['SERVICE_CODE', 'SERVICE_PLAINTEXT', 'ZONE', 'WEIGHT_OZ', 'WEIGHT_LB', 'PUBLISHED_RATES'])\n",
    "df7 = df7.reindex(columns=['SERVICE_CODE', 'SERVICE_PLAINTEXT', 'ZONE', 'WEIGHT_OZ', 'WEIGHT_LB', 'PUBLISHED_RATES'])\n",
    "df8 = df8.reindex(columns=['SERVICE_CODE', 'SERVICE_PLAINTEXT', 'ZONE', 'WEIGHT_OZ', 'WEIGHT_LB', 'PUBLISHED_RATES'])\n",
    "df9 = df9.reindex(columns=['SERVICE_CODE', 'SERVICE_PLAINTEXT', 'ZONE', 'WEIGHT_OZ', 'WEIGHT_LB', 'PUBLISHED_RATES'])\n",
    "df10 = df10.reindex(columns=['SERVICE_CODE', 'SERVICE_PLAINTEXT', 'ZONE', 'WEIGHT_OZ', 'WEIGHT_LB', 'PUBLISHED_RATES'])\n",
    "\n",
    "df16 = df16.reindex(columns=['SERVICE_CODE', 'SERVICE_PLAINTEXT', 'ZONE', 'WEIGHT_OZ', 'WEIGHT_LB', 'BARRETT_RATES'])\n",
    "df17 = df17.reindex(columns=['SERVICE_CODE', 'SERVICE_PLAINTEXT', 'ZONE', 'WEIGHT_OZ', 'WEIGHT_LB', 'BARRETT_RATES'])\n",
    "\n",
    "df20 = df20.reindex(columns=['SERVICE_CODE', 'SERVICE_PLAINTEXT', 'ZONE', 'CUBIC_FT', 'BARRETT_RATES'])\n",
    "df21 = df21.reindex(columns=['SERVICE_CODE', 'SERVICE_PLAINTEXT', 'ZONE', 'CUBIC_FT', 'BARRETT_RATES'])\n",
    "\n",
    "df22 = df22.reindex(columns=['SERVICE_CODE', 'SERVICE_PLAINTEXT', 'ZONE', 'WEIGHT_OZ', 'WEIGHT_LB', 'BARRETT_RATES'])\n",
    "df23 = df23.reindex(columns=['SERVICE_CODE', 'SERVICE_PLAINTEXT', 'ZONE', 'WEIGHT_OZ', 'WEIGHT_LB', 'BARRETT_RATES'])\n",
    "\n",
    "df24 = df24.reindex(columns=['SERVICE_CODE', 'SERVICE_PLAINTEXT', 'ZONE', 'CUBIC_FT', 'BARRETT_RATES'])\n",
    "df25 = df25.reindex(columns=['SERVICE_CODE', 'SERVICE_PLAINTEXT', 'ZONE', 'CUBIC_FT', 'BARRETT_RATES'])\n",
    "\n",
    "\n",
    "df18 = df18.reindex(columns=['SERVICE_CODE', 'SERVICE_PLAINTEXT', 'ZONE', 'WEIGHT_OZ', 'WEIGHT_LB', 'BARRETT_RATES'])\n",
    "df19 = df19.reindex(columns=['SERVICE_CODE', 'SERVICE_PLAINTEXT', 'ZONE', 'WEIGHT_OZ', 'WEIGHT_LB', 'BARRETT_RATES'])\n",
    "\n",
    "df11_1 = df11_1.reindex(columns=['SERVICE_CODE', 'SERVICE_PLAINTEXT', 'ZONE', 'WEIGHT_OZ', 'WEIGHT_LB', 'BARRETT_RATES'])\n",
    "df12_2 = df12_2.reindex(columns=['SERVICE_CODE', 'SERVICE_PLAINTEXT', 'ZONE', 'WEIGHT_OZ', 'WEIGHT_LB', 'BARRETT_RATES'])\n",
    "df13_3 = df13_3.reindex(columns=['SERVICE_CODE', 'SERVICE_PLAINTEXT', 'ZONE', 'WEIGHT_OZ', 'WEIGHT_LB', 'BARRETT_RATES'])\n",
    "df14_4 = df14_4.reindex(columns=['SERVICE_CODE', 'SERVICE_PLAINTEXT', 'ZONE', 'WEIGHT_OZ', 'WEIGHT_LB', 'BARRETT_RATES'])\n",
    "df15_5 = df15_5.reindex(columns=['SERVICE_CODE', 'SERVICE_PLAINTEXT', 'ZONE', 'WEIGHT_OZ', 'WEIGHT_LB', 'BARRETT_RATES'])\n",
    "\n",
    "df11_6 = df11_6.reindex(columns=['SERVICE_CODE', 'SERVICE_PLAINTEXT', 'ZONE', 'WEIGHT_OZ', 'WEIGHT_LB', 'BARRETT_RATES'])\n",
    "df12_7 = df12_7.reindex(columns=['SERVICE_CODE', 'SERVICE_PLAINTEXT', 'ZONE', 'WEIGHT_OZ', 'WEIGHT_LB', 'BARRETT_RATES'])\n",
    "df13_8 = df13_8.reindex(columns=['SERVICE_CODE', 'SERVICE_PLAINTEXT', 'ZONE', 'WEIGHT_OZ', 'WEIGHT_LB', 'BARRETT_RATES'])\n",
    "df14_9 = df14_9.reindex(columns=['SERVICE_CODE', 'SERVICE_PLAINTEXT', 'ZONE', 'WEIGHT_OZ', 'WEIGHT_LB', 'BARRETT_RATES'])\n",
    "#df15_10 = df15_10.reindex(columns=['SERVICE_CODE', 'SERVICE_PLAINTEXT', 'ZONE', 'WEIGHT_OZ', 'WEIGHT_LB', 'BARRETT_RATES'])\n",
    "\n",
    "df11_11 = df11_11.reindex(columns=['SERVICE_CODE', 'SERVICE_PLAINTEXT', 'ZONE', 'WEIGHT_OZ', 'WEIGHT_LB', 'BARRETT_RATES'])\n",
    "df12_12 = df12_12.reindex(columns=['SERVICE_CODE', 'SERVICE_PLAINTEXT', 'ZONE', 'WEIGHT_OZ', 'WEIGHT_LB', 'BARRETT_RATES'])\n",
    "df13_13 = df13_13.reindex(columns=['SERVICE_CODE', 'SERVICE_PLAINTEXT', 'ZONE', 'WEIGHT_OZ', 'WEIGHT_LB', 'BARRETT_RATES'])\n",
    "df14_14 = df14_14.reindex(columns=['SERVICE_CODE', 'SERVICE_PLAINTEXT', 'ZONE', 'WEIGHT_OZ', 'WEIGHT_LB', 'BARRETT_RATES'])\n",
    "df15_15 = df15_15.reindex(columns=['SERVICE_CODE', 'SERVICE_PLAINTEXT', 'ZONE', 'WEIGHT_OZ', 'WEIGHT_LB', 'BARRETT_RATES'])\n",
    "\n",
    "df11_16 = df11_16.reindex(columns=['SERVICE_CODE', 'SERVICE_PLAINTEXT', 'ZONE', 'WEIGHT_OZ', 'WEIGHT_LB', 'BARRETT_RATES'])\n",
    "df12_17 = df12_17.reindex(columns=['SERVICE_CODE', 'SERVICE_PLAINTEXT', 'ZONE', 'WEIGHT_OZ', 'WEIGHT_LB', 'BARRETT_RATES'])\n",
    "df13_18 = df13_18.reindex(columns=['SERVICE_CODE', 'SERVICE_PLAINTEXT', 'ZONE', 'WEIGHT_OZ', 'WEIGHT_LB', 'BARRETT_RATES'])\n",
    "df14_19 = df14_19.reindex(columns=['SERVICE_CODE', 'SERVICE_PLAINTEXT', 'ZONE', 'WEIGHT_OZ', 'WEIGHT_LB', 'BARRETT_RATES'])\n",
    "df15_20 = df15_20.reindex(columns=['SERVICE_CODE', 'SERVICE_PLAINTEXT', 'ZONE', 'WEIGHT_OZ', 'WEIGHT_LB', 'BARRETT_RATES'])\n",
    "\n",
    "df11_21 = df11_21.reindex(columns=['SERVICE_CODE', 'SERVICE_PLAINTEXT', 'ZONE', 'WEIGHT_OZ', 'WEIGHT_LB', 'BARRETT_RATES'])\n",
    "df12_22 = df12_22.reindex(columns=['SERVICE_CODE', 'SERVICE_PLAINTEXT', 'ZONE', 'WEIGHT_OZ', 'WEIGHT_LB', 'BARRETT_RATES'])\n",
    "df13_23 = df13_23.reindex(columns=['SERVICE_CODE', 'SERVICE_PLAINTEXT', 'ZONE', 'WEIGHT_OZ', 'WEIGHT_LB', 'BARRETT_RATES'])\n",
    "df14_24 = df14_24.reindex(columns=['SERVICE_CODE', 'SERVICE_PLAINTEXT', 'ZONE', 'WEIGHT_OZ', 'WEIGHT_LB', 'BARRETT_RATES'])\n",
    "df15_25 = df15_25.reindex(columns=['SERVICE_CODE', 'SERVICE_PLAINTEXT', 'ZONE', 'WEIGHT_OZ', 'WEIGHT_LB', 'BARRETT_RATES'])\n",
    "\n",
    "df11_26 = df11_26.reindex(columns=['SERVICE_CODE', 'SERVICE_PLAINTEXT', 'ZONE', 'WEIGHT_OZ', 'WEIGHT_LB', 'BARRETT_RATES'])\n",
    "df12_27 = df12_27.reindex(columns=['SERVICE_CODE', 'SERVICE_PLAINTEXT', 'ZONE', 'WEIGHT_OZ', 'WEIGHT_LB', 'BARRETT_RATES'])\n",
    "df13_28 = df13_28.reindex(columns=['SERVICE_CODE', 'SERVICE_PLAINTEXT', 'ZONE', 'WEIGHT_OZ', 'WEIGHT_LB', 'BARRETT_RATES'])\n",
    "df14_29 = df14_29.reindex(columns=['SERVICE_CODE', 'SERVICE_PLAINTEXT', 'ZONE', 'WEIGHT_OZ', 'WEIGHT_LB', 'BARRETT_RATES'])\n",
    "df15_30 = df15_30.reindex(columns=['SERVICE_CODE', 'SERVICE_PLAINTEXT', 'ZONE', 'WEIGHT_OZ', 'WEIGHT_LB', 'BARRETT_RATES'])\n",
    "\n",
    "df11_31 = df11_31.reindex(columns=['SERVICE_CODE', 'SERVICE_PLAINTEXT', 'ZONE', 'WEIGHT_OZ', 'WEIGHT_LB', 'BARRETT_RATES'])\n",
    "df12_32 = df12_32.reindex(columns=['SERVICE_CODE', 'SERVICE_PLAINTEXT', 'ZONE', 'WEIGHT_OZ', 'WEIGHT_LB', 'BARRETT_RATES'])\n",
    "df13_33 = df13_33.reindex(columns=['SERVICE_CODE', 'SERVICE_PLAINTEXT', 'ZONE', 'WEIGHT_OZ', 'WEIGHT_LB', 'BARRETT_RATES'])\n",
    "df14_34 = df14_34.reindex(columns=['SERVICE_CODE', 'SERVICE_PLAINTEXT', 'ZONE', 'WEIGHT_OZ', 'WEIGHT_LB', 'BARRETT_RATES'])\n",
    "df15_35 = df15_35.reindex(columns=['SERVICE_CODE', 'SERVICE_PLAINTEXT', 'ZONE', 'WEIGHT_OZ', 'WEIGHT_LB', 'BARRETT_RATES'])\n",
    "\n",
    "df11_36 = df11_36.reindex(columns=['SERVICE_CODE', 'SERVICE_PLAINTEXT', 'ZONE', 'WEIGHT_OZ', 'WEIGHT_LB', 'BARRETT_RATES'])\n",
    "df12_37 = df12_37.reindex(columns=['SERVICE_CODE', 'SERVICE_PLAINTEXT', 'ZONE', 'WEIGHT_OZ', 'WEIGHT_LB', 'BARRETT_RATES'])\n",
    "df13_38 = df13_38.reindex(columns=['SERVICE_CODE', 'SERVICE_PLAINTEXT', 'ZONE', 'WEIGHT_OZ', 'WEIGHT_LB', 'BARRETT_RATES'])\n",
    "df14_39 = df14_39.reindex(columns=['SERVICE_CODE', 'SERVICE_PLAINTEXT', 'ZONE', 'WEIGHT_OZ', 'WEIGHT_LB', 'BARRETT_RATES'])\n",
    "df15_40 = df15_40.reindex(columns=['SERVICE_CODE', 'SERVICE_PLAINTEXT', 'ZONE', 'WEIGHT_OZ', 'WEIGHT_LB', 'BARRETT_RATES'])\n",
    "\n",
    "df11_41 = df11_41.reindex(columns=['SERVICE_CODE', 'SERVICE_PLAINTEXT', 'ZONE', 'WEIGHT_OZ', 'WEIGHT_LB', 'BARRETT_RATES'])\n",
    "df12_42 = df12_42.reindex(columns=['SERVICE_CODE', 'SERVICE_PLAINTEXT', 'ZONE', 'WEIGHT_OZ', 'WEIGHT_LB', 'BARRETT_RATES'])\n",
    "df13_43 = df13_43.reindex(columns=['SERVICE_CODE', 'SERVICE_PLAINTEXT', 'ZONE', 'WEIGHT_OZ', 'WEIGHT_LB', 'BARRETT_RATES'])\n",
    "df14_44 = df14_44.reindex(columns=['SERVICE_CODE', 'SERVICE_PLAINTEXT', 'ZONE', 'WEIGHT_OZ', 'WEIGHT_LB', 'BARRETT_RATES'])\n",
    "df15_45 = df15_45.reindex(columns=['SERVICE_CODE', 'SERVICE_PLAINTEXT', 'ZONE', 'WEIGHT_OZ', 'WEIGHT_LB', 'BARRETT_RATES'])\n",
    "\n",
    "df11_46 = df11_46.reindex(columns=['SERVICE_CODE', 'SERVICE_PLAINTEXT', 'ZONE', 'WEIGHT_OZ', 'WEIGHT_LB', 'BARRETT_RATES'])\n",
    "df12_47 = df12_47.reindex(columns=['SERVICE_CODE', 'SERVICE_PLAINTEXT', 'ZONE', 'WEIGHT_OZ', 'WEIGHT_LB', 'BARRETT_RATES'])\n",
    "df13_48 = df13_48.reindex(columns=['SERVICE_CODE', 'SERVICE_PLAINTEXT', 'ZONE', 'WEIGHT_OZ', 'WEIGHT_LB', 'BARRETT_RATES'])\n",
    "df14_49 = df14_49.reindex(columns=['SERVICE_CODE', 'SERVICE_PLAINTEXT', 'ZONE', 'WEIGHT_OZ', 'WEIGHT_LB', 'BARRETT_RATES'])\n",
    "df15_50 = df15_50.reindex(columns=['SERVICE_CODE', 'SERVICE_PLAINTEXT', 'ZONE', 'WEIGHT_OZ', 'WEIGHT_LB', 'BARRETT_RATES'])\n",
    "\n",
    "#df11_51 = df11_51.reindex(columns=['SERVICE_CODE', 'SERVICE_PLAINTEXT', 'ZONE', 'WEIGHT_OZ', 'WEIGHT_LB', 'BARRETT_RATES'])\n",
    "#df12_52 = df12_52.reindex(columns=['SERVICE_CODE', 'SERVICE_PLAINTEXT', 'ZONE', 'WEIGHT_OZ', 'WEIGHT_LB', 'BARRETT_RATES'])\n",
    "#df13_53 = df13_53.reindex(columns=['SERVICE_CODE', 'SERVICE_PLAINTEXT', 'ZONE', 'WEIGHT_OZ', 'WEIGHT_LB', 'BARRETT_RATES'])\n",
    "#df14_54 = df14_54.reindex(columns=['SERVICE_CODE', 'SERVICE_PLAINTEXT', 'ZONE', 'WEIGHT_OZ', 'WEIGHT_LB', 'BARRETT_RATES'])\n",
    "#df15_55 = df15_55.reindex(columns=['SERVICE_CODE', 'SERVICE_PLAINTEXT', 'ZONE', 'WEIGHT_OZ', 'WEIGHT_LB', 'BARRETT_RATES'])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "tbl_publishedRates = pd.concat([df1, df2, df3, df4, df5, df6, df7, df8, df9, df10, df16, df17, df20, df21, df22, df23, df24, df25, df18, df19,\n",
    "                               df11_1, df12_2, df13_3, df14_4, df15_5, df11_6, df12_7, df13_8, df14_9, #df15_10,\n",
    "                               df11_11, df12_12, df13_13, df14_14, df15_15, df11_16, df12_17, df13_18, df14_19, df15_20, \n",
    "                               df11_21, df12_22, df13_23, df14_24, df15_25, df11_26, df12_27, df13_28, df14_29, df15_30, \n",
    "                               df11_31, df12_32, df13_33, df14_34, df15_35, df11_36, df12_37, df13_38, df14_39, df15_40, \n",
    "                               df11_41, df12_42, df13_43, df14_44, df15_45, df11_46, df12_47, df13_48, df14_49, df15_50], axis=0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1064,
   "id": "8137f7e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the version column\n",
    "tbl_publishedRates['VERSION'] ='2023'\n",
    "tbl_publishedRates['START_DATE'] ='2023/01/01'\n",
    "tbl_publishedRates['END_DATE'] ='2023/12/31'\n",
    "\n",
    "# Unify the Zone values\n",
    "def get_portion(value):\n",
    "    if isinstance(value, int):\n",
    "        return str(value)  # convert integer to string\n",
    "    if 'Zone' in value:\n",
    "        return value[5:]\n",
    "    elif 'US' in value:\n",
    "        return value[3:]\n",
    "    \n",
    "tbl_publishedRates['ZONE_new'] = tbl_publishedRates['ZONE'].apply(get_portion)\n",
    "\n",
    "# Dictionary to define the replacements\n",
    "replacements = {\n",
    "    '20': '', '30': '', '102': '2', '103': '3', '104': '4', '105': '5', '106': '6',\n",
    "    '107': '7', '108': '8', '202': '2', '203': '3', '204': '4', '205': '5', '206': '6',\n",
    "    '207': '7', '208': '8', '209': '9', '302': '2', '303': '3', '304': '4', '305': '5', '306': '6',\n",
    "    '307': '7', '308': '8', '124': '9', '125': '10', '126': '11', '224': '9',\n",
    "    '225': '10', '226': '11', '44': '9', '45': '10', '46': '11', '03': '3', '04': '4',\n",
    "    '05': '5', '06': '6', '07': '7', '08': '8', '09': '9'\n",
    "}\n",
    "\n",
    "# Function to replace based on the replacements dictionary\n",
    "def apply_replacements(zone):\n",
    "    if zone is None:\n",
    "        return zone  # Or return a default value like '' (empty string) if that's appropriate for your use case\n",
    "    for old, new in replacements.items():\n",
    "        zone = zone.replace(old, new)\n",
    "    return zone\n",
    "\n",
    "\n",
    "# Apply the replacement function\n",
    "tbl_publishedRates['ZONE_new'] = tbl_publishedRates['ZONE_new'].apply(apply_replacements)\n",
    "\n",
    "# Update 'ZONE' with 'ZONE_new' values where 'ZONE_new' is not None or NaN; otherwise, keep 'ZONE' values\n",
    "tbl_publishedRates['ZONE'] = tbl_publishedRates['ZONE_new'].combine_first(tbl_publishedRates['ZONE'])\n",
    "\n",
    "# Drop the 'ZONE_new' column as it's no longer needed\n",
    "tbl_publishedRates = tbl_publishedRates.drop('ZONE_new', axis=1)\n",
    "\n",
    "\n",
    "#adjust the index and finish creating this table.\n",
    "tbl_publishedRates.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# tbl_publishedRates.head()\n",
    "# tbl_publishedRates.to_csv('test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1065,
   "id": "6c7aa5d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<p style=\"font-family: 'Arial'; font-size: 20px; color: red; background-color: yellow;\">\n",
       "    Edit Location 2 of 8 (optional)\n",
       "</p>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "styled_html = HTML(\"\"\"\n",
    "<p style=\"font-family: 'Arial'; font-size: 20px; color: red; background-color: yellow;\">\n",
    "    Edit Location 2 of 8 (optional)\n",
    "</p>\n",
    "\"\"\")\n",
    "\n",
    "display(styled_html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1066,
   "id": "54818637",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Run Code 1 if need Auctane, Run Code 2 if need CPP. Default Code 1 Auctane. \n",
    "# Code 1\n",
    "# Keep rows where 'ServiceCode' is not in ['cpp1', 'cpp2', 'cpp3']\n",
    "tbl_publishedRates = tbl_publishedRates[~tbl_publishedRates['SERVICE_CODE'].isin(['USPSAG_CPP', 'USPSAP_CPP', 'USPSAG_CPPCI', 'USPSAP_CPPCI'])] # Edit, CPP\n",
    "\n",
    "\n",
    "# 2. Replacing values\n",
    "# Define a mapping of old values to new values\n",
    "replacement_dict = {\n",
    "    'USPSAG_AUCT': 'USPSAG',  # Edit, CPP\n",
    "    'USPSAP_AUCT': 'USPSAP',  # Edit, CPP\n",
    "    'USPSAG_AUCTCI': 'USPSAG_CI',  # Edit, CPP\n",
    "    'USPSAP_AUCTCI': 'USPSAP_CI' # Edit, CPP\n",
    "    \n",
    "}\n",
    "\n",
    "# Replace the values in 'ServiceCode' column\n",
    "tbl_publishedRates['SERVICE_CODE'] = tbl_publishedRates['SERVICE_CODE'].replace(replacement_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1067,
   "id": "962c032a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code 2\n",
    "#tbl_publishedRates = tbl_publishedRates[~tbl_publishedRates['SERVICE_CODE'].isin(['USPSAG_AUCT', 'USPSAP_AUCT', 'USPSAG_AUCTCI', 'USPSAP_AUCTCI'])]\n",
    "\n",
    "#replacement_dict = {\n",
    "#    'USPSAG_CPP': 'USPSAG',\n",
    "#    'USPSAP_CPP': 'USPSAP',\n",
    "#    'USPSAG_CPPCI': 'USPSAG_CI',\n",
    "#    'USPSAP_CPPCI': 'USPSAP_CI'\n",
    "    \n",
    "#}\n",
    "\n",
    "#tbl_publishedRates['SERVICE_CODE'] = tbl_publishedRates['SERVICE_CODE'].replace(replacement_dict)\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07e8001c",
   "metadata": {},
   "source": [
    "# Step 1 - b. Create Freight Rate Card (tbl_BarrettRates)\n",
    "*Add ups BarrettRates to tbl_publishedRates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1068,
   "id": "2e6725e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#The result is BarrettRate when margin = 0.\n",
    "#For UPS client freight & accessorial discounts, write \"YES\"/\"NO\" for rounding/not rounding to 2 decimal places.\n",
    "Round_Discount = \"NO\"\n",
    "\n",
    "#Set UPS margin for the client\n",
    "Margin_on_Min_GRND_BR = 0\n",
    "Margin_on_Min_AIR_BR = 0\n",
    "\n",
    "Margin_on_UPS_Parcel_Freight_GRND_BR = 0\n",
    "Margin_on_UPS_Parcel_Freight_AIR_BR = 0\n",
    "\n",
    "Margin_on_Value_Add_GRND_BR = 0\n",
    "Margin_on_Value_Add_AIR_BR = 0\n",
    "\n",
    "\n",
    "# UPSAccessorial_2023 has 2 tabs, UPS_Rates and UPS_Accessorial. UPS_Rates is used to create Barrett Rates\n",
    "xls = pd.ExcelFile('UPSAccessorial_2023.xlsx')\n",
    "UPS_Rates = pd.read_excel(xls, 'UPS_Rates')\n",
    "\n",
    "# This is the value of the left side of \"Rates\" tab from \"Customer Rate Contract Cards 2023\" doc\n",
    "# UPS_Rates\n",
    "\n",
    "\n",
    "#add 2 columns to this table with min and max weight\n",
    "def f(col):\n",
    "    if col['WEIGHT_BREAK'] == '1-5lb':\n",
    "        return 5\n",
    "    elif col['WEIGHT_BREAK'] == '6-10lb':\n",
    "        return 10\n",
    "    elif col['WEIGHT_BREAK'] == '11-20lb':\n",
    "        return 20\n",
    "    elif col['WEIGHT_BREAK'] == '21-30lb':\n",
    "        return 30\n",
    "    elif (col['WEIGHT_BREAK'] == '31+lb'):\n",
    "        return 999\n",
    "    elif (col['SERVICE_LEVEL'] == 'SurePost - Less than 1lb') & (col['WEIGHT_BREAK'] == '1-16oz'):\n",
    "        return 16\n",
    "    elif (col['SERVICE_LEVEL'] == 'SurePost - Greater than 1lb') & (col['WEIGHT_BREAK'] == '1-3lb'):\n",
    "        return 3\n",
    "    elif (col['SERVICE_LEVEL'] == 'SurePost - Greater than 1lb') & (col['WEIGHT_BREAK'] == '4-6lb'):\n",
    "        return 6\n",
    "    elif (col['SERVICE_LEVEL'] == 'SurePost - Greater than 1lb') & (col['WEIGHT_BREAK'] == '7-8lb'):\n",
    "        return 8\n",
    "    elif (col['SERVICE_LEVEL'] == 'SurePost - Greater than 1lb') & (col['WEIGHT_BREAK'] == '9lb'):\n",
    "        return 9  \n",
    "\n",
    "def g(col):\n",
    "    if col['WEIGHT_BREAK'] == '1-5lb':\n",
    "        return 1\n",
    "    elif col['WEIGHT_BREAK'] == '6-10lb':\n",
    "        return 6\n",
    "    elif col['WEIGHT_BREAK'] == '11-20lb':\n",
    "        return 11\n",
    "    elif col['WEIGHT_BREAK'] == '21-30lb':\n",
    "        return 21\n",
    "    elif col['WEIGHT_BREAK'] == '31+lb':\n",
    "        return 31\n",
    "    elif (col['SERVICE_LEVEL'] == 'SurePost - Less than 1lb') & (col['WEIGHT_BREAK'] == '1-16oz'):\n",
    "        return 1\n",
    "    elif (col['SERVICE_LEVEL'] == 'SurePost - Greater than 1lb') & (col['WEIGHT_BREAK'] == '1-3lb'):\n",
    "        return 1\n",
    "    elif (col['SERVICE_LEVEL'] == 'SurePost - Greater than 1lb') & (col['WEIGHT_BREAK'] == '4-6lb'):\n",
    "        return 4\n",
    "    elif (col['SERVICE_LEVEL'] == 'SurePost - Greater than 1lb') & (col['WEIGHT_BREAK'] == '7-8lb'):\n",
    "        return 7\n",
    "    elif (col['SERVICE_LEVEL'] == 'SurePost - Greater than 1lb') & (col['WEIGHT_BREAK'] == '9lb'):\n",
    "        return 9 \n",
    "\n",
    "UPS_Rates['WEIGHT_LOWER'] = UPS_Rates.apply(g, axis=1)\n",
    "UPS_Rates['WEIGHT_UPPER'] = UPS_Rates.apply(f, axis=1)\n",
    "\n",
    "\n",
    "# add client margin mins column\n",
    "def h(col):   \n",
    "    if col['SERVICE_LEVEL'] == 'Next Day Air Early AM':\n",
    "        return col['MIN_WITH_REBATE']\n",
    "    \n",
    "    elif (col['SERVICE_LEVEL'] == 'Next Day Air') | (col['SERVICE_LEVEL'] == 'Next Day Air Saver') | (col['SERVICE_LEVEL'] == '2nd A.M. Day Air') | (col['SERVICE_LEVEL'] == '2nd Day Air') | (col['SERVICE_LEVEL'] == '3 Day Select'): \n",
    "        try: \n",
    "            return min(col['MIN_WITH_REBATE'] + 4, col['MIN_WITH_REBATE'] / (1-Margin_on_Min_AIR_BR))\n",
    "        except ValueError:\n",
    "            return 0\n",
    "\n",
    "    elif (col['SERVICE_LEVEL'] == 'Ground Commercial') | (col['SERVICE_LEVEL'] == 'Ground Residential'): \n",
    "        try: \n",
    "            return min(10.1, col['MIN_WITH_REBATE'] / (1-Margin_on_Min_GRND_BR))\n",
    "        except ValueError:\n",
    "            return 0\n",
    "\n",
    "    elif (col['SERVICE_LEVEL'] == 'SurePost - Greater than 1lb'): \n",
    "        try: \n",
    "            return min(8.14, col['MIN_WITH_REBATE'] / (1-Margin_on_Min_GRND_BR))\n",
    "        except ValueError:\n",
    "            return 0\n",
    "    \n",
    "    elif (col['SERVICE_LEVEL'] == 'SurePost - Less than 1lb'): \n",
    "        try: \n",
    "            return min(8.14, col['MIN_WITH_REBATE'] / (1-Margin_on_Min_GRND_BR))\n",
    "        except ValueError:\n",
    "            return 0\n",
    "  \n",
    "    elif col['SERVICE_LEVEL'] == 'Ground CWT': \n",
    "        try: \n",
    "            return min(80, col['MIN_WITH_REBATE'] / (1-Margin_on_Min_GRND_BR))\n",
    "        except ValueError:\n",
    "            return 0\n",
    "  \n",
    "    elif (col['SERVICE_LEVEL'] == 'Standard to Canada') | (col['SERVICE_LEVEL'] == 'Standard to Mexico'):\n",
    "        try: \n",
    "            return min(col['MIN_WITH_REBATE'] + 4, col['MIN_WITH_REBATE'] / (1-Margin_on_Min_GRND_BR))\n",
    "        except ValueError:\n",
    "            return 0\n",
    "\n",
    "UPS_Rates['CLIENT_MARGIN_MINS'] = UPS_Rates.apply(h, axis=1)\n",
    "\n",
    "# add ADD_MARGIN_(COL_M/DECIMAL) column\n",
    "UPS_Rates['DISCOUNT_RECIPROCAL_(BDC_PAY_ON_THE_DOLLAR)'] = 1 - UPS_Rates['NET_CONTRACT_DISC']\n",
    "UPS_Rates.loc[UPS_Rates['SERVICE_LEVEL'] == 'Next Day Air Early AM', 'DISCOUNT_RECIPROCAL_(BDC_PAY_ON_THE_DOLLAR)'] = None\n",
    "\n",
    "def i(col):   \n",
    "    if (col['SERVICE_LEVEL'] == 'SurePost - Greater than 1lb') | (col['WEIGHT_BREAK'] == '9lb'):\n",
    "        return col['DISCOUNT_RECIPROCAL_(BDC_PAY_ON_THE_DOLLAR)'] / (1-Margin_on_UPS_Parcel_Freight_AIR_BR)\n",
    "    elif col['SERVICE_LEVEL'] == 'Next Day Air Early AM':\n",
    "        return\n",
    "    elif (col['SERVICE_LEVEL'] == 'Ground Commercial') | (col['SERVICE_LEVEL'] == 'Ground Residential') | (col['SERVICE_LEVEL'] == 'SurePost - Less than 1lb') | (col['SERVICE_LEVEL'] == 'SurePost - Greater than 1lb'): \n",
    "        return col['DISCOUNT_RECIPROCAL_(BDC_PAY_ON_THE_DOLLAR)'] / (1-Margin_on_UPS_Parcel_Freight_GRND_BR)\n",
    "    else:\n",
    "        return col['DISCOUNT_RECIPROCAL_(BDC_PAY_ON_THE_DOLLAR)'] / (1-Margin_on_UPS_Parcel_Freight_AIR_BR)\n",
    "\n",
    "UPS_Rates['ADD_MARGIN_(COL_M/DECIMAL)'] = UPS_Rates.apply(i, axis=1) \n",
    "\n",
    "# add DISCOUNT_TO_CLIENT(1-COLN) column \n",
    "def j(col):   \n",
    "    if (col['SERVICE_LEVEL'] == 'Standard to Canada') | (col['SERVICE_LEVEL'] == 'Standard to Mexico'):\n",
    "        return max(0, 1 - col['ADD_MARGIN_(COL_M/DECIMAL)'])\n",
    "    elif col['SERVICE_LEVEL'] == 'Next Day Air Early AM':\n",
    "        return\n",
    "    else:\n",
    "        return 1 - col['ADD_MARGIN_(COL_M/DECIMAL)']\n",
    "\n",
    "UPS_Rates['DISCOUNT_TO_CLIENT(1-COLN)'] = UPS_Rates.apply(j, axis=1)\n",
    "\n",
    "\n",
    "# This completes the left side of \"Rates\" tab from \"Customer Rate Contract Cards 2023\" doc\n",
    "# UPS_Rates\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1069,
   "id": "0e531eba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sliu\\AppData\\Local\\Temp\\ipykernel_18976\\1387918324.py:14: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  tbl_barrettRates_1['BARRETT_RATES_1'] = tbl_barrettRates_1['PUBLISHED_RATES']*(1-DISCOUNT_TO_CLIENT_1)\n",
      "C:\\Users\\sliu\\AppData\\Local\\Temp\\ipykernel_18976\\1387918324.py:15: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  tbl_barrettRates_1['BARRETT_RATES'] = round(tbl_barrettRates_1['BARRETT_RATES_1'].apply(lambda x: max(x, CLIENT_MARGIN_MINS_1)),2)\n",
      "C:\\Users\\sliu\\AppData\\Local\\Temp\\ipykernel_18976\\1387918324.py:27: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  tbl_barrettRates_2['BARRETT_RATES_1'] = tbl_barrettRates_2['PUBLISHED_RATES']*(1-DISCOUNT_TO_CLIENT_2)\n",
      "C:\\Users\\sliu\\AppData\\Local\\Temp\\ipykernel_18976\\1387918324.py:28: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  tbl_barrettRates_2['BARRETT_RATES'] = round(tbl_barrettRates_2['BARRETT_RATES_1'].apply(lambda x: max(x, CLIENT_MARGIN_MINS_2)),2)\n",
      "C:\\Users\\sliu\\AppData\\Local\\Temp\\ipykernel_18976\\1387918324.py:40: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  tbl_barrettRates_3['BARRETT_RATES_1'] = tbl_barrettRates_3['PUBLISHED_RATES']*(1-DISCOUNT_TO_CLIENT_3)\n",
      "C:\\Users\\sliu\\AppData\\Local\\Temp\\ipykernel_18976\\1387918324.py:41: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  tbl_barrettRates_3['BARRETT_RATES'] = round(tbl_barrettRates_3['BARRETT_RATES_1'].apply(lambda x: max(x, CLIENT_MARGIN_MINS_3)),2)\n",
      "C:\\Users\\sliu\\AppData\\Local\\Temp\\ipykernel_18976\\1387918324.py:53: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  tbl_barrettRates_4['BARRETT_RATES_1'] = tbl_barrettRates_4['PUBLISHED_RATES']*(1-DISCOUNT_TO_CLIENT_4)\n",
      "C:\\Users\\sliu\\AppData\\Local\\Temp\\ipykernel_18976\\1387918324.py:54: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  tbl_barrettRates_4['BARRETT_RATES'] = round(tbl_barrettRates_4['BARRETT_RATES_1'].apply(lambda x: max(x, CLIENT_MARGIN_MINS_4)),2)\n",
      "C:\\Users\\sliu\\AppData\\Local\\Temp\\ipykernel_18976\\1387918324.py:66: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  tbl_barrettRates_5['BARRETT_RATES_1'] = tbl_barrettRates_5['PUBLISHED_RATES']*(1-DISCOUNT_TO_CLIENT_5)\n",
      "C:\\Users\\sliu\\AppData\\Local\\Temp\\ipykernel_18976\\1387918324.py:67: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  tbl_barrettRates_5['BARRETT_RATES'] = round(tbl_barrettRates_5['BARRETT_RATES_1'].apply(lambda x: max(x, CLIENT_MARGIN_MINS_5)),2)\n",
      "C:\\Users\\sliu\\AppData\\Local\\Temp\\ipykernel_18976\\1387918324.py:80: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  tbl_barrettRates_6['BARRETT_RATES_1'] = tbl_barrettRates_6['PUBLISHED_RATES']*(1-DISCOUNT_TO_CLIENT_6)\n",
      "C:\\Users\\sliu\\AppData\\Local\\Temp\\ipykernel_18976\\1387918324.py:81: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  tbl_barrettRates_6['BARRETT_RATES'] = round(tbl_barrettRates_6['BARRETT_RATES_1'].apply(lambda x: max(x, CLIENT_MARGIN_MINS_6)),2)\n",
      "C:\\Users\\sliu\\AppData\\Local\\Temp\\ipykernel_18976\\1387918324.py:93: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  tbl_barrettRates_7['BARRETT_RATES_1'] = tbl_barrettRates_7['PUBLISHED_RATES']*(1-DISCOUNT_TO_CLIENT_7)\n",
      "C:\\Users\\sliu\\AppData\\Local\\Temp\\ipykernel_18976\\1387918324.py:94: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  tbl_barrettRates_7['BARRETT_RATES'] = round(tbl_barrettRates_7['BARRETT_RATES_1'].apply(lambda x: max(x, CLIENT_MARGIN_MINS_7)),2)\n",
      "C:\\Users\\sliu\\AppData\\Local\\Temp\\ipykernel_18976\\1387918324.py:106: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  tbl_barrettRates_8['BARRETT_RATES_1'] = tbl_barrettRates_8['PUBLISHED_RATES']*(1-DISCOUNT_TO_CLIENT_8)\n",
      "C:\\Users\\sliu\\AppData\\Local\\Temp\\ipykernel_18976\\1387918324.py:107: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  tbl_barrettRates_8['BARRETT_RATES'] = round(tbl_barrettRates_8['BARRETT_RATES_1'].apply(lambda x: max(x, CLIENT_MARGIN_MINS_8)),2)\n",
      "C:\\Users\\sliu\\AppData\\Local\\Temp\\ipykernel_18976\\1387918324.py:119: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  tbl_barrettRates_9['BARRETT_RATES_1'] = tbl_barrettRates_9['PUBLISHED_RATES']*(1-DISCOUNT_TO_CLIENT_9)\n",
      "C:\\Users\\sliu\\AppData\\Local\\Temp\\ipykernel_18976\\1387918324.py:120: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  tbl_barrettRates_9['BARRETT_RATES'] = round(tbl_barrettRates_9['BARRETT_RATES_1'].apply(lambda x: max(x, CLIENT_MARGIN_MINS_9)),2)\n",
      "C:\\Users\\sliu\\AppData\\Local\\Temp\\ipykernel_18976\\1387918324.py:132: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  tbl_barrettRates_10['BARRETT_RATES_1'] = tbl_barrettRates_10['PUBLISHED_RATES']*(1-DISCOUNT_TO_CLIENT_10)\n",
      "C:\\Users\\sliu\\AppData\\Local\\Temp\\ipykernel_18976\\1387918324.py:133: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  tbl_barrettRates_10['BARRETT_RATES'] = round(tbl_barrettRates_10['BARRETT_RATES_1'].apply(lambda x: max(x, CLIENT_MARGIN_MINS_10)),2)\n",
      "C:\\Users\\sliu\\AppData\\Local\\Temp\\ipykernel_18976\\1387918324.py:146: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  tbl_barrettRates_11['BARRETT_RATES_1'] = tbl_barrettRates_11['PUBLISHED_RATES']*(1-DISCOUNT_TO_CLIENT_11)\n",
      "C:\\Users\\sliu\\AppData\\Local\\Temp\\ipykernel_18976\\1387918324.py:147: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  tbl_barrettRates_11['BARRETT_RATES'] = round(tbl_barrettRates_11['BARRETT_RATES_1'].apply(lambda x: max(x, CLIENT_MARGIN_MINS_11)),2)\n",
      "C:\\Users\\sliu\\AppData\\Local\\Temp\\ipykernel_18976\\1387918324.py:159: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  tbl_barrettRates_12['BARRETT_RATES_1'] = tbl_barrettRates_12['PUBLISHED_RATES']*(1-DISCOUNT_TO_CLIENT_12)\n",
      "C:\\Users\\sliu\\AppData\\Local\\Temp\\ipykernel_18976\\1387918324.py:160: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  tbl_barrettRates_12['BARRETT_RATES'] = round(tbl_barrettRates_12['BARRETT_RATES_1'].apply(lambda x: max(x, CLIENT_MARGIN_MINS_12)),2)\n",
      "C:\\Users\\sliu\\AppData\\Local\\Temp\\ipykernel_18976\\1387918324.py:172: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  tbl_barrettRates_13['BARRETT_RATES_1'] = tbl_barrettRates_13['PUBLISHED_RATES']*(1-DISCOUNT_TO_CLIENT_13)\n",
      "C:\\Users\\sliu\\AppData\\Local\\Temp\\ipykernel_18976\\1387918324.py:173: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  tbl_barrettRates_13['BARRETT_RATES'] = round(tbl_barrettRates_13['BARRETT_RATES_1'].apply(lambda x: max(x, CLIENT_MARGIN_MINS_13)),2)\n",
      "C:\\Users\\sliu\\AppData\\Local\\Temp\\ipykernel_18976\\1387918324.py:185: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  tbl_barrettRates_14['BARRETT_RATES_1'] = tbl_barrettRates_14['PUBLISHED_RATES']*(1-DISCOUNT_TO_CLIENT_14)\n",
      "C:\\Users\\sliu\\AppData\\Local\\Temp\\ipykernel_18976\\1387918324.py:186: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  tbl_barrettRates_14['BARRETT_RATES'] = round(tbl_barrettRates_14['BARRETT_RATES_1'].apply(lambda x: max(x, CLIENT_MARGIN_MINS_14)),2)\n",
      "C:\\Users\\sliu\\AppData\\Local\\Temp\\ipykernel_18976\\1387918324.py:193: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  tbl_barrettRates_15['BARRETT_RATES'] = tbl_barrettRates_15['PUBLISHED_RATES']\n",
      "C:\\Users\\sliu\\AppData\\Local\\Temp\\ipykernel_18976\\1387918324.py:204: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  tbl_barrettRates_16['BARRETT_RATES_1'] = tbl_barrettRates_16['PUBLISHED_RATES']*(1-DISCOUNT_TO_CLIENT_16)\n",
      "C:\\Users\\sliu\\AppData\\Local\\Temp\\ipykernel_18976\\1387918324.py:205: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  tbl_barrettRates_16['BARRETT_RATES'] = round(tbl_barrettRates_16['BARRETT_RATES_1'].apply(lambda x: max(x, CLIENT_MARGIN_MINS_16)),2)\n",
      "C:\\Users\\sliu\\AppData\\Local\\Temp\\ipykernel_18976\\1387918324.py:213: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  tbl_barrettRates_17['BARRETT_RATES_1'] = tbl_barrettRates_17['PUBLISHED_RATES']*(1-DISCOUNT_TO_CLIENT_17)\n",
      "C:\\Users\\sliu\\AppData\\Local\\Temp\\ipykernel_18976\\1387918324.py:214: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  tbl_barrettRates_17['BARRETT_RATES'] = round(tbl_barrettRates_17['BARRETT_RATES_1'].apply(lambda x: max(x, CLIENT_MARGIN_MINS_17)),2)\n",
      "C:\\Users\\sliu\\AppData\\Local\\Temp\\ipykernel_18976\\1387918324.py:222: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  tbl_barrettRates_18['BARRETT_RATES_1'] = tbl_barrettRates_18['PUBLISHED_RATES']*(1-DISCOUNT_TO_CLIENT_18)\n",
      "C:\\Users\\sliu\\AppData\\Local\\Temp\\ipykernel_18976\\1387918324.py:223: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  tbl_barrettRates_18['BARRETT_RATES'] = round(tbl_barrettRates_18['BARRETT_RATES_1'].apply(lambda x: max(x, CLIENT_MARGIN_MINS_18)),2)\n",
      "C:\\Users\\sliu\\AppData\\Local\\Temp\\ipykernel_18976\\1387918324.py:235: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  tbl_barrettRates_19['BARRETT_RATES_1'] = tbl_barrettRates_19['PUBLISHED_RATES']*(1-DISCOUNT_TO_CLIENT_19)\n",
      "C:\\Users\\sliu\\AppData\\Local\\Temp\\ipykernel_18976\\1387918324.py:236: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  tbl_barrettRates_19['BARRETT_RATES'] = round(tbl_barrettRates_19['BARRETT_RATES_1'].apply(lambda x: max(x, CLIENT_MARGIN_MINS_19)),2)\n",
      "C:\\Users\\sliu\\AppData\\Local\\Temp\\ipykernel_18976\\1387918324.py:248: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  tbl_barrettRates_20['BARRETT_RATES_1'] = tbl_barrettRates_20['PUBLISHED_RATES']*(1-DISCOUNT_TO_CLIENT_20)\n",
      "C:\\Users\\sliu\\AppData\\Local\\Temp\\ipykernel_18976\\1387918324.py:249: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  tbl_barrettRates_20['BARRETT_RATES'] = round(tbl_barrettRates_20['BARRETT_RATES_1'].apply(lambda x: max(x, CLIENT_MARGIN_MINS_20)),2)\n",
      "C:\\Users\\sliu\\AppData\\Local\\Temp\\ipykernel_18976\\1387918324.py:261: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  tbl_barrettRates_21['BARRETT_RATES_1'] = tbl_barrettRates_21['PUBLISHED_RATES']*(1-DISCOUNT_TO_CLIENT_21)\n",
      "C:\\Users\\sliu\\AppData\\Local\\Temp\\ipykernel_18976\\1387918324.py:262: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  tbl_barrettRates_21['BARRETT_RATES'] = round(tbl_barrettRates_21['BARRETT_RATES_1'].apply(lambda x: max(x, CLIENT_MARGIN_MINS_21)),2)\n"
     ]
    }
   ],
   "source": [
    "#GRND ['WEIGHT_BREAK'] == '1-5lb'\n",
    "# slice the published rates\n",
    "tbl_barrettRates_1 = tbl_publishedRates[(tbl_publishedRates['SERVICE_CODE'] == 'GRND') & (tbl_publishedRates['WEIGHT_LB'] <= 5)]\n",
    "\n",
    "# get DISCOUNT_TO_CLIENT(1-COLN) and CLIENT_MARGIN_MINS\n",
    "DISCOUNT_TO_CLIENT_1 = UPS_Rates.loc[(UPS_Rates['SERVICE_CODE'] == 'GRND') & (UPS_Rates['WEIGHT_BREAK'] == '1-5lb'), 'DISCOUNT_TO_CLIENT(1-COLN)'].values[0]\n",
    "if Round_Discount == 'YES':\n",
    "    DISCOUNT_TO_CLIENT_1 = round(DISCOUNT_TO_CLIENT_1,2)\n",
    "else:\n",
    "    DISCOUNT_TO_CLIENT_1 = DISCOUNT_TO_CLIENT_1\n",
    "CLIENT_MARGIN_MINS_1 = UPS_Rates.loc[(UPS_Rates['SERVICE_CODE'] == 'GRND') & (UPS_Rates['WEIGHT_BREAK'] == '1-5lb'), 'CLIENT_MARGIN_MINS'].values[0]\n",
    "\n",
    "# use max(published rates * (1-round(DISCOUNT_TO_CLIENT)), CLIENT_MARGIN_MINS) to get the client rate\n",
    "tbl_barrettRates_1['BARRETT_RATES_1'] = tbl_barrettRates_1['PUBLISHED_RATES']*(1-DISCOUNT_TO_CLIENT_1)\n",
    "tbl_barrettRates_1['BARRETT_RATES'] = round(tbl_barrettRates_1['BARRETT_RATES_1'].apply(lambda x: max(x, CLIENT_MARGIN_MINS_1)),2)\n",
    "tbl_barrettRates_1 = tbl_barrettRates_1.drop('BARRETT_RATES_1', axis=1)\n",
    "\n",
    "\n",
    "#GRND ['WEIGHT_BREAK'] == '6-10lb'\n",
    "tbl_barrettRates_2 = tbl_publishedRates[(tbl_publishedRates['SERVICE_CODE'] == 'GRND') & (tbl_publishedRates['WEIGHT_LB'] <= 10) & (tbl_publishedRates['WEIGHT_LB'] >= 6)]\n",
    "DISCOUNT_TO_CLIENT_2 = UPS_Rates.loc[(UPS_Rates['SERVICE_CODE'] == 'GRND') & (UPS_Rates['WEIGHT_BREAK'] == '6-10lb'), 'DISCOUNT_TO_CLIENT(1-COLN)'].values[0]\n",
    "if Round_Discount == 'YES':\n",
    "    DISCOUNT_TO_CLIENT_2 = round(DISCOUNT_TO_CLIENT_2,2)\n",
    "else:\n",
    "    DISCOUNT_TO_CLIENT_2 = DISCOUNT_TO_CLIENT_2\n",
    "CLIENT_MARGIN_MINS_2 = UPS_Rates.loc[(UPS_Rates['SERVICE_CODE'] == 'GRND') & (UPS_Rates['WEIGHT_BREAK'] == '6-10lb'), 'CLIENT_MARGIN_MINS'].values[0]\n",
    "tbl_barrettRates_2['BARRETT_RATES_1'] = tbl_barrettRates_2['PUBLISHED_RATES']*(1-DISCOUNT_TO_CLIENT_2)\n",
    "tbl_barrettRates_2['BARRETT_RATES'] = round(tbl_barrettRates_2['BARRETT_RATES_1'].apply(lambda x: max(x, CLIENT_MARGIN_MINS_2)),2)\n",
    "tbl_barrettRates_2 = tbl_barrettRates_2.drop('BARRETT_RATES_1', axis=1)\n",
    "\n",
    "\n",
    "#GRND ['WEIGHT_BREAK'] == '11-20lb'\n",
    "tbl_barrettRates_3 = tbl_publishedRates[(tbl_publishedRates['SERVICE_CODE'] == 'GRND') & (tbl_publishedRates['WEIGHT_LB'] <= 20) & (tbl_publishedRates['WEIGHT_LB'] >= 11)]\n",
    "DISCOUNT_TO_CLIENT_3 = UPS_Rates.loc[(UPS_Rates['SERVICE_CODE'] == 'GRND') & (UPS_Rates['WEIGHT_BREAK'] == '11-20lb'), 'DISCOUNT_TO_CLIENT(1-COLN)'].values[0]\n",
    "if Round_Discount == 'YES':\n",
    "    DISCOUNT_TO_CLIENT_3 = round(DISCOUNT_TO_CLIENT_3,2)\n",
    "else:\n",
    "    DISCOUNT_TO_CLIENT_3 = DISCOUNT_TO_CLIENT_3\n",
    "CLIENT_MARGIN_MINS_3 = UPS_Rates.loc[(UPS_Rates['SERVICE_CODE'] == 'GRND') & (UPS_Rates['WEIGHT_BREAK'] == '11-20lb'), 'CLIENT_MARGIN_MINS'].values[0]\n",
    "tbl_barrettRates_3['BARRETT_RATES_1'] = tbl_barrettRates_3['PUBLISHED_RATES']*(1-DISCOUNT_TO_CLIENT_3)\n",
    "tbl_barrettRates_3['BARRETT_RATES'] = round(tbl_barrettRates_3['BARRETT_RATES_1'].apply(lambda x: max(x, CLIENT_MARGIN_MINS_3)),2)\n",
    "tbl_barrettRates_3 = tbl_barrettRates_3.drop('BARRETT_RATES_1', axis=1)\n",
    "\n",
    "\n",
    "#GRND ['WEIGHT_BREAK'] == '21-30lb'\n",
    "tbl_barrettRates_4 = tbl_publishedRates[(tbl_publishedRates['SERVICE_CODE'] == 'GRND') & (tbl_publishedRates['WEIGHT_LB'] <= 30) & (tbl_publishedRates['WEIGHT_LB'] >= 21)]\n",
    "DISCOUNT_TO_CLIENT_4 = UPS_Rates.loc[(UPS_Rates['SERVICE_CODE'] == 'GRND') & (UPS_Rates['WEIGHT_BREAK'] == '21-30lb'), 'DISCOUNT_TO_CLIENT(1-COLN)'].values[0]\n",
    "if Round_Discount == 'YES':\n",
    "    DISCOUNT_TO_CLIENT_4 = round(DISCOUNT_TO_CLIENT_4,2)\n",
    "else:\n",
    "    DISCOUNT_TO_CLIENT_4 = DISCOUNT_TO_CLIENT_4\n",
    "CLIENT_MARGIN_MINS_4 = UPS_Rates.loc[(UPS_Rates['SERVICE_CODE'] == 'GRND') & (UPS_Rates['WEIGHT_BREAK'] == '21-30lb'), 'CLIENT_MARGIN_MINS'].values[0]\n",
    "tbl_barrettRates_4['BARRETT_RATES_1'] = tbl_barrettRates_4['PUBLISHED_RATES']*(1-DISCOUNT_TO_CLIENT_4)\n",
    "tbl_barrettRates_4['BARRETT_RATES'] = round(tbl_barrettRates_4['BARRETT_RATES_1'].apply(lambda x: max(x, CLIENT_MARGIN_MINS_4)),2)\n",
    "tbl_barrettRates_4 = tbl_barrettRates_4.drop('BARRETT_RATES_1', axis=1)\n",
    "\n",
    "\n",
    "#GRND ['WEIGHT_BREAK'] == '31+lb'\n",
    "tbl_barrettRates_5 = tbl_publishedRates[(tbl_publishedRates['SERVICE_CODE'] == 'GRND') & (tbl_publishedRates['WEIGHT_LB'] >= 31)]\n",
    "DISCOUNT_TO_CLIENT_5 = UPS_Rates.loc[(UPS_Rates['SERVICE_CODE'] == 'GRND') & (UPS_Rates['WEIGHT_BREAK'] == '31+lb'), 'DISCOUNT_TO_CLIENT(1-COLN)'].values[0]\n",
    "if Round_Discount == 'YES':\n",
    "    DISCOUNT_TO_CLIENT_5 = round(DISCOUNT_TO_CLIENT_5,2)\n",
    "else:\n",
    "    DISCOUNT_TO_CLIENT_5 = DISCOUNT_TO_CLIENT_5\n",
    "CLIENT_MARGIN_MINS_5 = UPS_Rates.loc[(UPS_Rates['SERVICE_CODE'] == 'GRND') & (UPS_Rates['WEIGHT_BREAK'] == '31+lb'), 'CLIENT_MARGIN_MINS'].values[0]\n",
    "tbl_barrettRates_5['BARRETT_RATES_1'] = tbl_barrettRates_5['PUBLISHED_RATES']*(1-DISCOUNT_TO_CLIENT_5)\n",
    "tbl_barrettRates_5['BARRETT_RATES'] = round(tbl_barrettRates_5['BARRETT_RATES_1'].apply(lambda x: max(x, CLIENT_MARGIN_MINS_5)),2)\n",
    "tbl_barrettRates_5 = tbl_barrettRates_5.drop('BARRETT_RATES_1', axis=1)\n",
    "\n",
    "\n",
    "#GRES \n",
    "#GRES ['WEIGHT_BREAK'] == '1-5lb'\n",
    "tbl_barrettRates_6 = tbl_publishedRates[(tbl_publishedRates['SERVICE_CODE'] == 'GRES') & (tbl_publishedRates['WEIGHT_LB'] <= 5)]\n",
    "DISCOUNT_TO_CLIENT_6 = UPS_Rates.loc[(UPS_Rates['SERVICE_CODE'] == 'GRES') & (UPS_Rates['WEIGHT_BREAK'] == '1-5lb'), 'DISCOUNT_TO_CLIENT(1-COLN)'].values[0]\n",
    "if Round_Discount == 'YES':\n",
    "    DISCOUNT_TO_CLIENT_6 = round(DISCOUNT_TO_CLIENT_6,2)\n",
    "else:\n",
    "    DISCOUNT_TO_CLIENT_6 = DISCOUNT_TO_CLIENT_6\n",
    "CLIENT_MARGIN_MINS_6 = UPS_Rates.loc[(UPS_Rates['SERVICE_CODE'] == 'GRES') & (UPS_Rates['WEIGHT_BREAK'] == '1-5lb'), 'CLIENT_MARGIN_MINS'].values[0]\n",
    "tbl_barrettRates_6['BARRETT_RATES_1'] = tbl_barrettRates_6['PUBLISHED_RATES']*(1-DISCOUNT_TO_CLIENT_6)\n",
    "tbl_barrettRates_6['BARRETT_RATES'] = round(tbl_barrettRates_6['BARRETT_RATES_1'].apply(lambda x: max(x, CLIENT_MARGIN_MINS_6)),2)\n",
    "tbl_barrettRates_6 = tbl_barrettRates_6.drop('BARRETT_RATES_1', axis=1)\n",
    "\n",
    "\n",
    "#GRES ['WEIGHT_BREAK'] == '6-10lb'\n",
    "tbl_barrettRates_7 = tbl_publishedRates[(tbl_publishedRates['SERVICE_CODE'] == 'GRES') & (tbl_publishedRates['WEIGHT_LB'] <= 10) & (tbl_publishedRates['WEIGHT_LB'] >= 6)]\n",
    "DISCOUNT_TO_CLIENT_7 = UPS_Rates.loc[(UPS_Rates['SERVICE_CODE'] == 'GRES') & (UPS_Rates['WEIGHT_BREAK'] == '6-10lb'), 'DISCOUNT_TO_CLIENT(1-COLN)'].values[0]\n",
    "if Round_Discount == 'YES':\n",
    "    DISCOUNT_TO_CLIENT_7 = round(DISCOUNT_TO_CLIENT_7,2)\n",
    "else:\n",
    "    DISCOUNT_TO_CLIENT_7 = DISCOUNT_TO_CLIENT_7\n",
    "CLIENT_MARGIN_MINS_7 = UPS_Rates.loc[(UPS_Rates['SERVICE_CODE'] == 'GRES') & (UPS_Rates['WEIGHT_BREAK'] == '6-10lb'), 'CLIENT_MARGIN_MINS'].values[0]\n",
    "tbl_barrettRates_7['BARRETT_RATES_1'] = tbl_barrettRates_7['PUBLISHED_RATES']*(1-DISCOUNT_TO_CLIENT_7)\n",
    "tbl_barrettRates_7['BARRETT_RATES'] = round(tbl_barrettRates_7['BARRETT_RATES_1'].apply(lambda x: max(x, CLIENT_MARGIN_MINS_7)),2)\n",
    "tbl_barrettRates_7 = tbl_barrettRates_7.drop('BARRETT_RATES_1', axis=1)\n",
    "\n",
    "\n",
    "#GRES ['WEIGHT_BREAK'] == '11-20lb'\n",
    "tbl_barrettRates_8 = tbl_publishedRates[(tbl_publishedRates['SERVICE_CODE'] == 'GRES') & (tbl_publishedRates['WEIGHT_LB'] <= 20) & (tbl_publishedRates['WEIGHT_LB'] >= 11)]\n",
    "DISCOUNT_TO_CLIENT_8 = UPS_Rates.loc[(UPS_Rates['SERVICE_CODE'] == 'GRES') & (UPS_Rates['WEIGHT_BREAK'] == '11-20lb'), 'DISCOUNT_TO_CLIENT(1-COLN)'].values[0]\n",
    "if Round_Discount == 'YES':\n",
    "    DISCOUNT_TO_CLIENT_8 = round(DISCOUNT_TO_CLIENT_8,2)\n",
    "else:\n",
    "    DISCOUNT_TO_CLIENT_8 = DISCOUNT_TO_CLIENT_8\n",
    "CLIENT_MARGIN_MINS_8 = UPS_Rates.loc[(UPS_Rates['SERVICE_CODE'] == 'GRES') & (UPS_Rates['WEIGHT_BREAK'] == '11-20lb'), 'CLIENT_MARGIN_MINS'].values[0]\n",
    "tbl_barrettRates_8['BARRETT_RATES_1'] = tbl_barrettRates_8['PUBLISHED_RATES']*(1-DISCOUNT_TO_CLIENT_8)\n",
    "tbl_barrettRates_8['BARRETT_RATES'] = round(tbl_barrettRates_8['BARRETT_RATES_1'].apply(lambda x: max(x, CLIENT_MARGIN_MINS_8)),2)\n",
    "tbl_barrettRates_8 = tbl_barrettRates_8.drop('BARRETT_RATES_1', axis=1)\n",
    "\n",
    "\n",
    "#GRES ['WEIGHT_BREAK'] == '21-30lb'\n",
    "tbl_barrettRates_9 = tbl_publishedRates[(tbl_publishedRates['SERVICE_CODE'] == 'GRES') & (tbl_publishedRates['WEIGHT_LB'] <= 30) & (tbl_publishedRates['WEIGHT_LB'] >= 21)]\n",
    "DISCOUNT_TO_CLIENT_9 = UPS_Rates.loc[(UPS_Rates['SERVICE_CODE'] == 'GRES') & (UPS_Rates['WEIGHT_BREAK'] == '21-30lb'), 'DISCOUNT_TO_CLIENT(1-COLN)'].values[0]\n",
    "if Round_Discount == 'YES':\n",
    "    DISCOUNT_TO_CLIENT_9 = round(DISCOUNT_TO_CLIENT_9,2)\n",
    "else:\n",
    "    DISCOUNT_TO_CLIENT_9 = DISCOUNT_TO_CLIENT_9\n",
    "CLIENT_MARGIN_MINS_9 = UPS_Rates.loc[(UPS_Rates['SERVICE_CODE'] == 'GRES') & (UPS_Rates['WEIGHT_BREAK'] == '21-30lb'), 'CLIENT_MARGIN_MINS'].values[0]\n",
    "tbl_barrettRates_9['BARRETT_RATES_1'] = tbl_barrettRates_9['PUBLISHED_RATES']*(1-DISCOUNT_TO_CLIENT_9)\n",
    "tbl_barrettRates_9['BARRETT_RATES'] = round(tbl_barrettRates_9['BARRETT_RATES_1'].apply(lambda x: max(x, CLIENT_MARGIN_MINS_9)),2)\n",
    "tbl_barrettRates_9 = tbl_barrettRates_9.drop('BARRETT_RATES_1', axis=1)\n",
    "\n",
    "\n",
    "#GRES ['WEIGHT_BREAK'] == '31+lb'\n",
    "tbl_barrettRates_10 = tbl_publishedRates[(tbl_publishedRates['SERVICE_CODE'] == 'GRES') & (tbl_publishedRates['WEIGHT_LB'] >= 31)]\n",
    "DISCOUNT_TO_CLIENT_10 = UPS_Rates.loc[(UPS_Rates['SERVICE_CODE'] == 'GRES') & (UPS_Rates['WEIGHT_BREAK'] == '31+lb'), 'DISCOUNT_TO_CLIENT(1-COLN)'].values[0]\n",
    "if Round_Discount == 'YES':\n",
    "    DISCOUNT_TO_CLIENT_10 = round(DISCOUNT_TO_CLIENT_10,2)\n",
    "else:\n",
    "    DISCOUNT_TO_CLIENT_10 = DISCOUNT_TO_CLIENT_10\n",
    "CLIENT_MARGIN_MINS_10 = UPS_Rates.loc[(UPS_Rates['SERVICE_CODE'] == 'GRES') & (UPS_Rates['WEIGHT_BREAK'] == '31+lb'), 'CLIENT_MARGIN_MINS'].values[0]\n",
    "tbl_barrettRates_10['BARRETT_RATES_1'] = tbl_barrettRates_10['PUBLISHED_RATES']*(1-DISCOUNT_TO_CLIENT_10)\n",
    "tbl_barrettRates_10['BARRETT_RATES'] = round(tbl_barrettRates_10['BARRETT_RATES_1'].apply(lambda x: max(x, CLIENT_MARGIN_MINS_10)),2)\n",
    "tbl_barrettRates_10 = tbl_barrettRates_10.drop('BARRETT_RATES_1', axis=1)\n",
    "\n",
    "\n",
    "#SRPT\n",
    "#SRPT ['WEIGHT_BREAK'] == '1-3lb'\n",
    "tbl_barrettRates_11 = tbl_publishedRates[(tbl_publishedRates['SERVICE_CODE'] == 'SRPT') & (tbl_publishedRates['WEIGHT_LB'] <= 3)]\n",
    "DISCOUNT_TO_CLIENT_11 = UPS_Rates.loc[(UPS_Rates['SERVICE_CODE'] == 'SRPT') & (UPS_Rates['WEIGHT_BREAK'] == '1-3lb'), 'DISCOUNT_TO_CLIENT(1-COLN)'].values[0]\n",
    "if Round_Discount == 'YES':\n",
    "    DISCOUNT_TO_CLIENT_11 = round(DISCOUNT_TO_CLIENT_11,2)\n",
    "else:\n",
    "    DISCOUNT_TO_CLIENT_11 = DISCOUNT_TO_CLIENT_11\n",
    "CLIENT_MARGIN_MINS_11 = UPS_Rates.loc[(UPS_Rates['SERVICE_CODE'] == 'SRPT') & (UPS_Rates['WEIGHT_BREAK'] == '1-3lb'), 'CLIENT_MARGIN_MINS'].values[0]\n",
    "tbl_barrettRates_11['BARRETT_RATES_1'] = tbl_barrettRates_11['PUBLISHED_RATES']*(1-DISCOUNT_TO_CLIENT_11)\n",
    "tbl_barrettRates_11['BARRETT_RATES'] = round(tbl_barrettRates_11['BARRETT_RATES_1'].apply(lambda x: max(x, CLIENT_MARGIN_MINS_11)),2)\n",
    "tbl_barrettRates_11 = tbl_barrettRates_11.drop('BARRETT_RATES_1', axis=1)\n",
    "\n",
    "\n",
    "#SRPT ['WEIGHT_BREAK'] == '4-6lb'\n",
    "tbl_barrettRates_12 = tbl_publishedRates[(tbl_publishedRates['SERVICE_CODE'] == 'SRPT') & (tbl_publishedRates['WEIGHT_LB'] >= 4) & (tbl_publishedRates['WEIGHT_LB'] <= 6)]\n",
    "DISCOUNT_TO_CLIENT_12 = UPS_Rates.loc[(UPS_Rates['SERVICE_CODE'] == 'SRPT') & (UPS_Rates['WEIGHT_BREAK'] == '4-6lb'), 'DISCOUNT_TO_CLIENT(1-COLN)'].values[0]\n",
    "if Round_Discount == 'YES':\n",
    "    DISCOUNT_TO_CLIENT_12 = round(DISCOUNT_TO_CLIENT_12,2)\n",
    "else:\n",
    "    DISCOUNT_TO_CLIENT_12 = DISCOUNT_TO_CLIENT_12\n",
    "CLIENT_MARGIN_MINS_12 = UPS_Rates.loc[(UPS_Rates['SERVICE_CODE'] == 'SRPT') & (UPS_Rates['WEIGHT_BREAK'] == '4-6lb'), 'CLIENT_MARGIN_MINS'].values[0]\n",
    "tbl_barrettRates_12['BARRETT_RATES_1'] = tbl_barrettRates_12['PUBLISHED_RATES']*(1-DISCOUNT_TO_CLIENT_12)\n",
    "tbl_barrettRates_12['BARRETT_RATES'] = round(tbl_barrettRates_12['BARRETT_RATES_1'].apply(lambda x: max(x, CLIENT_MARGIN_MINS_12)),2)\n",
    "tbl_barrettRates_12 = tbl_barrettRates_12.drop('BARRETT_RATES_1', axis=1)\n",
    "\n",
    "\n",
    "#SRPT ['WEIGHT_BREAK'] == '7-8lb'\n",
    "tbl_barrettRates_13 = tbl_publishedRates[(tbl_publishedRates['SERVICE_CODE'] == 'SRPT') & (tbl_publishedRates['WEIGHT_LB'] >= 7) & (tbl_publishedRates['WEIGHT_LB'] <= 8)]\n",
    "DISCOUNT_TO_CLIENT_13 = UPS_Rates.loc[(UPS_Rates['SERVICE_CODE'] == 'SRPT') & (UPS_Rates['WEIGHT_BREAK'] == '7-8lb'), 'DISCOUNT_TO_CLIENT(1-COLN)'].values[0]\n",
    "if Round_Discount == 'YES':\n",
    "    DISCOUNT_TO_CLIENT_13 = round(DISCOUNT_TO_CLIENT_13,2)\n",
    "else:\n",
    "    DISCOUNT_TO_CLIENT_13 = DISCOUNT_TO_CLIENT_13\n",
    "CLIENT_MARGIN_MINS_13 = UPS_Rates.loc[(UPS_Rates['SERVICE_CODE'] == 'SRPT') & (UPS_Rates['WEIGHT_BREAK'] == '7-8lb'), 'CLIENT_MARGIN_MINS'].values[0]\n",
    "tbl_barrettRates_13['BARRETT_RATES_1'] = tbl_barrettRates_13['PUBLISHED_RATES']*(1-DISCOUNT_TO_CLIENT_13)\n",
    "tbl_barrettRates_13['BARRETT_RATES'] = round(tbl_barrettRates_13['BARRETT_RATES_1'].apply(lambda x: max(x, CLIENT_MARGIN_MINS_13)),2)\n",
    "tbl_barrettRates_13 = tbl_barrettRates_13.drop('BARRETT_RATES_1', axis=1)\n",
    "\n",
    "\n",
    "#SRPT ['WEIGHT_BREAK'] == '9lb'\n",
    "tbl_barrettRates_14 = tbl_publishedRates[(tbl_publishedRates['SERVICE_CODE'] == 'SRPT') & (tbl_publishedRates['WEIGHT_LB'] == 9)]\n",
    "DISCOUNT_TO_CLIENT_14 = UPS_Rates.loc[(UPS_Rates['SERVICE_CODE'] == 'SRPT') & (UPS_Rates['WEIGHT_BREAK'] == '9lb'), 'DISCOUNT_TO_CLIENT(1-COLN)'].values[0]\n",
    "if Round_Discount == 'YES':\n",
    "    DISCOUNT_TO_CLIENT_14 = round(DISCOUNT_TO_CLIENT_14,2)\n",
    "else:\n",
    "    DISCOUNT_TO_CLIENT_14 = DISCOUNT_TO_CLIENT_14\n",
    "CLIENT_MARGIN_MINS_14 = UPS_Rates.loc[(UPS_Rates['SERVICE_CODE'] == 'SRPT') & (UPS_Rates['WEIGHT_BREAK'] == '9lb'), 'CLIENT_MARGIN_MINS'].values[0]\n",
    "tbl_barrettRates_14['BARRETT_RATES_1'] = tbl_barrettRates_14['PUBLISHED_RATES']*(1-DISCOUNT_TO_CLIENT_14)\n",
    "tbl_barrettRates_14['BARRETT_RATES'] = round(tbl_barrettRates_14['BARRETT_RATES_1'].apply(lambda x: max(x, CLIENT_MARGIN_MINS_14)),2)\n",
    "tbl_barrettRates_14 = tbl_barrettRates_14.drop('BARRETT_RATES_1', axis=1)\n",
    "\n",
    "\n",
    "#Others:\n",
    "#UPS REDE, there is no discount, therefore REDE's client rate = published rate\n",
    "tbl_barrettRates_15 = tbl_publishedRates[(tbl_publishedRates['SERVICE_CODE'] == 'REDE')]\n",
    "tbl_barrettRates_15['BARRETT_RATES'] = tbl_barrettRates_15['PUBLISHED_RATES']\n",
    "\n",
    "\n",
    "#UPS RED\n",
    "tbl_barrettRates_16 = tbl_publishedRates[(tbl_publishedRates['SERVICE_CODE'] == 'RED')]\n",
    "DISCOUNT_TO_CLIENT_16 = UPS_Rates.loc[(UPS_Rates['SERVICE_CODE'] == 'RED'), 'DISCOUNT_TO_CLIENT(1-COLN)'].values[0]\n",
    "if Round_Discount == 'YES':\n",
    "    DISCOUNT_TO_CLIENT_16 = round(DISCOUNT_TO_CLIENT_16,2)\n",
    "else:\n",
    "    DISCOUNT_TO_CLIENT_16 = DISCOUNT_TO_CLIENT_16\n",
    "CLIENT_MARGIN_MINS_16 = UPS_Rates.loc[(UPS_Rates['SERVICE_CODE'] == 'RED'), 'CLIENT_MARGIN_MINS'].values[0]\n",
    "tbl_barrettRates_16['BARRETT_RATES_1'] = tbl_barrettRates_16['PUBLISHED_RATES']*(1-DISCOUNT_TO_CLIENT_16)\n",
    "tbl_barrettRates_16['BARRETT_RATES'] = round(tbl_barrettRates_16['BARRETT_RATES_1'].apply(lambda x: max(x, CLIENT_MARGIN_MINS_16)),2)\n",
    "tbl_barrettRates_16 = tbl_barrettRates_16.drop('BARRETT_RATES_1', axis=1)\n",
    "\n",
    "\n",
    "#UPS REDS, this does not have round up option (1/2)\n",
    "tbl_barrettRates_17 = tbl_publishedRates[(tbl_publishedRates['SERVICE_CODE'] == 'REDS')]\n",
    "DISCOUNT_TO_CLIENT_17 = UPS_Rates.loc[(UPS_Rates['SERVICE_CODE'] == 'REDS'), 'DISCOUNT_TO_CLIENT(1-COLN)'].values[0]\n",
    "CLIENT_MARGIN_MINS_17 = UPS_Rates.loc[(UPS_Rates['SERVICE_CODE'] == 'REDS'), 'CLIENT_MARGIN_MINS'].values[0]\n",
    "tbl_barrettRates_17['BARRETT_RATES_1'] = tbl_barrettRates_17['PUBLISHED_RATES']*(1-DISCOUNT_TO_CLIENT_17)\n",
    "tbl_barrettRates_17['BARRETT_RATES'] = round(tbl_barrettRates_17['BARRETT_RATES_1'].apply(lambda x: max(x, CLIENT_MARGIN_MINS_17)),2)\n",
    "tbl_barrettRates_17 = tbl_barrettRates_17.drop('BARRETT_RATES_1', axis=1)\n",
    "\n",
    "\n",
    "#UPS 2DAM, this does not have round up option (2/2)\n",
    "tbl_barrettRates_18 = tbl_publishedRates[(tbl_publishedRates['SERVICE_CODE'] == '2DAM')]\n",
    "DISCOUNT_TO_CLIENT_18 = UPS_Rates.loc[(UPS_Rates['SERVICE_CODE'] == '2DAM'), 'DISCOUNT_TO_CLIENT(1-COLN)'].values[0]\n",
    "CLIENT_MARGIN_MINS_18 = UPS_Rates.loc[(UPS_Rates['SERVICE_CODE'] == '2DAM'), 'CLIENT_MARGIN_MINS'].values[0]\n",
    "tbl_barrettRates_18['BARRETT_RATES_1'] = tbl_barrettRates_18['PUBLISHED_RATES']*(1-DISCOUNT_TO_CLIENT_18)\n",
    "tbl_barrettRates_18['BARRETT_RATES'] = round(tbl_barrettRates_18['BARRETT_RATES_1'].apply(lambda x: max(x, CLIENT_MARGIN_MINS_18)),2)\n",
    "tbl_barrettRates_18 = tbl_barrettRates_18.drop('BARRETT_RATES_1', axis=1)\n",
    "\n",
    "\n",
    "#UPS BLUE\n",
    "tbl_barrettRates_19 = tbl_publishedRates[(tbl_publishedRates['SERVICE_CODE'] == 'BLUE')]\n",
    "DISCOUNT_TO_CLIENT_19 = UPS_Rates.loc[(UPS_Rates['SERVICE_CODE'] == 'BLUE'), 'DISCOUNT_TO_CLIENT(1-COLN)'].values[0]\n",
    "if Round_Discount == 'YES':\n",
    "    DISCOUNT_TO_CLIENT_19 = round(DISCOUNT_TO_CLIENT_19,2)\n",
    "else:\n",
    "    DISCOUNT_TO_CLIENT_19 = DISCOUNT_TO_CLIENT_19\n",
    "CLIENT_MARGIN_MINS_19 = UPS_Rates.loc[(UPS_Rates['SERVICE_CODE'] == 'BLUE') , 'CLIENT_MARGIN_MINS'].values[0]\n",
    "tbl_barrettRates_19['BARRETT_RATES_1'] = tbl_barrettRates_19['PUBLISHED_RATES']*(1-DISCOUNT_TO_CLIENT_19)\n",
    "tbl_barrettRates_19['BARRETT_RATES'] = round(tbl_barrettRates_19['BARRETT_RATES_1'].apply(lambda x: max(x, CLIENT_MARGIN_MINS_19)),2)\n",
    "tbl_barrettRates_19 = tbl_barrettRates_19.drop('BARRETT_RATES_1', axis=1)\n",
    "\n",
    "\n",
    "#UPS ORNG\n",
    "tbl_barrettRates_20 = tbl_publishedRates[(tbl_publishedRates['SERVICE_CODE'] == 'ORNG')]\n",
    "DISCOUNT_TO_CLIENT_20 = UPS_Rates.loc[(UPS_Rates['SERVICE_CODE'] == 'ORNG'), 'DISCOUNT_TO_CLIENT(1-COLN)'].values[0]\n",
    "if Round_Discount == 'YES':\n",
    "    DISCOUNT_TO_CLIENT_20 = round(DISCOUNT_TO_CLIENT_20,2)\n",
    "else:\n",
    "    DISCOUNT_TO_CLIENT_20 = DISCOUNT_TO_CLIENT_20\n",
    "CLIENT_MARGIN_MINS_20 = UPS_Rates.loc[(UPS_Rates['SERVICE_CODE'] == 'ORNG'), 'CLIENT_MARGIN_MINS'].values[0]\n",
    "tbl_barrettRates_20['BARRETT_RATES_1'] = tbl_barrettRates_20['PUBLISHED_RATES']*(1-DISCOUNT_TO_CLIENT_20)\n",
    "tbl_barrettRates_20['BARRETT_RATES'] = round(tbl_barrettRates_20['BARRETT_RATES_1'].apply(lambda x: max(x, CLIENT_MARGIN_MINS_20)),2)\n",
    "tbl_barrettRates_20 = tbl_barrettRates_20.drop('BARRETT_RATES_1', axis=1)\n",
    "\n",
    "\n",
    "#UPS SRPT<1\n",
    "tbl_barrettRates_21 = tbl_publishedRates[(tbl_publishedRates['SERVICE_CODE'] == 'SRPT<1')]\n",
    "DISCOUNT_TO_CLIENT_21 = UPS_Rates.loc[(UPS_Rates['SERVICE_CODE'] == 'SRPT<1'), 'DISCOUNT_TO_CLIENT(1-COLN)'].values[0]\n",
    "if Round_Discount == 'YES':\n",
    "    DISCOUNT_TO_CLIENT_21 = round(DISCOUNT_TO_CLIENT_21,2)\n",
    "else:\n",
    "    DISCOUNT_TO_CLIENT_21 = DISCOUNT_TO_CLIENT_21\n",
    "CLIENT_MARGIN_MINS_21 = UPS_Rates.loc[(UPS_Rates['SERVICE_CODE'] == 'SRPT<1'), 'CLIENT_MARGIN_MINS'].values[0]\n",
    "tbl_barrettRates_21['BARRETT_RATES_1'] = tbl_barrettRates_21['PUBLISHED_RATES']*(1-DISCOUNT_TO_CLIENT_21)\n",
    "tbl_barrettRates_21['BARRETT_RATES'] = round(tbl_barrettRates_21['BARRETT_RATES_1'].apply(lambda x: max(x, CLIENT_MARGIN_MINS_21)),2)\n",
    "tbl_barrettRates_21 = tbl_barrettRates_21.drop('BARRETT_RATES_1', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1070,
   "id": "720bcc72",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get dhl, usps, and mi rates\n",
    "tbl_barrettRates_22 = tbl_publishedRates[(tbl_publishedRates['SERVICE_CODE'] == 'USPSAG') | \n",
    "                                         (tbl_publishedRates['SERVICE_CODE'] == 'USPSAP') | \n",
    "                                         (tbl_publishedRates['SERVICE_CODE'] == 'USPSAG_CI') | \n",
    "                                         (tbl_publishedRates['SERVICE_CODE'] == 'USPSAP_CI') | \n",
    "                                         (tbl_publishedRates['SERVICE_CODE'] == 'MIPLE') |\n",
    "                                         (tbl_publishedRates['SERVICE_CODE'] == 'MIPH') |\n",
    "                                         (tbl_publishedRates['SERVICE_CODE'].str.startswith('DHLG_'))   |\n",
    "                                         (tbl_publishedRates['SERVICE_CODE'].str.startswith('DHLG<1_')) |\n",
    "                                         (tbl_publishedRates['SERVICE_CODE'].str.startswith('DHLE_'))   |\n",
    "                                         (tbl_publishedRates['SERVICE_CODE'].str.startswith('DHLE<1_')) |\n",
    "                                         (tbl_publishedRates['SERVICE_CODE'].str.startswith('DHLEM_'))]\n",
    "\n",
    "\n",
    "#combine them to rename as BarrettRates\n",
    "tbl_BarrettRates = pd.concat([tbl_barrettRates_1, tbl_barrettRates_2, tbl_barrettRates_3, tbl_barrettRates_4, \n",
    "                    tbl_barrettRates_5, tbl_barrettRates_6, tbl_barrettRates_7, tbl_barrettRates_8,\n",
    "                    tbl_barrettRates_9, tbl_barrettRates_10, tbl_barrettRates_11, tbl_barrettRates_12,\n",
    "                    tbl_barrettRates_13, tbl_barrettRates_14, tbl_barrettRates_15, tbl_barrettRates_16,\n",
    "                    tbl_barrettRates_17, tbl_barrettRates_18, tbl_barrettRates_19, tbl_barrettRates_20,\n",
    "                    tbl_barrettRates_21, tbl_barrettRates_22])\n",
    "#Keep this table as it only contains published rates and Barrett rates\n",
    "#tbl_BarrettRates.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1071,
   "id": "6910593d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define two different mappings\n",
    "mapping_for_ground = {0.1: 'Tier1', 0.2: 'Tier2', 0.3: 'Tier3', 0.4: 'Tier4', 0.5: 'Tier5', 0.6: 'Tier6', 0.7: 'Tier7', \n",
    "                      0.8: 'Tier8', 0.9: 'Tier9', 1: 'Tier10'}\n",
    "\n",
    "mapping_for_priority = {0.1: 'Tier1', 0.2: 'Tier2', 0.3: 'Tier3', 0.4: 'Tier4', 0.5: 'Tier5'}\n",
    "\n",
    "def apply_custom_mapping(row):\n",
    "    if row['SERVICE_CODE'] == 'USPSAG_CI':\n",
    "        return mapping_for_ground.get(row['CUBIC_FT'], None)  # None for default\n",
    "    elif row['SERVICE_CODE'] == 'USPSAP_CI':\n",
    "        return mapping_for_priority.get(row['CUBIC_FT'], None)  # None for default\n",
    "    else:\n",
    "        return None  # Or some default value for other ServiceCodes\n",
    "\n",
    "# Apply the custom function to create a new column\n",
    "tbl_publishedRates['TIER'] = tbl_publishedRates.apply(apply_custom_mapping, axis=1)\n",
    "tbl_BarrettRates['TIER'] = tbl_BarrettRates.apply(apply_custom_mapping, axis=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1072,
   "id": "9c76a57a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Temp solution to handle rounding difference\n",
    "tbl_BarrettRates.to_csv('tbl_BarrettRates.csv')\n",
    "tbl_publishedRates.to_csv('tbl_publishedRates.csv')\n",
    "tbl_BarrettRates = pd.read_csv('C:\\\\Users\\\\sliu\\\\BD\\\\re-rate_model\\\\tbl_BarrettRates.csv')\n",
    "tbl_publishedRates = pd.read_csv('C:\\\\Users\\\\sliu\\\\BD\\\\re-rate_model\\\\tbl_publishedRates.csv')\n",
    "tbl_BarrettRates['ZONE'] = tbl_BarrettRates['ZONE'].astype(str)\n",
    "tbl_BarrettRates['VERSION'] = tbl_BarrettRates['VERSION'].astype(str)\n",
    "tbl_publishedRates['ZONE'] = tbl_publishedRates['ZONE'].astype(str)\n",
    "tbl_publishedRates['VERSION'] = tbl_publishedRates['VERSION'].astype(str)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "psychological-destination",
   "metadata": {},
   "source": [
    "# Step 1 - c. Create Freight Rate Card (tbl_clientRates)\n",
    "*Add ups, dhl, and usps clientRates in tbl_publishedRates. Read the margins located at the top of this model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1073,
   "id": "photographic-people",
   "metadata": {},
   "outputs": [],
   "source": [
    "# UPSAccessorial_2023 has 2 tabs, UPS_Rates and UPS_Accessorial. UPS_Rates is used to create Barrett Rates\n",
    "xls = pd.ExcelFile('UPSAccessorial_2023.xlsx')\n",
    "UPS_Rates = pd.read_excel(xls, 'UPS_Rates')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1074,
   "id": "upper-trick",
   "metadata": {},
   "outputs": [],
   "source": [
    "#add 2 columns to this table with min and max weight\n",
    "def f(col):\n",
    "    if col['WEIGHT_BREAK'] == '1-5lb':\n",
    "        return 5\n",
    "    elif col['WEIGHT_BREAK'] == '6-10lb':\n",
    "        return 10\n",
    "    elif col['WEIGHT_BREAK'] == '11-20lb':\n",
    "        return 20\n",
    "    elif col['WEIGHT_BREAK'] == '21-30lb':\n",
    "        return 30\n",
    "    elif (col['WEIGHT_BREAK'] == '31+lb'):\n",
    "        return 999\n",
    "    elif (col['SERVICE_LEVEL'] == 'SurePost - Less than 1lb') & (col['WEIGHT_BREAK'] == '1-16oz'):\n",
    "        return 16\n",
    "    elif (col['SERVICE_LEVEL'] == 'SurePost - Greater than 1lb') & (col['WEIGHT_BREAK'] == '1-3lb'):\n",
    "        return 3\n",
    "    elif (col['SERVICE_LEVEL'] == 'SurePost - Greater than 1lb') & (col['WEIGHT_BREAK'] == '4-6lb'):\n",
    "        return 6\n",
    "    elif (col['SERVICE_LEVEL'] == 'SurePost - Greater than 1lb') & (col['WEIGHT_BREAK'] == '7-8lb'):\n",
    "        return 8\n",
    "    elif (col['SERVICE_LEVEL'] == 'SurePost - Greater than 1lb') & (col['WEIGHT_BREAK'] == '9lb'):\n",
    "        return 9  \n",
    "\n",
    "def g(col):\n",
    "    if col['WEIGHT_BREAK'] == '1-5lb':\n",
    "        return 1\n",
    "    elif col['WEIGHT_BREAK'] == '6-10lb':\n",
    "        return 6\n",
    "    elif col['WEIGHT_BREAK'] == '11-20lb':\n",
    "        return 11\n",
    "    elif col['WEIGHT_BREAK'] == '21-30lb':\n",
    "        return 21\n",
    "    elif col['WEIGHT_BREAK'] == '31+lb':\n",
    "        return 31\n",
    "    elif (col['SERVICE_LEVEL'] == 'SurePost - Less than 1lb') & (col['WEIGHT_BREAK'] == '1-16oz'):\n",
    "        return 1\n",
    "    elif (col['SERVICE_LEVEL'] == 'SurePost - Greater than 1lb') & (col['WEIGHT_BREAK'] == '1-3lb'):\n",
    "        return 1\n",
    "    elif (col['SERVICE_LEVEL'] == 'SurePost - Greater than 1lb') & (col['WEIGHT_BREAK'] == '4-6lb'):\n",
    "        return 4\n",
    "    elif (col['SERVICE_LEVEL'] == 'SurePost - Greater than 1lb') & (col['WEIGHT_BREAK'] == '7-8lb'):\n",
    "        return 7\n",
    "    elif (col['SERVICE_LEVEL'] == 'SurePost - Greater than 1lb') & (col['WEIGHT_BREAK'] == '9lb'):\n",
    "        return 9 \n",
    "\n",
    "UPS_Rates['WEIGHT_LOWER'] = UPS_Rates.apply(g, axis=1)\n",
    "UPS_Rates['WEIGHT_UPPER'] = UPS_Rates.apply(f, axis=1)\n",
    "\n",
    "\n",
    "# add client margin mins column\n",
    "def h(col):   \n",
    "    if col['SERVICE_LEVEL'] == 'Next Day Air Early AM':\n",
    "        return col['MIN_WITH_REBATE']\n",
    "    \n",
    "    elif (col['SERVICE_LEVEL'] == 'Next Day Air') | (col['SERVICE_LEVEL'] == 'Next Day Air Saver') | (col['SERVICE_LEVEL'] == '2nd A.M. Day Air') | (col['SERVICE_LEVEL'] == '2nd Day Air') | (col['SERVICE_LEVEL'] == '3 Day Select'): \n",
    "        try: \n",
    "            return min(col['MIN_WITH_REBATE'] + 4, col['MIN_WITH_REBATE'] / (1-Margin_on_Min_AIR))\n",
    "        except ValueError:\n",
    "            return 0\n",
    "\n",
    "    elif (col['SERVICE_LEVEL'] == 'Ground Commercial') | (col['SERVICE_LEVEL'] == 'Ground Residential'): \n",
    "        try: \n",
    "            return min(10.1, col['MIN_WITH_REBATE'] / (1-Margin_on_Min_GRND))\n",
    "        except ValueError:\n",
    "            return 0\n",
    "\n",
    "    elif (col['SERVICE_LEVEL'] == 'SurePost - Greater than 1lb'): \n",
    "        try: \n",
    "            return min(8.14, col['MIN_WITH_REBATE'] / (1-Margin_on_Min_GRND))\n",
    "        except ValueError:\n",
    "            return 0\n",
    "        \n",
    "    elif (col['SERVICE_LEVEL'] == 'SurePost - Less than 1lb'): \n",
    "        try: \n",
    "            return min(8.14, col['MIN_WITH_REBATE'] / (1-Margin_on_Min_GRND))\n",
    "        except ValueError:\n",
    "            return 0\n",
    "  \n",
    "    elif col['SERVICE_LEVEL'] == 'Ground CWT': \n",
    "        try: \n",
    "            return min(80, col['MIN_WITH_REBATE'] / (1-Margin_on_Min_GRND))\n",
    "        except ValueError:\n",
    "            return 0\n",
    "  \n",
    "    elif (col['SERVICE_LEVEL'] == 'Standard to Canada') | (col['SERVICE_LEVEL'] == 'Standard to Mexico'):\n",
    "        try: \n",
    "            return min(col['MIN_WITH_REBATE'] + 4, col['MIN_WITH_REBATE'] / (1-Margin_on_Min_GRND))\n",
    "        except ValueError:\n",
    "            return 0\n",
    "\n",
    "UPS_Rates['CLIENT_MARGIN_MINS'] = UPS_Rates.apply(h, axis=1)\n",
    "\n",
    "# add ADD_MARGIN_(COL_M/DECIMAL) column\n",
    "UPS_Rates['DISCOUNT_RECIPROCAL_(BDC_PAY_ON_THE_DOLLAR)'] = 1 - UPS_Rates['NET_CONTRACT_DISC']\n",
    "UPS_Rates.loc[UPS_Rates['SERVICE_LEVEL'] == 'Next Day Air Early AM', 'DISCOUNT_RECIPROCAL_(BDC_PAY_ON_THE_DOLLAR)'] = None\n",
    "\n",
    "def i(col):   \n",
    "    if (col['SERVICE_LEVEL'] == 'SurePost - Greater than 1lb') | (col['WEIGHT_BREAK'] == '9lb'):\n",
    "        return col['DISCOUNT_RECIPROCAL_(BDC_PAY_ON_THE_DOLLAR)'] / (1-Margin_on_UPS_Parcel_Freight_AIR)\n",
    "    elif col['SERVICE_LEVEL'] == 'Next Day Air Early AM':\n",
    "        return\n",
    "    elif (col['SERVICE_LEVEL'] == 'Ground Commercial') | (col['SERVICE_LEVEL'] == 'Ground Residential') | (col['SERVICE_LEVEL'] == 'SurePost - Less than 1lb') | (col['SERVICE_LEVEL'] == 'SurePost - Greater than 1lb'): \n",
    "        return col['DISCOUNT_RECIPROCAL_(BDC_PAY_ON_THE_DOLLAR)'] / (1-Margin_on_UPS_Parcel_Freight_GRND)\n",
    "    else:\n",
    "        return col['DISCOUNT_RECIPROCAL_(BDC_PAY_ON_THE_DOLLAR)'] / (1-Margin_on_UPS_Parcel_Freight_AIR)\n",
    "\n",
    "UPS_Rates['ADD_MARGIN_(COL_M/DECIMAL)'] = UPS_Rates.apply(i, axis=1) \n",
    "\n",
    "# add DISCOUNT_TO_CLIENT(1-COLN) column \n",
    "def j(col):   \n",
    "    if (col['SERVICE_LEVEL'] == 'Standard to Canada') | (col['SERVICE_LEVEL'] == 'Standard to Mexico'):\n",
    "        return max(0, 1 - col['ADD_MARGIN_(COL_M/DECIMAL)'])\n",
    "    elif col['SERVICE_LEVEL'] == 'Next Day Air Early AM':\n",
    "        return\n",
    "    else:\n",
    "        return 1 - col['ADD_MARGIN_(COL_M/DECIMAL)']\n",
    "\n",
    "UPS_Rates['DISCOUNT_TO_CLIENT(1-COLN)'] = UPS_Rates.apply(j, axis=1)\n",
    "\n",
    "\n",
    "# This is the left side of \"Rates\" tab from \"Customer Rate Contract Cards 2023\" doc\n",
    "# UPS_Rates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1075,
   "id": "dominican-lodging",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sliu\\AppData\\Local\\Temp\\ipykernel_18976\\2791379907.py:14: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  tbl_clientRates_1['CLIENT_RATES_1'] = tbl_clientRates_1['PUBLISHED_RATES']*(1-DISCOUNT_TO_CLIENT_1)\n",
      "C:\\Users\\sliu\\AppData\\Local\\Temp\\ipykernel_18976\\2791379907.py:15: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  tbl_clientRates_1['CLIENT_RATES'] = round(tbl_clientRates_1['CLIENT_RATES_1'].apply(lambda x: max(x, CLIENT_MARGIN_MINS_1)),2)\n",
      "C:\\Users\\sliu\\AppData\\Local\\Temp\\ipykernel_18976\\2791379907.py:27: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  tbl_clientRates_2['CLIENT_RATES_1'] = tbl_clientRates_2['PUBLISHED_RATES']*(1-DISCOUNT_TO_CLIENT_2)\n",
      "C:\\Users\\sliu\\AppData\\Local\\Temp\\ipykernel_18976\\2791379907.py:28: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  tbl_clientRates_2['CLIENT_RATES'] = round(tbl_clientRates_2['CLIENT_RATES_1'].apply(lambda x: max(x, CLIENT_MARGIN_MINS_2)),2)\n",
      "C:\\Users\\sliu\\AppData\\Local\\Temp\\ipykernel_18976\\2791379907.py:40: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  tbl_clientRates_3['CLIENT_RATES_1'] = tbl_clientRates_3['PUBLISHED_RATES']*(1-DISCOUNT_TO_CLIENT_3)\n",
      "C:\\Users\\sliu\\AppData\\Local\\Temp\\ipykernel_18976\\2791379907.py:41: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  tbl_clientRates_3['CLIENT_RATES'] = round(tbl_clientRates_3['CLIENT_RATES_1'].apply(lambda x: max(x, CLIENT_MARGIN_MINS_3)),2)\n",
      "C:\\Users\\sliu\\AppData\\Local\\Temp\\ipykernel_18976\\2791379907.py:53: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  tbl_clientRates_4['CLIENT_RATES_1'] = tbl_clientRates_4['PUBLISHED_RATES']*(1-DISCOUNT_TO_CLIENT_4)\n",
      "C:\\Users\\sliu\\AppData\\Local\\Temp\\ipykernel_18976\\2791379907.py:54: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  tbl_clientRates_4['CLIENT_RATES'] = round(tbl_clientRates_4['CLIENT_RATES_1'].apply(lambda x: max(x, CLIENT_MARGIN_MINS_4)),2)\n",
      "C:\\Users\\sliu\\AppData\\Local\\Temp\\ipykernel_18976\\2791379907.py:66: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  tbl_clientRates_5['CLIENT_RATES_1'] = tbl_clientRates_5['PUBLISHED_RATES']*(1-DISCOUNT_TO_CLIENT_5)\n",
      "C:\\Users\\sliu\\AppData\\Local\\Temp\\ipykernel_18976\\2791379907.py:67: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  tbl_clientRates_5['CLIENT_RATES'] = round(tbl_clientRates_5['CLIENT_RATES_1'].apply(lambda x: max(x, CLIENT_MARGIN_MINS_5)),2)\n",
      "C:\\Users\\sliu\\AppData\\Local\\Temp\\ipykernel_18976\\2791379907.py:80: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  tbl_clientRates_6['CLIENT_RATES_1'] = tbl_clientRates_6['PUBLISHED_RATES']*(1-DISCOUNT_TO_CLIENT_6)\n",
      "C:\\Users\\sliu\\AppData\\Local\\Temp\\ipykernel_18976\\2791379907.py:81: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  tbl_clientRates_6['CLIENT_RATES'] = round(tbl_clientRates_6['CLIENT_RATES_1'].apply(lambda x: max(x, CLIENT_MARGIN_MINS_6)),2)\n",
      "C:\\Users\\sliu\\AppData\\Local\\Temp\\ipykernel_18976\\2791379907.py:93: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  tbl_clientRates_7['CLIENT_RATES_1'] = tbl_clientRates_7['PUBLISHED_RATES']*(1-DISCOUNT_TO_CLIENT_7)\n",
      "C:\\Users\\sliu\\AppData\\Local\\Temp\\ipykernel_18976\\2791379907.py:94: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  tbl_clientRates_7['CLIENT_RATES'] = round(tbl_clientRates_7['CLIENT_RATES_1'].apply(lambda x: max(x, CLIENT_MARGIN_MINS_7)),2)\n",
      "C:\\Users\\sliu\\AppData\\Local\\Temp\\ipykernel_18976\\2791379907.py:106: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  tbl_clientRates_8['CLIENT_RATES_1'] = tbl_clientRates_8['PUBLISHED_RATES']*(1-DISCOUNT_TO_CLIENT_8)\n",
      "C:\\Users\\sliu\\AppData\\Local\\Temp\\ipykernel_18976\\2791379907.py:107: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  tbl_clientRates_8['CLIENT_RATES'] = round(tbl_clientRates_8['CLIENT_RATES_1'].apply(lambda x: max(x, CLIENT_MARGIN_MINS_8)),2)\n",
      "C:\\Users\\sliu\\AppData\\Local\\Temp\\ipykernel_18976\\2791379907.py:119: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  tbl_clientRates_9['CLIENT_RATES_1'] = tbl_clientRates_9['PUBLISHED_RATES']*(1-DISCOUNT_TO_CLIENT_9)\n",
      "C:\\Users\\sliu\\AppData\\Local\\Temp\\ipykernel_18976\\2791379907.py:120: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  tbl_clientRates_9['CLIENT_RATES'] = round(tbl_clientRates_9['CLIENT_RATES_1'].apply(lambda x: max(x, CLIENT_MARGIN_MINS_9)),2)\n",
      "C:\\Users\\sliu\\AppData\\Local\\Temp\\ipykernel_18976\\2791379907.py:132: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  tbl_clientRates_10['CLIENT_RATES_1'] = tbl_clientRates_10['PUBLISHED_RATES']*(1-DISCOUNT_TO_CLIENT_10)\n",
      "C:\\Users\\sliu\\AppData\\Local\\Temp\\ipykernel_18976\\2791379907.py:133: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  tbl_clientRates_10['CLIENT_RATES'] = round(tbl_clientRates_10['CLIENT_RATES_1'].apply(lambda x: max(x, CLIENT_MARGIN_MINS_10)),2)\n",
      "C:\\Users\\sliu\\AppData\\Local\\Temp\\ipykernel_18976\\2791379907.py:146: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  tbl_clientRates_11['CLIENT_RATES_1'] = tbl_clientRates_11['PUBLISHED_RATES']*(1-DISCOUNT_TO_CLIENT_11)\n",
      "C:\\Users\\sliu\\AppData\\Local\\Temp\\ipykernel_18976\\2791379907.py:147: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  tbl_clientRates_11['CLIENT_RATES'] = round(tbl_clientRates_11['CLIENT_RATES_1'].apply(lambda x: max(x, CLIENT_MARGIN_MINS_11)),2)\n",
      "C:\\Users\\sliu\\AppData\\Local\\Temp\\ipykernel_18976\\2791379907.py:159: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  tbl_clientRates_12['CLIENT_RATES_1'] = tbl_clientRates_12['PUBLISHED_RATES']*(1-DISCOUNT_TO_CLIENT_12)\n",
      "C:\\Users\\sliu\\AppData\\Local\\Temp\\ipykernel_18976\\2791379907.py:160: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  tbl_clientRates_12['CLIENT_RATES'] = round(tbl_clientRates_12['CLIENT_RATES_1'].apply(lambda x: max(x, CLIENT_MARGIN_MINS_12)),2)\n",
      "C:\\Users\\sliu\\AppData\\Local\\Temp\\ipykernel_18976\\2791379907.py:172: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  tbl_clientRates_13['CLIENT_RATES_1'] = tbl_clientRates_13['PUBLISHED_RATES']*(1-DISCOUNT_TO_CLIENT_13)\n",
      "C:\\Users\\sliu\\AppData\\Local\\Temp\\ipykernel_18976\\2791379907.py:173: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  tbl_clientRates_13['CLIENT_RATES'] = round(tbl_clientRates_13['CLIENT_RATES_1'].apply(lambda x: max(x, CLIENT_MARGIN_MINS_13)),2)\n",
      "C:\\Users\\sliu\\AppData\\Local\\Temp\\ipykernel_18976\\2791379907.py:185: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  tbl_clientRates_14['CLIENT_RATES_1'] = tbl_clientRates_14['PUBLISHED_RATES']*(1-DISCOUNT_TO_CLIENT_14)\n",
      "C:\\Users\\sliu\\AppData\\Local\\Temp\\ipykernel_18976\\2791379907.py:186: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  tbl_clientRates_14['CLIENT_RATES'] = round(tbl_clientRates_14['CLIENT_RATES_1'].apply(lambda x: max(x, CLIENT_MARGIN_MINS_14)),2)\n",
      "C:\\Users\\sliu\\AppData\\Local\\Temp\\ipykernel_18976\\2791379907.py:193: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  tbl_clientRates_15['CLIENT_RATES'] = tbl_clientRates_15['PUBLISHED_RATES']\n",
      "C:\\Users\\sliu\\AppData\\Local\\Temp\\ipykernel_18976\\2791379907.py:204: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  tbl_clientRates_16['CLIENT_RATES_1'] = tbl_clientRates_16['PUBLISHED_RATES']*(1-DISCOUNT_TO_CLIENT_16)\n",
      "C:\\Users\\sliu\\AppData\\Local\\Temp\\ipykernel_18976\\2791379907.py:205: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  tbl_clientRates_16['CLIENT_RATES'] = round(tbl_clientRates_16['CLIENT_RATES_1'].apply(lambda x: max(x, CLIENT_MARGIN_MINS_16)),2)\n",
      "C:\\Users\\sliu\\AppData\\Local\\Temp\\ipykernel_18976\\2791379907.py:213: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  tbl_clientRates_17['CLIENT_RATES_1'] = tbl_clientRates_17['PUBLISHED_RATES']*(1-DISCOUNT_TO_CLIENT_17)\n",
      "C:\\Users\\sliu\\AppData\\Local\\Temp\\ipykernel_18976\\2791379907.py:214: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  tbl_clientRates_17['CLIENT_RATES'] = round(tbl_clientRates_17['CLIENT_RATES_1'].apply(lambda x: max(x, CLIENT_MARGIN_MINS_17)),2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sliu\\AppData\\Local\\Temp\\ipykernel_18976\\2791379907.py:222: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  tbl_clientRates_18['CLIENT_RATES_1'] = tbl_clientRates_18['PUBLISHED_RATES']*(1-DISCOUNT_TO_CLIENT_18)\n",
      "C:\\Users\\sliu\\AppData\\Local\\Temp\\ipykernel_18976\\2791379907.py:223: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  tbl_clientRates_18['CLIENT_RATES'] = round(tbl_clientRates_18['CLIENT_RATES_1'].apply(lambda x: max(x, CLIENT_MARGIN_MINS_18)),2)\n",
      "C:\\Users\\sliu\\AppData\\Local\\Temp\\ipykernel_18976\\2791379907.py:235: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  tbl_clientRates_19['CLIENT_RATES_1'] = tbl_clientRates_19['PUBLISHED_RATES']*(1-DISCOUNT_TO_CLIENT_19)\n",
      "C:\\Users\\sliu\\AppData\\Local\\Temp\\ipykernel_18976\\2791379907.py:236: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  tbl_clientRates_19['CLIENT_RATES'] = round(tbl_clientRates_19['CLIENT_RATES_1'].apply(lambda x: max(x, CLIENT_MARGIN_MINS_19)),2)\n",
      "C:\\Users\\sliu\\AppData\\Local\\Temp\\ipykernel_18976\\2791379907.py:248: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  tbl_clientRates_20['CLIENT_RATES_1'] = tbl_clientRates_20['PUBLISHED_RATES']*(1-DISCOUNT_TO_CLIENT_20)\n",
      "C:\\Users\\sliu\\AppData\\Local\\Temp\\ipykernel_18976\\2791379907.py:249: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  tbl_clientRates_20['CLIENT_RATES'] = round(tbl_clientRates_20['CLIENT_RATES_1'].apply(lambda x: max(x, CLIENT_MARGIN_MINS_20)),2)\n",
      "C:\\Users\\sliu\\AppData\\Local\\Temp\\ipykernel_18976\\2791379907.py:261: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  tbl_clientRates_21['CLIENT_RATES_1'] = tbl_clientRates_21['PUBLISHED_RATES']*(1-DISCOUNT_TO_CLIENT_21)\n",
      "C:\\Users\\sliu\\AppData\\Local\\Temp\\ipykernel_18976\\2791379907.py:262: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  tbl_clientRates_21['CLIENT_RATES'] = round(tbl_clientRates_21['CLIENT_RATES_1'].apply(lambda x: max(x, CLIENT_MARGIN_MINS_21)),2)\n"
     ]
    }
   ],
   "source": [
    "#GRND ['WEIGHT_BREAK'] == '1-5lb'\n",
    "# slice the published rates\n",
    "tbl_clientRates_1 = tbl_publishedRates[(tbl_publishedRates['SERVICE_CODE'] == 'GRND') & (tbl_publishedRates['WEIGHT_LB'] <= 5)]\n",
    "\n",
    "# get DISCOUNT_TO_CLIENT(1-COLN) and CLIENT_MARGIN_MINS\n",
    "DISCOUNT_TO_CLIENT_1 = UPS_Rates.loc[(UPS_Rates['SERVICE_CODE'] == 'GRND') & (UPS_Rates['WEIGHT_BREAK'] == '1-5lb'), 'DISCOUNT_TO_CLIENT(1-COLN)'].values[0]\n",
    "if Round_Discount == 'YES':\n",
    "    DISCOUNT_TO_CLIENT_1 = round(DISCOUNT_TO_CLIENT_1,2)\n",
    "else:\n",
    "    DISCOUNT_TO_CLIENT_1 = DISCOUNT_TO_CLIENT_1\n",
    "CLIENT_MARGIN_MINS_1 = UPS_Rates.loc[(UPS_Rates['SERVICE_CODE'] == 'GRND') & (UPS_Rates['WEIGHT_BREAK'] == '1-5lb'), 'CLIENT_MARGIN_MINS'].values[0]\n",
    "\n",
    "# use max(published rates * (1-round(DISCOUNT_TO_CLIENT)), CLIENT_MARGIN_MINS) to get the client rate\n",
    "tbl_clientRates_1['CLIENT_RATES_1'] = tbl_clientRates_1['PUBLISHED_RATES']*(1-DISCOUNT_TO_CLIENT_1)\n",
    "tbl_clientRates_1['CLIENT_RATES'] = round(tbl_clientRates_1['CLIENT_RATES_1'].apply(lambda x: max(x, CLIENT_MARGIN_MINS_1)),2)\n",
    "tbl_clientRates_1 = tbl_clientRates_1.drop('CLIENT_RATES_1', axis=1)\n",
    "\n",
    "\n",
    "#GRND ['WEIGHT_BREAK'] == '6-10lb'\n",
    "tbl_clientRates_2 = tbl_publishedRates[(tbl_publishedRates['SERVICE_CODE'] == 'GRND') & (tbl_publishedRates['WEIGHT_LB'] <= 10) & (tbl_publishedRates['WEIGHT_LB'] >= 6)]\n",
    "DISCOUNT_TO_CLIENT_2 = UPS_Rates.loc[(UPS_Rates['SERVICE_CODE'] == 'GRND') & (UPS_Rates['WEIGHT_BREAK'] == '6-10lb'), 'DISCOUNT_TO_CLIENT(1-COLN)'].values[0]\n",
    "if Round_Discount == 'YES':\n",
    "    DISCOUNT_TO_CLIENT_2 = round(DISCOUNT_TO_CLIENT_2,2)\n",
    "else:\n",
    "    DISCOUNT_TO_CLIENT_2 = DISCOUNT_TO_CLIENT_2\n",
    "CLIENT_MARGIN_MINS_2 = UPS_Rates.loc[(UPS_Rates['SERVICE_CODE'] == 'GRND') & (UPS_Rates['WEIGHT_BREAK'] == '6-10lb'), 'CLIENT_MARGIN_MINS'].values[0]\n",
    "tbl_clientRates_2['CLIENT_RATES_1'] = tbl_clientRates_2['PUBLISHED_RATES']*(1-DISCOUNT_TO_CLIENT_2)\n",
    "tbl_clientRates_2['CLIENT_RATES'] = round(tbl_clientRates_2['CLIENT_RATES_1'].apply(lambda x: max(x, CLIENT_MARGIN_MINS_2)),2)\n",
    "tbl_clientRates_2 = tbl_clientRates_2.drop('CLIENT_RATES_1', axis=1)\n",
    "\n",
    "\n",
    "#GRND ['WEIGHT_BREAK'] == '11-20lb'\n",
    "tbl_clientRates_3 = tbl_publishedRates[(tbl_publishedRates['SERVICE_CODE'] == 'GRND') & (tbl_publishedRates['WEIGHT_LB'] <= 20) & (tbl_publishedRates['WEIGHT_LB'] >= 11)]\n",
    "DISCOUNT_TO_CLIENT_3 = UPS_Rates.loc[(UPS_Rates['SERVICE_CODE'] == 'GRND') & (UPS_Rates['WEIGHT_BREAK'] == '11-20lb'), 'DISCOUNT_TO_CLIENT(1-COLN)'].values[0]\n",
    "if Round_Discount == 'YES':\n",
    "    DISCOUNT_TO_CLIENT_3 = round(DISCOUNT_TO_CLIENT_3,2)\n",
    "else:\n",
    "    DISCOUNT_TO_CLIENT_3 = DISCOUNT_TO_CLIENT_3\n",
    "CLIENT_MARGIN_MINS_3 = UPS_Rates.loc[(UPS_Rates['SERVICE_CODE'] == 'GRND') & (UPS_Rates['WEIGHT_BREAK'] == '11-20lb'), 'CLIENT_MARGIN_MINS'].values[0]\n",
    "tbl_clientRates_3['CLIENT_RATES_1'] = tbl_clientRates_3['PUBLISHED_RATES']*(1-DISCOUNT_TO_CLIENT_3)\n",
    "tbl_clientRates_3['CLIENT_RATES'] = round(tbl_clientRates_3['CLIENT_RATES_1'].apply(lambda x: max(x, CLIENT_MARGIN_MINS_3)),2)\n",
    "tbl_clientRates_3 = tbl_clientRates_3.drop('CLIENT_RATES_1', axis=1)\n",
    "\n",
    "\n",
    "#GRND ['WEIGHT_BREAK'] == '21-30lb'\n",
    "tbl_clientRates_4 = tbl_publishedRates[(tbl_publishedRates['SERVICE_CODE'] == 'GRND') & (tbl_publishedRates['WEIGHT_LB'] <= 30) & (tbl_publishedRates['WEIGHT_LB'] >= 21)]\n",
    "DISCOUNT_TO_CLIENT_4 = UPS_Rates.loc[(UPS_Rates['SERVICE_CODE'] == 'GRND') & (UPS_Rates['WEIGHT_BREAK'] == '21-30lb'), 'DISCOUNT_TO_CLIENT(1-COLN)'].values[0]\n",
    "if Round_Discount == 'YES':\n",
    "    DISCOUNT_TO_CLIENT_4 = round(DISCOUNT_TO_CLIENT_4,2)\n",
    "else:\n",
    "    DISCOUNT_TO_CLIENT_4 = DISCOUNT_TO_CLIENT_4\n",
    "CLIENT_MARGIN_MINS_4 = UPS_Rates.loc[(UPS_Rates['SERVICE_CODE'] == 'GRND') & (UPS_Rates['WEIGHT_BREAK'] == '21-30lb'), 'CLIENT_MARGIN_MINS'].values[0]\n",
    "tbl_clientRates_4['CLIENT_RATES_1'] = tbl_clientRates_4['PUBLISHED_RATES']*(1-DISCOUNT_TO_CLIENT_4)\n",
    "tbl_clientRates_4['CLIENT_RATES'] = round(tbl_clientRates_4['CLIENT_RATES_1'].apply(lambda x: max(x, CLIENT_MARGIN_MINS_4)),2)\n",
    "tbl_clientRates_4 = tbl_clientRates_4.drop('CLIENT_RATES_1', axis=1)\n",
    "\n",
    "\n",
    "#GRND ['WEIGHT_BREAK'] == '31+lb'\n",
    "tbl_clientRates_5 = tbl_publishedRates[(tbl_publishedRates['SERVICE_CODE'] == 'GRND') & (tbl_publishedRates['WEIGHT_LB'] >= 31)]\n",
    "DISCOUNT_TO_CLIENT_5 = UPS_Rates.loc[(UPS_Rates['SERVICE_CODE'] == 'GRND') & (UPS_Rates['WEIGHT_BREAK'] == '31+lb'), 'DISCOUNT_TO_CLIENT(1-COLN)'].values[0]\n",
    "if Round_Discount == 'YES':\n",
    "    DISCOUNT_TO_CLIENT_5 = round(DISCOUNT_TO_CLIENT_5,2)\n",
    "else:\n",
    "    DISCOUNT_TO_CLIENT_5 = DISCOUNT_TO_CLIENT_5\n",
    "CLIENT_MARGIN_MINS_5 = UPS_Rates.loc[(UPS_Rates['SERVICE_CODE'] == 'GRND') & (UPS_Rates['WEIGHT_BREAK'] == '31+lb'), 'CLIENT_MARGIN_MINS'].values[0]\n",
    "tbl_clientRates_5['CLIENT_RATES_1'] = tbl_clientRates_5['PUBLISHED_RATES']*(1-DISCOUNT_TO_CLIENT_5)\n",
    "tbl_clientRates_5['CLIENT_RATES'] = round(tbl_clientRates_5['CLIENT_RATES_1'].apply(lambda x: max(x, CLIENT_MARGIN_MINS_5)),2)\n",
    "tbl_clientRates_5 = tbl_clientRates_5.drop('CLIENT_RATES_1', axis=1)\n",
    "\n",
    "\n",
    "#GRES \n",
    "#GRES ['WEIGHT_BREAK'] == '1-5lb'\n",
    "tbl_clientRates_6 = tbl_publishedRates[(tbl_publishedRates['SERVICE_CODE'] == 'GRES') & (tbl_publishedRates['WEIGHT_LB'] <= 5)]\n",
    "DISCOUNT_TO_CLIENT_6 = UPS_Rates.loc[(UPS_Rates['SERVICE_CODE'] == 'GRES') & (UPS_Rates['WEIGHT_BREAK'] == '1-5lb'), 'DISCOUNT_TO_CLIENT(1-COLN)'].values[0]\n",
    "if Round_Discount == 'YES':\n",
    "    DISCOUNT_TO_CLIENT_6 = round(DISCOUNT_TO_CLIENT_6,2)\n",
    "else:\n",
    "    DISCOUNT_TO_CLIENT_6 = DISCOUNT_TO_CLIENT_6\n",
    "CLIENT_MARGIN_MINS_6 = UPS_Rates.loc[(UPS_Rates['SERVICE_CODE'] == 'GRES') & (UPS_Rates['WEIGHT_BREAK'] == '1-5lb'), 'CLIENT_MARGIN_MINS'].values[0]\n",
    "tbl_clientRates_6['CLIENT_RATES_1'] = tbl_clientRates_6['PUBLISHED_RATES']*(1-DISCOUNT_TO_CLIENT_6)\n",
    "tbl_clientRates_6['CLIENT_RATES'] = round(tbl_clientRates_6['CLIENT_RATES_1'].apply(lambda x: max(x, CLIENT_MARGIN_MINS_6)),2)\n",
    "tbl_clientRates_6 = tbl_clientRates_6.drop('CLIENT_RATES_1', axis=1)\n",
    "\n",
    "\n",
    "#GRES ['WEIGHT_BREAK'] == '6-10lb'\n",
    "tbl_clientRates_7 = tbl_publishedRates[(tbl_publishedRates['SERVICE_CODE'] == 'GRES') & (tbl_publishedRates['WEIGHT_LB'] <= 10) & (tbl_publishedRates['WEIGHT_LB'] >= 6)]\n",
    "DISCOUNT_TO_CLIENT_7 = UPS_Rates.loc[(UPS_Rates['SERVICE_CODE'] == 'GRES') & (UPS_Rates['WEIGHT_BREAK'] == '6-10lb'), 'DISCOUNT_TO_CLIENT(1-COLN)'].values[0]\n",
    "if Round_Discount == 'YES':\n",
    "    DISCOUNT_TO_CLIENT_7 = round(DISCOUNT_TO_CLIENT_7,2)\n",
    "else:\n",
    "    DISCOUNT_TO_CLIENT_7 = DISCOUNT_TO_CLIENT_7\n",
    "CLIENT_MARGIN_MINS_7 = UPS_Rates.loc[(UPS_Rates['SERVICE_CODE'] == 'GRES') & (UPS_Rates['WEIGHT_BREAK'] == '6-10lb'), 'CLIENT_MARGIN_MINS'].values[0]\n",
    "tbl_clientRates_7['CLIENT_RATES_1'] = tbl_clientRates_7['PUBLISHED_RATES']*(1-DISCOUNT_TO_CLIENT_7)\n",
    "tbl_clientRates_7['CLIENT_RATES'] = round(tbl_clientRates_7['CLIENT_RATES_1'].apply(lambda x: max(x, CLIENT_MARGIN_MINS_7)),2)\n",
    "tbl_clientRates_7 = tbl_clientRates_7.drop('CLIENT_RATES_1', axis=1)\n",
    "\n",
    "\n",
    "#GRES ['WEIGHT_BREAK'] == '11-20lb'\n",
    "tbl_clientRates_8 = tbl_publishedRates[(tbl_publishedRates['SERVICE_CODE'] == 'GRES') & (tbl_publishedRates['WEIGHT_LB'] <= 20) & (tbl_publishedRates['WEIGHT_LB'] >= 11)]\n",
    "DISCOUNT_TO_CLIENT_8 = UPS_Rates.loc[(UPS_Rates['SERVICE_CODE'] == 'GRES') & (UPS_Rates['WEIGHT_BREAK'] == '11-20lb'), 'DISCOUNT_TO_CLIENT(1-COLN)'].values[0]\n",
    "if Round_Discount == 'YES':\n",
    "    DISCOUNT_TO_CLIENT_8 = round(DISCOUNT_TO_CLIENT_8,2)\n",
    "else:\n",
    "    DISCOUNT_TO_CLIENT_8 = DISCOUNT_TO_CLIENT_8\n",
    "CLIENT_MARGIN_MINS_8 = UPS_Rates.loc[(UPS_Rates['SERVICE_CODE'] == 'GRES') & (UPS_Rates['WEIGHT_BREAK'] == '11-20lb'), 'CLIENT_MARGIN_MINS'].values[0]\n",
    "tbl_clientRates_8['CLIENT_RATES_1'] = tbl_clientRates_8['PUBLISHED_RATES']*(1-DISCOUNT_TO_CLIENT_8)\n",
    "tbl_clientRates_8['CLIENT_RATES'] = round(tbl_clientRates_8['CLIENT_RATES_1'].apply(lambda x: max(x, CLIENT_MARGIN_MINS_8)),2)\n",
    "tbl_clientRates_8 = tbl_clientRates_8.drop('CLIENT_RATES_1', axis=1)\n",
    "\n",
    "\n",
    "#GRES ['WEIGHT_BREAK'] == '21-30lb'\n",
    "tbl_clientRates_9 = tbl_publishedRates[(tbl_publishedRates['SERVICE_CODE'] == 'GRES') & (tbl_publishedRates['WEIGHT_LB'] <= 30) & (tbl_publishedRates['WEIGHT_LB'] >= 21)]\n",
    "DISCOUNT_TO_CLIENT_9 = UPS_Rates.loc[(UPS_Rates['SERVICE_CODE'] == 'GRES') & (UPS_Rates['WEIGHT_BREAK'] == '21-30lb'), 'DISCOUNT_TO_CLIENT(1-COLN)'].values[0]\n",
    "if Round_Discount == 'YES':\n",
    "    DISCOUNT_TO_CLIENT_9 = round(DISCOUNT_TO_CLIENT_9,2)\n",
    "else:\n",
    "    DISCOUNT_TO_CLIENT_9 = DISCOUNT_TO_CLIENT_9\n",
    "CLIENT_MARGIN_MINS_9 = UPS_Rates.loc[(UPS_Rates['SERVICE_CODE'] == 'GRES') & (UPS_Rates['WEIGHT_BREAK'] == '21-30lb'), 'CLIENT_MARGIN_MINS'].values[0]\n",
    "tbl_clientRates_9['CLIENT_RATES_1'] = tbl_clientRates_9['PUBLISHED_RATES']*(1-DISCOUNT_TO_CLIENT_9)\n",
    "tbl_clientRates_9['CLIENT_RATES'] = round(tbl_clientRates_9['CLIENT_RATES_1'].apply(lambda x: max(x, CLIENT_MARGIN_MINS_9)),2)\n",
    "tbl_clientRates_9 = tbl_clientRates_9.drop('CLIENT_RATES_1', axis=1)\n",
    "\n",
    "\n",
    "#GRES ['WEIGHT_BREAK'] == '31+lb'\n",
    "tbl_clientRates_10 = tbl_publishedRates[(tbl_publishedRates['SERVICE_CODE'] == 'GRES') & (tbl_publishedRates['WEIGHT_LB'] >= 31)]\n",
    "DISCOUNT_TO_CLIENT_10 = UPS_Rates.loc[(UPS_Rates['SERVICE_CODE'] == 'GRES') & (UPS_Rates['WEIGHT_BREAK'] == '31+lb'), 'DISCOUNT_TO_CLIENT(1-COLN)'].values[0]\n",
    "if Round_Discount == 'YES':\n",
    "    DISCOUNT_TO_CLIENT_10 = round(DISCOUNT_TO_CLIENT_10,2)\n",
    "else:\n",
    "    DISCOUNT_TO_CLIENT_10 = DISCOUNT_TO_CLIENT_10\n",
    "CLIENT_MARGIN_MINS_10 = UPS_Rates.loc[(UPS_Rates['SERVICE_CODE'] == 'GRES') & (UPS_Rates['WEIGHT_BREAK'] == '31+lb'), 'CLIENT_MARGIN_MINS'].values[0]\n",
    "tbl_clientRates_10['CLIENT_RATES_1'] = tbl_clientRates_10['PUBLISHED_RATES']*(1-DISCOUNT_TO_CLIENT_10)\n",
    "tbl_clientRates_10['CLIENT_RATES'] = round(tbl_clientRates_10['CLIENT_RATES_1'].apply(lambda x: max(x, CLIENT_MARGIN_MINS_10)),2)\n",
    "tbl_clientRates_10 = tbl_clientRates_10.drop('CLIENT_RATES_1', axis=1)\n",
    "\n",
    "\n",
    "#SRPT\n",
    "#SRPT ['WEIGHT_BREAK'] == '1-3lb'\n",
    "tbl_clientRates_11 = tbl_publishedRates[(tbl_publishedRates['SERVICE_CODE'] == 'SRPT') & (tbl_publishedRates['WEIGHT_LB'] <= 3)]\n",
    "DISCOUNT_TO_CLIENT_11 = UPS_Rates.loc[(UPS_Rates['SERVICE_CODE'] == 'SRPT') & (UPS_Rates['WEIGHT_BREAK'] == '1-3lb'), 'DISCOUNT_TO_CLIENT(1-COLN)'].values[0]\n",
    "if Round_Discount == 'YES':\n",
    "    DISCOUNT_TO_CLIENT_11 = round(DISCOUNT_TO_CLIENT_11,2)\n",
    "else:\n",
    "    DISCOUNT_TO_CLIENT_11 = DISCOUNT_TO_CLIENT_11\n",
    "CLIENT_MARGIN_MINS_11 = UPS_Rates.loc[(UPS_Rates['SERVICE_CODE'] == 'SRPT') & (UPS_Rates['WEIGHT_BREAK'] == '1-3lb'), 'CLIENT_MARGIN_MINS'].values[0]\n",
    "tbl_clientRates_11['CLIENT_RATES_1'] = tbl_clientRates_11['PUBLISHED_RATES']*(1-DISCOUNT_TO_CLIENT_11)\n",
    "tbl_clientRates_11['CLIENT_RATES'] = round(tbl_clientRates_11['CLIENT_RATES_1'].apply(lambda x: max(x, CLIENT_MARGIN_MINS_11)),2)\n",
    "tbl_clientRates_11 = tbl_clientRates_11.drop('CLIENT_RATES_1', axis=1)\n",
    "\n",
    "\n",
    "#SRPT ['WEIGHT_BREAK'] == '4-6lb'\n",
    "tbl_clientRates_12 = tbl_publishedRates[(tbl_publishedRates['SERVICE_CODE'] == 'SRPT') & (tbl_publishedRates['WEIGHT_LB'] >= 4) & (tbl_publishedRates['WEIGHT_LB'] <= 6)]\n",
    "DISCOUNT_TO_CLIENT_12 = UPS_Rates.loc[(UPS_Rates['SERVICE_CODE'] == 'SRPT') & (UPS_Rates['WEIGHT_BREAK'] == '4-6lb'), 'DISCOUNT_TO_CLIENT(1-COLN)'].values[0]\n",
    "if Round_Discount == 'YES':\n",
    "    DISCOUNT_TO_CLIENT_12 = round(DISCOUNT_TO_CLIENT_12,2)\n",
    "else:\n",
    "    DISCOUNT_TO_CLIENT_12 = DISCOUNT_TO_CLIENT_12\n",
    "CLIENT_MARGIN_MINS_12 = UPS_Rates.loc[(UPS_Rates['SERVICE_CODE'] == 'SRPT') & (UPS_Rates['WEIGHT_BREAK'] == '4-6lb'), 'CLIENT_MARGIN_MINS'].values[0]\n",
    "tbl_clientRates_12['CLIENT_RATES_1'] = tbl_clientRates_12['PUBLISHED_RATES']*(1-DISCOUNT_TO_CLIENT_12)\n",
    "tbl_clientRates_12['CLIENT_RATES'] = round(tbl_clientRates_12['CLIENT_RATES_1'].apply(lambda x: max(x, CLIENT_MARGIN_MINS_12)),2)\n",
    "tbl_clientRates_12 = tbl_clientRates_12.drop('CLIENT_RATES_1', axis=1)\n",
    "\n",
    "\n",
    "#SRPT ['WEIGHT_BREAK'] == '7-8lb'\n",
    "tbl_clientRates_13 = tbl_publishedRates[(tbl_publishedRates['SERVICE_CODE'] == 'SRPT') & (tbl_publishedRates['WEIGHT_LB'] >= 7) & (tbl_publishedRates['WEIGHT_LB'] <= 8)]\n",
    "DISCOUNT_TO_CLIENT_13 = UPS_Rates.loc[(UPS_Rates['SERVICE_CODE'] == 'SRPT') & (UPS_Rates['WEIGHT_BREAK'] == '7-8lb'), 'DISCOUNT_TO_CLIENT(1-COLN)'].values[0]\n",
    "if Round_Discount == 'YES':\n",
    "    DISCOUNT_TO_CLIENT_13 = round(DISCOUNT_TO_CLIENT_13,2)\n",
    "else:\n",
    "    DISCOUNT_TO_CLIENT_13 = DISCOUNT_TO_CLIENT_13\n",
    "CLIENT_MARGIN_MINS_13 = UPS_Rates.loc[(UPS_Rates['SERVICE_CODE'] == 'SRPT') & (UPS_Rates['WEIGHT_BREAK'] == '7-8lb'), 'CLIENT_MARGIN_MINS'].values[0]\n",
    "tbl_clientRates_13['CLIENT_RATES_1'] = tbl_clientRates_13['PUBLISHED_RATES']*(1-DISCOUNT_TO_CLIENT_13)\n",
    "tbl_clientRates_13['CLIENT_RATES'] = round(tbl_clientRates_13['CLIENT_RATES_1'].apply(lambda x: max(x, CLIENT_MARGIN_MINS_13)),2)\n",
    "tbl_clientRates_13 = tbl_clientRates_13.drop('CLIENT_RATES_1', axis=1)\n",
    "\n",
    "\n",
    "#SRPT ['WEIGHT_BREAK'] == '9lb'\n",
    "tbl_clientRates_14 = tbl_publishedRates[(tbl_publishedRates['SERVICE_CODE'] == 'SRPT') & (tbl_publishedRates['WEIGHT_LB'] == 9)]\n",
    "DISCOUNT_TO_CLIENT_14 = UPS_Rates.loc[(UPS_Rates['SERVICE_CODE'] == 'SRPT') & (UPS_Rates['WEIGHT_BREAK'] == '9lb'), 'DISCOUNT_TO_CLIENT(1-COLN)'].values[0]\n",
    "if Round_Discount == 'YES':\n",
    "    DISCOUNT_TO_CLIENT_14 = round(DISCOUNT_TO_CLIENT_14,2)\n",
    "else:\n",
    "    DISCOUNT_TO_CLIENT_14 = DISCOUNT_TO_CLIENT_14\n",
    "CLIENT_MARGIN_MINS_14 = UPS_Rates.loc[(UPS_Rates['SERVICE_CODE'] == 'SRPT') & (UPS_Rates['WEIGHT_BREAK'] == '9lb'), 'CLIENT_MARGIN_MINS'].values[0]\n",
    "tbl_clientRates_14['CLIENT_RATES_1'] = tbl_clientRates_14['PUBLISHED_RATES']*(1-DISCOUNT_TO_CLIENT_14)\n",
    "tbl_clientRates_14['CLIENT_RATES'] = round(tbl_clientRates_14['CLIENT_RATES_1'].apply(lambda x: max(x, CLIENT_MARGIN_MINS_14)),2)\n",
    "tbl_clientRates_14 = tbl_clientRates_14.drop('CLIENT_RATES_1', axis=1)\n",
    "\n",
    "\n",
    "#Others:\n",
    "#UPS REDE, there is no discount, therefore REDE's client rate = published rate\n",
    "tbl_clientRates_15 = tbl_publishedRates[(tbl_publishedRates['SERVICE_CODE'] == 'REDE')]\n",
    "tbl_clientRates_15['CLIENT_RATES'] = tbl_clientRates_15['PUBLISHED_RATES']\n",
    "\n",
    "\n",
    "#UPS RED\n",
    "tbl_clientRates_16 = tbl_publishedRates[(tbl_publishedRates['SERVICE_CODE'] == 'RED')]\n",
    "DISCOUNT_TO_CLIENT_16 = UPS_Rates.loc[(UPS_Rates['SERVICE_CODE'] == 'RED'), 'DISCOUNT_TO_CLIENT(1-COLN)'].values[0]\n",
    "if Round_Discount == 'YES':\n",
    "    DISCOUNT_TO_CLIENT_16 = round(DISCOUNT_TO_CLIENT_16,2)\n",
    "else:\n",
    "    DISCOUNT_TO_CLIENT_16 = DISCOUNT_TO_CLIENT_16\n",
    "CLIENT_MARGIN_MINS_16 = UPS_Rates.loc[(UPS_Rates['SERVICE_CODE'] == 'RED'), 'CLIENT_MARGIN_MINS'].values[0]\n",
    "tbl_clientRates_16['CLIENT_RATES_1'] = tbl_clientRates_16['PUBLISHED_RATES']*(1-DISCOUNT_TO_CLIENT_16)\n",
    "tbl_clientRates_16['CLIENT_RATES'] = round(tbl_clientRates_16['CLIENT_RATES_1'].apply(lambda x: max(x, CLIENT_MARGIN_MINS_16)),2)\n",
    "tbl_clientRates_16 = tbl_clientRates_16.drop('CLIENT_RATES_1', axis=1)\n",
    "\n",
    "\n",
    "#UPS REDS, this does not have round up option (1/2)\n",
    "tbl_clientRates_17 = tbl_publishedRates[(tbl_publishedRates['SERVICE_CODE'] == 'REDS')]\n",
    "DISCOUNT_TO_CLIENT_17 = UPS_Rates.loc[(UPS_Rates['SERVICE_CODE'] == 'REDS'), 'DISCOUNT_TO_CLIENT(1-COLN)'].values[0]\n",
    "CLIENT_MARGIN_MINS_17 = UPS_Rates.loc[(UPS_Rates['SERVICE_CODE'] == 'REDS'), 'CLIENT_MARGIN_MINS'].values[0]\n",
    "tbl_clientRates_17['CLIENT_RATES_1'] = tbl_clientRates_17['PUBLISHED_RATES']*(1-DISCOUNT_TO_CLIENT_17)\n",
    "tbl_clientRates_17['CLIENT_RATES'] = round(tbl_clientRates_17['CLIENT_RATES_1'].apply(lambda x: max(x, CLIENT_MARGIN_MINS_17)),2)\n",
    "tbl_clientRates_17 = tbl_clientRates_17.drop('CLIENT_RATES_1', axis=1)\n",
    "\n",
    "\n",
    "#UPS 2DAM, this does not have round up option (2/2)\n",
    "tbl_clientRates_18 = tbl_publishedRates[(tbl_publishedRates['SERVICE_CODE'] == '2DAM')]\n",
    "DISCOUNT_TO_CLIENT_18 = UPS_Rates.loc[(UPS_Rates['SERVICE_CODE'] == '2DAM'), 'DISCOUNT_TO_CLIENT(1-COLN)'].values[0]\n",
    "CLIENT_MARGIN_MINS_18 = UPS_Rates.loc[(UPS_Rates['SERVICE_CODE'] == '2DAM'), 'CLIENT_MARGIN_MINS'].values[0]\n",
    "tbl_clientRates_18['CLIENT_RATES_1'] = tbl_clientRates_18['PUBLISHED_RATES']*(1-DISCOUNT_TO_CLIENT_18)\n",
    "tbl_clientRates_18['CLIENT_RATES'] = round(tbl_clientRates_18['CLIENT_RATES_1'].apply(lambda x: max(x, CLIENT_MARGIN_MINS_18)),2)\n",
    "tbl_clientRates_18 = tbl_clientRates_18.drop('CLIENT_RATES_1', axis=1)\n",
    "\n",
    "\n",
    "#UPS BLUE\n",
    "tbl_clientRates_19 = tbl_publishedRates[(tbl_publishedRates['SERVICE_CODE'] == 'BLUE')]\n",
    "DISCOUNT_TO_CLIENT_19 = UPS_Rates.loc[(UPS_Rates['SERVICE_CODE'] == 'BLUE'), 'DISCOUNT_TO_CLIENT(1-COLN)'].values[0]\n",
    "if Round_Discount == 'YES':\n",
    "    DISCOUNT_TO_CLIENT_19 = round(DISCOUNT_TO_CLIENT_19,2)\n",
    "else:\n",
    "    DISCOUNT_TO_CLIENT_19 = DISCOUNT_TO_CLIENT_19\n",
    "CLIENT_MARGIN_MINS_19 = UPS_Rates.loc[(UPS_Rates['SERVICE_CODE'] == 'BLUE') , 'CLIENT_MARGIN_MINS'].values[0]\n",
    "tbl_clientRates_19['CLIENT_RATES_1'] = tbl_clientRates_19['PUBLISHED_RATES']*(1-DISCOUNT_TO_CLIENT_19)\n",
    "tbl_clientRates_19['CLIENT_RATES'] = round(tbl_clientRates_19['CLIENT_RATES_1'].apply(lambda x: max(x, CLIENT_MARGIN_MINS_19)),2)\n",
    "tbl_clientRates_19 = tbl_clientRates_19.drop('CLIENT_RATES_1', axis=1)\n",
    "\n",
    "\n",
    "#UPS ORNG\n",
    "tbl_clientRates_20 = tbl_publishedRates[(tbl_publishedRates['SERVICE_CODE'] == 'ORNG')]\n",
    "DISCOUNT_TO_CLIENT_20 = UPS_Rates.loc[(UPS_Rates['SERVICE_CODE'] == 'ORNG'), 'DISCOUNT_TO_CLIENT(1-COLN)'].values[0]\n",
    "if Round_Discount == 'YES':\n",
    "    DISCOUNT_TO_CLIENT_20 = round(DISCOUNT_TO_CLIENT_20,2)\n",
    "else:\n",
    "    DISCOUNT_TO_CLIENT_20 = DISCOUNT_TO_CLIENT_20\n",
    "CLIENT_MARGIN_MINS_20 = UPS_Rates.loc[(UPS_Rates['SERVICE_CODE'] == 'ORNG'), 'CLIENT_MARGIN_MINS'].values[0]\n",
    "tbl_clientRates_20['CLIENT_RATES_1'] = tbl_clientRates_20['PUBLISHED_RATES']*(1-DISCOUNT_TO_CLIENT_20)\n",
    "tbl_clientRates_20['CLIENT_RATES'] = round(tbl_clientRates_20['CLIENT_RATES_1'].apply(lambda x: max(x, CLIENT_MARGIN_MINS_20)),2)\n",
    "tbl_clientRates_20 = tbl_clientRates_20.drop('CLIENT_RATES_1', axis=1)\n",
    "\n",
    "\n",
    "#UPS SRPT<1\n",
    "tbl_clientRates_21 = tbl_publishedRates[(tbl_publishedRates['SERVICE_CODE'] == 'SRPT<1')]\n",
    "DISCOUNT_TO_CLIENT_21 = UPS_Rates.loc[(UPS_Rates['SERVICE_CODE'] == 'SRPT<1'), 'DISCOUNT_TO_CLIENT(1-COLN)'].values[0]\n",
    "if Round_Discount == 'YES':\n",
    "    DISCOUNT_TO_CLIENT_21 = round(DISCOUNT_TO_CLIENT_21,2)\n",
    "else:\n",
    "    DISCOUNT_TO_CLIENT_21 = DISCOUNT_TO_CLIENT_21\n",
    "CLIENT_MARGIN_MINS_21 = UPS_Rates.loc[(UPS_Rates['SERVICE_CODE'] == 'SRPT<1'), 'CLIENT_MARGIN_MINS'].values[0]\n",
    "tbl_clientRates_21['CLIENT_RATES_1'] = tbl_clientRates_21['PUBLISHED_RATES']*(1-DISCOUNT_TO_CLIENT_21)\n",
    "tbl_clientRates_21['CLIENT_RATES'] = round(tbl_clientRates_21['CLIENT_RATES_1'].apply(lambda x: max(x, CLIENT_MARGIN_MINS_21)),2)\n",
    "tbl_clientRates_21 = tbl_clientRates_21.drop('CLIENT_RATES_1', axis=1)\n",
    "\n",
    "\n",
    "\n",
    "tbl_ups_clientRates = pd.concat([tbl_clientRates_1, tbl_clientRates_2, tbl_clientRates_3, tbl_clientRates_4, \n",
    "                    tbl_clientRates_5, tbl_clientRates_6, tbl_clientRates_7, tbl_clientRates_8,\n",
    "                    tbl_clientRates_9, tbl_clientRates_10, tbl_clientRates_11, tbl_clientRates_12,\n",
    "                    tbl_clientRates_13, tbl_clientRates_14, tbl_clientRates_15, tbl_clientRates_16,\n",
    "                    tbl_clientRates_17, tbl_clientRates_18, tbl_clientRates_19, tbl_clientRates_20,\n",
    "                    tbl_clientRates_21])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1076,
   "id": "governmental-perspective",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sliu\\AppData\\Local\\Temp\\ipykernel_18976\\6757182.py:18: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  tbl_dhl_clientRates['CLIENT_RATES'] = round(tbl_dhl_clientRates['BARRETT_RATES']/(1 - Margin_on_DHL),2)\n",
      "C:\\Users\\sliu\\AppData\\Local\\Temp\\ipykernel_18976\\6757182.py:19: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  tbl_usps_clientRates['CLIENT_RATES'] = round(tbl_usps_clientRates['BARRETT_RATES']/(1 - Margin_on_USPS),2)\n",
      "C:\\Users\\sliu\\AppData\\Local\\Temp\\ipykernel_18976\\6757182.py:20: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  tbl_mi_clientRates['CLIENT_RATES'] = round(tbl_mi_clientRates['BARRETT_RATES']/(1 - Margin_on_MI),2)\n"
     ]
    }
   ],
   "source": [
    "# Create DHL and USPS Client Rate\n",
    "tbl_dhl_clientRates = tbl_publishedRates[(tbl_publishedRates['SERVICE_CODE'].str.startswith('DHLG_')) |\n",
    "                                         (tbl_publishedRates['SERVICE_CODE'].str.startswith('DHLG<1_')) | \n",
    "                                         (tbl_publishedRates['SERVICE_CODE'].str.startswith('DHLE_')) |\n",
    "                                         (tbl_publishedRates['SERVICE_CODE'].str.startswith('DHLE<1_')) |\n",
    "                                         (tbl_publishedRates['SERVICE_CODE'].str.startswith('DHLEM_'))]\n",
    "\n",
    "tbl_usps_clientRates = tbl_publishedRates[(tbl_publishedRates['SERVICE_CODE'] == 'USPSAG') | \n",
    "                                          (tbl_publishedRates['SERVICE_CODE'] == 'USPSAP') |\n",
    "                                          (tbl_publishedRates['SERVICE_CODE'] == 'USPSAG_CI') | \n",
    "                                          (tbl_publishedRates['SERVICE_CODE'] == 'USPSAP_CI')]\n",
    "\n",
    "\n",
    "tbl_mi_clientRates = tbl_publishedRates[(tbl_publishedRates['SERVICE_CODE'] == 'MIPLE') | \n",
    "                                          (tbl_publishedRates['SERVICE_CODE'] == 'MIPH')]\n",
    "\n",
    "\n",
    "tbl_dhl_clientRates['CLIENT_RATES'] = round(tbl_dhl_clientRates['BARRETT_RATES']/(1 - Margin_on_DHL),2)\n",
    "tbl_usps_clientRates['CLIENT_RATES'] = round(tbl_usps_clientRates['BARRETT_RATES']/(1 - Margin_on_USPS),2)\n",
    "tbl_mi_clientRates['CLIENT_RATES'] = round(tbl_mi_clientRates['BARRETT_RATES']/(1 - Margin_on_MI),2)\n",
    "\n",
    "\n",
    "tbl_clientRates = pd.concat([tbl_ups_clientRates, tbl_dhl_clientRates, tbl_usps_clientRates, tbl_mi_clientRates])\n",
    "tbl_clientRates = tbl_clientRates.reindex(columns=['SERVICE_CODE', 'SERVICE_PLAINTEXT', 'ZONE', 'WEIGHT_OZ', 'WEIGHT_LB', 'CUBIC_FT', 'PUBLISHED_RATES', 'CLIENT_RATES', 'VERSION', 'START_DATE', 'END_DATE'])\n",
    "\n",
    "#get the finalized tbl_publishedRates\n",
    "tbl_clientRates = tbl_BarrettRates.merge(tbl_clientRates[['SERVICE_CODE', 'SERVICE_PLAINTEXT', 'ZONE', 'WEIGHT_OZ', 'WEIGHT_LB', 'CUBIC_FT', 'VERSION', 'START_DATE', 'END_DATE', 'CLIENT_RATES']], left_on=['SERVICE_CODE', 'SERVICE_PLAINTEXT', 'ZONE', 'WEIGHT_OZ', 'WEIGHT_LB', 'VERSION', 'START_DATE', 'END_DATE', 'CUBIC_FT'], right_on=['SERVICE_CODE', 'SERVICE_PLAINTEXT', 'ZONE', 'WEIGHT_OZ', 'WEIGHT_LB', 'VERSION', 'START_DATE', 'END_DATE', 'CUBIC_FT'], how='left')\n",
    "#tbl_clientRates.to_csv('test.csv')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hollow-unemployment",
   "metadata": {},
   "source": [
    "# Step 2 - a. Create UPS accessorial rates\n",
    "*This rate is separated from freight rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1077,
   "id": "english-chester",
   "metadata": {},
   "outputs": [],
   "source": [
    "# UPSAccessorial_2023 has 2 tabs, UPS_Rates and UPS_Accessorial\n",
    "xls = pd.ExcelFile('UPSAccessorial_2023.xlsx')\n",
    "UPS_Accessorial = pd.read_excel(xls, 'UPS_Accessorial')\n",
    "\n",
    "# Add Net_charge column\n",
    "UPS_Accessorial['NET_CHARGE'] = UPS_Accessorial['PUBLISHED_CHARGE']*(1-UPS_Accessorial['DISCOUNT'])\n",
    "\n",
    "# Add ADD_MARGIN column\n",
    "def k(col):   \n",
    "    if UPS_Accessorial['ACCESSORIAL'].str.contains('Air').any():\n",
    "        return col['NET_CHARGE']/(1-Margin_on_Value_Add_AIR)\n",
    "    else:\n",
    "        return col['NET_CHARGE']/(1-Margin_on_Value_Add_GRND)\n",
    "\n",
    "UPS_Accessorial['ADD_MARGIN'] = UPS_Accessorial.apply(k, axis=1)  \n",
    "\n",
    "\n",
    "# Assign a number to a specific cell based on column name and other row values\n",
    "column_name = 'ADD_MARGIN'\n",
    "condition1 = UPS_Accessorial['ACCESSORIAL'] == 'Remote Area' \n",
    "condition2 = UPS_Accessorial['ACCESSORIAL'] == 'Remote Area Alaska' \n",
    "condition3 = UPS_Accessorial['ACCESSORIAL'] == 'Remote Area Hawaii' \n",
    "\n",
    "UPS_Accessorial.loc[condition1, column_name] = 13.05\n",
    "UPS_Accessorial.loc[condition2, column_name] = 38.00\n",
    "UPS_Accessorial.loc[condition3, column_name] = 13.05\n",
    "\n",
    "\n",
    "# Add PERCENTAGE column\n",
    "def l(col):   \n",
    "    if Round_Discount == 'YES':\n",
    "        return 1 - round((col['ADD_MARGIN']/col['PUBLISHED_CHARGE']),2)\n",
    "    else:\n",
    "        return 1 - col['ADD_MARGIN']/col['PUBLISHED_CHARGE']\n",
    "\n",
    "UPS_Accessorial['PERCENTAGE'] = UPS_Accessorial.apply(l, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "quick-belarus",
   "metadata": {},
   "source": [
    "# Step 2 - b. Prepare all other rates for rerate\n",
    "*These rates do not need a table to house"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1078,
   "id": "actual-calendar",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List all UPS Surcharge rates for rerate\n",
    "Resi_Surcharge_Air = UPS_Accessorial.loc[(UPS_Accessorial['ACCESSORIAL'] == 'Resi Surcharge (Air)'), 'ADD_MARGIN'].values[0]\n",
    "Resi_Surcharge_Ground = UPS_Accessorial.loc[(UPS_Accessorial['ACCESSORIAL'] == 'Resi Surcharge (Ground)'), 'ADD_MARGIN'].values[0]\n",
    "DAS_Comm_Ground = UPS_Accessorial.loc[(UPS_Accessorial['ACCESSORIAL'] == 'DAS Comm (Ground)'), 'ADD_MARGIN'].values[0]\n",
    "DAS_Comm_Extended_Ground = UPS_Accessorial.loc[(UPS_Accessorial['ACCESSORIAL'] == 'DAS Comm Extended (Ground)'), 'ADD_MARGIN'].values[0]\n",
    "DAS_Resi_Ground = UPS_Accessorial.loc[(UPS_Accessorial['ACCESSORIAL'] == 'DAS Resi (Ground)'), 'ADD_MARGIN'].values[0]\n",
    "DAS_Resi_Extended_Ground = UPS_Accessorial.loc[(UPS_Accessorial['ACCESSORIAL'] == 'DAS Resi Extended (Ground)'), 'ADD_MARGIN'].values[0]\n",
    "DAS_Comm_Air = UPS_Accessorial.loc[(UPS_Accessorial['ACCESSORIAL'] == 'DAS Comm (Air)'), 'ADD_MARGIN'].values[0]\n",
    "DAS_Comm_Extended_Air = UPS_Accessorial.loc[(UPS_Accessorial['ACCESSORIAL'] == 'DAS Comm Extended (Air)'), 'ADD_MARGIN'].values[0]\n",
    "DAS_Resi_Air = UPS_Accessorial.loc[(UPS_Accessorial['ACCESSORIAL'] == 'DAS Resi (Air)'), 'ADD_MARGIN'].values[0]\n",
    "DAS_Resi_Extended_Air = UPS_Accessorial.loc[(UPS_Accessorial['ACCESSORIAL'] == 'DAS Resi Extended (Air)'), 'ADD_MARGIN'].values[0]\n",
    "DAS_SurePost = UPS_Accessorial.loc[(UPS_Accessorial['ACCESSORIAL'] == 'DAS SurePost'), 'ADD_MARGIN'].values[0]\n",
    "DAS_Extended_SurePost = UPS_Accessorial.loc[(UPS_Accessorial['ACCESSORIAL'] == 'DAS Extended SurePost'), 'ADD_MARGIN'].values[0]\n",
    "Remote_Area = 13.05\n",
    "Remote_Area_Alaska = 38.00\n",
    "Remote_Area_Hawaii = 13.05\n",
    "\n",
    "\n",
    "# Get FSC ready for rerate\n",
    "xls = pd.ExcelFile('FSC.xlsx')\n",
    "UPS_FSC = pd.read_excel(xls, 'UPS_FSC')\n",
    "DHL_FSC = pd.read_excel(xls, 'DHL_FSC')\n",
    "\n",
    "#Get UPS_FSC doc ready\n",
    "UPS_FSC['SHIP_DATE_START'] = UPS_FSC['Ship Date']\n",
    "UPS_FSC['Ship Date'] = pd.to_datetime(UPS_FSC['Ship Date']) \n",
    "UPS_FSC['SHIP_DATE_END'] = UPS_FSC['Ship Date'] + pd.Timedelta(days=6)\n",
    "UPS_FSC = UPS_FSC.drop('Ship Date', axis=1)\n",
    "\n",
    "UPS_FSC['SHIP_DATE_START'] = pd.to_datetime(UPS_FSC['SHIP_DATE_START']).dt.date\n",
    "UPS_FSC['SHIP_DATE_END'] = pd.to_datetime(UPS_FSC['SHIP_DATE_END']).dt.date\n",
    "\n",
    "#Get DHL_FSC doc ready\n",
    "from datetime import timedelta\n",
    "from datetime import date\n",
    "DHL_FSC = DHL_FSC.sort_values(by='Ship Date', ascending=True)\n",
    "DHL_FSC['Ship Date'] = pd.to_datetime(DHL_FSC['Ship Date']) \n",
    "DHL_FSC['SHIP_DATE_START'] = DHL_FSC['Ship Date']\n",
    "DHL_FSC['SHIP_DATE_END'] = DHL_FSC['SHIP_DATE_START'].shift(-1) - timedelta(days=1)\n",
    "\n",
    "# Calculate a date that is 2 years in the future\n",
    "future_date = date.today().replace(year=date.today().year + 2)\n",
    "# Update the 'SHIP_DATE_END' column for rows with the latest 'SHIP_DATE_START'\n",
    "DHL_FSC.loc[DHL_FSC['SHIP_DATE_START'] == max(DHL_FSC['SHIP_DATE_START']), ['SHIP_DATE_END']] = future_date\n",
    "\n",
    "DHL_FSC['SHIP_DATE_END'] = pd.to_datetime(DHL_FSC['SHIP_DATE_END']) \n",
    "DHL_FSC = DHL_FSC.drop('Ship Date', axis=1)\n",
    "\n",
    "# List UPS AHS rates for rerate\n",
    "AH_Girth_2 = UPS_Accessorial.loc[(UPS_Accessorial['ACCESSORIAL'] == 'Additional Handling Girth 2'), 'ADD_MARGIN'].values[0]\n",
    "AH_Girth_3_4 = UPS_Accessorial.loc[(UPS_Accessorial['ACCESSORIAL'] == 'Additional Handling Girth 3-4'), 'ADD_MARGIN'].values[0]\n",
    "AH_Girth_5_up = UPS_Accessorial.loc[(UPS_Accessorial['ACCESSORIAL'] == 'Additional Handling Girth 5+'), 'ADD_MARGIN'].values[0]\n",
    "\n",
    "AH_L_2 = UPS_Accessorial.loc[(UPS_Accessorial['ACCESSORIAL'] == 'Additional Handling L 2'), 'ADD_MARGIN'].values[0]\n",
    "AH_L_3_4 = UPS_Accessorial.loc[(UPS_Accessorial['ACCESSORIAL'] == 'Additional Handling L 3-4'), 'ADD_MARGIN'].values[0]\n",
    "AH_L_5_up = UPS_Accessorial.loc[(UPS_Accessorial['ACCESSORIAL'] == 'Additional Handling L 5+'), 'ADD_MARGIN'].values[0]\n",
    "\n",
    "AH_W_2 = UPS_Accessorial.loc[(UPS_Accessorial['ACCESSORIAL'] == 'Additional Handling W 2'), 'ADD_MARGIN'].values[0]\n",
    "AH_W_3_4 = UPS_Accessorial.loc[(UPS_Accessorial['ACCESSORIAL'] == 'Additional Handling W 3-4'), 'ADD_MARGIN'].values[0]\n",
    "AH_W_5_up = UPS_Accessorial.loc[(UPS_Accessorial['ACCESSORIAL'] == 'Additional Handling W 5+'), 'ADD_MARGIN'].values[0]\n",
    "\n",
    "AH_WGT_2 = UPS_Accessorial.loc[(UPS_Accessorial['ACCESSORIAL'] == 'Additional Handling WGT 2'), 'ADD_MARGIN'].values[0]\n",
    "AH_WGT_3_4 = UPS_Accessorial.loc[(UPS_Accessorial['ACCESSORIAL'] == 'Additional Handling WGT 3-4'), 'ADD_MARGIN'].values[0]\n",
    "AH_WGT_5_up = UPS_Accessorial.loc[(UPS_Accessorial['ACCESSORIAL'] == 'Additional Handling WGT 5+'), 'ADD_MARGIN'].values[0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "induced-monroe",
   "metadata": {},
   "source": [
    "# Step 3 - a. Prepare PLD for rerate\n",
    "* Regardless of various types of PLD, Client PLD, Barrett PLD, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "earlier-grenada",
   "metadata": {},
   "source": [
    "# Load PLD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1079,
   "id": "lasting-primary",
   "metadata": {},
   "outputs": [],
   "source": [
    "#change the corresponding column names to below names before upload. \n",
    "#Requested columns and the corresponding type:     \n",
    "# ---  ------             --------------  -----         \n",
    "#1   TRACKING_NUMBER    \n",
    "#2   SHIP_DATE             \n",
    "#3   OLD_SERVICE --(optional)      \n",
    "#4   ZIP_CODE               \n",
    "#5   ZONE --(optional)               \n",
    "#6   DIMENSIONS or LWH           \n",
    "#7   ACTUAL_WEIGHT               \n",
    "#8   RESIDENTIAL_FLAG       \n",
    "#9   INTERNATIONAL_FLAG    \n",
    "\n",
    "#Load Sample PLD\n",
    "xls = pd.ExcelFile('QuotePLD_2023.xlsx')\n",
    "tbl_BP_Master = pd.read_excel(xls, 'Sheet1')\n",
    "#tbl_BP.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1080,
   "id": "c3c60bdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#When the original file is too large, make a copy\n",
    "tbl_BP = tbl_BP_Master.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1081,
   "id": "4996b6fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean the empties if necessary\n",
    "tbl_BP = tbl_BP.dropna(axis = 0, how = 'all')\n",
    "tbl_BP = tbl_BP.dropna(axis = 1, how = 'all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1082,
   "id": "4ed09bdf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(126511, 21)"
      ]
     },
     "execution_count": 1082,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tbl_BP.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5931f8f",
   "metadata": {},
   "source": [
    "# Clean PLD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1083,
   "id": "e684aa3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 126511 entries, 0 to 126510\n",
      "Data columns (total 21 columns):\n",
      " #   Column              Non-Null Count   Dtype         \n",
      "---  ------              --------------   -----         \n",
      " 0   CustomerID          126511 non-null  int64         \n",
      " 1   CustomerName        126511 non-null  object        \n",
      " 2   Facility            126511 non-null  object        \n",
      " 3   BarrettOrderNumber  126511 non-null  object        \n",
      " 4   Reference           126511 non-null  object        \n",
      " 5   PoNumber            117175 non-null  object        \n",
      " 6   TRACKING_NUMBER     126511 non-null  object        \n",
      " 7   SHIP_DATE           126511 non-null  datetime64[ns]\n",
      " 8   OLD_SERVICE         126511 non-null  object        \n",
      " 9   Shipper             126511 non-null  object        \n",
      " 10  ShipToName          126511 non-null  object        \n",
      " 11  Contact             27292 non-null   object        \n",
      " 12  City                126511 non-null  object        \n",
      " 13  State               126511 non-null  object        \n",
      " 14  ZIP_CODE            126511 non-null  int64         \n",
      " 15  Country             126511 non-null  object        \n",
      " 16  ZONE                125956 non-null  float64       \n",
      " 17  Quantity            119267 non-null  float64       \n",
      " 18  DIMENSIONS          126511 non-null  object        \n",
      " 19  ACTUAL_WEIGHT       126511 non-null  float64       \n",
      " 20  RESIDENTIAL_FLAG    126511 non-null  bool          \n",
      "dtypes: bool(1), datetime64[ns](1), float64(3), int64(2), object(14)\n",
      "memory usage: 19.4+ MB\n"
     ]
    }
   ],
   "source": [
    "#The column type must match\n",
    "#Requested columns and the corresponding type:     \n",
    "# ---  ------             --------------  -----         \n",
    "#1   TRACKING_NUMBER        ---------------------      int64 (or object) \"can manually create one before upload\"       \n",
    "#2   SHIP_DATE              ---------------------      datetime64[ns] \"can manually fill up before upload\"\n",
    "#3   OLD_SERVICE (optional) ---------------------      object        \n",
    "#4   ZIP_CODE               ---------------------      object\n",
    "#5   ZONE (optional)        ---------------------      object          \n",
    "#6   DIMENSIONS (or LWH)    ---------------------      object (or float64 or int64)       \n",
    "#7   ACTUAL_WEIGHT (lb as default)---------------      float64       \n",
    "#8   RESIDENTIAL_FLAG       ---------------------      bool   \n",
    "tbl_BP.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1084,
   "id": "4558a9d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#update column type\n",
    "tbl_BP['SHIP_DATE'] = pd.to_datetime(tbl_BP['SHIP_DATE'])\n",
    "tbl_BP['ZIP_CODE'] = tbl_BP['ZIP_CODE'].astype(str)\n",
    "tbl_BP['ZONE'] = tbl_BP['ZONE'].astype(str)\n",
    "tbl_BP['ACTUAL_WEIGHT'] = tbl_BP['ACTUAL_WEIGHT'].astype(float)\n",
    "tbl_BP['RESIDENTIAL_FLAG'] = tbl_BP['RESIDENTIAL_FLAG'].astype('bool')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1085,
   "id": "69616d63",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['77433', '12883', '6762', ..., '47841', '8609', '50060'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 1085,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Check the Zip_Code values\n",
    "tbl_BP['ZIP_CODE'].unique() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1086,
   "id": "ca339d7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sliu\\AppData\\Local\\Temp\\ipykernel_18976\\2751569339.py:2: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  tbl_BP['ZIP_CODE'] = tbl_BP['ZIP_CODE'].astype(str).str.replace('\\.0$', '').astype(int)\n"
     ]
    }
   ],
   "source": [
    "tbl_BP['ZIP_CODE'] = tbl_BP['ZIP_CODE'].replace('nan', np.nan).astype(float).fillna(0).astype(int)\n",
    "tbl_BP['ZIP_CODE'] = tbl_BP['ZIP_CODE'].astype(str).str.replace('\\.0$', '').astype(int)\n",
    "tbl_BP['ZIP_CODE'] = tbl_BP['ZIP_CODE'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1087,
   "id": "d056af57",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['4.0', '6.0', '7.0', '5.0', '2.0', '3.0', '8.0', 'nan', '1.0',\n",
       "       '9.0', '11.0', '12.0', '10.0', '13.0'], dtype=object)"
      ]
     },
     "execution_count": 1087,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Check the value of Zone if exists\n",
    "tbl_BP['ZONE'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1088,
   "id": "6a9e15cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sliu\\AppData\\Local\\Temp\\ipykernel_18976\\4054397438.py:2: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  tbl_BP['ZONE'] = tbl_BP['ZONE'].astype(str).str.replace('\\.0$', '').astype(int)\n"
     ]
    }
   ],
   "source": [
    "tbl_BP['ZONE'] = tbl_BP['ZONE'].replace('nan', np.nan).astype(float).fillna(0).astype(int)\n",
    "tbl_BP['ZONE'] = tbl_BP['ZONE'].astype(str).str.replace('\\.0$', '').astype(int)\n",
    "tbl_BP['ZONE'] = tbl_BP['ZONE'].astype(str)\n",
    "#tbl_BP['ZONE'] = tbl_BP['ZONE'].str.replace('102', '2')\n",
    "#tbl_BP['ZONE'] = tbl_BP['ZONE'].str.replace('108', '8')\n",
    "#tbl_BP['ZONE'] = tbl_BP['ZONE'].str.replace('104', '4')\n",
    "#tbl_BP['ZONE'] = tbl_BP['ZONE'].str.replace('44', '9')\n",
    "#tbl_BP['ZONE'] = tbl_BP['ZONE'].str.replace('46', '11')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1089,
   "id": "exact-candle",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Drop duplicates based on ID column by only keeping the unique ID associated with the most recent Ship Date.  \n",
    "#If there are rows where both tracking no and ship date are duplicated, only keep the first one.\n",
    "tbl_BP = tbl_BP.sort_values('SHIP_DATE', ascending=False)\n",
    "tbl_BP = tbl_BP.drop_duplicates(subset='TRACKING_NUMBER', keep='first')\n",
    "tbl_BP = tbl_BP.drop_duplicates(subset='TRACKING_NUMBER', keep='first')\n",
    "#tbl_BP.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adjusted-period",
   "metadata": {},
   "source": [
    "# Format PLD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1090,
   "id": "satisfied-mathematics",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Skip this if LWH Columns exist already. This line of code needs improvement. \n",
    "#turn dimensions into L W H columns. no need to run this if they are already separate. \n",
    "tbl_BP[['L', 'W', 'H']] = tbl_BP['DIMENSIONS'].str.split('x', expand=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1091,
   "id": "01ccea89",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Skip this if LWH Columns are integers. \n",
    "tbl_BP[['L', 'W', 'H']] = tbl_BP[['L', 'W', 'H']].apply(pd.to_numeric)\n",
    "tbl_BP['L'] = tbl_BP['L'].apply(lambda x: round(x,0) if x - math.floor(x) < 0.5 else np.ceil(x))\n",
    "tbl_BP['W'] = tbl_BP['W'].apply(lambda x: round(x,0) if x - math.floor(x) < 0.5 else np.ceil(x))\n",
    "tbl_BP['H'] = tbl_BP['H'].apply(lambda x: round(x,0) if x - math.floor(x) < 0.5 else np.ceil(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1092,
   "id": "fuzzy-telephone",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure that the columns are of a numeric type (float)\n",
    "tbl_BP['L'] = tbl_BP['L'].astype(float)\n",
    "tbl_BP['W'] = tbl_BP['W'].astype(float)\n",
    "tbl_BP['H'] = tbl_BP['H'].astype(float)\n",
    "\n",
    "# Create a temporary DataFrame with the columns 'L', 'W', and 'H'\n",
    "temp_df = tbl_BP[['L', 'W', 'H']]\n",
    "\n",
    "# Sort each row in descending order and reverse the columns to maintain the order L, W, H\n",
    "sorted_temp_df = np.sort(temp_df.values, axis=1)[:, ::-1]\n",
    "\n",
    "# Assign the sorted values back to the original DataFrame\n",
    "tbl_BP[['L', 'W', 'H']] = sorted_temp_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1093,
   "id": "aquatic-white",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create columns of GIRTH_AND_L and CUBIC_INCH\n",
    "tbl_BP['GIRTH_AND_L'] = round(tbl_BP['W'] * 2 + tbl_BP['H'] * 2 + tbl_BP['L'],0)\n",
    "tbl_BP['CUBIC_INCH'] = round(tbl_BP['W'] * tbl_BP['H'] * tbl_BP['L'],0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1094,
   "id": "modern-league",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create ACTUAL_WEIGHT_LB(rounded up to the nearest lb) and ACTUAL_WEIGHT_OZ(rounded up to the nearest oz). \n",
    "# Still keep the same value if the cell is empty string or has any missing values.  \n",
    "tbl_BP['ACTUAL_WEIGHT_LB'] = tbl_BP['ACTUAL_WEIGHT'].apply(lambda x: np.ceil(x) if pd.notna(x) and x != '' else x)\n",
    "tbl_BP['ACTUAL_WEIGHT_OZ_1'] = tbl_BP['ACTUAL_WEIGHT']*16\n",
    "tbl_BP['ACTUAL_WEIGHT_OZ'] = tbl_BP['ACTUAL_WEIGHT_OZ_1'].apply(lambda x: np.ceil(x) if pd.notna(x) and x != '' else x)\n",
    "tbl_BP = tbl_BP.drop('ACTUAL_WEIGHT_OZ_1', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1095,
   "id": "concerned-temperature",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Skip this if the left 5 digits are provided already\n",
    "# get the left 5 digits of the zip code or fill the zip to 5 digits\n",
    "#import re\n",
    "#split_col = tbl_BP['ZIP_CODE'].str.split('-')\n",
    "#left_col = split_col.str[0]\n",
    "#tbl_BP['ZIP_CODE'] = left_col\n",
    "#tbl_BP['ZIP_CODE'] = tbl_BP['ZIP_CODE'].apply(lambda x: re.match(r'\\d{5}', str(x)).group(0) if (pd.notna(x) and re.match(r'\\d{5}', str(x))) else '')\n",
    "# fill the 3 and 4 digits zip to 5 digits\n",
    "tbl_BP['ZIP_CODE'] = tbl_BP['ZIP_CODE'].astype(str)\n",
    "tbl_BP['ZIP_CODE'] = tbl_BP['ZIP_CODE'].apply(lambda x: '0' + x if len(x) == 4 else ('00' + x if (len(x) == 3) else x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "expected-assignment",
   "metadata": {},
   "source": [
    "# Step 3 - b. Rerate Begin\n",
    "#====================UPS====================\n",
    "\n",
    "#REDE (UPS NDA Early): commercial & residential, lb    \n",
    "\n",
    "#RED (UPS NDA): commercial & residential, lb\n",
    "\n",
    "#REDS (UPS NDA Saver): commercial & residential, lb\n",
    "\n",
    "#2DAM (UPS 2DA A.M.): commercial & residential, lb\n",
    "\n",
    "#BLUE (UPS 2DA): commercial & residential, lb\n",
    "\n",
    "#ORNG (UPS 3DA): commercial & residential, lb\n",
    "\n",
    "#GRND (UPS Ground Commercial): commercial only, lb\n",
    "\n",
    "#GRES (UPS Ground Residential): residential only, lb\n",
    "\n",
    "#SRPT<1 (UPS Surepost 1#>): commercial & residential, oz\n",
    "\n",
    "#SRPT (UPS Surepost): commercial & residential, lb\n",
    "\n",
    "#====================DHL====================\n",
    "\n",
    "#DHLG (DHL SmartMail Parcel Plus Ground 1-25): commercial & residential, lb\n",
    "\n",
    "#DHLG<1 (DHL SmartMail Parcel Ground < 1lb): commercial & residential, oz\n",
    "\n",
    "#DHLE (DHL SmartMail Parcel Plus Expedited 1-25): commercial & residential, lb\n",
    "\n",
    "#DHLE<1 (DHL SmartMail Parcel Expedited  < 1lb): commercial & residential, oz\n",
    "\n",
    "#DHLEM (DHL SmartMail Parcel Expedited Max): commercial & residential, both oz and lb\n",
    "\n",
    "#====================USPS====================\n",
    "\n",
    "#USPSAG (USPS Auctane GRND): commercial & residential, lb\n",
    "\n",
    "#USPSAP (USPS Auctane PM): commercial & residential, lb\n",
    "\n",
    "#==========================MI===========================================\n",
    "\n",
    "#MIPLE (MI Parcel Select Lightweight Expedited): commercial & residential, oz\n",
    "\n",
    "#MIPH (MI Parcel Select Heavyweight): commercial & residential, lb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e5e8b31",
   "metadata": {},
   "source": [
    "# ========= UPS Section =========="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 982,
   "id": "421de538",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add a column named DAS_CATEGORY to PLD with ups accessorial category DAS, DASE, RA, HI, or AK. One package (or row) can only have one single category.\n",
    "tbl_area_surcharge = pd.read_excel('C:\\\\Users\\\\sliu\\\\BD\\\\re-rate_model\\\\areaSurchargeZipsUs_2023.xlsx')\n",
    "tbl_area_surcharge['Zip'] = tbl_area_surcharge['Zip'].astype(str)\n",
    "tbl_area_surcharge['Zip'] = tbl_area_surcharge['Zip'].apply(lambda x: '0' + x if len(x) == 4 else ('00' + x if (len(x) == 3) else x))\n",
    "\n",
    "tbl_BP = tbl_BP.merge(tbl_area_surcharge[[\"Zip\", \"Type\"]], left_on='ZIP_CODE', right_on='Zip', how='left')\n",
    "tbl_BP['DAS_CATEGORY'] = tbl_BP['Type']\n",
    "tbl_BP = tbl_BP.drop('Type', axis=1)\n",
    "tbl_BP = tbl_BP.drop('Zip', axis=1)\n",
    "#tbl_BP.info()\n",
    "#tbl_BP['ZONE'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dying-interview",
   "metadata": {},
   "source": [
    "# REDE (UPS NDA Early) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 983,
   "id": "9133892c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get the right client rate and the corresponding PLD\n",
    "tbl_clientRates_1 = tbl_clientRates[(tbl_clientRates['SERVICE_CODE'] == 'REDE')]\n",
    "tbl_PLD_original_REDE = tbl_BP.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 984,
   "id": "d9d60753",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Rerate start\n",
    "#add billed weight column to PLD\n",
    "tbl_PLD_original_REDE['BILLED_WEIGHT_LB'] = tbl_PLD_original_REDE.apply(lambda row: max(row['ACTUAL_WEIGHT_LB'], np.ceil(row['CUBIC_INCH']/dim_factor_ups)), axis=1)\n",
    "\n",
    "\n",
    "#add Freight column to PLD\n",
    "tbl_PLD_original_REDE['ZONE'] = tbl_PLD_original_REDE['ZONE'].astype(str)\n",
    "tbl_PLD_original_REDE = tbl_PLD_original_REDE.merge(tbl_clientRates_1[[\"WEIGHT_LB\", \"ZONE\", \"CLIENT_RATES\"]], left_on=['BILLED_WEIGHT_LB', 'ZONE'], right_on=['WEIGHT_LB', 'ZONE'], how='left')\n",
    "tbl_PLD_original_REDE = tbl_PLD_original_REDE.drop('WEIGHT_LB', axis=1)\n",
    "tbl_PLD_original_REDE = tbl_PLD_original_REDE.rename(columns={'CLIENT_RATES': 'FRT_REDE'})\n",
    "   \n",
    "\n",
    "#add Residential Surcharge column to PLD\n",
    "tbl_PLD_original_REDE['RES_REDE'] = tbl_PLD_original_REDE.apply(lambda row: Resi_Surcharge_Air if row['RESIDENTIAL_FLAG'] is True else 0, axis=1)\n",
    "\n",
    "\n",
    "#add DAS column to PLD, Pay attention to the flags under Residential Flag\n",
    "tbl_PLD_original_REDE['DAS_REDE'] = tbl_PLD_original_REDE.apply(lambda row: \n",
    "                                                            DAS_Comm_Air if (row['DAS_CATEGORY'] == 'DAS' and row['RESIDENTIAL_FLAG'] is False) else \n",
    "                                                           (DAS_Comm_Extended_Air if (row['DAS_CATEGORY'] == 'DASE' and row['RESIDENTIAL_FLAG'] is False) else\n",
    "                                                           (DAS_Resi_Air if (row['DAS_CATEGORY'] == 'DAS' and row['RESIDENTIAL_FLAG'] is True) else \n",
    "                                                           (DAS_Resi_Extended_Air if (row['DAS_CATEGORY'] == 'DASE' and row['RESIDENTIAL_FLAG'] is True) else \n",
    "                                                           (Remote_Area if (row['DAS_CATEGORY'] == 'RA') else \n",
    "                                                           (Remote_Area_Alaska if (row['DAS_CATEGORY'] == 'AK') else\n",
    "                                                           (Remote_Area_Hawaii if (row['DAS_CATEGORY'] == 'HI') else 0)))))), axis=1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#add fuel surcharge % column to PLD\n",
    "tbl_PLD_original_REDE['SHIP_DATE'] = pd.to_datetime(tbl_PLD_original_REDE['SHIP_DATE']).dt.date\n",
    "# get unique values from column 'SHIP_DATE' and create a new dataframe\n",
    "new_df = pd.DataFrame({'SHIP_DATE': tbl_PLD_original_REDE['SHIP_DATE'].unique()})\n",
    "# use this new dataframe to merge to FSC\n",
    "df_merge = new_df.merge(UPS_FSC, how='cross')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#slice the merged dataframe to make sure ship_date is between the ship_data_start and end. Now we have a FSC file with Ship_date col to merge back to pld so that the pld can get FSC.\n",
    "df_merge['SHIP_DATE'] = pd.to_datetime(df_merge['SHIP_DATE']).dt.date\n",
    "df_merge = df_merge.query('SHIP_DATE >= SHIP_DATE_START and SHIP_DATE <= SHIP_DATE_END')\n",
    "df_merge = df_merge.drop_duplicates()\n",
    "# Get Domestic Air column as FSC. \n",
    "tbl_PLD_original_REDE = tbl_PLD_original_REDE.merge(df_merge, on=['SHIP_DATE'], how='left').drop(['SHIP_DATE_START', 'SHIP_DATE_END', 'Ground'], axis=1)\n",
    "tbl_PLD_original_REDE.rename(columns={'Domestic Air': 'FSC%_REDE'}, inplace=True)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#add AHS column to PLD\n",
    "tbl_PLD_original_REDE['ZONE'] = pd.to_numeric(tbl_PLD_original_REDE['ZONE'], errors='coerce').fillna(0).astype(int)\n",
    "\n",
    "tbl_PLD_original_REDE['AHS_1'] = tbl_PLD_original_REDE.apply(lambda row: \n",
    "                                                          AH_Girth_2 if (row['GIRTH_AND_L'] > 105 and row['ZONE'] == 2) else \n",
    "                                                           (AH_Girth_3_4 if (row['GIRTH_AND_L'] > 105 and row['ZONE'] > 2 and row['ZONE'] < 5) else\n",
    "                                                           (AH_Girth_5_up if (row['GIRTH_AND_L'] > 105 and row['ZONE'] > 4) else 0)), axis=1)\n",
    "                                                    \n",
    "tbl_PLD_original_REDE['AHS_2'] = tbl_PLD_original_REDE.apply(lambda row: \n",
    "                                                           AH_L_2 if (row['L'] > 48 and row['ZONE'] == 2) else \n",
    "                                                           (AH_L_3_4 if (row['L'] > 48 and row['ZONE'] > 2 and row['ZONE'] < 5) else \n",
    "                                                           (AH_L_5_up if (row['L'] > 48 and row['ZONE'] > 4) else 0)), axis=1)\n",
    "\n",
    "tbl_PLD_original_REDE['AHS_3'] = tbl_PLD_original_REDE.apply(lambda row:\n",
    "                                                           AH_W_2 if (row['W'] > 30 and row['ZONE'] == 2) else \n",
    "                                                           (AH_W_3_4 if (row['W'] > 30 and row['ZONE'] > 2 and row['ZONE'] < 5) else \n",
    "                                                           (AH_W_5_up if (row['W'] > 30 and row['ZONE'] > 4) else 0)), axis=1)\n",
    "                                        \n",
    "tbl_PLD_original_REDE['AHS_4'] = tbl_PLD_original_REDE.apply(lambda row:\n",
    "                                                           AH_WGT_2 if (row['ACTUAL_WEIGHT'] > 50 and row['ZONE'] == 2) else \n",
    "                                                           (AH_WGT_3_4 if (row['ACTUAL_WEIGHT'] > 50 and row['ZONE'] > 2 and row['ZONE'] < 5) else \n",
    "                                                           (AH_WGT_5_up if (row['ACTUAL_WEIGHT'] > 50 and row['ZONE'] > 4) else 0)), axis=1)\n",
    "\n",
    "tbl_PLD_original_REDE['AHS_REDE'] = tbl_PLD_original_REDE['AHS_1'] + tbl_PLD_original_REDE['AHS_2'] + tbl_PLD_original_REDE['AHS_3'] + tbl_PLD_original_REDE['AHS_4']\n",
    "tbl_PLD_original_REDE = tbl_PLD_original_REDE.drop(['AHS_1','AHS_2','AHS_3','AHS_4'], axis=1)\n",
    "\n",
    "#Get Total rate. Set total rate to null if the freight rate is null and/or some package parameters are out of carrier handling capability.  \n",
    "tbl_PLD_original_REDE['TOTAL_REDE'] = round(tbl_PLD_original_REDE.apply(lambda row: \n",
    "                                                                  (1 + row['FSC%_REDE']) * (row['FRT_REDE'] + row['RES_REDE'] + row['DAS_REDE'] + row['AHS_REDE']) if not pd.isna(row['FRT_REDE']) and row['FRT_REDE'] != '' else np.nan, axis=1),2)\n",
    "#tbl_PLD_original_REDE.to_csv('test.csv')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "broken-safety",
   "metadata": {},
   "source": [
    "# RED (UPS NDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 985,
   "id": "92c04bae",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get the right client rate and the corresponding PLD\n",
    "tbl_clientRates_2 = tbl_clientRates[(tbl_clientRates['SERVICE_CODE'] == 'RED')]\n",
    "tbl_PLD_original_RED = tbl_BP.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 986,
   "id": "meaning-force",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Rerate start\n",
    "#add billed weight column to PLD\n",
    "tbl_PLD_original_RED['BILLED_WEIGHT_LB'] = tbl_PLD_original_RED.apply(lambda row: max(row['ACTUAL_WEIGHT_LB'], np.ceil(row['CUBIC_INCH']/dim_factor_ups)), axis=1)\n",
    "\n",
    "#add Freight column to PLD\n",
    "tbl_PLD_original_RED['ZONE'] = tbl_PLD_original_RED['ZONE'].astype(str)\n",
    "tbl_PLD_original_RED = tbl_PLD_original_RED.merge(tbl_clientRates_2[[\"WEIGHT_LB\", \"ZONE\", \"CLIENT_RATES\"]], left_on=['BILLED_WEIGHT_LB', 'ZONE'], right_on=['WEIGHT_LB', 'ZONE'], how='left')\n",
    "tbl_PLD_original_RED = tbl_PLD_original_RED.drop('WEIGHT_LB', axis=1)\n",
    "tbl_PLD_original_RED = tbl_PLD_original_RED.rename(columns={'CLIENT_RATES': 'FRT_RED'})\n",
    "   \n",
    "    \n",
    "#add Residential Surcharge column to PLD\n",
    "tbl_PLD_original_RED['RES_RED'] = tbl_PLD_original_RED.apply(lambda row: Resi_Surcharge_Air if row['RESIDENTIAL_FLAG'] is True else 0, axis=1)\n",
    "\n",
    "#add DAS column to PLD, Pay attention to the flags under Residential Flag\n",
    "tbl_PLD_original_RED['DAS_RED'] = tbl_PLD_original_RED.apply(lambda row: \n",
    "                                                            DAS_Comm_Air if (row['DAS_CATEGORY'] == 'DAS' and row['RESIDENTIAL_FLAG'] is False) else \n",
    "                                                           (DAS_Comm_Extended_Air if (row['DAS_CATEGORY'] == 'DASE' and row['RESIDENTIAL_FLAG'] is False) else\n",
    "                                                           (DAS_Resi_Air if (row['DAS_CATEGORY'] == 'DAS' and row['RESIDENTIAL_FLAG'] is True) else \n",
    "                                                           (DAS_Resi_Extended_Air if (row['DAS_CATEGORY'] == 'DASE' and row['RESIDENTIAL_FLAG'] is True) else \n",
    "                                                           (Remote_Area if (row['DAS_CATEGORY'] == 'RA') else \n",
    "                                                           (Remote_Area_Alaska if (row['DAS_CATEGORY'] == 'AK') else\n",
    "                                                           (Remote_Area_Hawaii if (row['DAS_CATEGORY'] == 'HI') else 0)))))), axis=1)\n",
    "\n",
    "\n",
    "#add fuel surcharge % column to PLD\n",
    "tbl_PLD_original_RED['SHIP_DATE'] = pd.to_datetime(tbl_PLD_original_RED['SHIP_DATE']).dt.date\n",
    "# get unique values from column 'SHIP_DATE' and create a new dataframe\n",
    "new_df = pd.DataFrame({'SHIP_DATE': tbl_PLD_original_RED['SHIP_DATE'].unique()})\n",
    "# use this new dataframe to merge to FSC\n",
    "df_merge = new_df.merge(UPS_FSC, how='cross')\n",
    "df_merge['SHIP_DATE'] = pd.to_datetime(df_merge['SHIP_DATE']).dt.date\n",
    "#slice the merged dataframe to make sure ship_date is between the ship_data_start and end. Now we have a FSC file with Ship_date col to merge back to pld so that the pld can get FSC.\n",
    "df_merge = df_merge.query('SHIP_DATE >= SHIP_DATE_START and SHIP_DATE <= SHIP_DATE_END')\n",
    "df_merge = df_merge.drop_duplicates()\n",
    "# Get Domestic Air column as FSC. \n",
    "tbl_PLD_original_RED = tbl_PLD_original_RED.merge(df_merge, on=['SHIP_DATE'], how='left').drop(['SHIP_DATE_START', 'SHIP_DATE_END', 'Ground'], axis=1)\n",
    "tbl_PLD_original_RED.rename(columns={'Domestic Air': 'FSC%_RED'}, inplace=True)\n",
    "\n",
    "\n",
    "#add AHS column to PLD\n",
    "tbl_PLD_original_RED['ZONE'] = pd.to_numeric(tbl_PLD_original_RED['ZONE'], errors='coerce').fillna(0).astype(int)\n",
    "\n",
    "tbl_PLD_original_RED['AHS_1'] = tbl_PLD_original_RED.apply(lambda row: \n",
    "                                                           AH_Girth_2 if (row['GIRTH_AND_L'] > 105 and row['ZONE'] == 2) else \n",
    "                                                           (AH_Girth_3_4 if (row['GIRTH_AND_L'] > 105 and row['ZONE'] > 2 and row['ZONE'] < 5) else\n",
    "                                                           (AH_Girth_5_up if (row['GIRTH_AND_L'] > 105 and row['ZONE'] > 4) else 0)), axis=1)\n",
    "                                                    \n",
    "tbl_PLD_original_RED['AHS_2'] = tbl_PLD_original_RED.apply(lambda row: \n",
    "                                                           AH_L_2 if (row['L'] > 48 and row['ZONE'] == 2) else \n",
    "                                                           (AH_L_3_4 if (row['L'] > 48 and row['ZONE'] > 2 and row['ZONE'] < 5) else \n",
    "                                                           (AH_L_5_up if (row['L'] > 48 and row['ZONE'] > 4) else 0)), axis=1)\n",
    "\n",
    "tbl_PLD_original_RED['AHS_3'] = tbl_PLD_original_RED.apply(lambda row:\n",
    "                                                           AH_W_2 if (row['W'] > 30 and row['ZONE'] == 2) else \n",
    "                                                           (AH_W_3_4 if (row['W'] > 30 and row['ZONE'] > 2 and row['ZONE'] < 5) else \n",
    "                                                           (AH_W_5_up if (row['W'] > 30 and row['ZONE'] > 4) else 0)), axis=1)\n",
    "                                        \n",
    "tbl_PLD_original_RED['AHS_4'] = tbl_PLD_original_RED.apply(lambda row:\n",
    "                                                           AH_WGT_2 if (row['ACTUAL_WEIGHT'] > 50 and row['ZONE'] == 2) else \n",
    "                                                           (AH_WGT_3_4 if (row['ACTUAL_WEIGHT'] > 50 and row['ZONE'] > 2 and row['ZONE'] < 5) else \n",
    "                                                           (AH_WGT_5_up if (row['ACTUAL_WEIGHT'] > 50 and row['ZONE'] > 4) else 0)), axis=1)\n",
    "\n",
    "tbl_PLD_original_RED['AHS_RED'] = tbl_PLD_original_RED['AHS_1'] + tbl_PLD_original_RED['AHS_2'] + tbl_PLD_original_RED['AHS_3'] + tbl_PLD_original_RED['AHS_4']\n",
    "tbl_PLD_original_RED = tbl_PLD_original_RED.drop(['AHS_1','AHS_2','AHS_3','AHS_4'], axis=1)\n",
    "\n",
    "#Get Total rate. Set total rate to null if the freight rate is null and/or some package parameters are out of carrier handling capability.  \n",
    "tbl_PLD_original_RED['TOTAL_RED'] = round(tbl_PLD_original_RED.apply(lambda row: \n",
    "                                                                  (1 + row['FSC%_RED']) * (row['FRT_RED'] + row['RES_RED'] + row['DAS_RED'] + row['AHS_RED']) if not pd.isna(row['FRT_RED']) and row['FRT_RED'] != '' else np.nan, axis=1),2)\n",
    "#tbl_PLD_original_RED.to_csv('test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "raised-departure",
   "metadata": {},
   "source": [
    "# REDS (UPS NDA Saver)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 987,
   "id": "ef6fe621",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get the right client rate and the corresponding PLD\n",
    "tbl_clientRates_3 = tbl_clientRates[(tbl_clientRates['SERVICE_CODE'] == 'REDS')]\n",
    "tbl_PLD_original_REDS = tbl_BP.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 988,
   "id": "foster-stockholm",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Rerate start\n",
    "#add billed weight column to PLD\n",
    "tbl_PLD_original_REDS['BILLED_WEIGHT_LB'] = tbl_PLD_original_REDS.apply(lambda row: max(row['ACTUAL_WEIGHT_LB'], np.ceil(row['CUBIC_INCH']/dim_factor_ups)), axis=1)\n",
    "\n",
    "#add Freight column to PLD\n",
    "tbl_PLD_original_REDS['ZONE'] = tbl_PLD_original_REDS['ZONE'].astype(str)\n",
    "tbl_PLD_original_REDS = tbl_PLD_original_REDS.merge(tbl_clientRates_3[[\"WEIGHT_LB\", \"ZONE\", \"CLIENT_RATES\"]], left_on=['BILLED_WEIGHT_LB', 'ZONE'], right_on=['WEIGHT_LB', 'ZONE'], how='left')\n",
    "tbl_PLD_original_REDS = tbl_PLD_original_REDS.drop('WEIGHT_LB', axis=1)\n",
    "tbl_PLD_original_REDS = tbl_PLD_original_REDS.rename(columns={'CLIENT_RATES': 'FRT_REDS'})\n",
    "   \n",
    "    \n",
    "#add Residential Surcharge column to PLD\n",
    "tbl_PLD_original_REDS['RES_REDS'] = tbl_PLD_original_REDS.apply(lambda row: Resi_Surcharge_Air if row['RESIDENTIAL_FLAG'] is True else 0, axis=1)\n",
    "\n",
    "#add DAS column to PLD, Pay attention to the flags under Residential Flag\n",
    "tbl_PLD_original_REDS['DAS_REDS'] = tbl_PLD_original_REDS.apply(lambda row: \n",
    "                                                            DAS_Comm_Air if (row['DAS_CATEGORY'] == 'DAS' and row['RESIDENTIAL_FLAG'] is False) else \n",
    "                                                           (DAS_Comm_Extended_Air if (row['DAS_CATEGORY'] == 'DASE' and row['RESIDENTIAL_FLAG'] is False) else\n",
    "                                                           (DAS_Resi_Air if (row['DAS_CATEGORY'] == 'DAS' and row['RESIDENTIAL_FLAG'] is True) else \n",
    "                                                           (DAS_Resi_Extended_Air if (row['DAS_CATEGORY'] == 'DASE' and row['RESIDENTIAL_FLAG'] is True) else \n",
    "                                                           (Remote_Area if (row['DAS_CATEGORY'] == 'RA') else \n",
    "                                                           (Remote_Area_Alaska if (row['DAS_CATEGORY'] == 'AK') else\n",
    "                                                           (Remote_Area_Hawaii if (row['DAS_CATEGORY'] == 'HI') else 0)))))), axis=1)\n",
    "\n",
    "\n",
    "#add fuel surcharge % column to PLD\n",
    "tbl_PLD_original_REDS['SHIP_DATE'] = pd.to_datetime(tbl_PLD_original_REDS['SHIP_DATE']).dt.date\n",
    "# get unique values from column 'SHIP_DATE' and create a new dataframe\n",
    "new_df = pd.DataFrame({'SHIP_DATE': tbl_PLD_original_REDS['SHIP_DATE'].unique()})\n",
    "# use this new dataframe to merge to FSC\n",
    "df_merge = new_df.merge(UPS_FSC, how='cross')\n",
    "df_merge['SHIP_DATE'] = pd.to_datetime(df_merge['SHIP_DATE']).dt.date\n",
    "#slice the merged dataframe to make sure ship_date is between the ship_data_start and end. Now we have a FSC file with Ship_date col to merge back to pld so that the pld can get FSC.\n",
    "df_merge = df_merge.query('SHIP_DATE >= SHIP_DATE_START and SHIP_DATE <= SHIP_DATE_END')\n",
    "df_merge = df_merge.drop_duplicates()\n",
    "# Get Domestic Air column as FSC. \n",
    "tbl_PLD_original_REDS = tbl_PLD_original_REDS.merge(df_merge, on=['SHIP_DATE'], how='left').drop(['SHIP_DATE_START', 'SHIP_DATE_END', 'Ground'], axis=1)\n",
    "tbl_PLD_original_REDS.rename(columns={'Domestic Air': 'FSC%_REDS'}, inplace=True)\n",
    "\n",
    "\n",
    "#add AHS column to PLD\n",
    "tbl_PLD_original_REDS['ZONE'] = pd.to_numeric(tbl_PLD_original_REDS['ZONE'], errors='coerce').fillna(0).astype(int)\n",
    "\n",
    "tbl_PLD_original_REDS['AHS_1'] = tbl_PLD_original_REDS.apply(lambda row: \n",
    "                                                           AH_Girth_2 if (row['GIRTH_AND_L'] > 105 and row['ZONE'] == 2) else \n",
    "                                                           (AH_Girth_3_4 if (row['GIRTH_AND_L'] > 105 and row['ZONE'] > 2 and row['ZONE'] < 5) else\n",
    "                                                           (AH_Girth_5_up if (row['GIRTH_AND_L'] > 105 and row['ZONE'] > 4) else 0)), axis=1)\n",
    "                                                    \n",
    "tbl_PLD_original_REDS['AHS_2'] = tbl_PLD_original_REDS.apply(lambda row: \n",
    "                                                           AH_L_2 if (row['L'] > 48 and row['ZONE'] == 2) else \n",
    "                                                           (AH_L_3_4 if (row['L'] > 48 and row['ZONE'] > 2 and row['ZONE'] < 5) else \n",
    "                                                           (AH_L_5_up if (row['L'] > 48 and row['ZONE'] > 4) else 0)), axis=1)\n",
    "\n",
    "tbl_PLD_original_REDS['AHS_3'] = tbl_PLD_original_REDS.apply(lambda row:\n",
    "                                                           AH_W_2 if (row['W'] > 30 and row['ZONE'] == 2) else \n",
    "                                                           (AH_W_3_4 if (row['W'] > 30 and row['ZONE'] > 2 and row['ZONE'] < 5) else \n",
    "                                                           (AH_W_5_up if (row['W'] > 30 and row['ZONE'] > 4) else 0)), axis=1)\n",
    "                                        \n",
    "tbl_PLD_original_REDS['AHS_4'] = tbl_PLD_original_REDS.apply(lambda row:\n",
    "                                                           AH_WGT_2 if (row['ACTUAL_WEIGHT'] > 50 and row['ZONE'] == 2) else \n",
    "                                                           (AH_WGT_3_4 if (row['ACTUAL_WEIGHT'] > 50 and row['ZONE'] > 2 and row['ZONE'] < 5) else \n",
    "                                                           (AH_WGT_5_up if (row['ACTUAL_WEIGHT'] > 50 and row['ZONE'] > 4) else 0)), axis=1)\n",
    "\n",
    "tbl_PLD_original_REDS['AHS_REDS'] = tbl_PLD_original_REDS['AHS_1'] + tbl_PLD_original_REDS['AHS_2'] + tbl_PLD_original_REDS['AHS_3'] + tbl_PLD_original_REDS['AHS_4']\n",
    "tbl_PLD_original_REDS = tbl_PLD_original_REDS.drop(['AHS_1','AHS_2','AHS_3','AHS_4'], axis=1)\n",
    "\n",
    "#Get Total rate. Set total rate to null if the freight rate is null and/or some package parameters are out of carrier handling capability.  \n",
    "tbl_PLD_original_REDS['TOTAL_REDS'] = round(tbl_PLD_original_REDS.apply(lambda row: \n",
    "                                                                  (1 + row['FSC%_REDS']) * (row['FRT_REDS'] + row['RES_REDS'] + row['DAS_REDS'] + row['AHS_REDS']) if not pd.isna(row['FRT_REDS']) and row['FRT_REDS'] != '' else np.nan, axis=1),2)\n",
    "#tbl_PLD_original_REDS.to_csv('test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "scenic-transaction",
   "metadata": {},
   "source": [
    "# 2DAM (UPS 2DA A.M.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 989,
   "id": "f87cf032",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get the right client rate and the corresponding PLD\n",
    "tbl_clientRates_4 = tbl_clientRates[(tbl_clientRates['SERVICE_CODE'] == '2DAM')]\n",
    "tbl_PLD_original_2DAM = tbl_BP.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 990,
   "id": "elegant-nicaragua",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Rerate start\n",
    "#add billed weight column to PLD\n",
    "tbl_PLD_original_2DAM['BILLED_WEIGHT_LB'] = tbl_PLD_original_2DAM.apply(lambda row: max(row['ACTUAL_WEIGHT_LB'], np.ceil(row['CUBIC_INCH']/dim_factor_ups)), axis=1)\n",
    "\n",
    "#add Freight column to PLD\n",
    "tbl_PLD_original_2DAM['ZONE'] = tbl_PLD_original_2DAM['ZONE'].astype(str)\n",
    "tbl_PLD_original_2DAM = tbl_PLD_original_2DAM.merge(tbl_clientRates_4[[\"WEIGHT_LB\", \"ZONE\", \"CLIENT_RATES\"]], left_on=['BILLED_WEIGHT_LB', 'ZONE'], right_on=['WEIGHT_LB', 'ZONE'], how='left')\n",
    "tbl_PLD_original_2DAM = tbl_PLD_original_2DAM.drop('WEIGHT_LB', axis=1)\n",
    "tbl_PLD_original_2DAM = tbl_PLD_original_2DAM.rename(columns={'CLIENT_RATES': 'FRT_2DAM'})\n",
    "   \n",
    "    \n",
    "#add Residential Surcharge column to PLD\n",
    "tbl_PLD_original_2DAM['RES_2DAM'] = tbl_PLD_original_2DAM.apply(lambda row: Resi_Surcharge_Air if row['RESIDENTIAL_FLAG'] is True else 0, axis=1)\n",
    "\n",
    "#add DAS column to PLD, Pay attention to the flags under Residential Flag\n",
    "tbl_PLD_original_2DAM['DAS_2DAM'] = tbl_PLD_original_2DAM.apply(lambda row: \n",
    "                                                            DAS_Comm_Air if (row['DAS_CATEGORY'] == 'DAS' and row['RESIDENTIAL_FLAG'] is False) else \n",
    "                                                           (DAS_Comm_Extended_Air if (row['DAS_CATEGORY'] == 'DASE' and row['RESIDENTIAL_FLAG'] is False) else\n",
    "                                                           (DAS_Resi_Air if (row['DAS_CATEGORY'] == 'DAS' and row['RESIDENTIAL_FLAG'] is True) else \n",
    "                                                           (DAS_Resi_Extended_Air if (row['DAS_CATEGORY'] == 'DASE' and row['RESIDENTIAL_FLAG'] is True) else \n",
    "                                                           (Remote_Area if (row['DAS_CATEGORY'] == 'RA') else \n",
    "                                                           (Remote_Area_Alaska if (row['DAS_CATEGORY'] == 'AK') else\n",
    "                                                           (Remote_Area_Hawaii if (row['DAS_CATEGORY'] == 'HI') else 0)))))), axis=1)\n",
    "\n",
    "\n",
    "#add fuel surcharge % column to PLD\n",
    "tbl_PLD_original_2DAM['SHIP_DATE'] = pd.to_datetime(tbl_PLD_original_2DAM['SHIP_DATE']).dt.date\n",
    "# get unique values from column 'SHIP_DATE' and create a new dataframe\n",
    "new_df = pd.DataFrame({'SHIP_DATE': tbl_PLD_original_2DAM['SHIP_DATE'].unique()})\n",
    "# use this new dataframe to merge to FSC\n",
    "df_merge = new_df.merge(UPS_FSC, how='cross')\n",
    "df_merge['SHIP_DATE'] = pd.to_datetime(df_merge['SHIP_DATE']).dt.date\n",
    "#slice the merged dataframe to make sure ship_date is between the ship_data_start and end. Now we have a FSC file with Ship_date col to merge back to pld so that the pld can get FSC.\n",
    "df_merge = df_merge.query('SHIP_DATE >= SHIP_DATE_START and SHIP_DATE <= SHIP_DATE_END')\n",
    "df_merge = df_merge.drop_duplicates()\n",
    "# Get Domestic Air column as FSC. \n",
    "tbl_PLD_original_2DAM = tbl_PLD_original_2DAM.merge(df_merge, on=['SHIP_DATE'], how='left').drop(['SHIP_DATE_START', 'SHIP_DATE_END', 'Ground'], axis=1)\n",
    "tbl_PLD_original_2DAM.rename(columns={'Domestic Air': 'FSC%_2DAM'}, inplace=True)\n",
    "\n",
    "\n",
    "#add AHS column to PLD\n",
    "tbl_PLD_original_2DAM['ZONE'] = pd.to_numeric(tbl_PLD_original_2DAM['ZONE'], errors='coerce').fillna(0).astype(int)\n",
    "\n",
    "tbl_PLD_original_2DAM['AHS_1'] = tbl_PLD_original_2DAM.apply(lambda row: \n",
    "                                                           AH_Girth_2 if (row['GIRTH_AND_L'] > 105 and row['ZONE'] == 2) else \n",
    "                                                           (AH_Girth_3_4 if (row['GIRTH_AND_L'] > 105 and row['ZONE'] > 2 and row['ZONE'] < 5) else\n",
    "                                                           (AH_Girth_5_up if (row['GIRTH_AND_L'] > 105 and row['ZONE'] > 4) else 0)), axis=1)\n",
    "                                                    \n",
    "tbl_PLD_original_2DAM['AHS_2'] = tbl_PLD_original_2DAM.apply(lambda row: \n",
    "                                                           AH_L_2 if (row['L'] > 48 and row['ZONE'] == 2) else \n",
    "                                                           (AH_L_3_4 if (row['L'] > 48 and row['ZONE'] > 2 and row['ZONE'] < 5) else \n",
    "                                                           (AH_L_5_up if (row['L'] > 48 and row['ZONE'] > 4) else 0)), axis=1)\n",
    "\n",
    "tbl_PLD_original_2DAM['AHS_3'] = tbl_PLD_original_2DAM.apply(lambda row:\n",
    "                                                           AH_W_2 if (row['W'] > 30 and row['ZONE'] == 2) else \n",
    "                                                           (AH_W_3_4 if (row['W'] > 30 and row['ZONE'] > 2 and row['ZONE'] < 5) else \n",
    "                                                           (AH_W_5_up if (row['W'] > 30 and row['ZONE'] > 4) else 0)), axis=1)\n",
    "                                        \n",
    "tbl_PLD_original_2DAM['AHS_4'] = tbl_PLD_original_2DAM.apply(lambda row:\n",
    "                                                           AH_WGT_2 if (row['ACTUAL_WEIGHT'] > 50 and row['ZONE'] == 2) else \n",
    "                                                           (AH_WGT_3_4 if (row['ACTUAL_WEIGHT'] > 50 and row['ZONE'] > 2 and row['ZONE'] < 5) else \n",
    "                                                           (AH_WGT_5_up if (row['ACTUAL_WEIGHT'] > 50 and row['ZONE'] > 4) else 0)), axis=1)\n",
    "\n",
    "tbl_PLD_original_2DAM['AHS_2DAM'] = tbl_PLD_original_2DAM['AHS_1'] + tbl_PLD_original_2DAM['AHS_2'] + tbl_PLD_original_2DAM['AHS_3'] + tbl_PLD_original_2DAM['AHS_4']\n",
    "tbl_PLD_original_2DAM = tbl_PLD_original_2DAM.drop(['AHS_1','AHS_2','AHS_3','AHS_4'], axis=1)\n",
    "\n",
    "#Get Total rate. Set total rate to null if the freight rate is null and/or some package parameters are out of carrier handling capability.  \n",
    "tbl_PLD_original_2DAM['TOTAL_2DAM'] = round(tbl_PLD_original_2DAM.apply(lambda row: \n",
    "                                                                  (1 + row['FSC%_2DAM']) * (row['FRT_2DAM'] + row['RES_2DAM'] + row['DAS_2DAM'] + row['AHS_2DAM']) if not pd.isna(row['FRT_2DAM']) and row['FRT_2DAM'] != '' else np.nan, axis=1),2)\n",
    "#tbl_PLD_original_2DAM.to_csv('test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "insured-stations",
   "metadata": {},
   "source": [
    "# BLUE (UPS 2DA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 991,
   "id": "954a4bf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get the right client rate and the corresponding PLD\n",
    "tbl_clientRates_5 = tbl_clientRates[(tbl_clientRates['SERVICE_CODE'] == 'BLUE')]\n",
    "tbl_PLD_original_BLUE = tbl_BP.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 992,
   "id": "systematic-working",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Rerate start\n",
    "#add billed weight column to PLD\n",
    "tbl_PLD_original_BLUE['BILLED_WEIGHT_LB'] = tbl_PLD_original_BLUE.apply(lambda row: max(row['ACTUAL_WEIGHT_LB'], np.ceil(row['CUBIC_INCH']/dim_factor_ups)), axis=1)\n",
    "\n",
    "#add Freight column to PLD\n",
    "tbl_PLD_original_BLUE['ZONE'] = tbl_PLD_original_BLUE['ZONE'].astype(str)\n",
    "tbl_PLD_original_BLUE = tbl_PLD_original_BLUE.merge(tbl_clientRates_5[[\"WEIGHT_LB\", \"ZONE\", \"CLIENT_RATES\"]], left_on=['BILLED_WEIGHT_LB', 'ZONE'], right_on=['WEIGHT_LB', 'ZONE'], how='left')\n",
    "tbl_PLD_original_BLUE = tbl_PLD_original_BLUE.drop('WEIGHT_LB', axis=1)\n",
    "tbl_PLD_original_BLUE = tbl_PLD_original_BLUE.rename(columns={'CLIENT_RATES': 'FRT_BLUE'})\n",
    "   \n",
    "    \n",
    "#add Residential Surcharge column to PLD\n",
    "tbl_PLD_original_BLUE['RES_BLUE'] = tbl_PLD_original_BLUE.apply(lambda row: Resi_Surcharge_Air if row['RESIDENTIAL_FLAG'] is True else 0, axis=1)\n",
    "\n",
    "#add DAS column to PLD, Pay attention to the flags under Residential Flag\n",
    "tbl_PLD_original_BLUE['DAS_BLUE'] = tbl_PLD_original_BLUE.apply(lambda row: \n",
    "                                                            DAS_Comm_Air if (row['DAS_CATEGORY'] == 'DAS' and row['RESIDENTIAL_FLAG'] is False) else \n",
    "                                                           (DAS_Comm_Extended_Air if (row['DAS_CATEGORY'] == 'DASE' and row['RESIDENTIAL_FLAG'] is False) else\n",
    "                                                           (DAS_Resi_Air if (row['DAS_CATEGORY'] == 'DAS' and row['RESIDENTIAL_FLAG'] is True) else \n",
    "                                                           (DAS_Resi_Extended_Air if (row['DAS_CATEGORY'] == 'DASE' and row['RESIDENTIAL_FLAG'] is True) else \n",
    "                                                           (Remote_Area if (row['DAS_CATEGORY'] == 'RA') else \n",
    "                                                           (Remote_Area_Alaska if (row['DAS_CATEGORY'] == 'AK') else\n",
    "                                                           (Remote_Area_Hawaii if (row['DAS_CATEGORY'] == 'HI') else 0)))))), axis=1)\n",
    "\n",
    "\n",
    "#add fuel surcharge % column to PLD\n",
    "tbl_PLD_original_BLUE['SHIP_DATE'] = pd.to_datetime(tbl_PLD_original_BLUE['SHIP_DATE']).dt.date\n",
    "# get unique values from column 'SHIP_DATE' and create a new dataframe\n",
    "new_df = pd.DataFrame({'SHIP_DATE': tbl_PLD_original_BLUE['SHIP_DATE'].unique()})\n",
    "# use this new dataframe to merge to FSC\n",
    "df_merge = new_df.merge(UPS_FSC, how='cross')\n",
    "\n",
    "df_merge['SHIP_DATE'] = pd.to_datetime(df_merge['SHIP_DATE']).dt.date\n",
    "\n",
    "#slice the merged dataframe to make sure ship_date is between the ship_data_start and end. Now we have a FSC file with Ship_date col to merge back to pld so that the pld can get FSC.\n",
    "df_merge = df_merge.query('SHIP_DATE >= SHIP_DATE_START and SHIP_DATE <= SHIP_DATE_END')\n",
    "df_merge = df_merge.drop_duplicates()\n",
    "# Get Domestic Air column as FSC. \n",
    "tbl_PLD_original_BLUE = tbl_PLD_original_BLUE.merge(df_merge, on=['SHIP_DATE'], how='left').drop(['SHIP_DATE_START', 'SHIP_DATE_END', 'Ground'], axis=1)\n",
    "tbl_PLD_original_BLUE.rename(columns={'Domestic Air': 'FSC%_BLUE'}, inplace=True)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#add AHS column to PLD\n",
    "tbl_PLD_original_BLUE['ZONE'] = pd.to_numeric(tbl_PLD_original_BLUE['ZONE'], errors='coerce').fillna(0).astype(int)\n",
    "\n",
    "tbl_PLD_original_BLUE['AHS_1'] = tbl_PLD_original_BLUE.apply(lambda row: \n",
    "                                                           AH_Girth_2 if (row['GIRTH_AND_L'] > 105 and row['ZONE'] == 2) else \n",
    "                                                           (AH_Girth_3_4 if (row['GIRTH_AND_L'] > 105 and row['ZONE'] > 2 and row['ZONE'] < 5) else\n",
    "                                                           (AH_Girth_5_up if (row['GIRTH_AND_L'] > 105 and row['ZONE'] > 4) else 0)), axis=1)\n",
    "                                                    \n",
    "tbl_PLD_original_BLUE['AHS_2'] = tbl_PLD_original_BLUE.apply(lambda row: \n",
    "                                                           AH_L_2 if (row['L'] > 48 and row['ZONE'] == 2) else \n",
    "                                                           (AH_L_3_4 if (row['L'] > 48 and row['ZONE'] > 2 and row['ZONE'] < 5) else \n",
    "                                                           (AH_L_5_up if (row['L'] > 48 and row['ZONE'] > 4) else 0)), axis=1)\n",
    "\n",
    "tbl_PLD_original_BLUE['AHS_3'] = tbl_PLD_original_BLUE.apply(lambda row:\n",
    "                                                           AH_W_2 if (row['W'] > 30 and row['ZONE'] == 2) else \n",
    "                                                           (AH_W_3_4 if (row['W'] > 30 and row['ZONE'] > 2 and row['ZONE'] < 5) else \n",
    "                                                           (AH_W_5_up if (row['W'] > 30 and row['ZONE'] > 4) else 0)), axis=1)\n",
    "                                        \n",
    "tbl_PLD_original_BLUE['AHS_4'] = tbl_PLD_original_BLUE.apply(lambda row:\n",
    "                                                           AH_WGT_2 if (row['ACTUAL_WEIGHT'] > 50 and row['ZONE'] == 2) else \n",
    "                                                           (AH_WGT_3_4 if (row['ACTUAL_WEIGHT'] > 50 and row['ZONE'] > 2 and row['ZONE'] < 5) else \n",
    "                                                           (AH_WGT_5_up if (row['ACTUAL_WEIGHT'] > 50 and row['ZONE'] > 4) else 0)), axis=1)\n",
    "\n",
    "tbl_PLD_original_BLUE['AHS_BLUE'] = tbl_PLD_original_BLUE['AHS_1'] + tbl_PLD_original_BLUE['AHS_2'] + tbl_PLD_original_BLUE['AHS_3'] + tbl_PLD_original_BLUE['AHS_4']\n",
    "tbl_PLD_original_BLUE = tbl_PLD_original_BLUE.drop(['AHS_1','AHS_2','AHS_3','AHS_4'], axis=1)\n",
    "\n",
    "#Get Total rate. Set total rate to null if the freight rate is null and/or some package parameters are out of carrier handling capability.  \n",
    "tbl_PLD_original_BLUE['TOTAL_BLUE'] = round(tbl_PLD_original_BLUE.apply(lambda row: \n",
    "                                                                  (1 + row['FSC%_BLUE']) * (row['FRT_BLUE'] + row['RES_BLUE'] + row['DAS_BLUE'] + row['AHS_BLUE']) if not pd.isna(row['FRT_BLUE']) and row['FRT_BLUE'] != '' else np.nan, axis=1),2)\n",
    "#tbl_PLD_original_BLUE.to_csv('test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "final-third",
   "metadata": {},
   "source": [
    "# ORNG (UPS 3DA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 994,
   "id": "59483f8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get the right client rate and the corresponding PLD\n",
    "tbl_clientRates_6 = tbl_clientRates[(tbl_clientRates['SERVICE_CODE'] == 'ORNG')]\n",
    "tbl_PLD_original_ORNG = tbl_BP.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 995,
   "id": "empty-illustration",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Rerate start\n",
    "#add billed weight column to PLD\n",
    "tbl_PLD_original_ORNG['BILLED_WEIGHT_LB'] = tbl_PLD_original_ORNG.apply(lambda row: max(row['ACTUAL_WEIGHT_LB'], np.ceil(row['CUBIC_INCH']/dim_factor_ups)), axis=1)\n",
    "\n",
    "#add Freight column to PLD\n",
    "tbl_PLD_original_ORNG['ZONE'] = tbl_PLD_original_ORNG['ZONE'].astype(str)\n",
    "tbl_PLD_original_ORNG = tbl_PLD_original_ORNG.merge(tbl_clientRates_6[[\"WEIGHT_LB\", \"ZONE\", \"CLIENT_RATES\"]], left_on=['BILLED_WEIGHT_LB', 'ZONE'], right_on=['WEIGHT_LB', 'ZONE'], how='left')\n",
    "tbl_PLD_original_ORNG = tbl_PLD_original_ORNG.drop('WEIGHT_LB', axis=1)\n",
    "tbl_PLD_original_ORNG = tbl_PLD_original_ORNG.rename(columns={'CLIENT_RATES': 'FRT_ORNG'})\n",
    "   \n",
    "    \n",
    "#add Residential Surcharge column to PLD\n",
    "tbl_PLD_original_ORNG['RES_ORNG'] = tbl_PLD_original_ORNG.apply(lambda row: Resi_Surcharge_Air if row['RESIDENTIAL_FLAG'] is True else 0, axis=1)\n",
    "\n",
    "#add DAS column to PLD, Pay attention to the flags under Residential Flag\n",
    "tbl_PLD_original_ORNG['DAS_ORNG'] = tbl_PLD_original_ORNG.apply(lambda row: \n",
    "                                                            DAS_Comm_Air if (row['DAS_CATEGORY'] == 'DAS' and row['RESIDENTIAL_FLAG'] is False) else \n",
    "                                                           (DAS_Comm_Extended_Air if (row['DAS_CATEGORY'] == 'DASE' and row['RESIDENTIAL_FLAG'] is False) else\n",
    "                                                           (DAS_Resi_Air if (row['DAS_CATEGORY'] == 'DAS' and row['RESIDENTIAL_FLAG'] is True) else \n",
    "                                                           (DAS_Resi_Extended_Air if (row['DAS_CATEGORY'] == 'DASE' and row['RESIDENTIAL_FLAG'] is True) else \n",
    "                                                           (Remote_Area if (row['DAS_CATEGORY'] == 'RA') else \n",
    "                                                           (Remote_Area_Alaska if (row['DAS_CATEGORY'] == 'AK') else\n",
    "                                                           (Remote_Area_Hawaii if (row['DAS_CATEGORY'] == 'HI') else 0)))))), axis=1)\n",
    "\n",
    "\n",
    "#add fuel surcharge % column to PLD\n",
    "tbl_PLD_original_ORNG['SHIP_DATE'] = pd.to_datetime(tbl_PLD_original_ORNG['SHIP_DATE']).dt.date\n",
    "# get unique values from column 'SHIP_DATE' and create a new dataframe\n",
    "new_df = pd.DataFrame({'SHIP_DATE': tbl_PLD_original_ORNG['SHIP_DATE'].unique()})\n",
    "# use this new dataframe to merge to FSC\n",
    "df_merge = new_df.merge(UPS_FSC, how='cross')\n",
    "df_merge['SHIP_DATE'] = pd.to_datetime(df_merge['SHIP_DATE']).dt.date\n",
    "#slice the merged dataframe to make sure ship_date is between the ship_data_start and end. Now we have a FSC file with Ship_date col to merge back to pld so that the pld can get FSC.\n",
    "df_merge = df_merge.query('SHIP_DATE >= SHIP_DATE_START and SHIP_DATE <= SHIP_DATE_END')\n",
    "df_merge = df_merge.drop_duplicates()\n",
    "# Get Domestic Air column as FSC. \n",
    "tbl_PLD_original_ORNG = tbl_PLD_original_ORNG.merge(df_merge, on=['SHIP_DATE'], how='left').drop(['SHIP_DATE_START', 'SHIP_DATE_END', 'Ground'], axis=1)\n",
    "tbl_PLD_original_ORNG.rename(columns={'Domestic Air': 'FSC%_ORNG'}, inplace=True)\n",
    "\n",
    "\n",
    "\n",
    "#add AHS column to PLD\n",
    "tbl_PLD_original_ORNG['ZONE'] = pd.to_numeric(tbl_PLD_original_ORNG['ZONE'], errors='coerce').fillna(0).astype(int)\n",
    "\n",
    "tbl_PLD_original_ORNG['AHS_1'] = tbl_PLD_original_ORNG.apply(lambda row: \n",
    "                                                           AH_Girth_2 if (row['GIRTH_AND_L'] > 105 and row['ZONE'] == 2) else \n",
    "                                                           (AH_Girth_3_4 if (row['GIRTH_AND_L'] > 105 and row['ZONE'] > 2 and row['ZONE'] < 5) else\n",
    "                                                           (AH_Girth_5_up if (row['GIRTH_AND_L'] > 105 and row['ZONE'] > 4) else 0)), axis=1)\n",
    "                                                    \n",
    "tbl_PLD_original_ORNG['AHS_2'] = tbl_PLD_original_ORNG.apply(lambda row: \n",
    "                                                           AH_L_2 if (row['L'] > 48 and row['ZONE'] == 2) else \n",
    "                                                           (AH_L_3_4 if (row['L'] > 48 and row['ZONE'] > 2 and row['ZONE'] < 5) else \n",
    "                                                           (AH_L_5_up if (row['L'] > 48 and row['ZONE'] > 4) else 0)), axis=1)\n",
    "\n",
    "tbl_PLD_original_ORNG['AHS_3'] = tbl_PLD_original_ORNG.apply(lambda row:\n",
    "                                                           AH_W_2 if (row['W'] > 30 and row['ZONE'] == 2) else \n",
    "                                                           (AH_W_3_4 if (row['W'] > 30 and row['ZONE'] > 2 and row['ZONE'] < 5) else \n",
    "                                                           (AH_W_5_up if (row['W'] > 30 and row['ZONE'] > 4) else 0)), axis=1)\n",
    "                                        \n",
    "tbl_PLD_original_ORNG['AHS_4'] = tbl_PLD_original_ORNG.apply(lambda row:\n",
    "                                                           AH_WGT_2 if (row['ACTUAL_WEIGHT'] > 50 and row['ZONE'] == 2) else \n",
    "                                                           (AH_WGT_3_4 if (row['ACTUAL_WEIGHT'] > 50 and row['ZONE'] > 2 and row['ZONE'] < 5) else \n",
    "                                                           (AH_WGT_5_up if (row['ACTUAL_WEIGHT'] > 50 and row['ZONE'] > 4) else 0)), axis=1)\n",
    "\n",
    "tbl_PLD_original_ORNG['AHS_ORNG'] = tbl_PLD_original_ORNG['AHS_1'] + tbl_PLD_original_ORNG['AHS_2'] + tbl_PLD_original_ORNG['AHS_3'] + tbl_PLD_original_ORNG['AHS_4']\n",
    "tbl_PLD_original_ORNG = tbl_PLD_original_ORNG.drop(['AHS_1','AHS_2','AHS_3','AHS_4'], axis=1)\n",
    "\n",
    "#Get Total rate. Set total rate to null if the freight rate is null and/or some package parameters are out of carrier handling capability.  \n",
    "tbl_PLD_original_ORNG['TOTAL_ORNG'] = round(tbl_PLD_original_ORNG.apply(lambda row: \n",
    "                                                                  (1 + row['FSC%_ORNG']) * (row['FRT_ORNG'] + row['RES_ORNG'] + row['DAS_ORNG'] + row['AHS_ORNG']) if not pd.isna(row['FRT_ORNG']) and row['FRT_ORNG'] != '' else np.nan, axis=1),2)\n",
    "#tbl_PLD_original_ORNG.to_csv('test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "registered-prison",
   "metadata": {},
   "source": [
    "# GRND (UPS Ground Commercial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 996,
   "id": "235ec428",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get the right client rate and the corresponding PLD\n",
    "tbl_clientRates_7 = tbl_clientRates[(tbl_clientRates['SERVICE_CODE'] == 'GRND')]\n",
    "tbl_PLD_original_GRND = tbl_BP[(tbl_BP['RESIDENTIAL_FLAG'] == False)].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 997,
   "id": "linear-rwanda",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Rerate start\n",
    "#add billed weight column to PLD\n",
    "tbl_PLD_original_GRND['BILLED_WEIGHT_LB'] = tbl_PLD_original_GRND.apply(lambda row: max(row['ACTUAL_WEIGHT_LB'], np.ceil(row['CUBIC_INCH']/dim_factor_ups)), axis=1)\n",
    "\n",
    "#add Freight column to PLD\n",
    "tbl_PLD_original_GRND['ZONE'] = tbl_PLD_original_GRND['ZONE'].astype(str)\n",
    "tbl_PLD_original_GRND = tbl_PLD_original_GRND.merge(tbl_clientRates_7[[\"WEIGHT_LB\", \"ZONE\", \"CLIENT_RATES\"]], left_on=['BILLED_WEIGHT_LB', 'ZONE'], right_on=['WEIGHT_LB', 'ZONE'], how='left')\n",
    "tbl_PLD_original_GRND = tbl_PLD_original_GRND.drop('WEIGHT_LB', axis=1)\n",
    "tbl_PLD_original_GRND = tbl_PLD_original_GRND.rename(columns={'CLIENT_RATES': 'FRT_GRND'})\n",
    "   \n",
    "    \n",
    "#add Residential Surcharge column to PLD\n",
    "tbl_PLD_original_GRND['RES_GRND'] = tbl_PLD_original_GRND.apply(lambda row: Resi_Surcharge_Ground if row['RESIDENTIAL_FLAG'] is True else 0, axis=1)\n",
    "\n",
    "#add DAS column to PLD, Pay attention to the flags under Residential Flag\n",
    "tbl_PLD_original_GRND['DAS_GRND'] = tbl_PLD_original_GRND.apply(lambda row: \n",
    "                                                            DAS_Comm_Ground if (row['DAS_CATEGORY'] == 'DAS' and row['RESIDENTIAL_FLAG'] is False) else \n",
    "                                                           (DAS_Comm_Extended_Ground if (row['DAS_CATEGORY'] == 'DASE' and row['RESIDENTIAL_FLAG'] is False) else\n",
    "                                                           (DAS_Resi_Ground if (row['DAS_CATEGORY'] == 'DAS' and row['RESIDENTIAL_FLAG'] is True) else \n",
    "                                                           (DAS_Resi_Extended_Ground if (row['DAS_CATEGORY'] == 'DASE' and row['RESIDENTIAL_FLAG'] is True) else \n",
    "                                                           (Remote_Area if (row['DAS_CATEGORY'] == 'RA') else \n",
    "                                                           (Remote_Area_Alaska if (row['DAS_CATEGORY'] == 'AK') else\n",
    "                                                           (Remote_Area_Hawaii if (row['DAS_CATEGORY'] == 'HI') else 0)))))), axis=1)\n",
    "\n",
    "\n",
    "#add fuel surcharge % column to PLD\n",
    "tbl_PLD_original_GRND['SHIP_DATE'] = pd.to_datetime(tbl_PLD_original_GRND['SHIP_DATE']).dt.date\n",
    "# get unique values from column 'SHIP_DATE' and create a new dataframe\n",
    "new_df = pd.DataFrame({'SHIP_DATE': tbl_PLD_original_GRND['SHIP_DATE'].unique()})\n",
    "# use this new dataframe to merge to FSC\n",
    "df_merge = new_df.merge(UPS_FSC, how='cross')\n",
    "df_merge['SHIP_DATE'] = pd.to_datetime(df_merge['SHIP_DATE']).dt.date\n",
    "#slice the merged dataframe to make sure ship_date is between the ship_data_start and end. Now we have a FSC file with Ship_date col to merge back to pld so that the pld can get FSC.\n",
    "df_merge = df_merge.query('SHIP_DATE >= SHIP_DATE_START and SHIP_DATE <= SHIP_DATE_END')\n",
    "df_merge = df_merge.drop_duplicates()\n",
    "# Get Ground column as FSC. \n",
    "tbl_PLD_original_GRND = tbl_PLD_original_GRND.merge(df_merge, on=['SHIP_DATE'], how='left').drop(['SHIP_DATE_START', 'SHIP_DATE_END', 'Domestic Air'], axis=1)\n",
    "tbl_PLD_original_GRND.rename(columns={'Ground': 'FSC%_GRND'}, inplace=True)\n",
    "\n",
    "\n",
    "\n",
    "#add AHS column to PLD\n",
    "tbl_PLD_original_GRND['ZONE'] = pd.to_numeric(tbl_PLD_original_GRND['ZONE'], errors='coerce').fillna(0).astype(int)\n",
    "\n",
    "tbl_PLD_original_GRND['AHS_1'] = tbl_PLD_original_GRND.apply(lambda row: \n",
    "                                                           AH_Girth_2 if (row['GIRTH_AND_L'] > 105 and row['ZONE'] == 2) else \n",
    "                                                           (AH_Girth_3_4 if (row['GIRTH_AND_L'] > 105 and row['ZONE'] > 2 and row['ZONE'] < 5) else\n",
    "                                                           (AH_Girth_5_up if (row['GIRTH_AND_L'] > 105 and row['ZONE'] > 4) else 0)), axis=1)\n",
    "                                                    \n",
    "tbl_PLD_original_GRND['AHS_2'] = tbl_PLD_original_GRND.apply(lambda row: \n",
    "                                                           AH_L_2 if (row['L'] > 48 and row['ZONE'] == 2) else \n",
    "                                                           (AH_L_3_4 if (row['L'] > 48 and row['ZONE'] > 2 and row['ZONE'] < 5) else \n",
    "                                                           (AH_L_5_up if (row['L'] > 48 and row['ZONE'] > 4) else 0)), axis=1)\n",
    "\n",
    "tbl_PLD_original_GRND['AHS_3'] = tbl_PLD_original_GRND.apply(lambda row:\n",
    "                                                           AH_W_2 if (row['W'] > 30 and row['ZONE'] == 2) else \n",
    "                                                           (AH_W_3_4 if (row['W'] > 30 and row['ZONE'] > 2 and row['ZONE'] < 5) else \n",
    "                                                           (AH_W_5_up if (row['W'] > 30 and row['ZONE'] > 4) else 0)), axis=1)\n",
    "                                        \n",
    "tbl_PLD_original_GRND['AHS_4'] = tbl_PLD_original_GRND.apply(lambda row:\n",
    "                                                           AH_WGT_2 if (row['ACTUAL_WEIGHT'] > 50 and row['ZONE'] == 2) else \n",
    "                                                           (AH_WGT_3_4 if (row['ACTUAL_WEIGHT'] > 50 and row['ZONE'] > 2 and row['ZONE'] < 5) else \n",
    "                                                           (AH_WGT_5_up if (row['ACTUAL_WEIGHT'] > 50 and row['ZONE'] > 4) else 0)), axis=1)\n",
    "\n",
    "tbl_PLD_original_GRND['AHS_GRND'] = tbl_PLD_original_GRND['AHS_1'] + tbl_PLD_original_GRND['AHS_2'] + tbl_PLD_original_GRND['AHS_3'] + tbl_PLD_original_GRND['AHS_4']\n",
    "tbl_PLD_original_GRND = tbl_PLD_original_GRND.drop(['AHS_1','AHS_2','AHS_3','AHS_4'], axis=1)\n",
    "\n",
    "#Get Total rate. Set total rate to null if the freight rate is null and/or some package parameters are out of carrier handling capability.  \n",
    "tbl_PLD_original_GRND['TOTAL_GRND'] = round(tbl_PLD_original_GRND.apply(lambda row: \n",
    "                                                                  (1 + row['FSC%_GRND']) * (row['FRT_GRND'] + row['RES_GRND'] + row['DAS_GRND'] + row['AHS_GRND']) if not pd.isna(row['FRT_GRND']) and row['FRT_GRND'] != '' else np.nan, axis=1),2)\n",
    "#tbl_PLD_original_GRND.to_csv('test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sensitive-belgium",
   "metadata": {},
   "source": [
    "# GRES (UPS Ground Residential)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 998,
   "id": "51a4c030",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get the right client rate and the corresponding PLD\n",
    "tbl_clientRates_8 = tbl_clientRates[(tbl_clientRates['SERVICE_CODE'] == 'GRES')]\n",
    "tbl_PLD_original_GRES = tbl_BP[(tbl_BP['RESIDENTIAL_FLAG'] == True)].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 999,
   "id": "durable-liver",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Rerate start\n",
    "#add billed weight column to PLD\n",
    "tbl_PLD_original_GRES['BILLED_WEIGHT_LB'] = tbl_PLD_original_GRES.apply(lambda row: max(row['ACTUAL_WEIGHT_LB'], np.ceil(row['CUBIC_INCH']/dim_factor_ups)), axis=1)\n",
    "\n",
    "#add Freight column to PLD\n",
    "tbl_PLD_original_GRES['ZONE'] = tbl_PLD_original_GRES['ZONE'].astype(str)\n",
    "tbl_PLD_original_GRES = tbl_PLD_original_GRES.merge(tbl_clientRates_8[[\"WEIGHT_LB\", \"ZONE\", \"CLIENT_RATES\"]], left_on=['BILLED_WEIGHT_LB', 'ZONE'], right_on=['WEIGHT_LB', 'ZONE'], how='left')\n",
    "tbl_PLD_original_GRES = tbl_PLD_original_GRES.drop('WEIGHT_LB', axis=1)\n",
    "tbl_PLD_original_GRES = tbl_PLD_original_GRES.rename(columns={'CLIENT_RATES': 'FRT_GRES'})\n",
    "   \n",
    "    \n",
    "#add Residential Surcharge column to PLD\n",
    "tbl_PLD_original_GRES['RES_GRES'] = tbl_PLD_original_GRES.apply(lambda row: Resi_Surcharge_Ground if row['RESIDENTIAL_FLAG'] is True else 0, axis=1)\n",
    "\n",
    "#add DAS column to PLD, Pay attention to the flags under Residential Flag\n",
    "tbl_PLD_original_GRES['DAS_GRES'] = tbl_PLD_original_GRES.apply(lambda row: \n",
    "                                                            DAS_Comm_Ground if (row['DAS_CATEGORY'] == 'DAS' and row['RESIDENTIAL_FLAG'] is False) else \n",
    "                                                           (DAS_Comm_Extended_Ground if (row['DAS_CATEGORY'] == 'DASE' and row['RESIDENTIAL_FLAG'] is False) else\n",
    "                                                           (DAS_Resi_Ground if (row['DAS_CATEGORY'] == 'DAS' and row['RESIDENTIAL_FLAG'] is True) else \n",
    "                                                           (DAS_Resi_Extended_Ground if (row['DAS_CATEGORY'] == 'DASE' and row['RESIDENTIAL_FLAG'] is True) else \n",
    "                                                           (Remote_Area if (row['DAS_CATEGORY'] == 'RA') else \n",
    "                                                           (Remote_Area_Alaska if (row['DAS_CATEGORY'] == 'AK') else\n",
    "                                                           (Remote_Area_Hawaii if (row['DAS_CATEGORY'] == 'HI') else 0)))))), axis=1)\n",
    "\n",
    "\n",
    "#add fuel surcharge % column to PLD\n",
    "tbl_PLD_original_GRES['SHIP_DATE'] = pd.to_datetime(tbl_PLD_original_GRES['SHIP_DATE']).dt.date\n",
    "# get unique values from column 'SHIP_DATE' and create a new dataframe\n",
    "new_df = pd.DataFrame({'SHIP_DATE': tbl_PLD_original_GRES['SHIP_DATE'].unique()})\n",
    "# use this new dataframe to merge to FSC\n",
    "df_merge = new_df.merge(UPS_FSC, how='cross')\n",
    "df_merge['SHIP_DATE'] = pd.to_datetime(df_merge['SHIP_DATE']).dt.date\n",
    "#slice the merged dataframe to make sure ship_date is between the ship_data_start and end. Now we have a FSC file with Ship_date col to merge back to pld so that the pld can get FSC.\n",
    "df_merge = df_merge.query('SHIP_DATE >= SHIP_DATE_START and SHIP_DATE <= SHIP_DATE_END')\n",
    "df_merge = df_merge.drop_duplicates()\n",
    "# Get Ground column as FSC. \n",
    "tbl_PLD_original_GRES = tbl_PLD_original_GRES.merge(df_merge, on=['SHIP_DATE'], how='left').drop(['SHIP_DATE_START', 'SHIP_DATE_END', 'Domestic Air'], axis=1)\n",
    "tbl_PLD_original_GRES.rename(columns={'Ground': 'FSC%_GRES'}, inplace=True)\n",
    "\n",
    "\n",
    "\n",
    "#add AHS column to PLD\n",
    "tbl_PLD_original_GRES['ZONE'] = pd.to_numeric(tbl_PLD_original_GRES['ZONE'], errors='coerce').fillna(0).astype(int)\n",
    "\n",
    "tbl_PLD_original_GRES['AHS_1'] = tbl_PLD_original_GRES.apply(lambda row: \n",
    "                                                           AH_Girth_2 if (row['GIRTH_AND_L'] > 105 and row['ZONE'] == 2) else \n",
    "                                                           (AH_Girth_3_4 if (row['GIRTH_AND_L'] > 105 and row['ZONE'] > 2 and row['ZONE'] < 5) else\n",
    "                                                           (AH_Girth_5_up if (row['GIRTH_AND_L'] > 105 and row['ZONE'] > 4) else 0)), axis=1)\n",
    "                                                    \n",
    "tbl_PLD_original_GRES['AHS_2'] = tbl_PLD_original_GRES.apply(lambda row: \n",
    "                                                           AH_L_2 if (row['L'] > 48 and row['ZONE'] == 2) else \n",
    "                                                           (AH_L_3_4 if (row['L'] > 48 and row['ZONE'] > 2 and row['ZONE'] < 5) else \n",
    "                                                           (AH_L_5_up if (row['L'] > 48 and row['ZONE'] > 4) else 0)), axis=1)\n",
    "\n",
    "tbl_PLD_original_GRES['AHS_3'] = tbl_PLD_original_GRES.apply(lambda row:\n",
    "                                                           AH_W_2 if (row['W'] > 30 and row['ZONE'] == 2) else \n",
    "                                                           (AH_W_3_4 if (row['W'] > 30 and row['ZONE'] > 2 and row['ZONE'] < 5) else \n",
    "                                                           (AH_W_5_up if (row['W'] > 30 and row['ZONE'] > 4) else 0)), axis=1)\n",
    "                                        \n",
    "tbl_PLD_original_GRES['AHS_4'] = tbl_PLD_original_GRES.apply(lambda row:\n",
    "                                                           AH_WGT_2 if (row['ACTUAL_WEIGHT'] > 50 and row['ZONE'] == 2) else \n",
    "                                                           (AH_WGT_3_4 if (row['ACTUAL_WEIGHT'] > 50 and row['ZONE'] > 2 and row['ZONE'] < 5) else \n",
    "                                                           (AH_WGT_5_up if (row['ACTUAL_WEIGHT'] > 50 and row['ZONE'] > 4) else 0)), axis=1)\n",
    "\n",
    "tbl_PLD_original_GRES['AHS_GRES'] = tbl_PLD_original_GRES['AHS_1'] + tbl_PLD_original_GRES['AHS_2'] + tbl_PLD_original_GRES['AHS_3'] + tbl_PLD_original_GRES['AHS_4']\n",
    "tbl_PLD_original_GRES = tbl_PLD_original_GRES.drop(['AHS_1','AHS_2','AHS_3','AHS_4'], axis=1)\n",
    "\n",
    "#Get Total rate. Set total rate to null if the freight rate is null and/or some package parameters are out of carrier handling capability.  \n",
    "tbl_PLD_original_GRES['TOTAL_GRES'] = round(tbl_PLD_original_GRES.apply(lambda row: \n",
    "                                                                  (1 + row['FSC%_GRES']) * (row['FRT_GRES'] + row['RES_GRES'] + row['DAS_GRES'] + row['AHS_GRES']) if not pd.isna(row['FRT_GRES']) and row['FRT_GRES'] != '' else np.nan, axis=1),2)\n",
    "#tbl_PLD_original_GRES.to_csv('test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hungarian-fishing",
   "metadata": {},
   "source": [
    "# SRPT<1 (UPS Surepost 1#>)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1000,
   "id": "4cb0cab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get the right client rate and the corresponding PLD\n",
    "tbl_clientRates_9 = tbl_clientRates[(tbl_clientRates['SERVICE_CODE'] == 'SRPT<1')]\n",
    "tbl_PLD_original_SRPT1 = tbl_BP[(tbl_BP['ACTUAL_WEIGHT'] < 1)].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1001,
   "id": "listed-graduation",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Rerate start\n",
    "#add billed weight column to PLD\n",
    "tbl_PLD_original_SRPT1['BILLED_WEIGHT_OZ'] = tbl_PLD_original_SRPT1.apply(lambda row: max(row['ACTUAL_WEIGHT_LB'], np.ceil(row['CUBIC_INCH']/dim_factor_ups_srpt))*16 if (row['CUBIC_INCH'] >= 3456) else\n",
    "                                                                          (row['ACTUAL_WEIGHT_OZ']), axis=1)\n",
    "\n",
    "\n",
    "#add Freight column to PLD\n",
    "tbl_PLD_original_SRPT1['ZONE'] = tbl_PLD_original_SRPT1['ZONE'].astype(str)\n",
    "tbl_PLD_original_SRPT1 = tbl_PLD_original_SRPT1.merge(tbl_clientRates_9[[\"WEIGHT_OZ\", \"ZONE\", \"CLIENT_RATES\"]], left_on=['BILLED_WEIGHT_OZ', 'ZONE'], right_on=['WEIGHT_OZ', 'ZONE'], how='left')\n",
    "tbl_PLD_original_SRPT1 = tbl_PLD_original_SRPT1.drop('WEIGHT_OZ', axis=1)\n",
    "tbl_PLD_original_SRPT1 = tbl_PLD_original_SRPT1.rename(columns={'CLIENT_RATES': 'FRT_SRPT1'})\n",
    "   \n",
    "    \n",
    "#add Residential Surcharge column to PLD\n",
    "tbl_PLD_original_SRPT1['RES_SRPT1'] = tbl_PLD_original_SRPT1.apply(lambda row: 0 if row['RESIDENTIAL_FLAG'] == True else 0, axis=1)\n",
    "\n",
    "#add DAS column to PLD, Pay attention to the flags under Residential Flag\n",
    "tbl_PLD_original_SRPT1['DAS_SRPT1'] = tbl_PLD_original_SRPT1.apply(lambda row: \n",
    "                                                            DAS_SurePost if (row['DAS_CATEGORY'] == 'DAS') else \n",
    "                                                           (DAS_Extended_SurePost if (row['DAS_CATEGORY'] == 'DASE') else \n",
    "                                                           (Remote_Area if (row['DAS_CATEGORY'] == 'RA') else \n",
    "                                                           (Remote_Area_Alaska if (row['DAS_CATEGORY'] == 'AK') else\n",
    "                                                           (Remote_Area_Hawaii if (row['DAS_CATEGORY'] == 'HI') else 0)))), axis=1)\n",
    "\n",
    "\n",
    "\n",
    "#add fuel surcharge % column to PLD\n",
    "tbl_PLD_original_SRPT1['SHIP_DATE'] = pd.to_datetime(tbl_PLD_original_SRPT1['SHIP_DATE']).dt.date\n",
    "# get unique values from column 'SHIP_DATE' and create a new dataframe\n",
    "new_df = pd.DataFrame({'SHIP_DATE': tbl_PLD_original_SRPT1['SHIP_DATE'].unique()})\n",
    "# use this new dataframe to merge to FSC\n",
    "df_merge = new_df.merge(UPS_FSC, how='cross')\n",
    "df_merge['SHIP_DATE'] = pd.to_datetime(df_merge['SHIP_DATE']).dt.date\n",
    "#slice the merged dataframe to make sure ship_date is between the ship_data_start and end. Now we have a FSC file with Ship_date col to merge back to pld so that the pld can get FSC.\n",
    "df_merge = df_merge.query('SHIP_DATE >= SHIP_DATE_START and SHIP_DATE <= SHIP_DATE_END')\n",
    "df_merge = df_merge.drop_duplicates()\n",
    "# Get Ground column as FSC. \n",
    "tbl_PLD_original_SRPT1 = tbl_PLD_original_SRPT1.merge(df_merge, on=['SHIP_DATE'], how='left').drop(['SHIP_DATE_START', 'SHIP_DATE_END', 'Domestic Air'], axis=1)\n",
    "tbl_PLD_original_SRPT1.rename(columns={'Ground': 'FSC%_SRPT1'}, inplace=True)\n",
    "\n",
    "\n",
    "\n",
    "#add AHS column to PLD\n",
    "tbl_PLD_original_SRPT1['ZONE'] = pd.to_numeric(tbl_PLD_original_SRPT1['ZONE'], errors='coerce').fillna(0).astype(int)\n",
    "\n",
    "tbl_PLD_original_SRPT1['AHS_1'] = tbl_PLD_original_SRPT1.apply(lambda row: \n",
    "                                                           AH_Girth_2 if (row['GIRTH_AND_L'] > 105 and row['ZONE'] == 2) else \n",
    "                                                           (AH_Girth_3_4 if (row['GIRTH_AND_L'] > 105 and row['ZONE'] > 2 and row['ZONE'] < 5) else\n",
    "                                                           (AH_Girth_5_up if (row['GIRTH_AND_L'] > 105 and row['ZONE'] > 4) else 0)), axis=1)\n",
    "                                                    \n",
    "tbl_PLD_original_SRPT1['AHS_2'] = tbl_PLD_original_SRPT1.apply(lambda row: \n",
    "                                                           AH_L_2 if (row['L'] > 48 and row['ZONE'] == 2) else \n",
    "                                                           (AH_L_3_4 if (row['L'] > 48 and row['ZONE'] > 2 and row['ZONE'] < 5) else \n",
    "                                                           (AH_L_5_up if (row['L'] > 48 and row['ZONE'] > 4) else 0)), axis=1)\n",
    "\n",
    "tbl_PLD_original_SRPT1['AHS_3'] = tbl_PLD_original_SRPT1.apply(lambda row:\n",
    "                                                           AH_W_2 if (row['W'] > 30 and row['ZONE'] == 2) else \n",
    "                                                           (AH_W_3_4 if (row['W'] > 30 and row['ZONE'] > 2 and row['ZONE'] < 5) else \n",
    "                                                           (AH_W_5_up if (row['W'] > 30 and row['ZONE'] > 4) else 0)), axis=1)\n",
    "                                        \n",
    "tbl_PLD_original_SRPT1['AHS_4'] = tbl_PLD_original_SRPT1.apply(lambda row:\n",
    "                                                           AH_WGT_2 if (row['ACTUAL_WEIGHT'] > 50 and row['ZONE'] == 2) else \n",
    "                                                           (AH_WGT_3_4 if (row['ACTUAL_WEIGHT'] > 50 and row['ZONE'] > 2 and row['ZONE'] < 5) else \n",
    "                                                           (AH_WGT_5_up if (row['ACTUAL_WEIGHT'] > 50 and row['ZONE'] > 4) else 0)), axis=1)\n",
    "\n",
    "tbl_PLD_original_SRPT1['AHS_SRPT1'] = tbl_PLD_original_SRPT1['AHS_1'] + tbl_PLD_original_SRPT1['AHS_2'] + tbl_PLD_original_SRPT1['AHS_3'] + tbl_PLD_original_SRPT1['AHS_4']\n",
    "tbl_PLD_original_SRPT1 = tbl_PLD_original_SRPT1.drop(['AHS_1','AHS_2','AHS_3','AHS_4'], axis=1)\n",
    "\n",
    "#Get Total rate. Set total rate to null if the freight rate is null and/or some package parameters are out of carrier handling capability.  \n",
    "tbl_PLD_original_SRPT1['TOTAL_SRPT1'] = round(tbl_PLD_original_SRPT1.apply(lambda row: \n",
    "                                                                  (1 + row['FSC%_SRPT1']) * (row['FRT_SRPT1'] + row['RES_SRPT1'] + row['DAS_SRPT1'] + row['AHS_SRPT1']) if not pd.isna(row['FRT_SRPT1']) and row['FRT_SRPT1'] != '' else np.nan, axis=1),2)\n",
    "#tbl_PLD_original_SRPT1.to_csv('test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sorted-tobacco",
   "metadata": {},
   "source": [
    "# SRPT (UPS Surepost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1002,
   "id": "c395a83b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get the right client rate and the corresponding PLD\n",
    "#get client rate and a copy of PLD\n",
    "tbl_clientRates_10 = tbl_clientRates[(tbl_clientRates['SERVICE_CODE'] == 'SRPT')]\n",
    "tbl_PLD_original_SRPT = tbl_BP.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1003,
   "id": "quarterly-taiwan",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Rerate start\n",
    "#add billed weight column to PLD\n",
    "tbl_PLD_original_SRPT['BILLED_WEIGHT_LB'] = tbl_PLD_original_SRPT.apply(lambda row: max(row['ACTUAL_WEIGHT_LB'], np.ceil(row['CUBIC_INCH']/dim_factor_ups_srpt)) if (row['CUBIC_INCH'] >= 3456) else\n",
    "                                                                          (row['ACTUAL_WEIGHT_LB']), axis=1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#add Freight column to PLD\n",
    "tbl_PLD_original_SRPT['ZONE'] = tbl_PLD_original_SRPT['ZONE'].astype(str)\n",
    "tbl_PLD_original_SRPT = tbl_PLD_original_SRPT.merge(tbl_clientRates_10[[\"WEIGHT_LB\", \"ZONE\", \"CLIENT_RATES\"]], left_on=['BILLED_WEIGHT_LB', 'ZONE'], right_on=['WEIGHT_LB', 'ZONE'], how='left')\n",
    "tbl_PLD_original_SRPT = tbl_PLD_original_SRPT.drop('WEIGHT_LB', axis=1)\n",
    "tbl_PLD_original_SRPT = tbl_PLD_original_SRPT.rename(columns={'CLIENT_RATES': 'FRT_SRPT'})\n",
    "   \n",
    "\n",
    "#add Residential Surcharge column to PLD\n",
    "tbl_PLD_original_SRPT['RES_SRPT'] = tbl_PLD_original_SRPT.apply(lambda row: 0 if row['RESIDENTIAL_FLAG'] == True else 0, axis=1)\n",
    "\n",
    "#add DAS column to PLD, Pay attention to the flags under Residential Flag\n",
    "tbl_PLD_original_SRPT['DAS_SRPT'] = tbl_PLD_original_SRPT.apply(lambda row: \n",
    "                                                            DAS_SurePost if (row['DAS_CATEGORY'] == 'DAS') else \n",
    "                                                           (DAS_Extended_SurePost if (row['DAS_CATEGORY'] == 'DASE') else \n",
    "                                                           (Remote_Area if (row['DAS_CATEGORY'] == 'RA') else \n",
    "                                                           (Remote_Area_Alaska if (row['DAS_CATEGORY'] == 'AK') else\n",
    "                                                           (Remote_Area_Hawaii if (row['DAS_CATEGORY'] == 'HI') else 0)))), axis=1)\n",
    "\n",
    "\n",
    "#add fuel surcharge % column to PLD\n",
    "tbl_PLD_original_SRPT['SHIP_DATE'] = pd.to_datetime(tbl_PLD_original_SRPT['SHIP_DATE']).dt.date\n",
    "# get unique values from column 'SHIP_DATE' and create a new dataframe\n",
    "new_df = pd.DataFrame({'SHIP_DATE': tbl_PLD_original_SRPT['SHIP_DATE'].unique()})\n",
    "# use this new dataframe to merge to FSC\n",
    "df_merge = new_df.merge(UPS_FSC, how='cross')\n",
    "df_merge['SHIP_DATE'] = pd.to_datetime(df_merge['SHIP_DATE']).dt.date\n",
    "#slice the merged dataframe to make sure ship_date is between the ship_data_start and end. Now we have a FSC file with Ship_date col to merge back to pld so that the pld can get FSC.\n",
    "df_merge = df_merge.query('SHIP_DATE >= SHIP_DATE_START and SHIP_DATE <= SHIP_DATE_END')\n",
    "df_merge = df_merge.drop_duplicates()\n",
    "# Get Ground column as FSC. \n",
    "tbl_PLD_original_SRPT = tbl_PLD_original_SRPT.merge(df_merge, on=['SHIP_DATE'], how='left').drop(['SHIP_DATE_START', 'SHIP_DATE_END', 'Domestic Air'], axis=1)\n",
    "tbl_PLD_original_SRPT.rename(columns={'Ground': 'FSC%_SRPT'}, inplace=True)\n",
    "\n",
    "\n",
    "\n",
    "#add AHS column to PLD\n",
    "tbl_PLD_original_SRPT['ZONE'] = pd.to_numeric(tbl_PLD_original_SRPT['ZONE'], errors='coerce').fillna(0).astype(int)\n",
    "\n",
    "tbl_PLD_original_SRPT['AHS_1'] = tbl_PLD_original_SRPT.apply(lambda row: \n",
    "                                                           AH_Girth_2 if (row['GIRTH_AND_L'] > 105 and row['ZONE'] == 2) else \n",
    "                                                           (AH_Girth_3_4 if (row['GIRTH_AND_L'] > 105 and row['ZONE'] > 2 and row['ZONE'] < 5) else\n",
    "                                                           (AH_Girth_5_up if (row['GIRTH_AND_L'] > 105 and row['ZONE'] > 4) else 0)), axis=1)\n",
    "                                                    \n",
    "tbl_PLD_original_SRPT['AHS_2'] = tbl_PLD_original_SRPT.apply(lambda row: \n",
    "                                                           AH_L_2 if (row['L'] > 48 and row['ZONE'] == 2) else \n",
    "                                                           (AH_L_3_4 if (row['L'] > 48 and row['ZONE'] > 2 and row['ZONE'] < 5) else \n",
    "                                                           (AH_L_5_up if (row['L'] > 48 and row['ZONE'] > 4) else 0)), axis=1)\n",
    "\n",
    "tbl_PLD_original_SRPT['AHS_3'] = tbl_PLD_original_SRPT.apply(lambda row:\n",
    "                                                           AH_W_2 if (row['W'] > 30 and row['ZONE'] == 2) else \n",
    "                                                           (AH_W_3_4 if (row['W'] > 30 and row['ZONE'] > 2 and row['ZONE'] < 5) else \n",
    "                                                           (AH_W_5_up if (row['W'] > 30 and row['ZONE'] > 4) else 0)), axis=1)\n",
    "                                        \n",
    "tbl_PLD_original_SRPT['AHS_4'] = tbl_PLD_original_SRPT.apply(lambda row:\n",
    "                                                           AH_WGT_2 if (row['ACTUAL_WEIGHT'] > 50 and row['ZONE'] == 2) else \n",
    "                                                           (AH_WGT_3_4 if (row['ACTUAL_WEIGHT'] > 50 and row['ZONE'] > 2 and row['ZONE'] < 5) else \n",
    "                                                           (AH_WGT_5_up if (row['ACTUAL_WEIGHT'] > 50 and row['ZONE'] > 4) else 0)), axis=1)\n",
    "\n",
    "tbl_PLD_original_SRPT['AHS_SRPT'] = tbl_PLD_original_SRPT['AHS_1'] + tbl_PLD_original_SRPT['AHS_2'] + tbl_PLD_original_SRPT['AHS_3'] + tbl_PLD_original_SRPT['AHS_4']\n",
    "tbl_PLD_original_SRPT = tbl_PLD_original_SRPT.drop(['AHS_1','AHS_2','AHS_3','AHS_4'], axis=1)\n",
    "\n",
    "#Get Total rate. Set total rate to null if the freight rate is null and/or some package parameters are out of carrier handling capability.  \n",
    "tbl_PLD_original_SRPT['TOTAL_SRPT'] = round(tbl_PLD_original_SRPT.apply(lambda row: \n",
    "                                                                  (1 + row['FSC%_SRPT']) * (row['FRT_SRPT'] + row['RES_SRPT'] + row['DAS_SRPT'] + row['AHS_SRPT']) if not pd.isna(row['FRT_SRPT']) and row['FRT_SRPT'] != '' else np.nan, axis=1),2)\n",
    "#tbl_PLD_original_SRPT.to_csv('test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "arbitrary-petersburg",
   "metadata": {},
   "source": [
    "# DHLG (DHL SmartMail Parcel Plus Ground)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58aa8d45",
   "metadata": {},
   "source": [
    "5114358: Franklin (FRA) -> 02038_Outbound  \n",
    "5122444: Somerset, no DHLEM (JER) -> 08873_Outbound  \n",
    "5122723: 8150 Nail Olive Branch (MS2) -> 38654_Outbound  \n",
    "5122739: Nail rd Olive branch (MS3) -> 38654_Outbound  \n",
    "5122855: Garland TX (TX1) -> 75041_Outbound  \n",
    "5122890: Oofos -> is/will be in Byhalia, MS)  \n",
    "5122893: Memphis (TN2, TN3) -> 38141_Outbound  \n",
    "5123280: (MAN)  \n",
    "5123282: Bridgewater (MA5)  \n",
    "5123283: Curtis Bay (GBM) -> 21226_Outbound  \n",
    "5123556: Montebello -> 90640\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d56e1e3",
   "metadata": {},
   "source": [
    "# DHLG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1004,
   "id": "9f413c1e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<p style=\"font-family: 'Arial'; font-size: 20px; color: red; background-color: yellow;\">\n",
       "    Edit Location 3 of 8 (Only if you use DHL, default facility is MS3)\n",
       "</p>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "styled_html = HTML(\"\"\"\n",
    "<p style=\"font-family: 'Arial'; font-size: 20px; color: red; background-color: yellow;\">\n",
    "    Edit Location 3 of 8 (Only if you use DHL, default facility is MS3)\n",
    "</p>\n",
    "\"\"\")\n",
    "\n",
    "display(styled_html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1005,
   "id": "8b0416f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#select the rate to use, #Decide the part of PLD to rerate\n",
    "tbl_clientRates_11 = tbl_clientRates[(tbl_clientRates['SERVICE_CODE'] == 'DHLG_5122739')] # update the number here. \n",
    "tbl_PLD_original_DHLG = tbl_BP.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1006,
   "id": "48414e56",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Start to rerate\n",
    "#add billed weight column to PLD\n",
    "tbl_PLD_original_DHLG['BILLED_WEIGHT_LB'] = tbl_PLD_original_DHLG.apply(lambda row: max(row['ACTUAL_WEIGHT_LB'], np.ceil(row['CUBIC_INCH']/dim_factor_dhl)) if (row['CUBIC_INCH'] >= 1728 and row['ACTUAL_WEIGHT'] >= 1) else\n",
    "                                                                          (row['ACTUAL_WEIGHT_LB']), axis=1)\n",
    "\n",
    "#add Freight column to PLD\n",
    "tbl_PLD_original_DHLG['ZONE'] = tbl_PLD_original_DHLG['ZONE'].astype(str)\n",
    "tbl_PLD_original_DHLG = tbl_PLD_original_DHLG.merge(tbl_clientRates_11[[\"WEIGHT_LB\", \"ZONE\", \"CLIENT_RATES\"]], left_on=['BILLED_WEIGHT_LB', 'ZONE'], right_on=['WEIGHT_LB', 'ZONE'], how='left')\n",
    "tbl_PLD_original_DHLG = tbl_PLD_original_DHLG.drop('WEIGHT_LB', axis=1)\n",
    "tbl_PLD_original_DHLG = tbl_PLD_original_DHLG.rename(columns={'CLIENT_RATES': 'FRT_DHLG'})\n",
    "\n",
    "\n",
    "#add fuel surcharge % column to PLD\n",
    "tbl_PLD_original_DHLG['SHIP_DATE'] = pd.to_datetime(tbl_PLD_original_DHLG['SHIP_DATE']).dt.date\n",
    "# get unique values from column 'SHIP_DATE' and create a new dataframe\n",
    "new_df = pd.DataFrame({'SHIP_DATE': tbl_PLD_original_DHLG['SHIP_DATE'].unique()})\n",
    "# use this new dataframe to merge to FSC\n",
    "df_merge = new_df.merge(DHL_FSC, how='cross')\n",
    "df_merge['SHIP_DATE'] = pd.to_datetime(df_merge['SHIP_DATE']).dt.date\n",
    "#slice the merged dataframe to make sure ship_date is between the ship_data_start and end. Now we have a FSC file with Ship_date col to merge back to pld so that the pld can get FSC.\n",
    "df_merge = df_merge.query('SHIP_DATE >= SHIP_DATE_START and SHIP_DATE <= SHIP_DATE_END')\n",
    "df_merge = df_merge.drop_duplicates()\n",
    "\n",
    "\n",
    "\n",
    "# Get Domestic Air column as FSC. \n",
    "tbl_PLD_original_DHLG = tbl_PLD_original_DHLG.merge(df_merge, on=['SHIP_DATE'], how='left').drop(['SHIP_DATE_START', 'SHIP_DATE_END'], axis=1)\n",
    "tbl_PLD_original_DHLG.rename(columns={'FSC': 'FSC%_DHLG'}, inplace=True)\n",
    "tbl_PLD_original_DHLG['FSC_DHLG'] = tbl_PLD_original_DHLG.apply(lambda row: row['ACTUAL_WEIGHT']*row['FSC%_DHLG'] if (row['ACTUAL_WEIGHT'] <= 1) else\n",
    "                                                                          (row['BILLED_WEIGHT_LB']*row['FSC%_DHLG']), axis=1)\n",
    "\n",
    "\n",
    "#add NQD Surcharge column to PLD\n",
    "tbl_PLD_original_DHLG['SHIP_DATE'] = pd.to_datetime(tbl_PLD_original_DHLG['SHIP_DATE'])\n",
    "\n",
    "# billed weight calculation situations: \n",
    "# situation 1: actual weight <= 1 lb, use np.ceil(row['BILLED_WEIGHT_OZ']/16) or below, use 1 lb\n",
    "# situation 2: actual weight > 1 lb, and cubic inch < 1728, use round up (actual weight)\n",
    "# situation 3: actual weight > 1 lb, and cubic inch >= 1728, use billed weight\n",
    "\n",
    "#dim\n",
    "tbl_PLD_original_DHLG['NQD_DHLG_1'] = tbl_PLD_original_DHLG.apply(lambda row: \n",
    "                                                            2.5 * row['BILLED_WEIGHT_LB'] if (row['ACTUAL_WEIGHT'] >= 1 and row['CUBIC_INCH'] >= 1728 and row['SHIP_DATE'] >= nqd_dhl_date_3 and row['SHIP_DATE'] <= nqd_dhl_date_4) else \n",
    "                                                           (2 * row['BILLED_WEIGHT_LB'] if (row['ACTUAL_WEIGHT'] >= 1 and row['CUBIC_INCH'] >= 1728 and row['SHIP_DATE'] >= nqd_dhl_date_1 and row['SHIP_DATE'] <= nqd_dhl_date_2) else \n",
    "                                                           0), axis=1)\n",
    "\n",
    "#girth + L(As > 84 is unaccepted, needs a special character to mark it, however, we cannot add up this)\n",
    "tbl_PLD_original_DHLG['NQD_DHLG_2'] = tbl_PLD_original_DHLG.apply(lambda row:\n",
    "                                                           np.nan if (row['GIRTH_AND_L'] > 84) else (0), axis=1)\n",
    "\n",
    "#longest side_1\n",
    "tbl_PLD_original_DHLG['NQD_DHLG_3'] = tbl_PLD_original_DHLG.apply(lambda row: \n",
    "                                                            2.5 * row['BILLED_WEIGHT_LB'] if (max(row['L'], row['W'], row['H']) > 27 and row['SHIP_DATE'] >= nqd_dhl_date_3 and row['SHIP_DATE'] <= nqd_dhl_date_4) else \n",
    "                                                           (2 * row['BILLED_WEIGHT_LB'] if (max(row['L'], row['W'], row['H']) > 27 and row['SHIP_DATE'] >= nqd_dhl_date_1 and row['SHIP_DATE'] <= nqd_dhl_date_2) else\n",
    "                                                           0), axis=1)\n",
    "\n",
    "#longest side_2\n",
    "tbl_PLD_original_DHLG['NQD_DHLG_4'] = tbl_PLD_original_DHLG.apply(lambda row:\n",
    "                                                           4.5 if (max(row['L'], row['W'], row['H']) > 22 and max(row['L'], row['W'], row['H']) <= 30) else\n",
    "                                                           (15.5 if (max(row['L'], row['W'], row['H']) > 30) else \n",
    "                                                           0), axis=1)\n",
    "#volume\n",
    "tbl_PLD_original_DHLG['NQD_DHLG_5'] = tbl_PLD_original_DHLG.apply(lambda row: \n",
    "                                                            15.5 if (row['CUBIC_INCH'] > 3456) else \n",
    "                                                           (0), axis=1)\n",
    "\n",
    "\n",
    "tbl_PLD_original_DHLG['NQD_DHLG_OTHER'] = tbl_PLD_original_DHLG['NQD_DHLG_1'] + tbl_PLD_original_DHLG['NQD_DHLG_3'] + tbl_PLD_original_DHLG['NQD_DHLG_4'] + tbl_PLD_original_DHLG['NQD_DHLG_5']\n",
    "tbl_PLD_original_DHLG['NQD_DHLG_GIRTH_AND_L'] = tbl_PLD_original_DHLG['NQD_DHLG_2']\n",
    "tbl_PLD_original_DHLG = tbl_PLD_original_DHLG.drop(['NQD_DHLG_1','NQD_DHLG_2','NQD_DHLG_3','NQD_DHLG_4','NQD_DHLG_5'], axis=1)\n",
    "\n",
    "\n",
    "#Get Total\n",
    "tbl_PLD_original_DHLG['TOTAL_DHLG'] = round(tbl_PLD_original_DHLG.apply(lambda row: \n",
    "                                                                  np.nan if ((row['FRT_DHLG'] == '') | (pd.isna(row['FRT_DHLG'])) | (row['NQD_DHLG_GIRTH_AND_L'] == '') | (pd.isna(row['NQD_DHLG_GIRTH_AND_L'] == '')))\n",
    "                                                                  else\n",
    "                                                                  (row['FRT_DHLG'] + (row['FSC_DHLG'] + row['NQD_DHLG_OTHER'] + row['NQD_DHLG_GIRTH_AND_L'])/(1-Margin_on_DHL)), axis=1),2)\n",
    "\n",
    "\n",
    "#tbl_PLD_original_DHLG.to_csv('test.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aging-anchor",
   "metadata": {},
   "source": [
    "# DHLG<1 (DHL SmartMail Parcel Ground)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a9cd1ae",
   "metadata": {},
   "source": [
    "5114358: Franklin (FRA) -> 02038_Outbound  \n",
    "5122444: Somerset, no DHLEM (JER) -> 08873_Outbound  \n",
    "5122723: 8150 Nail Olive Branch (MS2) -> 38654_Outbound  \n",
    "5122739: Nail rd Olive branch (MS3) -> 38654_Outbound  \n",
    "5122855: Garland TX (TX1) -> 75041_Outbound  \n",
    "5122890: Oofos -> is/will be in Byhalia, MS)  \n",
    "5122893: Memphis (TN2, TN3) -> 38141_Outbound  \n",
    "5123280: (MAN)  \n",
    "5123282: Bridgewater (MA5)  \n",
    "5123283: Curtis Bay (GBM) -> 21226_Outbound"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9da1e82",
   "metadata": {},
   "source": [
    "# DHLG<1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1007,
   "id": "92caac1a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<p style=\"font-family: 'Arial'; font-size: 20px; color: red; background-color: yellow;\">\n",
       "    Edit Location 4 of 8 (Only if you use DHL, default facility is MS3)\n",
       "</p>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "styled_html = HTML(\"\"\"\n",
    "<p style=\"font-family: 'Arial'; font-size: 20px; color: red; background-color: yellow;\">\n",
    "    Edit Location 4 of 8 (Only if you use DHL, default facility is MS3)\n",
    "</p>\n",
    "\"\"\")\n",
    "\n",
    "display(styled_html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1008,
   "id": "bd8910ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "#select the rate to use, #Decide the part of PLD to rerate\n",
    "tbl_clientRates_12 = tbl_clientRates[(tbl_clientRates['SERVICE_CODE'] == 'DHLG<1_5122739')] # Edit the number here. \n",
    "tbl_PLD_original_DHLG1 = tbl_BP[(tbl_BP['ACTUAL_WEIGHT'] < 1)].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1009,
   "id": "detailed-kazakhstan",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Start to rerate\n",
    "#add billed weight column to PLD\n",
    "tbl_PLD_original_DHLG1['BILLED_WEIGHT_OZ'] = tbl_PLD_original_DHLG1.apply(lambda row: max(row['ACTUAL_WEIGHT_LB'], np.ceil(row['CUBIC_INCH']/dim_factor_dhl))*16 if (row['CUBIC_INCH'] >= 1728 and row['ACTUAL_WEIGHT'] >= 1) else\n",
    "                                                                          (row['ACTUAL_WEIGHT_OZ']), axis=1)\n",
    "\n",
    "\n",
    "#add Freight column to PLD\n",
    "tbl_PLD_original_DHLG1['ZONE'] = tbl_PLD_original_DHLG1['ZONE'].astype(str)\n",
    "tbl_PLD_original_DHLG1 = tbl_PLD_original_DHLG1.merge(tbl_clientRates_12[[\"WEIGHT_OZ\", \"ZONE\", \"CLIENT_RATES\"]], left_on=['BILLED_WEIGHT_OZ', 'ZONE'], right_on=['WEIGHT_OZ', 'ZONE'], how='left')\n",
    "tbl_PLD_original_DHLG1 = tbl_PLD_original_DHLG1.drop('WEIGHT_OZ', axis=1)\n",
    "tbl_PLD_original_DHLG1 = tbl_PLD_original_DHLG1.rename(columns={'CLIENT_RATES': 'FRT_DHLG1'})\n",
    "\n",
    "#add fuel surcharge % column to PLD\n",
    "tbl_PLD_original_DHLG1['SHIP_DATE'] = pd.to_datetime(tbl_PLD_original_DHLG1['SHIP_DATE']).dt.date\n",
    "# get unique values from column 'SHIP_DATE' and create a new dataframe\n",
    "# when actual weight < 1, use actual weight non round up to get FSC, otherwise, use billed weight to get FSC\n",
    "new_df = pd.DataFrame({'SHIP_DATE': tbl_PLD_original_DHLG1['SHIP_DATE'].unique()})\n",
    "# use this new dataframe to merge to FSC\n",
    "df_merge = new_df.merge(DHL_FSC, how='cross')\n",
    "df_merge['SHIP_DATE'] = pd.to_datetime(df_merge['SHIP_DATE']).dt.date\n",
    "#slice the merged dataframe to make sure ship_date is between the ship_data_start and end. Now we have a FSC file with Ship_date col to merge back to pld so that the pld can get FSC.\n",
    "df_merge = df_merge.query('SHIP_DATE >= SHIP_DATE_START and SHIP_DATE <= SHIP_DATE_END')\n",
    "df_merge = df_merge.drop_duplicates()\n",
    "# Get Domestic Air column as FSC. \n",
    "tbl_PLD_original_DHLG1 = tbl_PLD_original_DHLG1.merge(df_merge, on=['SHIP_DATE'], how='left').drop(['SHIP_DATE_START', 'SHIP_DATE_END'], axis=1)\n",
    "tbl_PLD_original_DHLG1.rename(columns={'FSC': 'FSC%_DHLG1'}, inplace=True)\n",
    "tbl_PLD_original_DHLG1['FSC_DHLG1'] = tbl_PLD_original_DHLG1.apply(lambda row: row['ACTUAL_WEIGHT']*row['FSC%_DHLG1'], axis=1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#add NQD Surcharge column to PLD\n",
    "tbl_PLD_original_DHLG1['SHIP_DATE'] = pd.to_datetime(tbl_PLD_original_DHLG1['SHIP_DATE'])\n",
    "\n",
    "# billed weight calculation situations: \n",
    "# situation 1: actual weight <= 1 lb, use np.ceil(row['BILLED_WEIGHT_OZ']/16) or below, use 1 lb\n",
    "# situation 2: actual weight > 1 lb, and cubic inch < 1728, use round up (actual weight)\n",
    "# situation 3: actual weight > 1 lb, and cubic inch >= 1728, use billed weight\n",
    "\n",
    "#dim, billed weight is used to determine NQD, there is no <1 lb here\n",
    "tbl_PLD_original_DHLG1['NQD_DHLG1_1'] = tbl_PLD_original_DHLG1.apply(lambda row: \n",
    "                                                            2.5 * max(row['ACTUAL_WEIGHT_LB'], np.ceil(row['CUBIC_INCH']/dim_factor_dhl)) if (row['ACTUAL_WEIGHT'] >= 1 and row['CUBIC_INCH'] >= 1728 and row['SHIP_DATE'] >= nqd_dhl_date_3 and row['SHIP_DATE'] <= nqd_dhl_date_4) else \n",
    "                                                           (2 * max(row['ACTUAL_WEIGHT_LB'], np.ceil(row['CUBIC_INCH']/dim_factor_dhl)) if (row['ACTUAL_WEIGHT'] >= 1 and row['CUBIC_INCH'] >= 1728 and row['SHIP_DATE'] >= nqd_dhl_date_1 and row['SHIP_DATE'] <= nqd_dhl_date_2) else \n",
    "                                                           0), axis=1)\n",
    "\n",
    "\n",
    "#girth + L(As > 84 is unaccepted, needs a special character to mark it, however, we cannot add up this, so we make it null)\n",
    "#1 over 1lbs, under lbs\n",
    "#and those < 1 lb will use lb to determine NQD\n",
    "# situation 1: actual weight <= 1 lb, use np.ceil(row['BILLED_WEIGHT_OZ']/16)\n",
    "# situation 2: actual weight > 1 lb, and cubic inch < 1728, use actual weight\n",
    "# situation 3: actual weight > 1 lb, and cubic inch >= 1728, use billed weight\n",
    "tbl_PLD_original_DHLG1['NQD_DHLG1_2'] = tbl_PLD_original_DHLG1.apply(lambda row:\n",
    "                                                            np.nan if (row['GIRTH_AND_L'] > 84) else (0), axis=1)\n",
    "\n",
    "#longest side_1\n",
    "tbl_PLD_original_DHLG1['NQD_DHLG1_3'] = tbl_PLD_original_DHLG1.apply(lambda row: \n",
    "                                                            2.5 * row['ACTUAL_WEIGHT_LB'] if (max(row['L'], row['W'], row['H']) > 27 and row['SHIP_DATE'] >= nqd_dhl_date_3 and row['SHIP_DATE'] <= nqd_dhl_date_4) else \n",
    "                                                           (2 * row['ACTUAL_WEIGHT_LB'] if (max(row['L'], row['W'], row['H']) > 27 and row['SHIP_DATE'] >= nqd_dhl_date_1 and row['SHIP_DATE'] <= nqd_dhl_date_2) else\n",
    "                                                           0), axis=1)\n",
    "\n",
    "#longest side_2\n",
    "tbl_PLD_original_DHLG1['NQD_DHLG1_4'] = tbl_PLD_original_DHLG1.apply(lambda row:\n",
    "                                                           4.5 if (max(row['L'], row['W'], row['H']) > 22 and max(row['L'], row['W'], row['H']) <= 30) else\n",
    "                                                           (15.5 if (max(row['L'], row['W'], row['H']) > 30) else \n",
    "                                                           0), axis=1)\n",
    "#volume\n",
    "tbl_PLD_original_DHLG1['NQD_DHLG1_5'] = tbl_PLD_original_DHLG1.apply(lambda row: \n",
    "                                                            15.5 if (row['CUBIC_INCH'] > 3456) else \n",
    "                                                           (0), axis=1)\n",
    "\n",
    "\n",
    "tbl_PLD_original_DHLG1['NQD_DHLG1_OTHER'] = tbl_PLD_original_DHLG1['NQD_DHLG1_1'] + tbl_PLD_original_DHLG1['NQD_DHLG1_3'] + tbl_PLD_original_DHLG1['NQD_DHLG1_4'] + tbl_PLD_original_DHLG1['NQD_DHLG1_5']\n",
    "tbl_PLD_original_DHLG1['NQD_DHLG1_GIRTH_AND_L'] = tbl_PLD_original_DHLG1['NQD_DHLG1_2']\n",
    "tbl_PLD_original_DHLG1 = tbl_PLD_original_DHLG1.drop(['NQD_DHLG1_1','NQD_DHLG1_2','NQD_DHLG1_3','NQD_DHLG1_4','NQD_DHLG1_5'], axis=1)\n",
    "\n",
    "\n",
    "#Get Total\n",
    "tbl_PLD_original_DHLG1['TOTAL_DHLG1'] = round(tbl_PLD_original_DHLG1.apply(lambda row: \n",
    "                                                                  np.nan if ((row['FRT_DHLG1'] == '') | (pd.isna(row['FRT_DHLG1'])) | (row['NQD_DHLG1_GIRTH_AND_L'] == '') | (pd.isna(row['NQD_DHLG1_GIRTH_AND_L'] == '')))\n",
    "                                                                  else\n",
    "                                                                  (row['FRT_DHLG1'] + (row['FSC_DHLG1'] + row['NQD_DHLG1_OTHER'] + row['NQD_DHLG1_GIRTH_AND_L'])/(1-Margin_on_DHL)), axis=1),2)\n",
    "\n",
    "#tbl_PLD_original_DHLG1.to_csv('test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "coastal-discipline",
   "metadata": {},
   "source": [
    "# DHLE (DHL SmartMail Parcel Plus Expedited)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcdefa52",
   "metadata": {},
   "source": [
    "5114358: Franklin (FRA) -> 02038_Outbound  \n",
    "5122444: Somerset, no DHLEM (JER) -> 08873_Outbound  \n",
    "5122723: 8150 Nail Olive Branch (MS2) -> 38654_Outbound  \n",
    "5122739: Nail rd Olive branch (MS3) -> 38654_Outbound  \n",
    "5122855: Garland TX (TX1) -> 75041_Outbound  \n",
    "5122890: Oofos -> is/will be in Byhalia, MS)  \n",
    "5122893: Memphis (TN2, TN3) -> 38141_Outbound  \n",
    "5123280: (MAN)  \n",
    "5123282: Bridgewater (MA5)  \n",
    "5123283: Curtis Bay (GBM) -> 21226_Outbound"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1010,
   "id": "709125a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<p style=\"font-family: 'Arial'; font-size: 20px; color: red; background-color: yellow;\">\n",
       "    Edit Location 5 of 8 (Only if you use DHL, default facility is MS3)\n",
       "</p>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "styled_html = HTML(\"\"\"\n",
    "<p style=\"font-family: 'Arial'; font-size: 20px; color: red; background-color: yellow;\">\n",
    "    Edit Location 5 of 8 (Only if you use DHL, default facility is MS3)\n",
    "</p>\n",
    "\"\"\")\n",
    "\n",
    "display(styled_html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1011,
   "id": "71f5d336",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#get client rate and a copy of PLD\n",
    "tbl_clientRates_13 = tbl_clientRates[(tbl_clientRates['SERVICE_CODE'] == 'DHLE_5122739')] # Edit number here\n",
    "tbl_PLD_original_DHLE = tbl_BP.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1012,
   "id": "antique-opera",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Start to rerate\n",
    "#add billed weight column to PLD\n",
    "tbl_PLD_original_DHLE['BILLED_WEIGHT_LB'] = tbl_PLD_original_DHLE.apply(lambda row: max(row['ACTUAL_WEIGHT_LB'], np.ceil(row['CUBIC_INCH']/dim_factor_dhl)) if (row['CUBIC_INCH'] >= 1728 and row['ACTUAL_WEIGHT'] >= 1) else\n",
    "                                                                          (row['ACTUAL_WEIGHT_LB']), axis=1)\n",
    "\n",
    "#add Freight column to PLD\n",
    "tbl_PLD_original_DHLE['ZONE'] = tbl_PLD_original_DHLE['ZONE'].astype(str)\n",
    "tbl_PLD_original_DHLE = tbl_PLD_original_DHLE.merge(tbl_clientRates_13[[\"WEIGHT_LB\", \"ZONE\", \"CLIENT_RATES\"]], left_on=['BILLED_WEIGHT_LB', 'ZONE'], right_on=['WEIGHT_LB', 'ZONE'], how='left')\n",
    "tbl_PLD_original_DHLE = tbl_PLD_original_DHLE.drop('WEIGHT_LB', axis=1)\n",
    "tbl_PLD_original_DHLE = tbl_PLD_original_DHLE.rename(columns={'CLIENT_RATES': 'FRT_DHLE'})\n",
    "\n",
    "\n",
    "\n",
    "#add fuel surcharge % column to PLD\n",
    "tbl_PLD_original_DHLE['SHIP_DATE'] = pd.to_datetime(tbl_PLD_original_DHLE['SHIP_DATE']).dt.date\n",
    "# get unique values from column 'SHIP_DATE' and create a new dataframe\n",
    "new_df = pd.DataFrame({'SHIP_DATE': tbl_PLD_original_DHLE['SHIP_DATE'].unique()})\n",
    "# use this new dataframe to merge to FSC\n",
    "df_merge = new_df.merge(DHL_FSC, how='cross')\n",
    "df_merge['SHIP_DATE'] = pd.to_datetime(df_merge['SHIP_DATE']).dt.date\n",
    "#slice the merged dataframe to make sure ship_date is between the ship_data_start and end. Now we have a FSC file with Ship_date col to merge back to pld so that the pld can get FSC.\n",
    "df_merge = df_merge.query('SHIP_DATE >= SHIP_DATE_START and SHIP_DATE <= SHIP_DATE_END')\n",
    "df_merge = df_merge.drop_duplicates()\n",
    "# Get Domestic Air column as FSC. \n",
    "tbl_PLD_original_DHLE = tbl_PLD_original_DHLE.merge(df_merge, on=['SHIP_DATE'], how='left').drop(['SHIP_DATE_START', 'SHIP_DATE_END'], axis=1)\n",
    "tbl_PLD_original_DHLE.rename(columns={'FSC': 'FSC%_DHLE'}, inplace=True)\n",
    "tbl_PLD_original_DHLE['FSC_DHLE'] = tbl_PLD_original_DHLE.apply(lambda row: row['ACTUAL_WEIGHT']*row['FSC%_DHLE'] if (row['ACTUAL_WEIGHT'] <= 1) else\n",
    "                                                                          (row['BILLED_WEIGHT_LB']*row['FSC%_DHLE']), axis=1)\n",
    "\n",
    "\n",
    "#add NQD Surcharge column to PLD\n",
    "tbl_PLD_original_DHLE['SHIP_DATE'] = pd.to_datetime(tbl_PLD_original_DHLE['SHIP_DATE'])\n",
    "\n",
    "# billed weight calculation situations: \n",
    "# situation 1: actual weight <= 1 lb, use np.ceil(row['BILLED_WEIGHT_OZ']/16) or below, use 1 lb\n",
    "# situation 2: actual weight > 1 lb, and cubic inch < 1728, use round up (actual weight)\n",
    "# situation 3: actual weight > 1 lb, and cubic inch >= 1728, use billed weight\n",
    "\n",
    "#dim\n",
    "tbl_PLD_original_DHLE['NQD_DHLE_1'] = tbl_PLD_original_DHLE.apply(lambda row: \n",
    "                                                            2.5 * row['BILLED_WEIGHT_LB'] if (row['ACTUAL_WEIGHT'] >= 1 and row['CUBIC_INCH'] >= 1728 and row['SHIP_DATE'] >= nqd_dhl_date_3 and row['SHIP_DATE'] <= nqd_dhl_date_4) else \n",
    "                                                           (2 * row['BILLED_WEIGHT_LB'] if (row['ACTUAL_WEIGHT'] >= 1 and row['CUBIC_INCH'] >= 1728 and row['SHIP_DATE'] >= nqd_dhl_date_1 and row['SHIP_DATE'] <= nqd_dhl_date_2) else \n",
    "                                                           0), axis=1)\n",
    "\n",
    "\n",
    "#girth + L(As > 84 is unaccepted, needs a special character to mark it, however, we cannot add up this)\n",
    "tbl_PLD_original_DHLE['NQD_DHLE_2'] = tbl_PLD_original_DHLE.apply(lambda row:\n",
    "                                                            np.nan if (row['GIRTH_AND_L'] > 84) else (0), axis=1)\n",
    "\n",
    "#longest side_1\n",
    "tbl_PLD_original_DHLE['NQD_DHLE_3'] = tbl_PLD_original_DHLE.apply(lambda row: \n",
    "                                                            2.5 * row['BILLED_WEIGHT_LB'] if (max(row['L'], row['W'], row['H']) > 27 and row['SHIP_DATE'] >= nqd_dhl_date_3 and row['SHIP_DATE'] <= nqd_dhl_date_4) else \n",
    "                                                           (2 * row['BILLED_WEIGHT_LB'] if (max(row['L'], row['W'], row['H']) > 27 and row['SHIP_DATE'] >= nqd_dhl_date_1 and row['SHIP_DATE'] <= nqd_dhl_date_2) else\n",
    "                                                           0), axis=1)\n",
    "\n",
    "#longest side_2\n",
    "tbl_PLD_original_DHLE['NQD_DHLE_4'] = tbl_PLD_original_DHLE.apply(lambda row:\n",
    "                                                           4.5 if (max(row['L'], row['W'], row['H']) > 22 and max(row['L'], row['W'], row['H']) <= 30) else\n",
    "                                                           (15.5 if (max(row['L'], row['W'], row['H']) > 30) else \n",
    "                                                           0), axis=1)\n",
    "#volume\n",
    "tbl_PLD_original_DHLE['NQD_DHLE_5'] = tbl_PLD_original_DHLE.apply(lambda row: \n",
    "                                                            15.5 if (row['CUBIC_INCH'] > 3456) else \n",
    "                                                           (0), axis=1)\n",
    "\n",
    "\n",
    "tbl_PLD_original_DHLE['NQD_DHLE_OTHER'] = tbl_PLD_original_DHLE['NQD_DHLE_1'] + tbl_PLD_original_DHLE['NQD_DHLE_3'] + tbl_PLD_original_DHLE['NQD_DHLE_4'] + tbl_PLD_original_DHLE['NQD_DHLE_5']\n",
    "tbl_PLD_original_DHLE['NQD_DHLE_GIRTH_AND_L'] = tbl_PLD_original_DHLE['NQD_DHLE_2']\n",
    "tbl_PLD_original_DHLE = tbl_PLD_original_DHLE.drop(['NQD_DHLE_1','NQD_DHLE_2','NQD_DHLE_3','NQD_DHLE_4','NQD_DHLE_5'], axis=1)\n",
    "\n",
    "\n",
    "#Get Total\n",
    "# note that for those with non-round up actual weight, if it is < 1, we still assign it the freight rate. \n",
    "\n",
    "tbl_PLD_original_DHLE['TOTAL_DHLE'] = round(tbl_PLD_original_DHLE.apply(lambda row: \n",
    "                                                                  np.nan if ((row['FRT_DHLE'] == '') | (pd.isna(row['FRT_DHLE'])) | (row['NQD_DHLE_GIRTH_AND_L'] == '') | (pd.isna(row['NQD_DHLE_GIRTH_AND_L'] == '')))\n",
    "                                                                  else\n",
    "                                                                  (row['FRT_DHLE'] + (row['FSC_DHLE'] + row['NQD_DHLE_OTHER'] + row['NQD_DHLE_GIRTH_AND_L'])/(1-Margin_on_DHL)), axis=1),2)\n",
    "\n",
    "#tbl_PLD_original_DHLE.to_csv('test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cordless-yukon",
   "metadata": {},
   "source": [
    "# DHLE<1 (DHL SmartMail Parcel Expedited)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bac591ed",
   "metadata": {},
   "source": [
    "5114358: Franklin (FRA) -> 02038_Outbound  \n",
    "5122444: Somerset, no DHLEM (JER) -> 08873_Outbound  \n",
    "5122723: 8150 Nail Olive Branch (MS2) -> 38654_Outbound  \n",
    "5122739: Nail rd Olive branch (MS3) -> 38654_Outbound  \n",
    "5122855: Garland TX (TX1) -> 75041_Outbound  \n",
    "5122890: Oofos -> is/will be in Byhalia, MS)  \n",
    "5122893: Memphis (TN2, TN3) -> 38141_Outbound  \n",
    "5123280: (MAN)  \n",
    "5123282: Bridgewater (MA5)  \n",
    "5123283: Curtis Bay (GBM) -> 21226_Outbound"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "735d9575",
   "metadata": {},
   "source": [
    "# DHLE<1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1013,
   "id": "d09732cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<p style=\"font-family: 'Arial'; font-size: 20px; color: red; background-color: yellow;\">\n",
       "    Edit Location 6 of 8 (Only if you use DHL, default facility is MS3)\n",
       "</p>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "styled_html = HTML(\"\"\"\n",
    "<p style=\"font-family: 'Arial'; font-size: 20px; color: red; background-color: yellow;\">\n",
    "    Edit Location 6 of 8 (Only if you use DHL, default facility is MS3)\n",
    "</p>\n",
    "\"\"\")\n",
    "\n",
    "display(styled_html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1014,
   "id": "a7ace654",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Select the rate to use, #Decide the part of PLD to rerate\n",
    "#get client rate and a copy of PLD\n",
    "tbl_clientRates_14 = tbl_clientRates[(tbl_clientRates['SERVICE_CODE'] == 'DHLE<1_5122739')] # Edit the number here. \n",
    "tbl_PLD_original_DHLE1 = tbl_BP[(tbl_BP['ACTUAL_WEIGHT'] < 1)].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1015,
   "id": "corresponding-monroe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Start to rerate\n",
    "#add billed weight column to PLD\n",
    "tbl_PLD_original_DHLE1['BILLED_WEIGHT_OZ'] = tbl_PLD_original_DHLE1.apply(lambda row: max(row['ACTUAL_WEIGHT_LB'], np.ceil(row['CUBIC_INCH']/dim_factor_dhl))*16 if (row['CUBIC_INCH'] >= 1728 and row['ACTUAL_WEIGHT'] >= 1) else\n",
    "                                                                          (row['ACTUAL_WEIGHT_OZ']), axis=1)\n",
    "\n",
    "\n",
    "#add Freight column to PLD\n",
    "tbl_PLD_original_DHLE1['ZONE'] = tbl_PLD_original_DHLE1['ZONE'].astype(str)\n",
    "tbl_PLD_original_DHLE1 = tbl_PLD_original_DHLE1.merge(tbl_clientRates_14[[\"WEIGHT_OZ\", \"ZONE\", \"CLIENT_RATES\"]], left_on=['BILLED_WEIGHT_OZ', 'ZONE'], right_on=['WEIGHT_OZ', 'ZONE'], how='left')\n",
    "tbl_PLD_original_DHLE1 = tbl_PLD_original_DHLE1.drop('WEIGHT_OZ', axis=1)\n",
    "tbl_PLD_original_DHLE1 = tbl_PLD_original_DHLE1.rename(columns={'CLIENT_RATES': 'FRT_DHLE1'})\n",
    "\n",
    "\n",
    "#add fuel surcharge % column to PLD\n",
    "tbl_PLD_original_DHLE1['SHIP_DATE'] = pd.to_datetime(tbl_PLD_original_DHLE1['SHIP_DATE']).dt.date\n",
    "# get unique values from column 'SHIP_DATE' and create a new dataframe\n",
    "# when actual weight < 1, use actual weight non round up to get FSC, otherwise, use billed weight to get FSC\n",
    "new_df = pd.DataFrame({'SHIP_DATE': tbl_PLD_original_DHLE1['SHIP_DATE'].unique()})\n",
    "# use this new dataframe to merge to FSC\n",
    "df_merge = new_df.merge(DHL_FSC, how='cross')\n",
    "df_merge['SHIP_DATE'] = pd.to_datetime(df_merge['SHIP_DATE']).dt.date\n",
    "#slice the merged dataframe to make sure ship_date is between the ship_data_start and end. Now we have a FSC file with Ship_date col to merge back to pld so that the pld can get FSC.\n",
    "df_merge = df_merge.query('SHIP_DATE >= SHIP_DATE_START and SHIP_DATE <= SHIP_DATE_END')\n",
    "df_merge = df_merge.drop_duplicates()\n",
    "# Get Domestic Air column as FSC. \n",
    "tbl_PLD_original_DHLE1 = tbl_PLD_original_DHLE1.merge(df_merge, on=['SHIP_DATE'], how='left').drop(['SHIP_DATE_START', 'SHIP_DATE_END'], axis=1)\n",
    "tbl_PLD_original_DHLE1.rename(columns={'FSC': 'FSC%_DHLE1'}, inplace=True)\n",
    "tbl_PLD_original_DHLE1['FSC_DHLE1'] = tbl_PLD_original_DHLE1.apply(lambda row: row['ACTUAL_WEIGHT']*row['FSC%_DHLE1'], axis=1)\n",
    "\n",
    "\n",
    "#add NQD Surcharge column to PLD\n",
    "tbl_PLD_original_DHLE1['SHIP_DATE'] = pd.to_datetime(tbl_PLD_original_DHLE1['SHIP_DATE'])\n",
    "\n",
    "# billed weight calculation situations: \n",
    "# situation 1: actual weight <= 1 lb, use np.ceil(row['BILLED_WEIGHT_OZ']/16) or below, use 1 lb\n",
    "# situation 2: actual weight > 1 lb, and cubic inch < 1728, use round up (actual weight)\n",
    "# situation 3: actual weight > 1 lb, and cubic inch >= 1728, use billed weight\n",
    "\n",
    "#dim, billed weight is used to determine NQD, there is no <1 lb here\n",
    "tbl_PLD_original_DHLE1['NQD_DHLE1_1'] = tbl_PLD_original_DHLE1.apply(lambda row: \n",
    "                                                            2.5 * max(row['ACTUAL_WEIGHT_LB'], np.ceil(row['CUBIC_INCH']/dim_factor_dhl)) if (row['ACTUAL_WEIGHT'] >= 1 and row['CUBIC_INCH'] >= 1728 and row['SHIP_DATE'] >= nqd_dhl_date_3 and row['SHIP_DATE'] <= nqd_dhl_date_4) else \n",
    "                                                           (2 * max(row['ACTUAL_WEIGHT_LB'], np.ceil(row['CUBIC_INCH']/dim_factor_dhl)) if (row['ACTUAL_WEIGHT'] >= 1 and row['CUBIC_INCH'] >= 1728 and row['SHIP_DATE'] >= nqd_dhl_date_1 and row['SHIP_DATE'] <= nqd_dhl_date_2) else \n",
    "                                                           0), axis=1)\n",
    "\n",
    "\n",
    "#girth + L(As > 84 is unaccepted, needs a special character to mark it, however, we cannot add up this, so we make it null)\n",
    "#1 over 1lbs, under lbs\n",
    "#and those < 1 lb will use lb to determine NQD\n",
    "# situation 1: actual weight <= 1 lb, use np.ceil(row['BILLED_WEIGHT_OZ']/16)\n",
    "# situation 2: actual weight > 1 lb, and cubic inch < 1728, use actual weight\n",
    "# situation 3: actual weight > 1 lb, and cubic inch >= 1728, use billed weight\n",
    "tbl_PLD_original_DHLE1['NQD_DHLE1_2'] = tbl_PLD_original_DHLE1.apply(lambda row:\n",
    "                                                            np.nan if (row['GIRTH_AND_L'] > 84) else (0), axis=1)\n",
    "\n",
    "#longest side_1\n",
    "tbl_PLD_original_DHLE1['NQD_DHLE1_3'] = tbl_PLD_original_DHLE1.apply(lambda row: \n",
    "                                                            2.5 * row['ACTUAL_WEIGHT_LB'] if (max(row['L'], row['W'], row['H']) > 27 and row['SHIP_DATE'] >= nqd_dhl_date_3 and row['SHIP_DATE'] <= nqd_dhl_date_4) else \n",
    "                                                           (2 * row['ACTUAL_WEIGHT_LB'] if (max(row['L'], row['W'], row['H']) > 27 and row['SHIP_DATE'] >= nqd_dhl_date_1 and row['SHIP_DATE'] <= nqd_dhl_date_2) else\n",
    "                                                           0), axis=1)\n",
    "\n",
    "#longest side_2\n",
    "tbl_PLD_original_DHLE1['NQD_DHLE1_4'] = tbl_PLD_original_DHLE1.apply(lambda row:\n",
    "                                                           4.5 if (max(row['L'], row['W'], row['H']) > 22 and max(row['L'], row['W'], row['H']) <= 30) else\n",
    "                                                           (15.5 if (max(row['L'], row['W'], row['H']) > 30) else \n",
    "                                                           0), axis=1)\n",
    "#volume\n",
    "tbl_PLD_original_DHLE1['NQD_DHLE1_5'] = tbl_PLD_original_DHLE1.apply(lambda row: \n",
    "                                                            15.5 if (row['CUBIC_INCH'] > 3456) else \n",
    "                                                           (0), axis=1)\n",
    "\n",
    "\n",
    "tbl_PLD_original_DHLE1['NQD_DHLE1_OTHER'] = tbl_PLD_original_DHLE1['NQD_DHLE1_1'] + tbl_PLD_original_DHLE1['NQD_DHLE1_3'] + tbl_PLD_original_DHLE1['NQD_DHLE1_4'] + tbl_PLD_original_DHLE1['NQD_DHLE1_5']\n",
    "tbl_PLD_original_DHLE1['NQD_DHLE1_GIRTH_AND_L'] = tbl_PLD_original_DHLE1['NQD_DHLE1_2']\n",
    "tbl_PLD_original_DHLE1 = tbl_PLD_original_DHLE1.drop(['NQD_DHLE1_1','NQD_DHLE1_2','NQD_DHLE1_3','NQD_DHLE1_4','NQD_DHLE1_5'], axis=1)\n",
    "\n",
    "\n",
    "#Get Total\n",
    "tbl_PLD_original_DHLE1['TOTAL_DHLE1'] = round(tbl_PLD_original_DHLE1.apply(lambda row: \n",
    "                                                                  np.nan if ((row['FRT_DHLE1'] == '') | (pd.isna(row['FRT_DHLE1'])) | (row['NQD_DHLE1_GIRTH_AND_L'] == '') | (pd.isna(row['NQD_DHLE1_GIRTH_AND_L'] == '')))\n",
    "                                                                  else\n",
    "                                                                  (row['FRT_DHLE1'] + (row['FSC_DHLE1'] + row['NQD_DHLE1_OTHER'] + row['NQD_DHLE1_GIRTH_AND_L'])/(1-Margin_on_DHL)), axis=1),2)\n",
    "\n",
    "#tbl_PLD_original_DHLE1.to_csv('test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "numeric-radical",
   "metadata": {},
   "source": [
    "# DHLEM (DHL SmartMail Parcel Expedited Max)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad7deb40",
   "metadata": {},
   "source": [
    "5114358: Franklin (FRA) -> 02038_Outbound  \n",
    "5122444: Somerset, no DHLEM (JER) -> 08873_Outbound  \n",
    "5122723: 8150 Nail Olive Branch (MS2) -> 38654_Outbound  \n",
    "5122739: Nail rd Olive branch (MS3) -> 38654_Outbound  \n",
    "5122855: Garland TX (TX1) -> 75041_Outbound  \n",
    "5122890: Oofos -> is/will be in Byhalia, MS)  \n",
    "5122893: Memphis (TN2, TN3) -> 38141_Outbound  \n",
    "5123280: (MAN)  \n",
    "5123282: Bridgewater (MA5)  \n",
    "5123283: Curtis Bay (GBM) -> 21226_Outbound"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b56c6cac",
   "metadata": {},
   "source": [
    "# DHLEM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1016,
   "id": "b875d300",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<p style=\"font-family: 'Arial'; font-size: 20px; color: red; background-color: yellow;\">\n",
       "    Edit Location 7 of 8 (Only if you use DHL, default facility is MS3)\n",
       "</p>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "styled_html = HTML(\"\"\"\n",
    "<p style=\"font-family: 'Arial'; font-size: 20px; color: red; background-color: yellow;\">\n",
    "    Edit Location 7 of 8 (Only if you use DHL, default facility is MS3)\n",
    "</p>\n",
    "\"\"\")\n",
    "\n",
    "display(styled_html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1017,
   "id": "02ef5014",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Select the rate to use, #Decide the part of PLD to rerate\n",
    "#get client rate and a copy of PLD\n",
    "tbl_clientRates_15 = tbl_clientRates[(tbl_clientRates['SERVICE_CODE'] == 'DHLEM_5122739')] # Edit number here\n",
    "tbl_PLD_original_DHLEM1 = tbl_BP[(tbl_BP['ACTUAL_WEIGHT'] <= 1)].copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1018,
   "id": "employed-silly",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Start to rerate\n",
    "#add billed weight column to PLD\n",
    "tbl_PLD_original_DHLEM1['BILLED_WEIGHT_OZ'] = tbl_PLD_original_DHLEM1.apply(lambda row: max(row['ACTUAL_WEIGHT_LB'], np.ceil(row['CUBIC_INCH']/dim_factor_dhl))*16 if (row['CUBIC_INCH'] >= 1728 and row['ACTUAL_WEIGHT'] >= 1) else\n",
    "                                                                          (row['ACTUAL_WEIGHT_OZ']), axis=1)\n",
    "\n",
    "#add Freight column to PLD\n",
    "tbl_PLD_original_DHLEM1['ZONE'] = tbl_PLD_original_DHLEM1['ZONE'].astype(str)\n",
    "tbl_PLD_original_DHLEM1 = tbl_PLD_original_DHLEM1.merge(tbl_clientRates_15[[\"WEIGHT_OZ\", \"ZONE\", \"CLIENT_RATES\"]], left_on=['BILLED_WEIGHT_OZ', 'ZONE'], right_on=['WEIGHT_OZ', 'ZONE'], how='left')\n",
    "tbl_PLD_original_DHLEM1 = tbl_PLD_original_DHLEM1.drop('WEIGHT_OZ', axis=1)\n",
    "tbl_PLD_original_DHLEM1 = tbl_PLD_original_DHLEM1.rename(columns={'CLIENT_RATES': 'FRT_DHLEM'})\n",
    "\n",
    "\n",
    "#add fuel surcharge % column to PLD\n",
    "tbl_PLD_original_DHLEM1['SHIP_DATE'] = pd.to_datetime(tbl_PLD_original_DHLEM1['SHIP_DATE']).dt.date\n",
    "# get unique values from column 'SHIP_DATE' and create a new dataframe\n",
    "# when actual weight < 1, use actual weight non round up to get FSC, otherwise, use billed weight to get FSC\n",
    "new_df = pd.DataFrame({'SHIP_DATE': tbl_PLD_original_DHLEM1['SHIP_DATE'].unique()})\n",
    "# use this new dataframe to merge to FSC\n",
    "df_merge = new_df.merge(DHL_FSC, how='cross')\n",
    "df_merge['SHIP_DATE'] = pd.to_datetime(df_merge['SHIP_DATE']).dt.date\n",
    "#slice the merged dataframe to make sure ship_date is between the ship_data_start and end. Now we have a FSC file with Ship_date col to merge back to pld so that the pld can get FSC.\n",
    "df_merge = df_merge.query('SHIP_DATE >= SHIP_DATE_START and SHIP_DATE <= SHIP_DATE_END')\n",
    "df_merge = df_merge.drop_duplicates()\n",
    "# Get Domestic Air column as FSC. \n",
    "tbl_PLD_original_DHLEM1 = tbl_PLD_original_DHLEM1.merge(df_merge, on=['SHIP_DATE'], how='left').drop(['SHIP_DATE_START', 'SHIP_DATE_END'], axis=1)\n",
    "tbl_PLD_original_DHLEM1.rename(columns={'FSC': 'FSC%_DHLEM'}, inplace=True)\n",
    "tbl_PLD_original_DHLEM1['FSC_DHLEM'] = tbl_PLD_original_DHLEM1.apply(lambda row: row['ACTUAL_WEIGHT']*row['FSC%_DHLEM'], axis=1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#add NQD Surcharge column to PLD\n",
    "tbl_PLD_original_DHLEM1['SHIP_DATE'] = pd.to_datetime(tbl_PLD_original_DHLEM1['SHIP_DATE'])\n",
    "\n",
    "# billed weight calculation situations: \n",
    "# situation 1: actual weight <= 1 lb, use np.ceil(row['BILLED_WEIGHT_OZ']/16) or below, use 1 lb\n",
    "# situation 2: actual weight > 1 lb, and cubic inch < 1728, use round up (actual weight)\n",
    "# situation 3: actual weight > 1 lb, and cubic inch >= 1728, use billed weight\n",
    "\n",
    "#dim, billed weight is used to determine NQD, there is no <1 lb here\n",
    "tbl_PLD_original_DHLEM1['NQD_DHLEM_1'] = tbl_PLD_original_DHLEM1.apply(lambda row: \n",
    "                                                            2.5 * max(row['ACTUAL_WEIGHT_LB'], np.ceil(row['CUBIC_INCH']/dim_factor_dhl)) if (row['ACTUAL_WEIGHT'] >= 1 and row['CUBIC_INCH'] >= 1728 and row['SHIP_DATE'] >= nqd_dhl_date_3 and row['SHIP_DATE'] <= nqd_dhl_date_4) else \n",
    "                                                           (2 * max(row['ACTUAL_WEIGHT_LB'], np.ceil(row['CUBIC_INCH']/dim_factor_dhl)) if (row['ACTUAL_WEIGHT'] >= 1 and row['CUBIC_INCH'] >= 1728 and row['SHIP_DATE'] >= nqd_dhl_date_1 and row['SHIP_DATE'] <= nqd_dhl_date_2) else \n",
    "                                                           0), axis=1)\n",
    "\n",
    "\n",
    "#girth + L(As > 84 is unaccepted, needs a special character to mark it, however, we cannot add up this, so we make it null)\n",
    "#1 over 1lbs, under lbs\n",
    "#and those < 1 lb will use lb to determine NQD\n",
    "# situation 1: actual weight <= 1 lb, use np.ceil(row['BILLED_WEIGHT_OZ']/16)\n",
    "# situation 2: actual weight > 1 lb, and cubic inch < 1728, use actual weight\n",
    "# situation 3: actual weight > 1 lb, and cubic inch >= 1728, use billed weight\n",
    "tbl_PLD_original_DHLEM1['NQD_DHLEM_2'] = tbl_PLD_original_DHLEM1.apply(lambda row:\n",
    "                                                            np.nan if (row['GIRTH_AND_L'] > 84) else (0), axis=1)\n",
    "\n",
    "#longest side_1\n",
    "tbl_PLD_original_DHLEM1['NQD_DHLEM_3'] = tbl_PLD_original_DHLEM1.apply(lambda row: \n",
    "                                                            2.5 * row['ACTUAL_WEIGHT_LB'] if (max(row['L'], row['W'], row['H']) > 27 and row['SHIP_DATE'] >= nqd_dhl_date_3 and row['SHIP_DATE'] <= nqd_dhl_date_4) else \n",
    "                                                           (2 * row['ACTUAL_WEIGHT_LB'] if (max(row['L'], row['W'], row['H']) > 27 and row['SHIP_DATE'] >= nqd_dhl_date_1 and row['SHIP_DATE'] <= nqd_dhl_date_2) else\n",
    "                                                           0), axis=1)\n",
    "\n",
    "#longest side_2\n",
    "tbl_PLD_original_DHLEM1['NQD_DHLEM_4'] = tbl_PLD_original_DHLEM1.apply(lambda row:\n",
    "                                                           4.5 if (max(row['L'], row['W'], row['H']) > 22 and max(row['L'], row['W'], row['H']) <= 30) else\n",
    "                                                           (15.5 if (max(row['L'], row['W'], row['H']) > 30) else \n",
    "                                                           0), axis=1)\n",
    "#volume\n",
    "tbl_PLD_original_DHLEM1['NQD_DHLEM_5'] = tbl_PLD_original_DHLEM1.apply(lambda row: \n",
    "                                                            15.5 if (row['CUBIC_INCH'] > 3456) else \n",
    "                                                           (0), axis=1)\n",
    "\n",
    "\n",
    "tbl_PLD_original_DHLEM1['NQD_DHLEM_OTHER'] = tbl_PLD_original_DHLEM1['NQD_DHLEM_1'] + tbl_PLD_original_DHLEM1['NQD_DHLEM_3'] + tbl_PLD_original_DHLEM1['NQD_DHLEM_4'] + tbl_PLD_original_DHLEM1['NQD_DHLEM_5']\n",
    "tbl_PLD_original_DHLEM1['NQD_DHLEM_GIRTH_AND_L'] = tbl_PLD_original_DHLEM1['NQD_DHLEM_2']\n",
    "tbl_PLD_original_DHLEM1 = tbl_PLD_original_DHLEM1.drop(['NQD_DHLEM_1','NQD_DHLEM_2','NQD_DHLEM_3','NQD_DHLEM_4','NQD_DHLEM_5'], axis=1)\n",
    "\n",
    "\n",
    "#Get Total\n",
    "tbl_PLD_original_DHLEM1['TOTAL_DHLEM'] = round(tbl_PLD_original_DHLEM1.apply(lambda row: \n",
    "                                                                  np.nan if ((row['FRT_DHLEM'] == '') | (pd.isna(row['FRT_DHLEM'])) | (row['NQD_DHLEM_GIRTH_AND_L'] == '') | (pd.isna(row['NQD_DHLEM_GIRTH_AND_L'] == '')))\n",
    "                                                                  else\n",
    "                                                                  (row['FRT_DHLEM'] + (row['FSC_DHLEM'] + row['NQD_DHLEM_OTHER'] + row['NQD_DHLEM_GIRTH_AND_L'])/(1-Margin_on_DHL)), axis=1),2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1019,
   "id": "e09b6449",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<p style=\"font-family: 'Arial'; font-size: 20px; color: red; background-color: yellow;\">\n",
       "    Edit Location 8 of 8 (Only if you use DHL, default facility is MS3)\n",
       "</p>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "styled_html = HTML(\"\"\"\n",
    "<p style=\"font-family: 'Arial'; font-size: 20px; color: red; background-color: yellow;\">\n",
    "    Edit Location 8 of 8 (Only if you use DHL, default facility is MS3)\n",
    "</p>\n",
    "\"\"\")\n",
    "\n",
    "display(styled_html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1020,
   "id": "314e4025",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Select the rate to use, #Decide the part of PLD to rerate\n",
    "#get client rate and a copy of PLD\n",
    "tbl_clientRates_16 = tbl_clientRates[(tbl_clientRates['SERVICE_CODE'] == 'DHLEM_5122739')] # Edit the number here\n",
    "tbl_PLD_original_DHLEM = tbl_BP[(tbl_BP['ACTUAL_WEIGHT'] > 1)].copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1021,
   "id": "fifteen-hopkins",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Start to rerate\n",
    "#add billed weight column to PLD\n",
    "tbl_PLD_original_DHLEM['BILLED_WEIGHT_LB'] = tbl_PLD_original_DHLEM.apply(lambda row: max(row['ACTUAL_WEIGHT_LB'], np.ceil(row['CUBIC_INCH']/dim_factor_dhl)) if (row['CUBIC_INCH'] >= 1728 and row['ACTUAL_WEIGHT'] >= 1) else\n",
    "                                                                          (row['ACTUAL_WEIGHT_LB']), axis=1)\n",
    "\n",
    "#add Freight column to PLD\n",
    "tbl_PLD_original_DHLEM['ZONE'] = tbl_PLD_original_DHLEM['ZONE'].astype(str)\n",
    "tbl_PLD_original_DHLEM = tbl_PLD_original_DHLEM.merge(tbl_clientRates_16[[\"WEIGHT_LB\", \"ZONE\", \"CLIENT_RATES\"]], left_on=['BILLED_WEIGHT_LB', 'ZONE'], right_on=['WEIGHT_LB', 'ZONE'], how='left')\n",
    "tbl_PLD_original_DHLEM = tbl_PLD_original_DHLEM.drop('WEIGHT_LB', axis=1)\n",
    "tbl_PLD_original_DHLEM = tbl_PLD_original_DHLEM.rename(columns={'CLIENT_RATES': 'FRT_DHLEM'})\n",
    "\n",
    "\n",
    "\n",
    "#add fuel surcharge % column to PLD\n",
    "tbl_PLD_original_DHLEM['SHIP_DATE'] = pd.to_datetime(tbl_PLD_original_DHLEM['SHIP_DATE']).dt.date\n",
    "# get unique values from column 'SHIP_DATE' and create a new dataframe\n",
    "new_df = pd.DataFrame({'SHIP_DATE': tbl_PLD_original_DHLEM['SHIP_DATE'].unique()})\n",
    "# use this new dataframe to merge to FSC\n",
    "df_merge = new_df.merge(DHL_FSC, how='cross')\n",
    "df_merge['SHIP_DATE'] = pd.to_datetime(df_merge['SHIP_DATE']).dt.date\n",
    "#slice the merged dataframe to make sure ship_date is between the ship_data_start and end. Now we have a FSC file with Ship_date col to merge back to pld so that the pld can get FSC.\n",
    "df_merge = df_merge.query('SHIP_DATE >= SHIP_DATE_START and SHIP_DATE <= SHIP_DATE_END')\n",
    "df_merge = df_merge.drop_duplicates()\n",
    "# Get Domestic Air column as FSC. \n",
    "tbl_PLD_original_DHLEM = tbl_PLD_original_DHLEM.merge(df_merge, on=['SHIP_DATE'], how='left').drop(['SHIP_DATE_START', 'SHIP_DATE_END'], axis=1)\n",
    "tbl_PLD_original_DHLEM.rename(columns={'FSC': 'FSC%_DHLEM'}, inplace=True)\n",
    "tbl_PLD_original_DHLEM['FSC_DHLEM'] = tbl_PLD_original_DHLEM.apply(lambda row: row['ACTUAL_WEIGHT']*row['FSC%_DHLEM'] if (row['ACTUAL_WEIGHT'] <= 1) else\n",
    "                                                                          (row['BILLED_WEIGHT_LB']*row['FSC%_DHLEM']), axis=1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#add NQD Surcharge column to PLD\n",
    "tbl_PLD_original_DHLEM['SHIP_DATE'] = pd.to_datetime(tbl_PLD_original_DHLEM['SHIP_DATE'])\n",
    "\n",
    "# billed weight calculation situations: \n",
    "# situation 1: actual weight <= 1 lb, use np.ceil(row['BILLED_WEIGHT_OZ']/16) or below, use 1 lb\n",
    "# situation 2: actual weight > 1 lb, and cubic inch < 1728, use round up (actual weight)\n",
    "# situation 3: actual weight > 1 lb, and cubic inch >= 1728, use billed weight\n",
    "\n",
    "#dim\n",
    "tbl_PLD_original_DHLEM['NQD_DHLEM_1'] = tbl_PLD_original_DHLEM.apply(lambda row: \n",
    "                                                            2.5 * row['BILLED_WEIGHT_LB'] if (row['ACTUAL_WEIGHT'] >= 1 and row['CUBIC_INCH'] >= 1728 and row['SHIP_DATE'] >= nqd_dhl_date_3 and row['SHIP_DATE'] <= nqd_dhl_date_4) else \n",
    "                                                           (2 * row['BILLED_WEIGHT_LB'] if (row['ACTUAL_WEIGHT'] >= 1 and row['CUBIC_INCH'] >= 1728 and row['SHIP_DATE'] >= nqd_dhl_date_1 and row['SHIP_DATE'] <= nqd_dhl_date_2) else \n",
    "                                                           0), axis=1)\n",
    "\n",
    "\n",
    "#girth + L(As > 84 is unaccepted, needs a special character to mark it, however, we cannot add up this)\n",
    "tbl_PLD_original_DHLEM['NQD_DHLEM_2'] = tbl_PLD_original_DHLEM.apply(lambda row:\n",
    "                                                           np.nan if (row['GIRTH_AND_L'] > 84) else (0), axis=1)\n",
    "\n",
    "#longest side_1\n",
    "tbl_PLD_original_DHLEM['NQD_DHLEM_3'] = tbl_PLD_original_DHLEM.apply(lambda row: \n",
    "                                                            2.5 * row['BILLED_WEIGHT_LB'] if (max(row['L'], row['W'], row['H']) > 27 and row['SHIP_DATE'] >= nqd_dhl_date_3 and row['SHIP_DATE'] <= nqd_dhl_date_4) else \n",
    "                                                           (2 * row['BILLED_WEIGHT_LB'] if (max(row['L'], row['W'], row['H']) > 27 and row['SHIP_DATE'] >= nqd_dhl_date_1 and row['SHIP_DATE'] <= nqd_dhl_date_2) else\n",
    "                                                           0), axis=1)\n",
    "\n",
    "#longest side_2\n",
    "tbl_PLD_original_DHLEM['NQD_DHLEM_4'] = tbl_PLD_original_DHLEM.apply(lambda row:\n",
    "                                                           4.5 if (max(row['L'], row['W'], row['H']) > 22 and max(row['L'], row['W'], row['H']) <= 30) else\n",
    "                                                           (15.5 if (max(row['L'], row['W'], row['H']) > 30) else \n",
    "                                                           0), axis=1)\n",
    "#volume\n",
    "tbl_PLD_original_DHLEM['NQD_DHLEM_5'] = tbl_PLD_original_DHLEM.apply(lambda row: \n",
    "                                                            15.5 if (row['CUBIC_INCH'] > 3456) else \n",
    "                                                           (0), axis=1)\n",
    "\n",
    "\n",
    "tbl_PLD_original_DHLEM['NQD_DHLEM_OTHER'] = tbl_PLD_original_DHLEM['NQD_DHLEM_1'] + tbl_PLD_original_DHLEM['NQD_DHLEM_3'] + tbl_PLD_original_DHLEM['NQD_DHLEM_4'] + tbl_PLD_original_DHLEM['NQD_DHLEM_5']\n",
    "tbl_PLD_original_DHLEM['NQD_DHLEM_GIRTH_AND_L'] = tbl_PLD_original_DHLEM['NQD_DHLEM_2']\n",
    "tbl_PLD_original_DHLEM = tbl_PLD_original_DHLEM.drop(['NQD_DHLEM_1','NQD_DHLEM_2','NQD_DHLEM_3','NQD_DHLEM_4','NQD_DHLEM_5'], axis=1)\n",
    "\n",
    "\n",
    "#Get Total\n",
    "# note that for those with non-round up actual weight, if it is < 1, we still assign it the freight rate. \n",
    "\n",
    "tbl_PLD_original_DHLEM['TOTAL_DHLEM'] = round(tbl_PLD_original_DHLEM.apply(lambda row: \n",
    "                                                                  np.nan if ((row['FRT_DHLEM'] == '') | (pd.isna(row['FRT_DHLEM'])) | (row['NQD_DHLEM_GIRTH_AND_L'] == '') | (pd.isna(row['NQD_DHLEM_GIRTH_AND_L'] == '')))\n",
    "                                                                  else\n",
    "                                                                  (row['FRT_DHLEM'] + (row['FSC_DHLEM'] + row['NQD_DHLEM_OTHER'] + row['NQD_DHLEM_GIRTH_AND_L'])/(1-Margin_on_DHL)), axis=1),2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1022,
   "id": "rolled-latter",
   "metadata": {},
   "outputs": [],
   "source": [
    "tbl_PLD_original_DHLEM = pd.concat([tbl_PLD_original_DHLEM1, tbl_PLD_original_DHLEM], axis=0)\n",
    "#tbl_PLD_original_DHLEM.to_csv('test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "engaged-register",
   "metadata": {},
   "source": [
    "# USPSAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1096,
   "id": "489d3def",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get the right client rate and the corresponding PLD\n",
    "#get client rate and a copy of PLD\n",
    "tbl_clientRates_17 = tbl_clientRates[(tbl_clientRates['SERVICE_CODE'] == 'USPSAG')]\n",
    "tbl_PLD_original_USPSAG = tbl_BP.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1097,
   "id": "9a6934f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sliu\\AppData\\Local\\Temp\\ipykernel_18976\\481323264.py:22: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  tbl_PLD_original_USPSAG_none3['ZONE'] = tbl_PLD_original_USPSAG_none3['ZONE'].astype(str)\n",
      "C:\\Users\\sliu\\AppData\\Local\\Temp\\ipykernel_18976\\481323264.py:31: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  tbl_PLD_original_USPSAG_3['ZONE'] = tbl_PLD_original_USPSAG_3['ZONE'].astype(str)\n"
     ]
    }
   ],
   "source": [
    "#Rerate start\n",
    "#add billed weight column to PLD\n",
    "tbl_PLD_original_USPSAG['BILLED_WEIGHT_LB'] = tbl_PLD_original_USPSAG.apply(lambda row: max(row['ACTUAL_WEIGHT_LB'], np.ceil(row['CUBIC_INCH']/dim_factor_usps)) if (row['CUBIC_INCH'] > 1728) else\n",
    "                                                                          (row['ACTUAL_WEIGHT_LB']), axis=1)\n",
    "\n",
    "tbl_PLD_original_USPSAG['BILLED_WEIGHT_OZ'] = tbl_PLD_original_USPSAG.apply(lambda row: max(row['ACTUAL_WEIGHT_LB'], np.ceil(row['CUBIC_INCH']/dim_factor_usps))*16 if (row['CUBIC_INCH'] > 1728) else\n",
    "                                                                          (row['ACTUAL_WEIGHT_OZ']), axis=1)\n",
    "\n",
    "tbl_PLD_original_USPSAG['BILLED_WEIGHT_LB'] = tbl_PLD_original_USPSAG.apply(lambda row:\n",
    "                                                            row['BILLED_WEIGHT_OZ']/16 if (row['BILLED_WEIGHT_OZ'] <= 16) else \n",
    "                                                           (row['BILLED_WEIGHT_LB']), axis=1)\n",
    "\n",
    "#usps will only rate for those not exceeding 70 lbs\n",
    "tbl_PLD_original_USPSAG = tbl_PLD_original_USPSAG[(tbl_PLD_original_USPSAG['BILLED_WEIGHT_LB'] <= 70)]\n",
    "\n",
    "\n",
    "#add Freight column to PLD\n",
    "tbl_PLD_original_USPSAG_3 = tbl_PLD_original_USPSAG[(tbl_PLD_original_USPSAG['GIRTH_AND_L'] > 108) & (tbl_PLD_original_USPSAG['GIRTH_AND_L'] <= 130)]\n",
    "tbl_PLD_original_USPSAG_none3 = tbl_PLD_original_USPSAG[(tbl_PLD_original_USPSAG['GIRTH_AND_L'] <= 108)]\n",
    "\n",
    "#main case\n",
    "tbl_PLD_original_USPSAG_none3['ZONE'] = tbl_PLD_original_USPSAG_none3['ZONE'].astype(str)\n",
    "tbl_PLD_original_USPSAG_none3 = tbl_PLD_original_USPSAG_none3.merge(tbl_clientRates_17[[\"WEIGHT_LB\", \"ZONE\", \"CLIENT_RATES\"]], left_on=['BILLED_WEIGHT_LB', 'ZONE'], right_on=['WEIGHT_LB', 'ZONE'], how='left')\n",
    "tbl_PLD_original_USPSAG_none3 = tbl_PLD_original_USPSAG_none3.drop('WEIGHT_LB', axis=1)\n",
    "tbl_PLD_original_USPSAG_none3 = tbl_PLD_original_USPSAG_none3.rename(columns={'CLIENT_RATES': 'FRT_USPSAG'})\n",
    "\n",
    "#3. for parcels that measure in combined length and girth more than 108 inches but not more than 130 inches, use oversized prices, \n",
    "#regardless of weight, based on the applicable zone.\n",
    "\n",
    "tbl_oversize = tbl_clientRates_17[(tbl_clientRates_17['WEIGHT_LB'] == 9999)]\n",
    "tbl_PLD_original_USPSAG_3['ZONE'] = tbl_PLD_original_USPSAG_3['ZONE'].astype(str)\n",
    "tbl_PLD_original_USPSAG_3 = tbl_PLD_original_USPSAG_3.merge(tbl_oversize[[\"ZONE\", \"CLIENT_RATES\"]], left_on=['ZONE'], right_on=['ZONE'], how='left')\n",
    "tbl_PLD_original_USPSAG_3 = tbl_PLD_original_USPSAG_3.rename(columns={'CLIENT_RATES': 'FRT_USPSAG'})\n",
    "\n",
    "#combine again\n",
    "tbl_PLD_original_USPSAG = pd.concat([tbl_PLD_original_USPSAG_3, tbl_PLD_original_USPSAG_none3], axis=0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1098,
   "id": "226c98eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#6. Parcels that exceed 22 inches but not greater than 30 inches in length, add $4 per package. \n",
    "#7. parcels that exceed 30 inches in length, add $7 per package. \n",
    "\n",
    "tbl_PLD_original_USPSAG['EXTRA_USPSAG_1'] = tbl_PLD_original_USPSAG.apply(lambda row:\n",
    "                                                            4 if (row['L'] > 22 and row['L'] <= 30) else \n",
    "                                                           (7 if (row['L'] > 30) else\n",
    "                                                           (0)), axis=1)\n",
    "\n",
    "\n",
    "#8. parcels that exceed 2 cubic feet (3456 cubic inches), add $15 per package. \n",
    "tbl_PLD_original_USPSAG['EXTRA_USPSAG_2'] = tbl_PLD_original_USPSAG.apply(lambda row:\n",
    "                                                            15 if (row['CUBIC_INCH'] > 3456) else (0), axis=1)\n",
    "\n",
    "#Get Total\n",
    "# note that for those with non-round up actual weight, if it is < 1, we still assign it the freight rate. \n",
    "\n",
    "#Get Total, revise the logic that if the freight rate is 0 or some rate is 0, the total will be 0. ups can handle everything. \n",
    "tbl_PLD_original_USPSAG['TOTAL_USPSAG'] = round(tbl_PLD_original_USPSAG.apply(lambda row: \n",
    "                                                                  (row['FRT_USPSAG'] + (row['EXTRA_USPSAG_1'] + row['EXTRA_USPSAG_2'])/(1-Margin_on_USPS)) if row['FRT_USPSAG'] != '' else '', axis=1),2)                         \n",
    "#tbl_PLD_original_USPSAG.to_csv('test.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vocal-astronomy",
   "metadata": {},
   "source": [
    "# USPSAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1099,
   "id": "fc9dca26",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get the right client rate and the corresponding PLD\n",
    "#get client rate and a copy of PLD\n",
    "tbl_clientRates_18 = tbl_clientRates[(tbl_clientRates['SERVICE_CODE'] == 'USPSAP')]\n",
    "tbl_PLD_original_USPSAP = tbl_BP.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1100,
   "id": "educated-possible",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sliu\\AppData\\Local\\Temp\\ipykernel_18976\\1624375625.py:19: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  tbl_PLD_original_USPSAP_none3['ZONE'] = tbl_PLD_original_USPSAP_none3['ZONE'].astype(str)\n",
      "C:\\Users\\sliu\\AppData\\Local\\Temp\\ipykernel_18976\\1624375625.py:28: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  tbl_PLD_original_USPSAP_3['ZONE'] = tbl_PLD_original_USPSAP_3['ZONE'].astype(str)\n"
     ]
    }
   ],
   "source": [
    "#Rerate start\n",
    "#add billed weight column to PLD\n",
    "tbl_PLD_original_USPSAP['BILLED_WEIGHT_LB'] = tbl_PLD_original_USPSAP.apply(lambda row: max(row['ACTUAL_WEIGHT_LB'], np.ceil(row['CUBIC_INCH']/dim_factor_usps)) if (row['CUBIC_INCH'] > 1728) else\n",
    "                                                                          (row['ACTUAL_WEIGHT_LB']), axis=1)\n",
    "\n",
    "tbl_PLD_original_USPSAP['BILLED_WEIGHT_OZ'] = tbl_PLD_original_USPSAP.apply(lambda row: max(row['ACTUAL_WEIGHT_LB'], np.ceil(row['CUBIC_INCH']/dim_factor_usps))*16 if (row['CUBIC_INCH'] > 1728) else\n",
    "                                                                          (row['ACTUAL_WEIGHT_OZ']), axis=1)\n",
    "\n",
    "tbl_PLD_original_USPSAP['BILLED_WEIGHT_LB'] = tbl_PLD_original_USPSAP.apply(lambda row:\n",
    "                                                            0.5 if (row['BILLED_WEIGHT_OZ'] <= 8) else\n",
    "                                                            (1 if (row['BILLED_WEIGHT_OZ'] > 8 and row['BILLED_WEIGHT_OZ'] <= 16) else             \n",
    "                                                            (row['BILLED_WEIGHT_LB'])), axis=1)\n",
    "\n",
    "#add Freight column to PLD\n",
    "tbl_PLD_original_USPSAP_3 = tbl_PLD_original_USPSAP[(tbl_PLD_original_USPSAP['GIRTH_AND_L'] > 108) & (tbl_PLD_original_USPSAP['GIRTH_AND_L'] <= 130)]\n",
    "tbl_PLD_original_USPSAP_none3 = tbl_PLD_original_USPSAP[(tbl_PLD_original_USPSAP['GIRTH_AND_L'] <= 108)]\n",
    "\n",
    "#main case\n",
    "tbl_PLD_original_USPSAP_none3['ZONE'] = tbl_PLD_original_USPSAP_none3['ZONE'].astype(str)\n",
    "tbl_PLD_original_USPSAP_none3 = tbl_PLD_original_USPSAP_none3.merge(tbl_clientRates_18[[\"WEIGHT_LB\", \"ZONE\", \"CLIENT_RATES\"]], left_on=['BILLED_WEIGHT_LB', 'ZONE'], right_on=['WEIGHT_LB', 'ZONE'], how='left')\n",
    "tbl_PLD_original_USPSAP_none3 = tbl_PLD_original_USPSAP_none3.drop('WEIGHT_LB', axis=1)\n",
    "tbl_PLD_original_USPSAP_none3 = tbl_PLD_original_USPSAP_none3.rename(columns={'CLIENT_RATES': 'FRT_USPSAP'})\n",
    "\n",
    "#3. for parcels that measure in combined length and girth more than 108 inches but not more than 130 inches, use oversized prices, \n",
    "#regardless of weight, based on the applicable zone.\n",
    "\n",
    "tbl_oversize = tbl_clientRates_18[(tbl_clientRates_18['WEIGHT_LB'] == 9999)]\n",
    "tbl_PLD_original_USPSAP_3['ZONE'] = tbl_PLD_original_USPSAP_3['ZONE'].astype(str)\n",
    "tbl_PLD_original_USPSAP_3 = tbl_PLD_original_USPSAP_3.merge(tbl_oversize[[\"ZONE\", \"CLIENT_RATES\"]], left_on=['ZONE'], right_on=['ZONE'], how='left')\n",
    "tbl_PLD_original_USPSAP_3 = tbl_PLD_original_USPSAP_3.rename(columns={'CLIENT_RATES': 'FRT_USPSAP'})\n",
    "\n",
    "#combine again\n",
    "tbl_PLD_original_USPSAP = pd.concat([tbl_PLD_original_USPSAP_3, tbl_PLD_original_USPSAP_none3], axis=0)\n",
    "\n",
    "#6. Parcels that exceed 22 inches but not greater than 30 inches in length, add $4 per package. \n",
    "#7. parcels that exceed 30 inches in length, add $7 per package. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1101,
   "id": "9c8fd31e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tbl_PLD_original_USPSAP['EXTRA_USPSAP_1'] = tbl_PLD_original_USPSAP.apply(lambda row:\n",
    "                                                            4 if (row['L'] > 22 and row['L'] <= 30) else \n",
    "                                                           (7 if (row['L'] > 30) else\n",
    "                                                            (0)), axis=1)\n",
    "\n",
    "\n",
    "#8. parcels that exceed 2 cubic feet (3456 cubic inches), add $15 per package. \n",
    "tbl_PLD_original_USPSAP['EXTRA_USPSAP_2'] = tbl_PLD_original_USPSAP.apply(lambda row:\n",
    "                                                            15 if (row['CUBIC_INCH'] > 3456) else (0), axis=1)\n",
    "\n",
    "#Get Total\n",
    "# note that for those with non-round up actual weight, if it is < 1, we still assign it the freight rate. \n",
    "\n",
    "#Get Total, revise the logic that if the freight rate is 0 or some rate is 0, the total will be 0. ups can handle everything. \n",
    "tbl_PLD_original_USPSAP['TOTAL_USPSAP'] = round(tbl_PLD_original_USPSAP.apply(lambda row: \n",
    "                                                                  (row['FRT_USPSAP'] + (row['EXTRA_USPSAP_1'] + row['EXTRA_USPSAP_2'])/(1-Margin_on_USPS)) if row['FRT_USPSAP'] != '' else '', axis=1),2)                         \n",
    "#tbl_PLD_original_USPSAP.to_csv('test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ff4beff",
   "metadata": {},
   "source": [
    "# MI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "391514a5",
   "metadata": {},
   "source": [
    "#MIPLE (MI Parcel Select Lightweight Expedited): commercial & residential, oz\n",
    "\n",
    "#MIPH (MI Parcel Select Heavyweight): commercial & residential, lb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db5813bb",
   "metadata": {},
   "source": [
    "# MIPLE (MI Parcel Select Lightweight Expedited)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1029,
   "id": "c6b2671a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#select the rate to use, #Decide the part of PLD to rerate\n",
    "tbl_clientRates_19 = tbl_clientRates[(tbl_clientRates['SERVICE_CODE'] == 'MIPLE')]\n",
    "tbl_PLD_original_MIPLE = tbl_BP[(tbl_BP['ACTUAL_WEIGHT'] < 1) & (tbl_BP['L'] <= 26) & (tbl_BP['W'] <= 26) & (tbl_BP['H'] <= 26)].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1030,
   "id": "e3fedb27",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Start to rerate\n",
    "#add billed weight column to PLD\n",
    "tbl_PLD_original_MIPLE['BILLED_WEIGHT_OZ'] = tbl_PLD_original_MIPLE.apply(lambda row: max(row['ACTUAL_WEIGHT_LB'], np.ceil(row['CUBIC_INCH']/dim_factor_mi))*16 if (row['CUBIC_INCH'] > 1728 and row['ACTUAL_WEIGHT'] >= 1) else\n",
    "                                                                          (row['ACTUAL_WEIGHT_OZ']), axis=1)\n",
    "\n",
    "\n",
    "#add Freight column to PLD\n",
    "tbl_PLD_original_MIPLE['ZONE'] = tbl_PLD_original_MIPLE['ZONE'].astype(str)\n",
    "tbl_PLD_original_MIPLE = tbl_PLD_original_MIPLE.merge(tbl_clientRates_19[[\"WEIGHT_OZ\", \"ZONE\", \"CLIENT_RATES\"]], left_on=['BILLED_WEIGHT_OZ', 'ZONE'], right_on=['WEIGHT_OZ', 'ZONE'], how='left')\n",
    "tbl_PLD_original_MIPLE = tbl_PLD_original_MIPLE.drop('WEIGHT_OZ', axis=1)\n",
    "tbl_PLD_original_MIPLE = tbl_PLD_original_MIPLE.rename(columns={'CLIENT_RATES': 'FRT_MIPLE'})\n",
    "\n",
    "#add fuel surcharge column to PLD\n",
    "tbl_PLD_original_MIPLE['FSC_MIPLE'] = tbl_PLD_original_MIPLE.apply(lambda row: row['FRT_MIPLE']*0.065, axis=1)\n",
    "\n",
    "#add NQD Surcharge column to PLD\n",
    "\n",
    "# A Non-standard piece fee of $4.00 will apply to any piece with a single side over 22\" or with total dimensions over 2 ft.\n",
    "tbl_PLD_original_MIPLE['NQD_MIPLE'] = tbl_PLD_original_MIPLE.apply(lambda row:\n",
    "                                                            4 if any([row['L'] > 22, row['W'] > 22, row['H'] > 22, row['CUBIC_INCH'] > 3456]) else\n",
    "                                                           (0), axis=1)\n",
    "\n",
    "#Get Total\n",
    "tbl_PLD_original_MIPLE['TOTAL_MIPLE'] = round(tbl_PLD_original_MIPLE.apply(lambda row: \n",
    "                                                                  np.nan if ((row['FRT_MIPLE'] == '') | (pd.isna(row['FRT_MIPLE'])))\n",
    "                                                                  else\n",
    "                                                                  ((row['FRT_MIPLE'] + row['FSC_MIPLE']) + (row['NQD_MIPLE'])/(1-Margin_on_MI)), axis=1),2)\n",
    "\n",
    "#tbl_PLD_original_MIPLE.to_csv('test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa199e87",
   "metadata": {},
   "source": [
    "# MIPH (MI Parcel Select Heavyweight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1031,
   "id": "5dc36d74",
   "metadata": {},
   "outputs": [],
   "source": [
    "#select the rate to use, #Decide the part of PLD to rerate\n",
    "tbl_clientRates_20 = tbl_clientRates[(tbl_clientRates['SERVICE_CODE'] == 'MIPH')]\n",
    "tbl_PLD_original_MIPH = tbl_BP[(tbl_BP['L'] <= 26) & (tbl_BP['W'] <= 26) & (tbl_BP['H'] <= 26)].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1032,
   "id": "391126f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Start to rerate\n",
    "#add billed weight column to PLD\n",
    "tbl_PLD_original_MIPH['BILLED_WEIGHT_LB'] = tbl_PLD_original_MIPH.apply(lambda row: max(row['ACTUAL_WEIGHT_LB'], np.ceil(row['CUBIC_INCH']/dim_factor_mi)) if (row['CUBIC_INCH'] > 1728 and row['ACTUAL_WEIGHT'] >= 1) else\n",
    "                                                                          (row['ACTUAL_WEIGHT_LB']), axis=1)\n",
    "\n",
    "\n",
    "\n",
    "#add Freight column to PLD\n",
    "tbl_PLD_original_MIPH['ZONE'] = tbl_PLD_original_MIPH['ZONE'].astype(str)\n",
    "tbl_PLD_original_MIPH = tbl_PLD_original_MIPH.merge(tbl_clientRates_20[[\"WEIGHT_LB\", \"ZONE\", \"CLIENT_RATES\"]], left_on=['BILLED_WEIGHT_LB', 'ZONE'], right_on=['WEIGHT_LB', 'ZONE'], how='left')\n",
    "tbl_PLD_original_MIPH = tbl_PLD_original_MIPH.drop('WEIGHT_LB', axis=1)\n",
    "tbl_PLD_original_MIPH = tbl_PLD_original_MIPH.rename(columns={'CLIENT_RATES': 'FRT_MIPH'})\n",
    "\n",
    "#add fuel surcharge column to PLD\n",
    "tbl_PLD_original_MIPH['FSC_MIPH'] = tbl_PLD_original_MIPH.apply(lambda row: row['FRT_MIPH']*0.065, axis=1)\n",
    "\n",
    "\n",
    "#add NQD Surcharge column to PLD\n",
    "\n",
    "# A Non-standard piece fee of $4.00 will apply to any piece with a single side over 22\" or with total dimensions over 2 ft.\n",
    "tbl_PLD_original_MIPH['NQD_MIPH'] = tbl_PLD_original_MIPH.apply(lambda row:\n",
    "                                                            4 if any([row['L'] > 22, row['W'] > 22, row['H'] > 22, row['CUBIC_INCH'] > 3456]) else\n",
    "                                                           (0), axis=1)\n",
    "\n",
    "#Get Total\n",
    "tbl_PLD_original_MIPH['TOTAL_MIPH'] = round(tbl_PLD_original_MIPH.apply(lambda row: \n",
    "                                                                  np.nan if ((row['FRT_MIPH'] == '') | (pd.isna(row['FRT_MIPH'])))\n",
    "                                                                  else\n",
    "                                                                  ((row['FRT_MIPH'] + row['FSC_MIPH']) + (row['NQD_MIPH'])/(1-Margin_on_MI)), axis=1),2)\n",
    "\n",
    "#tbl_PLD_original_MIPH.to_csv('test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "smooth-return",
   "metadata": {},
   "source": [
    "# 1.  Model Output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b332202d",
   "metadata": {},
   "source": [
    "# Specify the columns to keep in the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1102,
   "id": "progressive-battery",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['CustomerID', 'CustomerName', 'Facility', 'BarrettOrderNumber',\n",
       "       'Reference', 'PoNumber', 'TRACKING_NUMBER', 'SHIP_DATE', 'OLD_SERVICE',\n",
       "       'Shipper', 'ShipToName', 'Contact', 'City', 'State', 'ZIP_CODE',\n",
       "       'Country', 'ZONE', 'Quantity', 'DIMENSIONS', 'ACTUAL_WEIGHT',\n",
       "       'RESIDENTIAL_FLAG', 'L', 'W', 'H', 'GIRTH_AND_L', 'CUBIC_INCH',\n",
       "       'ACTUAL_WEIGHT_LB', 'ACTUAL_WEIGHT_OZ'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 1102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tbl_BP.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1103,
   "id": "advance-cabinet",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a copy of the DataFrame with only the specified columns\n",
    "tbl_PLD_original_rerated = tbl_BP.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f6645fb",
   "metadata": {},
   "source": [
    "# Specify the prioritized Services"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1035,
   "id": "6d2c87ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "tbl_PLD_original_REDE = tbl_PLD_original_REDE.drop_duplicates()\n",
    "tbl_PLD_original_RED = tbl_PLD_original_RED.drop_duplicates()\n",
    "tbl_PLD_original_REDS = tbl_PLD_original_REDS.drop_duplicates()\n",
    "tbl_PLD_original_2DAM = tbl_PLD_original_2DAM.drop_duplicates()\n",
    "tbl_PLD_original_BLUE = tbl_PLD_original_BLUE.drop_duplicates()\n",
    "tbl_PLD_original_ORNG = tbl_PLD_original_ORNG.drop_duplicates()\n",
    "tbl_PLD_original_GRND = tbl_PLD_original_GRND.drop_duplicates()\n",
    "tbl_PLD_original_GRES = tbl_PLD_original_GRES.drop_duplicates()\n",
    "tbl_PLD_original_SRPT1 = tbl_PLD_original_SRPT1.drop_duplicates()\n",
    "tbl_PLD_original_SRPT = tbl_PLD_original_SRPT.drop_duplicates()\n",
    "tbl_PLD_original_DHLG = tbl_PLD_original_DHLG.drop_duplicates()\n",
    "tbl_PLD_original_DHLG1 = tbl_PLD_original_DHLG1.drop_duplicates()\n",
    "tbl_PLD_original_DHLE = tbl_PLD_original_DHLE.drop_duplicates()\n",
    "tbl_PLD_original_DHLE1 = tbl_PLD_original_DHLE1.drop_duplicates()\n",
    "tbl_PLD_original_DHLEM = tbl_PLD_original_DHLEM.drop_duplicates()\n",
    "tbl_PLD_original_USPSAG = tbl_PLD_original_USPSAG.drop_duplicates()\n",
    "tbl_PLD_original_USPSAP = tbl_PLD_original_USPSAP.drop_duplicates()\n",
    "tbl_PLD_original_MIPH = tbl_PLD_original_MIPH.drop_duplicates()\n",
    "tbl_PLD_original_MIPLE = tbl_PLD_original_MIPLE.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1036,
   "id": "c1ce348c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List only the total rates of all carrier services. \n",
    "tbl_PLD_original_rerated['TOTAL_REDE'] = tbl_PLD_original_rerated['TRACKING_NUMBER'].map(tbl_PLD_original_REDE.set_index('TRACKING_NUMBER')['TOTAL_REDE'])\n",
    "tbl_PLD_original_rerated['TOTAL_RED'] = tbl_PLD_original_rerated['TRACKING_NUMBER'].map(tbl_PLD_original_RED.set_index('TRACKING_NUMBER')['TOTAL_RED'])\n",
    "tbl_PLD_original_rerated['TOTAL_REDS'] = tbl_PLD_original_rerated['TRACKING_NUMBER'].map(tbl_PLD_original_REDS.set_index('TRACKING_NUMBER')['TOTAL_REDS'])\n",
    "tbl_PLD_original_rerated['TOTAL_2DAM'] = tbl_PLD_original_rerated['TRACKING_NUMBER'].map(tbl_PLD_original_2DAM.set_index('TRACKING_NUMBER')['TOTAL_2DAM'])\n",
    "tbl_PLD_original_rerated['TOTAL_BLUE'] = tbl_PLD_original_rerated['TRACKING_NUMBER'].map(tbl_PLD_original_BLUE.set_index('TRACKING_NUMBER')['TOTAL_BLUE'])\n",
    "tbl_PLD_original_rerated['TOTAL_ORNG'] = tbl_PLD_original_rerated['TRACKING_NUMBER'].map(tbl_PLD_original_ORNG.set_index('TRACKING_NUMBER')['TOTAL_ORNG'])\n",
    "tbl_PLD_original_rerated['TOTAL_GRND'] = tbl_PLD_original_rerated['TRACKING_NUMBER'].map(tbl_PLD_original_GRND.set_index('TRACKING_NUMBER')['TOTAL_GRND'])\n",
    "tbl_PLD_original_rerated['TOTAL_GRES'] = tbl_PLD_original_rerated['TRACKING_NUMBER'].map(tbl_PLD_original_GRES.set_index('TRACKING_NUMBER')['TOTAL_GRES'])\n",
    "tbl_PLD_original_rerated['TOTAL_SRPT1'] = tbl_PLD_original_rerated['TRACKING_NUMBER'].map(tbl_PLD_original_SRPT1.set_index('TRACKING_NUMBER')['TOTAL_SRPT1'])\n",
    "tbl_PLD_original_rerated['TOTAL_SRPT'] = tbl_PLD_original_rerated['TRACKING_NUMBER'].map(tbl_PLD_original_SRPT.set_index('TRACKING_NUMBER')['TOTAL_SRPT'])\n",
    "\n",
    "tbl_PLD_original_rerated['TOTAL_DHLG'] = tbl_PLD_original_rerated['TRACKING_NUMBER'].map(tbl_PLD_original_DHLG.set_index('TRACKING_NUMBER')['TOTAL_DHLG'])\n",
    "tbl_PLD_original_rerated['TOTAL_DHLG1'] = tbl_PLD_original_rerated['TRACKING_NUMBER'].map(tbl_PLD_original_DHLG1.set_index('TRACKING_NUMBER')['TOTAL_DHLG1'])\n",
    "tbl_PLD_original_rerated['TOTAL_DHLE'] = tbl_PLD_original_rerated['TRACKING_NUMBER'].map(tbl_PLD_original_DHLE.set_index('TRACKING_NUMBER')['TOTAL_DHLE'])\n",
    "tbl_PLD_original_rerated['TOTAL_DHLE1'] = tbl_PLD_original_rerated['TRACKING_NUMBER'].map(tbl_PLD_original_DHLE1.set_index('TRACKING_NUMBER')['TOTAL_DHLE1'])\n",
    "tbl_PLD_original_rerated['TOTAL_DHLEM'] = tbl_PLD_original_rerated['TRACKING_NUMBER'].map(tbl_PLD_original_DHLEM.set_index('TRACKING_NUMBER')['TOTAL_DHLEM'])\n",
    "\n",
    "tbl_PLD_original_rerated['TOTAL_USPSAG'] = tbl_PLD_original_rerated['TRACKING_NUMBER'].map(tbl_PLD_original_USPSAG.set_index('TRACKING_NUMBER')['TOTAL_USPSAG'])\n",
    "tbl_PLD_original_rerated['TOTAL_USPSAP'] = tbl_PLD_original_rerated['TRACKING_NUMBER'].map(tbl_PLD_original_USPSAP.set_index('TRACKING_NUMBER')['TOTAL_USPSAP'])\n",
    "\n",
    "tbl_PLD_original_rerated['TOTAL_MIPLE'] = tbl_PLD_original_rerated['TRACKING_NUMBER'].map(tbl_PLD_original_MIPLE.set_index('TRACKING_NUMBER')['TOTAL_MIPLE'])\n",
    "tbl_PLD_original_rerated['TOTAL_MIPH'] = tbl_PLD_original_rerated['TRACKING_NUMBER'].map(tbl_PLD_original_MIPH.set_index('TRACKING_NUMBER')['TOTAL_MIPH'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb53050f",
   "metadata": {},
   "source": [
    "# Base Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1106,
   "id": "permanent-reminder",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Base model output. Make sure to check the obnormal rates due to obnormal dim, etc. \n",
    "tbl_PLD_original_rerated.to_csv('test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac201fc0",
   "metadata": {},
   "source": [
    "# ******End******"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ffcda5f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
