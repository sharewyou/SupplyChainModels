{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "50f0a14c",
   "metadata": {},
   "source": [
    "# RateCurrent_Automation\n",
    "**Rates packages for small parcel customers under the actual services (UPS, DHL, USPS, and UPS Intl).**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77149f36",
   "metadata": {},
   "source": [
    "**Structure**:  \n",
    "Section 1. Read in and Re-format Freight Rates  \n",
    "Section 2. Read in and Re-format all other 4 input tables  \n",
    "Section 3. Re-format and Rate PLD\n",
    "           \n",
    "           a. Load PLD from snowflake query\n",
    "           b. Patch PLD\n",
    "           c. Flag PLD (Flag instead of delete data)\n",
    "           d. Add columns to PLD\n",
    "           e. Add Freight Rates (Published Freight \"UPS only\" & Barrett Freight \"except UPS\") to PLD\n",
    "           f. Add Customer Freight to PLD (and Barrett Freight \"UPS only\" to PLD)\n",
    "           g. Add UPS Published & Barrett & Customer Resi, Das, and Ahs fee\n",
    "           h. Add dhl & usps accessorials\n",
    "           i. Get FSC (only for UPS and DHL)\n",
    "           j. Get Total\n",
    "           k. Handle Special Cases and Flag Rate\n",
    "           l. Calculate the savings and Savings % and Format\n",
    "\n",
    "\n",
    "**Input tables**:  \n",
    "Doc. 1. FreightCharge_xxxx: freight rate cards (insert updated version once a year)  \n",
    "Doc. 2. ClientCharge_xxxx: client rate and parameters (insert newer version once a quarter at Max.)  \n",
    "Doc. 3. FuelSurcharge_weekly: fuel surcharge rate (update once a week)  \n",
    "Doc. 4. ZipToZone: get zone from destination zip based on different facilities  \n",
    "Doc. 5. ZipToDas_xxxx: a table to assign the type of DAS Category (insert newer version once a year)  \n",
    "Doc. 6. PLD: snowflake package level detail as an input table\n",
    "\n",
    "\n",
    "\n",
    "**Input Data Cleaning & Formatting**\n",
    "\n",
    "| Column Input | Column Type | Function | Null Case | Error Case   |\n",
    "|:--------|:---------|:-------------|:------------|:------------|\n",
    "| TrackingNumber     | any (int64, object, etc)  | filter out duplications         | fill out number from 1,2,...         | no concern   |\n",
    "| Facility & ZipCode     | object                                                                                                              | get zone   | no concern if Zone exists         | no concern if Zone exists   |\n",
    "| ShipDate     | datetime                                                                                                             | get fsc%  | use yesterday's Date         | use yesterday's Date   |\n",
    "| Service     | object                                                                                                                | get target service for rate         | cannot rate         | cannot rate   |\n",
    "| ZipCode     | object | get das | cannot rate | cannot rate |\n",
    "| Zone     | object | get Freight | cannot rate if “Zip Code” and “Facility” do not exist | cannot rate if “Zip Code” and “Facility” do not exist |\n",
    "| Dimensions    | object | get billed weight | cannot rate | cannot rate | \n",
    "| ActualWeight    | float | get billed weight | cannot rate | cannot rate 0 lb or over 150 lbs|\n",
    "| ResidentialFlag    | bool| get target service for rate | no concern | no concern |\n",
    "\n",
    "\n",
    "**FreightCharge note**\n",
    "\n",
    "| Carrier | Service | Residential | WeightType | RateType   |\n",
    "|:--------|:---------|:-------------|:------------|:------------|\n",
    "| UPS     | REDE (UPS NDA Early) / RED (UPS NDA) / REDS (UPS NDA Saver) / 2DAM (UPS 2DA A.M.) / BLUE (UPS 2DA) / ORNG (UPS 3DA) / SRPT (UPS Surepost) / CNST (UPS Standard to Canada) / WWE (World Wide Expedited) / WWXS (World Wide Express Saver) / WWX (World Wide Express)  | both         | lb         | Published   |\n",
    "| UPS     | GRND (UPS Ground Commercial)                                                                                                              | commercial   | lb         | Published   |\n",
    "| UPS     | GRES (UPS Ground Residential)                                                                                                             | residential  | lb         | Published   |\n",
    "| UPS     | SRPT<1 (UPS Surepost 1#>)                                                                                                                 | both         | oz         | Published   |\n",
    "| DHL     | DHLG (DHL SmartMail Parcel Plus Ground 1-25)/DHLE (DHL SmartMail Parcel Plus Expedited 1-25) | both | lb | Barrett |\n",
    "| DHL     | DHLG<1 (DHL SmartMail Parcel Ground < 1lb)/DHLE<1 (DHL SmartMail Parcel Expedited  < 1lb)/DHLEM (DHL SmartMail Parcel Expedited Max) | both | oz | Barrett |\n",
    "| USPS    | USPSAP_CPP (USPS Auctane PM) | both | lb* | Barrett* | \n",
    "| USPS    | USPSFC_CPP (USPS First Class) | both | oz | Barrett* |\n",
    "| USPS    | USPSPS_CPP (USPS Parcel Select)| both | lb | Barrett* |\n",
    "| USPS    | USPSAP_AUCT (USPS Auctane PM) | both | lb* | Barrett | \n",
    "| USPS    | USPSFC_AUCT (USPS First Class) | both | oz | Barrett |\n",
    "| USPS    | USPSPS_AUCT (USPS Parcel Select)| both | lb | Barrett |\n",
    "| MI      | MIPLE (MI Parcel Select Lightweight Expedited)|  both | oz | Barrett |\n",
    "| MI      | MIPH (MI Parcel Select Heavyweight)|  both | lb | Barrett |\n",
    "\n",
    "*For convenience of exchange, flag as Barrett Rate. Although AP has one \"<1lb\" weight but still flagged as \"lb\". \n",
    "\n",
    "**Client rate calculation based on \"Barrett Dim\" Pub rate and Barrett rate**\n",
    "\n",
    "| Carrier | Service | Client Rate Calculation   |\n",
    "|:--------|:---------|:-------------|\n",
    "| UPS     | REDE / RED / REDS / 2DAM / BLUE / ORNG / CNST / WWE / WWXS / WWX |  max(Pub Rate *(1 - Round(Discount%,2)), Min) ** |\n",
    "| UPS     | GRND / SRPT / GRES  |  max(Pub Rate *(1 - Round(Discount%,2)), Min) **Consider weight break in Lb |\n",
    "| UPS     | SRPT<1  |  max(Pub Rate *(1 - Round(Discount%,2)), Min) **Consider weight break in Oz |\n",
    "| DHL/MI | All | Barrett Rate / (1-Margin%) | \n",
    "| USPS | All | CPP or Barrett Rate / (1-Margin%) | \n",
    "\n",
    "*The output breakdown rates are rounded to 2 decimal places. The total rate is a sum up of the breakdowns and then rounded again. And the Savings and Margin are rounded again upon the calculation of the other columns.  \n",
    "*4172 applies to 417801 and 4178 (417801 and 4178 do not exist in data)  \n",
    "*The \"version switch + number\" located in this code flags the place to edit to switch to a different version. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "056ef519",
   "metadata": {},
   "source": [
    "*********Parameter Set Up*********"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "70f6b342",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import re\n",
    "from datetime import timedelta, date\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "#Dim factor\n",
    "dim_factor_ups_pub = 139\n",
    "dim_factor_ups = 280\n",
    "dim_factor_ups_srpt = 139\n",
    "dim_factor_ups_intl = 139\n",
    "dim_factor_dhl = 166\n",
    "dim_factor_usps = 166\n",
    "dim_factor_mi = 166\n",
    "\n",
    "#Dim Allowance\n",
    "ups_allowance = 3456\n",
    "dhl_allowance = 1728\n",
    "usps_allowance = 1728\n",
    "mi_allowance = 1728\n",
    "\n",
    "#ups parameters\n",
    "ups_fsc_disc = 0.8\n",
    "intl_ground_res = 5.5 #5.85 in 2024 (version switch 1)\n",
    "intl_air_res = 5.85 #6.20 in 2024 (version switch 2)\n",
    "intl_ahs_weight_limit = 70 #55 in 2024 (version switch 3)\n",
    "intl_ahs = 24 #31 in 2024 (version switch 4)\n",
    "\n",
    "#dhl parameters\n",
    "dhl_cubic_inch_threshold_1 = 1728\n",
    "dhl_cubic_inch_threshold_2 = 3456\n",
    "dhl_weight_threshold = 1\n",
    "dhl_g_and_l_threshold_1 = 50\n",
    "dhl_g_and_l_threshold_2 = 84\n",
    "dhl_max_l_threshold_1 = 22\n",
    "dhl_max_l_threshold_2 = 27\n",
    "dhl_max_l_threshold_3 = 30\n",
    "\n",
    "dhl_nqd_lengthfee_1 = 4.5\n",
    "dhl_nqd_lengthfee_2 = 15.5\n",
    "dhl_nqd_volumefee = 15.5\n",
    "\n",
    "# update to 2024 when needed\n",
    "dates = ['2023-01-01', '2023-09-30', '2023-10-01', '2023-12-31'] #(version switch 5)\n",
    "nqd_dhl_date_1, nqd_dhl_date_2, nqd_dhl_date_3, nqd_dhl_date_4 = [pd.to_datetime(date) for date in dates]\n",
    "\n",
    "#usps parameters\n",
    "usps_l_threshold_1 = 22\n",
    "usps_l_threshold_2 = 30\n",
    "usps_cubic_inch_threshold = 3456\n",
    "\n",
    "usps_nonestandard_lengthfee_1 = 4 #2024: 4 (version switch 6)\n",
    "usps_nonestandard_lengthfee_2 = 7 #2024: 18 (version switch 7)\n",
    "usps_nonestandard_volumefee = 15 #2024: 30 (version switch 8)\n",
    "\n",
    "\n",
    "# Barrett min international charge 2023\n",
    "mapping_min_br_intl_2023 = {# Canada Standard\n",
    "                '51': 15.61, '52': 15.61, '53': 15.61, '54': 15.61, '55': 15.61, '56': 15.61,\n",
    "    # World Wide Expedited\n",
    "                '71': 29.98, '72': 31.76, '74': 26.58, '601/631': 34.59, '602/632': 28.37, '603/633': 37.54, '604/634': 40.24, '605/635': 35.10,\n",
    "'606/636': 47.57, '607/637': 39.33, '608/638': 36.60, '609/639': 33.30, '611/641': 36.95, '612/642': 36.79, '613/643': 32.19,\n",
    "'620': 27.66, '621': 30.79, # World Wide Express Saver, \n",
    "                '481': 31.569, '482': 34.899, '484': 32.157, '401': 38.868, '402': 34.44, '403': 44.517, '404': 42.48, '405': 41.076,\n",
    "'406': 51.042, '407': 61.182, '408': 63.648, '409': 36.132, '411': 43.464, '412': 40.509, '413': 35.625, '420': 32.718,\n",
    "'421': 33.849, # World Wide Express\n",
    "                '81': 32.952, '82': 36.297, '84': 32.769, '901': 40.035, '902': 34.956, '903': 46.5, '904': 43.035, '905': 41.721, \n",
    "'906': 51.759, '907': 63.147, '908': 64.563, '909': 36.603, '911': 44.097, '912': 41.877, '913': 37.083, '920': 33.21,\n",
    "'921': 34.911}\n",
    "\n",
    "\n",
    "# Barrett min international charge 2024\n",
    "mapping_min_br_intl_2024 = {# Canada Standard\n",
    "                '51': 16.52, '52': 16.52, '53': 16.52, '54': 16.52, '55': 16.52, '56': 16.52,\n",
    "    # World Wide Expedited\n",
    "                '71': 31.48, '72': 33.50, '74': 27.91, '601/631': 36.32, '602/632': 28.66, '603/633': 37.54, '604/634': 41.85, '605/635': 37.21,\n",
    "'606/636': 51.38, '607/637': 39.33, '608/638': 36.60, '609/639': 35.30, '611/641': 38.80, '612/642': 39.36, '613/643': 34.12,\n",
    "'620': 28.44, '621': 32.09, # World Wide Express Saver, \n",
    "                '481': 33.62, '482': 36.99, '484': 34.09, '401': 41.59, '402': 37.20, '403': 47.63, '404': 44.60, '405': 43.95,\n",
    "'406': 53.60, '407': 65.47, '408': 69.38, '409': 38.66, '411': 47.38, '412': 42.54, '413': 37.05, '420': 34.00,\n",
    "'421': 35.17, # World Wide Express\n",
    "                '81': 34.97, '82': 38.84, '84': 35.28, '901': 42.63, '902': 39.06, '903': 49.54, '904': 46.39, '905': 46.15, \n",
    "'906': 55.74, '907': 68.74, '908': 72.85, '909': 40.60, '911': 50.22, '912': 45.09, '913': 38.72, '920': 35.66,\n",
    "'921': 36.90}\n",
    "\n",
    "# update to 2023 or 2024 when needed\n",
    "mapping_min_br_intl = mapping_min_br_intl_2023 #(version switch 9)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e556ee2",
   "metadata": {},
   "source": [
    "**Section 1. Read in and Re-format Freight Rates**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "27468289",
   "metadata": {},
   "outputs": [],
   "source": [
    "# *************** Read Doc. 1 *************************\n",
    "xls = pd.ExcelFile('FreightCharge_2023.xlsx') # (version switch 10)\n",
    "\n",
    "# Group all rate cards based on their WeightType & RateType & handle DHL independently\n",
    "# Lb and Published Rate\n",
    "tabs_1 = ['REDE', 'RED', 'REDS', '2DAM', 'BLUE', 'ORNG', 'GRND', 'GRES', 'SRPT', 'CNST', 'WWE', 'WWXS', 'WWX']\n",
    "# Lb and Barrett Rate\n",
    "tabs_2 = ['USPSPS_AUCT', 'USPSAP_AUCT', 'USPSPS_CPP', 'USPSAP_CPP', 'MIPH']\n",
    "# Oz and Published Rate\n",
    "tabs_3 = ['SRPT<1'] \n",
    "# Oz and Barrett Rate\n",
    "tabs_4 = ['USPSFC_AUCT', 'USPSFC_CPP', 'MIPLE']\n",
    "# Lb and Barrett Rate (DHL)\n",
    "dhl_ids_1 = ['5114358', '5122444', '5122723', '5122739', '5122855', '5122890', '5122893', '5123280', '5123282', '5123283'#, '5123556' # (version switch 11) no '5123556' in 2023 but in 2024\n",
    "            ]\n",
    "dhl_types_1 = ['DHLG', 'DHLE']\n",
    "dhl_tabs_1 = [f\"{type}_{id}\" for id in dhl_ids_1 for type in dhl_types_1]\n",
    "# Oz and Barrett Rate (DHL)\n",
    "dhl_ids_2 = ['5114358', '5122444', '5122723', '5122739', '5122855', '5122890', '5122893', '5123280', '5123282', '5123283'#, '5123556' # (version switch 12) no '5123556' in 2023 but in 2024\n",
    "            ]\n",
    "dhl_types_2 = ['DHLG<1', 'DHLE<1', 'DHLEM']\n",
    "dhl_tabs_2 = [f\"{type}_{id}\" for id in dhl_ids_2 for type in dhl_types_2 if not (type == 'DHLEM' and id == '5122444')  # (version switch 13) DHLEM & '5122444' exists in 2024 but not in 2023\n",
    "             ]\n",
    "\n",
    "# Combine all the groups into a single list\n",
    "all_tabs = tabs_1 + tabs_2 + tabs_3 + tabs_4 + dhl_tabs_1 + dhl_tabs_2\n",
    "\n",
    "# Regroup/re-categorize the tabs into 4 groups\n",
    "lb_br_tabs = tabs_2 + dhl_tabs_1    \n",
    "lb_pr_tabs = tabs_1\n",
    "oz_br_tabs = tabs_4 + dhl_tabs_2    \n",
    "oz_pr_tabs = tabs_3\n",
    "\n",
    "# Load data using a dictionary comprehension. dfs['REDE']\n",
    "dfs = {tab: pd.read_excel(xls, tab) for tab in all_tabs}\n",
    "\n",
    "# Process each dataframe based on tab name\n",
    "for tab, df in dfs.items():\n",
    "    # Determine weight column name\n",
    "    if tab in oz_br_tabs or tab in oz_pr_tabs:\n",
    "        weight_col = 'WeightOz'\n",
    "    elif tab in lb_br_tabs or tab in lb_pr_tabs:\n",
    "        weight_col = 'WeightLb'\n",
    "    else:\n",
    "        raise ValueError(f\"Tab {tab} is not categorized properly by weight.\")\n",
    "    \n",
    "    # Determine rate column name\n",
    "    if tab in oz_br_tabs or tab in lb_br_tabs:\n",
    "        rate_col_name = 'BarrettRate'\n",
    "    elif tab in oz_pr_tabs or tab in lb_pr_tabs:\n",
    "        rate_col_name = 'PublishedRate'\n",
    "    else:\n",
    "        raise ValueError(f\"Tab {tab} is not categorized properly by rate.\")\n",
    "\n",
    "    # Apply unstack operation\n",
    "    dfs[tab] = (df.set_index('Zones')\n",
    "                .unstack()\n",
    "                .rename_axis(('Zone', weight_col))\n",
    "                .reset_index(name=rate_col_name))\n",
    "\n",
    "# Concatenate all transformed dataframes into one, with an additional column indicating the tab name\n",
    "FreightCharge = pd.concat(dfs.values(), keys=dfs.keys(), ignore_index=False)\n",
    "FreightCharge.reset_index(level=0, inplace=True)\n",
    "FreightCharge.rename(columns={'level_0': 'ServiceCode'}, inplace=True)\n",
    "\n",
    "# Fill missing values in 'Weight_Lb' using 'Weight_Oz' / 16\n",
    "FreightCharge['WeightLb'] = np.where(\n",
    "    FreightCharge['WeightLb'].isna(), \n",
    "    FreightCharge['WeightOz'] / 16,     \n",
    "    FreightCharge['WeightLb']           \n",
    ")\n",
    "\n",
    "# Fill missing values in 'Weight_Oz' using 'Weight_Lb' * 16\n",
    "FreightCharge['WeightOz'] = np.where(\n",
    "    FreightCharge['WeightOz'].isna(),  \n",
    "    FreightCharge['WeightLb'] * 16,    \n",
    "    FreightCharge['WeightOz']          \n",
    ")\n",
    "\n",
    "\n",
    "# Generate Service detailed names based on ServiceCode section\n",
    "subset_numbers = ['5114358', '5122723', '5122739', '5122855', '5122890', '5122893', '5123280', '5123282', '5123283'# , '5123556' # (version switch 14) '5123556' exists in 2024 but not in 2023\n",
    "                 ]\n",
    "\n",
    "def generate_service_details(ServiceCode):\n",
    "    number = ServiceCode.split('_')[-1]\n",
    "    if number not in subset_numbers:\n",
    "        return None\n",
    "\n",
    "    if \"DHLG_\" in ServiceCode:\n",
    "        return f'DHL SmartMail Parcel Plus Ground 1-25 {number}'\n",
    "    elif \"DHLG<1_\" in ServiceCode:\n",
    "        return f'DHL SmartMail Parcel Ground < 1lb {number}'\n",
    "    elif \"DHLE_\" in ServiceCode:\n",
    "        return f'DHL SmartMail Parcel Plus Expedited 1-25 {number}'\n",
    "    elif \"DHLE<1_\" in ServiceCode:\n",
    "        return f'DHL SmartMail Parcel Expedited < 1lb {number}'\n",
    "    elif \"DHLEM_\" in ServiceCode:\n",
    "        return f'DHL SmartMail Parcel Expedited Max {number}'\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "FreightCharge['Service'] = FreightCharge['ServiceCode'].apply(generate_service_details)\n",
    "\n",
    "tab_to_description = {\n",
    "    'REDE': 'UPS NDA Early',\n",
    "    'RED': 'UPS NDA',\n",
    "    'REDS': 'UPS NDA Saver',\n",
    "    '2DAM': 'UPS 2DA A.M.',\n",
    "    'BLUE': 'UPS 2DA',\n",
    "    'ORNG': 'UPS 3DA',\n",
    "    'GRND': 'UPS Ground Commercial',\n",
    "    'GRES': 'UPS Ground Residential',\n",
    "    'SRPT<1': 'UPS Surepost 1#>',\n",
    "    'SRPT': 'UPS Surepost',\n",
    "    'CNST': 'UPS Standard to Canada',\n",
    "    'WWE': 'UPS Worldwide Express',\n",
    "    'WWXS': 'UPS Worldwide Express Saver',\n",
    "    'WWX': 'UPS Worldwide Expedited',\n",
    "    'USPSFC_CPP': 'USPS First Class CPP',\n",
    "    'USPSPS_CPP': 'USPS Parcel Select CPP',\n",
    "    'USPSAP_CPP': 'USPS Priority Mail CPP',\n",
    "    'USPSFC_AUCT': 'USPS First Class Auctane',\n",
    "    'USPSPS_AUCT': 'USPS Parcel Select Auctane',\n",
    "    'USPSAP_AUCT': 'USPS Priority Mail Auctane',\n",
    "    'MIPLE': 'MI Parcel Select Lightweight Expedited',\n",
    "    'MIPH': 'MI Parcel Select Heavyweight',\n",
    "    'DHLG_5122444': 'DHL SmartMail Parcel Plus Ground 1-25 5122444',\n",
    "    'DHLG<1_5122444': 'DHL SmartMail Parcel Ground < 1lb 5122444',\n",
    "    'DHLE_5122444': 'DHL SmartMail Parcel Plus Expedited 1-25 5122444',\n",
    "    'DHLE<1_5122444': 'DHL SmartMail Parcel Expedited < 1lb 5122444'\n",
    "    #,'DHLEM_5122444': 'DHL SmartMail Parcel Expedited Max 5122444' # (version switch 15) 'DHLEM_5122444' exists in 2024 but not in 2023\n",
    "    \n",
    "}\n",
    "\n",
    "\n",
    "# Function to update the rest of the Services\n",
    "def update_service_details(row):\n",
    "    # If the Service is empty or None\n",
    "    if pd.isnull(row['Service']) or row['Service'] == \"\":\n",
    "        return tab_to_description.get(row['ServiceCode'], row['Service'])\n",
    "    else:\n",
    "        # Return existing value\n",
    "        return row['Service']\n",
    "\n",
    "FreightCharge['Service'] = FreightCharge.apply(update_service_details, axis=1)\n",
    "\n",
    "# Add the version column. Needs to update once a year.  (version switch 16)\n",
    "FreightCharge['Version'] ='2023'\n",
    "FreightCharge['StartDate'] ='2023/01/01'\n",
    "FreightCharge['EndDate'] ='2023/12/31'\n",
    "\n",
    "# Define the replacement mapping\n",
    "replacement_mapping = {\n",
    "    '102': '2', '103': '3', '104': '4', '105': '5', '106': '6', '107': '7', '108': '8', \n",
    "    '202': '2', '203': '3', '204': '4', '205': '5', '206': '6', '207': '7', '208': '8', \n",
    "    '302': '2', '303': '3', '304': '4', '305': '5', '306': '6', '307': '7', '308': '8',\n",
    "    '44': '9', '45': '10', '46': '11', '124': '9', '125': '10', '126': '11', '224': '9', '225': '10', '226': '11',\n",
    "    '03': '3', '04': '4', '05': '5', '06': '6', '07': '7', '08': '8', '09': '9'\n",
    "}\n",
    "\n",
    "def get_portion(value, replacement_mapping):\n",
    "    # Convert value to string to ensure compatibility with re.match\n",
    "    value_str = str(value)\n",
    "    \n",
    "    if re.match(r'\\d+/\\d+', value_str):\n",
    "        return value_str  # Return the value as-is\n",
    "\n",
    "    if re.match(r'[49]\\d\\d$', value_str):\n",
    "        return value_str  # Keep it as it is\n",
    "    \n",
    "    # Original processing logic\n",
    "    if 'Zone' in value_str:\n",
    "        modified_value = value_str[5:]\n",
    "    elif 'US' in value_str:\n",
    "        modified_value = value_str[3:] \n",
    "        # Check if the value is in the format \"xxx/xxx\"\n",
    "    else:\n",
    "        modified_value = value_str\n",
    "        \n",
    "    # Apply replacements from the mapping\n",
    "    for key, replacement in replacement_mapping.items():\n",
    "        modified_value = modified_value.replace(key, replacement)\n",
    "\n",
    "    return modified_value\n",
    "\n",
    "\n",
    "# Apply the modified get_portion function to each value in the Zone column\n",
    "FreightCharge['ZONE_new'] = FreightCharge['Zone'].apply(lambda x: get_portion(x, replacement_mapping))\n",
    "\n",
    "\n",
    "FreightCharge['Zone'] = FreightCharge['ZONE_new']\n",
    "FreightCharge = FreightCharge.drop('ZONE_new', axis=1)\n",
    "\n",
    "\n",
    "#adjust the index and finish re-formatting this table.\n",
    "FreightCharge.reset_index(drop=True, inplace=True)\n",
    "FreightCharge = FreightCharge[['ServiceCode', 'Service', 'Zone', 'WeightOz', 'WeightLb', 'PublishedRate', 'BarrettRate', 'Version', 'StartDate', 'EndDate']]\n",
    "#FreightCharge['Zone'] = FreightCharge['Zone'].astype('int64')\n",
    "FreightCharge['Version'] = FreightCharge['Version'].astype('int64')\n",
    "\n",
    "#check the table type and content\n",
    "#FreightCharge.info() \n",
    "#FreightCharge.head()\n",
    "\n",
    "\n",
    "# Keep rows where 'ServiceCode' is not in ['CPP']. CPP rate is less likely to be used than AUCT rate. \n",
    "FreightCharge = FreightCharge[~FreightCharge['ServiceCode'].isin(['USPSFC_CPP', 'USPSPS_CPP', 'USPSAP_CPP'])]\n",
    "         \n",
    "# Replacing values.\n",
    "replacement_dict = {\n",
    "    'USPSFC_AUCT': 'USPSFC',\n",
    "    'USPSPS_AUCT': 'USPSPS',\n",
    "    'USPSAP_AUCT': 'USPSAP'\n",
    "}\n",
    "\n",
    "# Replace the values in 'ServiceCode' column\n",
    "FreightCharge['ServiceCode'] = FreightCharge['ServiceCode'].replace(replacement_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "93bece89",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Run 2023 rates and save to FreightCharge_2023\n",
    "#FreightCharge_2023 = FreightCharge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "7f36b8e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Run 2024 rates and save to FreightCharge_2024\n",
    "#FreightCharge_2024 = FreightCharge\n",
    "#Combine 2023 and 2024. \n",
    "#FreightChargeMaster = pd.concat([FreightCharge_2023, FreightCharge_2024])\n",
    "#Select the version to use in this model\n",
    "#FreightCharge = FreightChargeMaster[FreightChargeMaster['Version'] == '2023'] # (version switch 17)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a555853d",
   "metadata": {},
   "source": [
    "**Section 2. Read in and Re-format all other 4 input tables**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "ece3b3b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ********** Read Doc. 2 **********\n",
    "ClientCharge_2023 = pd.read_excel('C:\\\\Users\\\\sliu\\\\BD\\\\re-rate_model\\\\ClientCharge_2023.xlsx')\n",
    "ClientCharge_2024 = pd.read_excel('C:\\\\Users\\\\sliu\\\\BD\\\\re-rate_model\\\\ClientCharge_2024.xlsx')\n",
    "ClientChargeMaster = pd.concat([ClientCharge_2023, ClientCharge_2024])\n",
    "#Select the version to use in this model\n",
    "ClientCharge = ClientChargeMaster[ClientChargeMaster['StartDate'] == '2023-01-01'] # (version switch 18)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "44b525f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ********** Read Doc. 3 **********\n",
    "xls = pd.ExcelFile('FuelSurcharge_weekly.xlsx')\n",
    "\n",
    "# Re-format the original fsc input doc by adding the range of period based on Ship Date\n",
    "def prepare_FSC_data(df, type=\"UPS\"):\n",
    "    # Convert Ship Date to datetime once\n",
    "    df['Ship Date'] = pd.to_datetime(df['Ship Date'])\n",
    "    \n",
    "    df['ShipDateStart'] = df['Ship Date']\n",
    "    \n",
    "    if type == \"UPS\":\n",
    "        df['ShipDateEnd'] = df['ShipDateStart'] + pd.Timedelta(days=6)\n",
    "    elif type == \"DHL\":\n",
    "        df['ShipDateEnd'] = df['ShipDateStart'].shift(-1) - timedelta(days=1)\n",
    "        df.loc[df['ShipDateStart'] == max(df['ShipDateStart']), ['ShipDateEnd']] = date.today()\n",
    "\n",
    "    # Drop Ship Date column\n",
    "    df.drop('Ship Date', axis=1, inplace=True)\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "UPS_FSC = prepare_FSC_data(pd.read_excel(xls, 'UPS_FSC'), type=\"UPS\")\n",
    "DHL_FSC = prepare_FSC_data(pd.read_excel(xls, 'DHL_FSC').sort_values(by='Ship Date', ascending=True), type=\"DHL\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "54a83a7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PLD already has a Zone column, but we still need to fill out the empty Zone values in PLD. \n",
    "# ********** Read Doc. 4 **********\n",
    "xls = pd.ExcelFile('ZipToZone.xlsx')\n",
    "\n",
    "# List of sheet names. Using a dictionary comprehension to load sheets into a dictionary of dataframes and then concatenate togeth\n",
    "sheets = [\n",
    "    '02038_Outbound', '08873_Outbound', '21226_Outbound', '38141_Outbound', '38654_Outbound', \n",
    "    '75041_Outbound', '90640_Outbound', '02324_Outbound', '06096_Outbound', '18105_Outbound', \n",
    "    '21240_Outbound', '43004_Outbound'\n",
    "]\n",
    "\n",
    "dfs = {sheet: pd.read_excel(xls, sheet) for sheet in sheets}\n",
    "\n",
    "ZipToZone = pd.concat([dfs['02038_Outbound'], dfs['08873_Outbound'], dfs['21226_Outbound'], dfs['38141_Outbound'], dfs['38654_Outbound'], \n",
    "                       dfs['75041_Outbound'], dfs['90640_Outbound'], dfs['02324_Outbound'], dfs['06096_Outbound'], dfs['18105_Outbound'], \n",
    "                       dfs['21240_Outbound'], dfs['43004_Outbound']])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "8ec790d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ********** Read Doc. 5 ********** # get 2023 or 2024 as needed. \n",
    "ZipToDas_2023 = pd.read_excel('C:\\\\Users\\\\sliu\\\\BD\\\\re-rate_model\\\\ZipToDas_2023.xlsx')\n",
    "ZipToDas_2024 = pd.read_excel('C:\\\\Users\\\\sliu\\\\BD\\\\re-rate_model\\\\ZipToDas_2024.xlsx')\n",
    "ZipToDas_2023['Version'] = '2023'\n",
    "ZipToDas_2024['Version'] = '2024'\n",
    "ZipToDasMaster = pd.concat([ZipToDas_2023, ZipToDas_2024])\n",
    "#Select the version to use in this model\n",
    "ZipToDas = ZipToDasMaster[ZipToDasMaster['Version'] == '2023'] # (version switch 19final)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "induced-monroe",
   "metadata": {},
   "source": [
    "**Section 3. Re-format and Rate PLD**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e55fcd6",
   "metadata": {},
   "source": [
    "***a. Load PLD from snowflake query***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "lasting-primary",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load Doc. 6\n",
    "PLDfromSnowFlake = pd.read_excel('C:\\\\Users\\\\sliu\\\\BD\\\\re-rate_model\\\\Coop2024_with2023.xlsx')\n",
    "#tbl_BP.head()\n",
    "#tbl_BP.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "c3c60bdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Make a copy especially when the original file is too large\n",
    "tbl_BP = PLDfromSnowFlake.copy()\n",
    "#Clean the empty rows/columns just in case\n",
    "tbl_BP = tbl_BP.dropna(axis = 0, how = 'all')\n",
    "tbl_BP = tbl_BP.dropna(axis = 1, how = 'all')\n",
    "# Dropping duplicated rows\n",
    "tbl_BP = tbl_BP.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "68a7a037",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['CustomerID', 'CustomerName', 'Facility', 'BarrettOrderNumber',\n",
       "       'Reference', 'PoNumber', 'TrackingNumber', 'ShipDate', 'Service',\n",
       "       'Shipper', 'ShipToName', 'ShipToContact', 'ShipToCity', 'ShipToState',\n",
       "       'ZipCode', 'ShipToCountry', 'Zone', 'Quantity', 'Dimensions',\n",
       "       'ActualWeight', 'ResidentialFlag'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tbl_BP.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "e684aa3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 44621 entries, 0 to 44620\n",
      "Data columns (total 21 columns):\n",
      " #   Column              Non-Null Count  Dtype         \n",
      "---  ------              --------------  -----         \n",
      " 0   CustomerID          44621 non-null  int64         \n",
      " 1   CustomerName        44621 non-null  object        \n",
      " 2   Facility            44621 non-null  object        \n",
      " 3   BarrettOrderNumber  44621 non-null  object        \n",
      " 4   Reference           44621 non-null  object        \n",
      " 5   PoNumber            44613 non-null  object        \n",
      " 6   TrackingNumber      44621 non-null  object        \n",
      " 7   ShipDate            44621 non-null  datetime64[ns]\n",
      " 8   Service             44621 non-null  object        \n",
      " 9   Shipper             44621 non-null  object        \n",
      " 10  ShipToName          44621 non-null  object        \n",
      " 11  ShipToContact       1043 non-null   object        \n",
      " 12  ShipToCity          44621 non-null  object        \n",
      " 13  ShipToState         44621 non-null  object        \n",
      " 14  ZipCode             44621 non-null  int64         \n",
      " 15  ShipToCountry       44621 non-null  object        \n",
      " 16  Zone                44621 non-null  int64         \n",
      " 17  Quantity            44419 non-null  float64       \n",
      " 18  Dimensions          44621 non-null  object        \n",
      " 19  ActualWeight        44621 non-null  float64       \n",
      " 20  ResidentialFlag     44621 non-null  bool          \n",
      "dtypes: bool(1), datetime64[ns](1), float64(2), int64(3), object(14)\n",
      "memory usage: 7.2+ MB\n"
     ]
    }
   ],
   "source": [
    "#Required column names and type:     \n",
    "# ---  ------             --------------  -----         \n",
    "#1   TrackingNumber        ---------------------      int64 (or object)      \n",
    "#2   ShipDate              ---------------------      datetime64[ns]\n",
    "#3   Service               ---------------------      object        \n",
    "#4   ZipCode               ---------------------       object\n",
    "#5   Zone                  ---------------------      object          \n",
    "#6   Dimensions            ---------------------      object     \n",
    "#7   ActualWeight (lb as default)---------------      float64       \n",
    "#8   ResidentialFlag       ---------------------      bool   \n",
    "tbl_BP.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "140ebc6d",
   "metadata": {},
   "source": [
    "***b. Patch PLD***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "7c0a8cf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Patch Tracking Number\n",
    "# Generating a sequence of integers to replace NaN values\n",
    "sequence = iter(range(1, len(tbl_BP) + 1))\n",
    "\n",
    "# Using list comprehension to replace NaN values directly\n",
    "tbl_BP['TrackingNumber'] = [next(sequence) if pd.isna(x) else x for x in tbl_BP['TrackingNumber']]\n",
    "\n",
    "\n",
    "#Patch Ship Date\n",
    "# Calculating yesterday's date\n",
    "yesterday_date = datetime.now() - timedelta(days=1)\n",
    "\n",
    "# Filling null values with yesterday's date\n",
    "tbl_BP['ShipDate'] = tbl_BP['ShipDate'].fillna(yesterday_date)\n",
    "\n",
    "\n",
    "# Patch Zone\n",
    "# Add 'FromZip' column basd on the value of Facility in the same df.\n",
    "tbl_BP['FromZip'] = tbl_BP.apply(\n",
    "    lambda row: \n",
    "    '02038' if row['Facility'] in ('FRA', 'MA5', 'MA6', 'BRI', 'MAN', 'MA7', 'NBO') else\n",
    "    '08873' if (row['Facility'] in ('JER', 'NJ2', 'NJ3', 'NY1')) else\n",
    "    '21226' if (row['Facility'] in ('GBM', 'GTZ', 'IDR')) else\n",
    "    '38141' if (row['Facility'] in ('TN1', 'TN2', 'TN3', 'MEM')) else\n",
    "    '38654' if (row['Facility'] in ('MS3', 'MS2', 'MS1', 'MST', 'MS4', 'MS5', 'MS6')) else\n",
    "    '75041' if (row['Facility'] in ('TX1')) else\n",
    "    '90640' if (row['Facility'] in ('MBC', 'CA3')) else\n",
    "    '', \n",
    "    axis=1\n",
    ")\n",
    "\n",
    "\n",
    "# Fill \"Zone\" values if this column in the original PLD is insufficient\n",
    "# Unify the columns types for merge and fill Zip Code to 5 digits. e.g. 01234 rather than 1234 \n",
    "columns_to_fill = [(tbl_BP, 'ZipCode'), (tbl_BP, 'FromZip'), (ZipToZone, 'Origin'), (ZipToZone, 'Dest Zip')]\n",
    "def zfill_column(df, col_name):\n",
    "    df[col_name] = df[col_name].astype(str).str.zfill(5)\n",
    "\n",
    "for df, col in columns_to_fill:\n",
    "    zfill_column(df, col)\n",
    "    \n",
    "tbl_BP = tbl_BP.merge(ZipToZone[[\"Origin\", \"Dest Zip\", \"GNDZone\"]], left_on=['FromZip', 'ZipCode'], right_on=['Origin','Dest Zip'], how='left').rename(columns={'GNDZone': 'ZoneExtra'}).drop(['Origin','Dest Zip'], axis=1)\n",
    "tbl_BP['ZoneExtra'] = tbl_BP['ZoneExtra'].astype('str')\n",
    "tbl_BP['ZoneExtra'] = (tbl_BP['ZoneExtra'].str.replace('44', '9') #the ZipToZone has 44, 45, and 46 as Zones. \n",
    "                              .str.replace('45', '10')\n",
    "                              .str.replace('46', '11'))\n",
    "\n",
    "\n",
    "# Replace 'Zone' values that are 0 or null with 'Zone_extra' values, where 'Zone_extra' is not 0 or null\n",
    "tbl_BP.loc[(tbl_BP['Zone'] == 0) | tbl_BP['Zone'].isnull(), 'Zone'] = tbl_BP['ZoneExtra']\n",
    "tbl_BP = tbl_BP.drop('ZoneExtra', axis=1)\n",
    "\n",
    "#Reformat so that they can be used to merge later. \n",
    "tbl_BP['Zone'] = tbl_BP['Zone'].astype(str)\n",
    "# Function to convert values\n",
    "def convert_to_int(val):\n",
    "    try:\n",
    "        # Convert to float first to handle values like '6.0'\n",
    "        return str(int(float(val)))\n",
    "    except ValueError:\n",
    "        # Return the original value if conversion is not possible\n",
    "        return val\n",
    "\n",
    "# Apply the function to the 'Zone' column\n",
    "tbl_BP['Zone'] = tbl_BP['Zone'].apply(convert_to_int)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d32ffe7",
   "metadata": {},
   "source": [
    "***c. Flag PLD (Flag instead of delete data)***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "ed3e08ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize 'RateFlag' column with a default value (e.g., 0)\n",
    "tbl_BP['RateFlag'] = 1\n",
    "\n",
    "# Setting 'RateFlag' to 0 for rows that meet specific conditions\n",
    "tbl_BP.loc[tbl_BP.ShipDate.isnull() |\n",
    "           tbl_BP.ZipCode.isnull() | \n",
    "           (tbl_BP.ZipCode == '0') |\n",
    "           tbl_BP.Zone.isnull() | \n",
    "           (tbl_BP.Zone == '0') |\n",
    "           tbl_BP.Dimensions.isnull() | \n",
    "           (tbl_BP.Dimensions == 0) |\n",
    "           tbl_BP.ActualWeight.isnull() | \n",
    "           (tbl_BP.ActualWeight == 0) |\n",
    "           (tbl_BP.ActualWeight > 150), 'RateFlag'] = 0\n",
    "\n",
    "\n",
    "# Sorting by 'TrackingNumber' and then by 'ShipDate' in descending order\n",
    "tbl_BP = tbl_BP.sort_values(by=['TrackingNumber', 'ShipDate'], ascending=[True, False])\n",
    "\n",
    "# Updating 'RateFlag' for duplicates, but only where 'RateFlag' is not already 0, # For rows where 'RateFlag' is already 0, it remains unchanged\n",
    "# In cases where duplicates are found, the first occurrence is considered as non-duplicate, and all subsequent occurrences are considered duplicates.\n",
    "mask = tbl_BP.duplicated(subset='TrackingNumber', keep='first')     \n",
    "tbl_BP.loc[mask, 'RateFlag'] = 0\n",
    "\n",
    "\n",
    "# Splitting the 'Dimensions' column into three separate parts\n",
    "dimensions_split = tbl_BP['Dimensions'].str.split('x', expand=True)\n",
    "# Converting the split parts to numeric, errors='coerce' will handle non-numeric values\n",
    "dimensions_numeric = dimensions_split.apply(pd.to_numeric, errors='coerce')\n",
    "# Checking if any of the three parts are 0 or non-numeric\n",
    "invalid_dimensions = dimensions_numeric.isna().any(axis=1) | (dimensions_numeric == 0).any(axis=1)\n",
    "# Updating 'RateFlag' to 0 for rows with a dimension of 0\n",
    "tbl_BP.loc[invalid_dimensions, 'RateFlag'] = 0\n",
    "# Updating 'RateFlag' to 1 for other rows, without affecting rows already marked as 0\n",
    "tbl_BP.loc[~invalid_dimensions & (tbl_BP['RateFlag'] != 0), 'RateFlag'] = 1\n",
    "\n",
    "\n",
    "# List of valid values for 'Service'\n",
    "valid_services = [\n",
    "    \"UPS Next Day Air Early\", \n",
    "    \"UPS Next Day Air Saver\", \n",
    "    \"UPS Next Day Air\", \n",
    "    \"UPS 2nd Day Air A.M.\", \n",
    "    \"UPS 2nd Day Air\",\n",
    "    \"UPS 3 Day Select\", \n",
    "    \"UPS Ground\", \n",
    "    \"UPS SurePost 1 lb or Greater\", \n",
    "    \"UPS SurePost Less than 1 lb\", \n",
    "    \"UPS Standard\", \n",
    "    \"UPS Worldwide Express Saver\", \n",
    "    \"UPS Worldwide Express\", \n",
    "    \"UPS Worldwide Expedited\", \n",
    "    \"DHL SmartMail Parcel Expedited Max\",\n",
    "    \"DHL SmartMail Parcel Plus Expedited\", \n",
    "    \"DHL SmartMail Parcel Expedited\", \n",
    "    \"DHL SmartMail Parcel Plus Ground\", \n",
    "    \"DHL SmartMail Parcel Ground\", \n",
    "    \"USPS 1st Class (Endicia)\", \n",
    "    \"USPS First Class Mail\", \n",
    "    \"USPS Parcel Select (Endicia)\",\n",
    "    \"USPS Express (Endicia)\", \n",
    "    \"USPS Priority (Endicia)\", \n",
    "    \"USPS Priority Mail\"\n",
    "   # \"UPS Mail Innovations First Class\",\n",
    "   # \"UPS Mail Innovations Expedited\"\n",
    "]  # Add your valid services here\n",
    "\n",
    "# Update 'RateFlag' to 0 for rows where 'Service' is not in the list of valid services\n",
    "tbl_BP.loc[~tbl_BP['Service'].isin(valid_services), 'RateFlag'] = 0\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "826613d7",
   "metadata": {},
   "source": [
    "***d. Add columns to PLD*** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "ca10c8d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sliu\\AppData\\Local\\Temp\\ipykernel_21416\\3559374997.py:70: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  ZipToDas['Zip'] = ZipToDas['Zip'].astype(str).apply(lambda x: '0' + x if len(x) == 4 else ('00' + x if len(x) == 3 else x))\n"
     ]
    }
   ],
   "source": [
    "#turn dimensions into L W H columns.\n",
    "tbl_BP[['L', 'W', 'H']] = tbl_BP['Dimensions'].str.split('x', expand=True)\n",
    "\n",
    "#Turn float to integers using the excel way of rounding up. \n",
    "tbl_BP[['L', 'W', 'H']] = tbl_BP[['L', 'W', 'H']].apply(pd.to_numeric, errors='coerce')\n",
    "tbl_BP['L'] = tbl_BP['L'].apply(lambda x: round(x,0) if pd.notna(x) and x - math.floor(x) < 0.5 else np.ceil(x) if pd.notna(x) else np.nan)\n",
    "tbl_BP['W'] = tbl_BP['W'].apply(lambda x: round(x,0) if pd.notna(x) and x - math.floor(x) < 0.5 else np.ceil(x) if pd.notna(x) else np.nan)\n",
    "tbl_BP['H'] = tbl_BP['H'].apply(lambda x: round(x,0) if pd.notna(x) and x - math.floor(x) < 0.5 else np.ceil(x) if pd.notna(x) else np.nan)\n",
    "\n",
    "\n",
    "# Create a temporary DataFrame with the columns 'L', 'W', and 'H'\n",
    "temp_df = tbl_BP[['L', 'W', 'H']]\n",
    "\n",
    "# Sort each row from large to small, L, W, H\n",
    "sorted_temp_df = np.sort(temp_df.values, axis=1)[:, ::-1]\n",
    "\n",
    "# Assign the sorted values back to the original DataFrame\n",
    "tbl_BP[['L', 'W', 'H']] = sorted_temp_df\n",
    "\n",
    "# Create columns of GirthAndL and CubicInch\n",
    "tbl_BP['GirthAndL'] = round(tbl_BP['W'] * 2 + tbl_BP['H'] * 2 + tbl_BP['L'],0)\n",
    "tbl_BP['CubicInch'] = round(tbl_BP['W'] * tbl_BP['H'] * tbl_BP['L'],0)\n",
    "\n",
    "\n",
    "\n",
    "# Add ServiceCode column to PLD\n",
    "# Do not change the order of this code without knowing the logic here. \n",
    "shipping_map = {\n",
    "    \"UPS Next Day Air Early\": \"REDE\",\n",
    "    \"UPS Next Day Air Saver\": \"REDS\",\n",
    "    \"UPS Next Day Air\": \"RED\",\n",
    "    \"UPS 2nd Day Air A.M.\": \"2DAM\",\n",
    "    \"UPS 2nd Day Air\": \"BLUE\",\n",
    "    \"UPS 3 Day Select\": \"ORNG\",\n",
    "    \"UPS Ground\": \"GRND\",\n",
    "    \"UPS SurePost 1 lb or Greater\": \"SRPT\",\n",
    "    \"UPS SurePost Less than 1 lb\": \"SRPT<1\",\n",
    "    \"UPS Standard\": \"CNST\",\n",
    "    \"UPS Worldwide Express Saver\": \"WWXS\",\n",
    "    \"UPS Worldwide Express Plus\": \"OTHER\",\n",
    "    \"UPS Worldwide Express\": \"WWX\",\n",
    "    \"UPS Worldwide Expedited\": \"WWE\",\n",
    "    \"DHL SmartMail Parcel Expedited Max\": \"DHLEM\",\n",
    "    \"DHL SmartMail Parcel Plus Expedited\": \"DHLE\",\n",
    "    \"DHL SmartMail Parcel Expedited\": \"DHLE<1\",\n",
    "    \"DHL SmartMail Parcel Plus Ground\": \"DHLG\",\n",
    "    \"DHL SmartMail Parcel Ground\": \"DHLG<1\",\n",
    "    \"USPS 1st Class (Endicia)\": \"USPSFC\",\n",
    "    \"USPS First Class Mail\": \"USPSFC\",\n",
    "    \"USPS Parcel Select (Endicia)\": \"USPSPS\",\n",
    "    \"USPS Express (Endicia)\": \"USPSAP\",\n",
    "    \"USPS Priority (Endicia)\": \"USPSAP\",\n",
    "    \"USPS Priority Mail\": \"USPSAP\",\n",
    "    \"UPS Mail Innovations First Class\": \"MIPH\",\n",
    "    \"UPS Mail Innovations Expedited\": \"MIPLE\",\n",
    "}\n",
    "\n",
    "        \n",
    "def label_shipping(row):\n",
    "    for key, value in shipping_map.items():\n",
    "        if key in row:\n",
    "            return value\n",
    "    return \"OTHER\"\n",
    "\n",
    "tbl_BP['ServiceCode'] = tbl_BP['Service'].apply(label_shipping)\n",
    "tbl_BP['ServiceCode'] = tbl_BP.apply(lambda row: 'GRES' if (row['ResidentialFlag'] == True and row['ServiceCode'] == 'GRND') else row['ServiceCode'], axis=1)\n",
    "\n",
    "\n",
    "# Add Das Code\n",
    "ZipToDas['Zip'] = ZipToDas['Zip'].astype(str).apply(lambda x: '0' + x if len(x) == 4 else ('00' + x if len(x) == 3 else x))\n",
    "tbl_BP = tbl_BP.merge(ZipToDas[[\"Zip\", \"Type\"]], left_on='ZipCode', right_on='Zip', how='left').rename(columns={'Type': 'DASCategory'})\n",
    "tbl_BP.drop(columns=['Zip'], inplace=True)\n",
    "tbl_BP.loc[~tbl_BP['ServiceCode'].isin(['REDE', 'RED', 'REDS', '2DAM', 'BLUE', 'ORNG', 'GRND', 'GRES', 'SRPT', 'SRPT<1']), 'DASCategory'] = None\n",
    "\n",
    "\n",
    "# Add FSC Rate\n",
    "# Specify the ServiceCodes for the three scenarios\n",
    "upsAir_calculation_1 = [\"REDE\", \"RED\", \"REDS\", \"2DAM\", \"BLUE\", \"ORNG\"]  \n",
    "upsGround_calculation_2 = [\"GRND\", \"GRES\", \"SRPT\", \"SRPT<1\"]\n",
    "upsIntl_calculation_3 = [\"WWXS\", \"WWX\", \"WWE\"]\n",
    "upsCan_calculation_4 = [\"CNST\"]\n",
    "dhl_calculation_5 = [\"DHLG\", \"DHLG<1\", \"DHLE\", \"DHLE<1\", \"DHLEM\"]\n",
    "\n",
    "\n",
    "# ups \n",
    "tbl_BP['ShipDate'] = pd.to_datetime(tbl_BP['ShipDate'])\n",
    "# get unique values from column 'SHIP_DATE' and create a new dataframe. \n",
    "new_df = pd.DataFrame({'ShipDate': tbl_BP['ShipDate'].unique()})\n",
    "# use this new dataframe to merge to Doc. 3 UPS section\n",
    "df_merge = new_df.merge(UPS_FSC, how='cross')\n",
    "#slice the merged dataframe to make sure ship_date is between the ship_data_start and end. Now we have Doc. 3 with Ship_date column to merge back to pld so that the pld can get FSC%.\n",
    "df_merge = df_merge.query('ShipDate >= ShipDateStart and ShipDate <= ShipDateEnd')\n",
    "# Get dhl FSC% to pld\n",
    "tbl_BP = tbl_BP.merge(df_merge, on=['ShipDate'], how='left').drop(['ShipDateStart', 'ShipDateEnd'], axis=1)\n",
    "\n",
    "\n",
    "# dhl \n",
    "df_merge = new_df.merge(DHL_FSC, how='cross')\n",
    "df_merge = df_merge.query('ShipDate >= ShipDateStart and ShipDate <= ShipDateEnd')\n",
    "tbl_BP = tbl_BP.merge(df_merge, on=['ShipDate'], how='left').drop(['ShipDateStart', 'ShipDateEnd'], axis=1)\n",
    "\n",
    "\n",
    "# pick rates from the 3 FSC% columns based on ServiceCode\n",
    "tbl_BP['FscRate'] = np.where(tbl_BP['ServiceCode'].isin(upsAir_calculation_1), tbl_BP['Domestic Air'], \n",
    "                         np.where(tbl_BP['ServiceCode'].isin(upsGround_calculation_2), tbl_BP['Ground'], \n",
    "                                  np.where(tbl_BP['ServiceCode'].isin(upsIntl_calculation_3), tbl_BP['International Air Export'], \n",
    "                                           np.where(tbl_BP['ServiceCode'].isin(upsCan_calculation_4), tbl_BP['International Ground Export Import'],\n",
    "                                                    np.where(tbl_BP['ServiceCode'].isin(dhl_calculation_5), tbl_BP['FSC'],\n",
    "                                           np.nan)))))\n",
    "\n",
    "# Drop the original rate columns\n",
    "tbl_BP = tbl_BP.drop(['Domestic Air', 'Ground', 'International Air Export', 'International Ground Export Import', 'FSC'], axis=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "25d0bd5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add dim weight and actual weight rounded up column to simply the process\n",
    "def cust_dimmed_weight(row):    \n",
    "    if row['ServiceCode'] in ['REDE','RED','REDS','2DAM','BLUE','ORNG','GRND','GRES']:\n",
    "        return np.ceil(row['CubicInch'] / dim_factor_ups)\n",
    "    elif row['ServiceCode'] in ['SRPT', 'SRPT<1']:\n",
    "        return np.ceil(row['CubicInch'] / dim_factor_ups_srpt)\n",
    "    elif row['ServiceCode'] in ['CNST','WWX','WWXS','WWE']:\n",
    "        return np.ceil(row['CubicInch'] / dim_factor_ups_intl)\n",
    "    elif row['ServiceCode'] in ('DHLG', 'DHLE', 'DHLG<1', 'DHLE<1','DHLEM'):\n",
    "        return np.ceil(row['CubicInch'] / dim_factor_dhl)\n",
    "    elif row['ServiceCode'] in ('USPSFC', 'USPSPS', 'USPSAP'):\n",
    "        return np.ceil(row['CubicInch'] / dim_factor_usps)\n",
    "    elif row['ServiceCode'] in ('MIPH', 'MIPLE'):\n",
    "        return np.ceil(row['CubicInch'] / dim_factor_mi)\n",
    "    else:\n",
    "        return np.nan  # Return NaN or an appropriate default value for other cases\n",
    "\n",
    "# Apply the function to each row to create the new column\n",
    "tbl_BP['CustomerDimWeight'] = tbl_BP.apply(cust_dimmed_weight, axis=1)\n",
    "\n",
    "# Create Round up Columns\n",
    "tbl_BP['CustomerActualWeightLb'] = np.ceil(tbl_BP['ActualWeight'])\n",
    "tbl_BP['CustomerActualWeightOz'] = np.ceil(tbl_BP['ActualWeight']*16)\n",
    "\n",
    "\n",
    "# Add PublishedBilledWeightLb and PublishedBilledWeightOz columns for UPS\n",
    "def compute_weights(row):\n",
    "    if row['ServiceCode'] in ['REDE','RED','REDS','2DAM','BLUE','ORNG','GRND','GRES','CNST','WWX','WWXS','WWE']:\n",
    "        return max(row['CustomerActualWeightLb'], np.ceil(row['CubicInch']/dim_factor_ups_pub)), None\n",
    "    elif row['ServiceCode'] == 'SRPT' and row['CubicInch'] >= ups_allowance:\n",
    "        return max(row['CustomerActualWeightLb'], np.ceil(row['CubicInch']/dim_factor_ups_pub)), None\n",
    "    elif row['ServiceCode'] == 'SRPT' and row['CubicInch'] < ups_allowance:\n",
    "        return row['CustomerActualWeightLb'], None\n",
    "    elif row['ServiceCode'] == 'SRPT<1' and row['CubicInch'] >= ups_allowance:\n",
    "        return None, max(row['CustomerActualWeightLb'], np.ceil(row['CubicInch']/dim_factor_ups_pub))*16\n",
    "    elif row['ServiceCode'] == 'SRPT<1' and row['CubicInch'] < ups_allowance:\n",
    "        return None, row['CustomerActualWeightOz']\n",
    "    else:\n",
    "        return None, None\n",
    "\n",
    "tbl_BP['PublishedBilledWeightLb'], tbl_BP['PublishedBilledWeightOz'] = zip(*tbl_BP.apply(compute_weights, axis=1))\n",
    "tbl_BP.loc[tbl_BP['PublishedBilledWeightOz'] > 16, 'PublishedBilledWeightOz'] = np.nan\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "94e1dc78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add customer billed weight. weight lb\n",
    "def cust_weight_lb(row):    \n",
    "    if row['ServiceCode'] in ['REDE','RED','REDS','2DAM','BLUE','ORNG','GRND','GRES','CNST','WWX','WWXS','WWE']:\n",
    "        return max(row['CustomerActualWeightLb'], row['CustomerDimWeight'])\n",
    "    elif row['ServiceCode'] == 'SRPT' and row['CubicInch'] >= ups_allowance:\n",
    "        return max(row['CustomerActualWeightLb'], row['CustomerDimWeight'])\n",
    "    elif row['ServiceCode'] == 'SRPT' and row['CubicInch'] < ups_allowance:\n",
    "        return row['CustomerActualWeightLb']\n",
    "    elif row['ServiceCode'] in ['DHLG', 'DHLE'] and (row['CubicInch'] >= dhl_allowance and row['ActualWeight'] >= 1):\n",
    "        return max(row['CustomerActualWeightLb'], row['CustomerDimWeight'])\n",
    "    elif row['ServiceCode'] in ['DHLG', 'DHLE'] and (row['CubicInch'] < dhl_allowance or row['ActualWeight'] < 1):\n",
    "        return row['CustomerActualWeightLb']\n",
    "    elif row['ServiceCode'] == 'USPSPS' and row['CubicInch'] > usps_allowance:\n",
    "        return max(row['CustomerActualWeightLb'], row['CustomerDimWeight'])\n",
    "    elif row['ServiceCode'] == 'USPSPS' and row['CubicInch'] <= usps_allowance:\n",
    "        return row['CustomerActualWeightLb']\n",
    "    elif row['ServiceCode'] == 'MIPH' and (row['CubicInch'] > mi_allowance and row['ActualWeight'] >= 1):\n",
    "        return max(row['CustomerActualWeightLb'], row['CustomerDimWeight'])\n",
    "    elif row['ServiceCode'] == 'MIPH' and (row['CubicInch'] <= mi_allowance or row['ActualWeight'] < 1):\n",
    "        return row['CustomerActualWeightLb']\n",
    "    elif row['ServiceCode'] == 'DHLEM' and (row['CubicInch'] >= dhl_allowance and row['ActualWeight'] >= 1):\n",
    "        return max(row['CustomerActualWeightLb'], row['CustomerDimWeight'])\n",
    "    elif row['ServiceCode'] == 'DHLEM' and (row['CubicInch'] < dhl_allowance or row['ActualWeight'] < 1) and row['CustomerActualWeightOz'] >= 16:\n",
    "        return row['CustomerActualWeightLb']\n",
    "    elif row['ServiceCode'] == 'USPSAP' and (row['CubicInch'] > usps_allowance):\n",
    "        return max(row['CustomerActualWeightLb'], row['CustomerDimWeight'])\n",
    "    elif row['ServiceCode'] == 'USPSAP' and (row['CubicInch'] <= usps_allowance) and row['CustomerActualWeightOz'] > 8:\n",
    "        return row['CustomerActualWeightLb']\n",
    "    elif row['ServiceCode'] == 'USPSAP' and (row['CubicInch'] <= usps_allowance) and row['CustomerActualWeightOz'] <= 8:\n",
    "        return 0.5\n",
    "    \n",
    "# weight oz\n",
    "def cust_weight_oz(row):    \n",
    "    if row['ServiceCode'] == 'SRPT<1' and row['CubicInch'] >= ups_allowance:\n",
    "        return max(row['CustomerActualWeightLb'], row['CustomerDimWeight'])*16\n",
    "    if row['ServiceCode'] == 'SRPT<1' and row['CubicInch'] < ups_allowance:\n",
    "        return row['CustomerActualWeightOz']\n",
    "    elif row['ServiceCode'] in ['DHLG<1', 'DHLE<1'] and (row['CubicInch'] >= dhl_allowance and row['ActualWeight'] >= 1):\n",
    "        return max(row['CustomerActualWeightLb'], row['CustomerDimWeight'])*16\n",
    "    elif row['ServiceCode'] in ['DHLG<1', 'DHLE<1'] and (row['CubicInch'] < dhl_allowance or row['ActualWeight'] < 1):\n",
    "        return row['CustomerActualWeightOz']\n",
    "    elif row['ServiceCode'] == 'USPSFC' and row['CubicInch'] > usps_allowance:\n",
    "        return max(row['CustomerActualWeightLb'], row['CustomerDimWeight'])*16\n",
    "    elif row['ServiceCode'] == 'USPSFC' and row['CubicInch'] <= usps_allowance:\n",
    "        return row['CustomerActualWeightOz']\n",
    "    elif row['ServiceCode'] == 'MIPLE' and (row['CubicInch'] > mi_allowance and row['ActualWeight'] >= 1):\n",
    "        return max(row['CustomerActualWeightLb'], row['CustomerDimWeight'])*16\n",
    "    elif row['ServiceCode'] == 'MIPLE' and (row['CubicInch'] <= mi_allowance or row['ActualWeight'] < 1):\n",
    "        return row['CustomerActualWeightOz']\n",
    "    elif row['ServiceCode'] == 'DHLEM' and (row['CubicInch'] < dhl_allowance or row['ActualWeight'] < 1) and row['CustomerActualWeightOz'] < 16:\n",
    "        return row['CustomerActualWeightOz']     \n",
    "    \n",
    "tbl_BP['CustomerBilledWeightLb'] = tbl_BP.apply(cust_weight_lb, axis=1)\n",
    "tbl_BP['CustomerBilledWeightOz'] = tbl_BP.apply(cust_weight_oz, axis=1)\n",
    "tbl_BP = tbl_BP.drop(['CustomerDimWeight', 'CustomerActualWeightLb', 'CustomerActualWeightOz'], axis=1)\n",
    "tbl_BP.loc[tbl_BP['CustomerBilledWeightOz'] > 16, 'CustomerBilledWeightOz'] = np.nan\n",
    "#Unqualified service cases:\n",
    "#If billweight > 1, 'SRPT<1', 'DHLG<1', 'DHLE<1', and 'USPSFC_CPP' are not eligible. These packages will not be rated.   \n",
    "\n",
    "tbl_BP['PublishedBilledWeightOz'] = tbl_BP['PublishedBilledWeightOz'].astype(float)\n",
    "tbl_BP['PublishedBilledWeightLb'] = tbl_BP['PublishedBilledWeightLb'].astype(float)\n",
    "tbl_BP['CustomerBilledWeightOz'] = tbl_BP['CustomerBilledWeightOz'].astype(float)\n",
    "tbl_BP['CustomerBilledWeightLb'] = tbl_BP['CustomerBilledWeightLb'].astype(float)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be3daa12",
   "metadata": {},
   "source": [
    "***e. Add Freight Rates (Published Freight \"UPS only\" & Barrett Freight \"except UPS\") to PLD***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "9ba28e20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add Freight Rates to PLD. \n",
    "\n",
    "# Published Rates & Lb\n",
    "group_pr_lb = ['REDE', 'RED', 'REDS', '2DAM', 'BLUE', 'ORNG', 'GRND', 'GRES', 'SRPT', 'CNST', 'WWX', 'WWXS', 'WWE'] \n",
    "# Published Rates & Oz\n",
    "group_pr_oz = ['SRPT<1']  \n",
    "# Barrett Rates & Lb\n",
    "group_br_lb = ['MIPH', 'USPSPS', 'USPSAP']  \n",
    "# Barrett Rates & Oz\n",
    "group_br_oz = ['MIPLE', 'USPSFC'] \n",
    "#Barrett Rate & Lb, DHL. separate because DHL needs additional merge to Facility.\n",
    "group_br_lb_dhl = ['DHLG', 'DHLE', 'DHLEM'] \n",
    "#Barrett Rate & Oz, DHL\n",
    "group_br_oz_dhl = ['DHLG<1', 'DHLE<1', 'DHLEM']  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "149bc130",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Merge for Published Rates & Lb\n",
    "tbl_BP = tbl_BP.merge(\n",
    "    FreightCharge[FreightCharge['PublishedRate'].notna() & FreightCharge['ServiceCode'].isin(group_pr_lb)][['ServiceCode', 'WeightLb', 'Zone', 'PublishedRate']],\n",
    "    left_on=['ServiceCode', 'PublishedBilledWeightLb', 'Zone'],\n",
    "    right_on=['ServiceCode', 'WeightLb', 'Zone'],\n",
    "    how='left'\n",
    ").rename(columns={'PublishedRate': 'PublishedRate_Lb'}).drop(columns='WeightLb')\n",
    "\n",
    "\n",
    "# 2. Merge for Published Rates & Oz\n",
    "tbl_BP = tbl_BP.merge(\n",
    "    FreightCharge[FreightCharge['PublishedRate'].notna() & FreightCharge['ServiceCode'].isin(group_pr_oz)][['ServiceCode', 'WeightOz', 'Zone', 'PublishedRate']],\n",
    "    left_on=['ServiceCode', 'PublishedBilledWeightOz', 'Zone'],\n",
    "    right_on=['ServiceCode', 'WeightOz', 'Zone'],\n",
    "    how='left'\n",
    ").rename(columns={'PublishedRate': 'PublishedRate_Oz'}).drop(columns='WeightOz')\n",
    "\n",
    "\n",
    "# 3. Merge for Barrett Rates & Lb\n",
    "tbl_BP = tbl_BP.merge(\n",
    "    FreightCharge[FreightCharge['BarrettRate'].notna() & FreightCharge['ServiceCode'].isin(group_br_lb)][['ServiceCode', 'WeightLb', 'Zone', 'BarrettRate']],\n",
    "    left_on=['ServiceCode', 'CustomerBilledWeightLb', 'Zone'],\n",
    "    right_on=['ServiceCode', 'WeightLb', 'Zone'],\n",
    "    how='left'\n",
    ").rename(columns={'BarrettRate': 'BarrettRate_Lb'}).drop(columns='WeightLb')\n",
    "\n",
    "\n",
    "# 4. Merge for Barrett Rates & Oz\n",
    "tbl_BP = tbl_BP.merge(\n",
    "    FreightCharge[FreightCharge['BarrettRate'].notna() & FreightCharge['ServiceCode'].isin(group_br_oz)][['ServiceCode', 'WeightOz', 'Zone', 'BarrettRate']],\n",
    "    left_on=['ServiceCode', 'CustomerBilledWeightOz', 'Zone'],\n",
    "    right_on=['ServiceCode', 'WeightOz', 'Zone'],\n",
    "    how='left'\n",
    ").rename(columns={'BarrettRate': 'BarrettRate_Oz'}).drop(columns='WeightOz')\n",
    "\n",
    "# 5. Merge for Barrett Rate & Lb, DHL\n",
    "# Define the mask to select rows where ServiceCode starts with 'DHL'\n",
    "mask = FreightCharge['ServiceCode'].str.startswith('DHL')\n",
    "FreightCharge[['ServiceCode_DHL', 'Facility_DHL']] = FreightCharge.loc[mask, 'ServiceCode'].str.extract(r'(DHLG<1|DHLE<1|DHLG|DHLE|DHLEM)_([0-9]{7})')\n",
    "key_mapping = {\n",
    "    'FRA': '5114358', 'MA5': '5114358', 'MA6': '5114358', 'BRI': '5123282', 'MAN': '5123280', 'MA7': '5114358', 'NBO': '5114358', \n",
    "    'JER': '5122444', 'NJ2': '5122444', 'NJ3': '5122444', 'NY1': '5122444', 'GBM': '5123283', 'GTZ': '5123283', 'IDR': '5123283', \n",
    "    'TN1': '5122893', 'TN2': '5122893', 'TN3': '5122893', 'MEM': '5122893', 'MS3': '5122723', 'MS2': '5122723', 'MS1': '5122723', \n",
    "    'MST': '5122723', 'MS4': '5122723', 'MS5': '5122723', 'TX1': '5122855', 'MBC': '5123556'\n",
    "}\n",
    "\n",
    "tbl_BP['key_mapped'] = tbl_BP['Facility'].map(key_mapping)\n",
    "\n",
    "tbl_BP = tbl_BP.merge(\n",
    "    FreightCharge[FreightCharge['ServiceCode_DHL'].notna() & FreightCharge['ServiceCode_DHL'].isin(group_br_lb_dhl)][['ServiceCode_DHL', 'Facility_DHL', 'WeightLb', 'Zone', 'BarrettRate']],\n",
    "    left_on=['ServiceCode', 'key_mapped', 'CustomerBilledWeightLb', 'Zone'],\n",
    "    right_on=['ServiceCode_DHL', 'Facility_DHL', 'WeightLb', 'Zone'],\n",
    "    how='left'\n",
    ").rename(columns={'BarrettRate': 'BarrettRate_Lb_DHL'}).drop(columns=['ServiceCode_DHL', 'Facility_DHL', 'WeightLb'])\n",
    "\n",
    "\n",
    "# 6. Merge for Barrett Rate & Oz, DHL\n",
    "tbl_BP = tbl_BP.merge(\n",
    "    FreightCharge[FreightCharge['ServiceCode_DHL'].notna() & FreightCharge['ServiceCode_DHL'].isin(group_br_oz_dhl)][['ServiceCode_DHL', 'Facility_DHL', 'WeightOz', 'Zone', 'BarrettRate']],\n",
    "    left_on=['ServiceCode', 'key_mapped', 'CustomerBilledWeightOz', 'Zone'],\n",
    "    right_on=['ServiceCode_DHL', 'Facility_DHL', 'WeightOz', 'Zone'],\n",
    "    how='left'\n",
    ").rename(columns={'BarrettRate': 'BarrettRate_Oz_DHL'}).drop(columns=['ServiceCode_DHL', 'Facility_DHL', 'WeightOz', 'key_mapped'])\n",
    "\n",
    "\n",
    "# Special case. Set the 'BarrettRate_Lb' values to NaN for rows that meet this condition as we want to use oversize rate only.\n",
    "mask_1 = (tbl_BP['ServiceCode'] == 'USPSPS') & (tbl_BP['GirthAndL'] > 108) & (tbl_BP['GirthAndL'] <= 130)\n",
    "mask_2 = (tbl_BP['ServiceCode'] == 'USPSPS') & (tbl_BP['GirthAndL'] > 130)\n",
    "tbl_BP.loc[mask_1, 'BarrettRate_Lb'] = np.nan\n",
    "tbl_BP.loc[mask_2, 'BarrettRate_Lb'] = np.nan\n",
    "\n",
    "#uspsps. Use the mask to merge only those rows directly in tbl_BP\n",
    "merged_subset = tbl_BP[mask_1].merge(\n",
    "    FreightCharge[(FreightCharge['ServiceCode'] == 'USPSPS') & (FreightCharge['WeightLb'] == 9999)][['ServiceCode', 'Zone', 'BarrettRate']],\n",
    "    left_on=['ServiceCode', 'Zone'],\n",
    "    right_on=['ServiceCode', 'Zone'],\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# Then assign the 'BarrettRate' values from the merged subset back to 'BarrettRate_Lb' in the original tbl_BP where the mask is True\n",
    "tbl_BP.loc[mask_1, 'BarrettRate_Lb'] = merged_subset['BarrettRate']\n",
    "\n",
    "\n",
    "# Combine all the Rate columns into one 'Rate' column\n",
    "tbl_BP['PublishedFreight'] = tbl_BP['PublishedRate_Lb'].combine_first(tbl_BP['PublishedRate_Oz'])\n",
    "tbl_BP['BarrettFreight'] = tbl_BP['BarrettRate_Lb'].combine_first(tbl_BP['BarrettRate_Oz']).combine_first(tbl_BP['BarrettRate_Lb_DHL']).combine_first(tbl_BP['BarrettRate_Oz_DHL'])\n",
    "\n",
    "# Drop the intermediate Rate columns\n",
    "columns_to_drop = ['PublishedRate_Lb', 'PublishedRate_Oz', 'BarrettRate_Lb', 'BarrettRate_Oz', 'BarrettRate_Lb_DHL', 'BarrettRate_Oz_DHL']\n",
    "tbl_BP.drop(columns=columns_to_drop, inplace=True)\n",
    "\n",
    "\n",
    "# Filter rows where 'exclude_rates' column value is in the provided list\n",
    "condition = ~tbl_BP['ServiceCode'].isin([\"REDE\", \"RED\", \"REDS\", \"2DAM\", \"BLUE\", \"ORNG\", \"GRND\", \"GRES\", \"SRPT\", \"SRPT<1\", \"CNST\", \"WWX\", \"WWXS\", \"WWE\"])\n",
    "\n",
    "# If the condition is met, then set 'PublishedFreight' to NaN because they are actually Barrett Rate here if not UPS \n",
    "tbl_BP.loc[condition, 'PublishedFreight'] = np.nan\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dbcac7f",
   "metadata": {},
   "source": [
    "***f. Add Customer Freight to PLD (and Barrett Freight \"UPS only\" to PLD)***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "da8db3b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add UPS Pub Rates from the same rate card location to PLD only for UPS Customer Rate calculation. \n",
    "\n",
    "# temp Published Rates & Lb\n",
    "group_pr_lb = ['REDE', 'RED', 'REDS', '2DAM', 'BLUE', 'ORNG', 'GRND', 'GRES', 'SRPT', 'CNST', 'WWX', 'WWXS', 'WWE'] \n",
    "\n",
    "# temp Published Rates & Oz\n",
    "group_pr_oz = ['SRPT<1']  \n",
    "\n",
    "\n",
    "# 1. Merge for Published Rates & Lb\n",
    "tbl_BP = tbl_BP.merge(\n",
    "    FreightCharge[FreightCharge['PublishedRate'].notna() & FreightCharge['ServiceCode'].isin(group_pr_lb)][['ServiceCode', 'WeightLb', 'Zone', 'PublishedRate']],\n",
    "    left_on=['ServiceCode', 'CustomerBilledWeightLb', 'Zone'],\n",
    "    right_on=['ServiceCode', 'WeightLb', 'Zone'],\n",
    "    how='left'\n",
    ").rename(columns={'PublishedRate': 'PublishedRate_Lb'}).drop(columns='WeightLb')\n",
    "\n",
    "\n",
    "# 2. Merge for Published Rates & Oz\n",
    "tbl_BP = tbl_BP.merge(\n",
    "    FreightCharge[FreightCharge['PublishedRate'].notna() & FreightCharge['ServiceCode'].isin(group_pr_oz)][['ServiceCode', 'WeightOz', 'Zone', 'PublishedRate']],\n",
    "    left_on=['ServiceCode', 'CustomerBilledWeightOz', 'Zone'],\n",
    "    right_on=['ServiceCode', 'WeightOz', 'Zone'],\n",
    "    how='left'\n",
    ").rename(columns={'PublishedRate': 'PublishedRate_Oz'}).drop(columns='WeightOz')\n",
    "\n",
    "\n",
    "# Combine all the Rate columns into one 'Rate' column\n",
    "tbl_BP['PublishedFreightTemp'] = tbl_BP['PublishedRate_Lb'].combine_first(tbl_BP['PublishedRate_Oz'])\n",
    "\n",
    "# Drop the intermediate Rate columns\n",
    "columns_to_drop = ['PublishedRate_Lb', 'PublishedRate_Oz']\n",
    "tbl_BP.drop(columns=columns_to_drop, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "b090ca85",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sliu\\AppData\\Local\\Temp\\ipykernel_21416\\27793880.py:12: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  ClientCharge[['min_weight', 'max_weight']] = ClientCharge['ServiceNote'].str.extract(pattern)\n",
      "C:\\Users\\sliu\\AppData\\Local\\Temp\\ipykernel_21416\\27793880.py:12: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  ClientCharge[['min_weight', 'max_weight']] = ClientCharge['ServiceNote'].str.extract(pattern)\n",
      "C:\\Users\\sliu\\AppData\\Local\\Temp\\ipykernel_21416\\27793880.py:29: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  ClientCharge['min_weight'] = ClientCharge['min_weight'].astype(float).astype('Int64')\n",
      "C:\\Users\\sliu\\AppData\\Local\\Temp\\ipykernel_21416\\27793880.py:30: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  ClientCharge['max_weight'] = ClientCharge['max_weight'].astype(float).astype('Int64')\n",
      "C:\\Users\\sliu\\AppData\\Local\\Temp\\ipykernel_21416\\27793880.py:42: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  filtered_ClientCharge['Price'] = filtered_ClientCharge['Price'].astype(float)\n",
      "C:\\Users\\sliu\\AppData\\Local\\Temp\\ipykernel_21416\\27793880.py:43: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  filtered_ClientCharge['PriceMin'] = pd.to_numeric(filtered_ClientCharge['PriceMin'], errors='coerce')\n",
      "C:\\Users\\sliu\\AppData\\Local\\Temp\\ipykernel_21416\\27793880.py:44: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  filtered_ClientCharge['PriceMin'] = filtered_ClientCharge['PriceMin'].astype(float)\n"
     ]
    }
   ],
   "source": [
    "# Specify the ServiceCodes for the 4 different ways of calculating client rates. See the table above.  \n",
    "group_disc_cr = ['REDE', 'RED', 'REDS', '2DAM', 'BLUE', 'ORNG']  \n",
    "group_wtbreaklb_disc_cr = ['GRND', 'GRES', 'SRPT']\n",
    "group_wtbreakoz_disc_cr = ['SRPT<1']\n",
    "group_marg_cr = ['DHLG', 'DHLG<1', 'DHLE', 'DHLE<1', 'DHLEM', 'USPSPS', 'USPSFC', 'USPSAP']\n",
    "group_intl_disc_cr = ['CNST', 'WWX', 'WWXS', 'WWE']\n",
    "\n",
    "# Use a regular expression pattern to extract the minimum and maximum weights directly into 'min_weight' and 'max_weight' for merge\n",
    "pattern = r'(\\d+)(?:-(\\d+))?(?:lb|oz)?\\+?'\n",
    "\n",
    "# Extract and assign the min_weight and max_weight\n",
    "ClientCharge[['min_weight', 'max_weight']] = ClientCharge['ServiceNote'].str.extract(pattern)\n",
    "\n",
    "# Dictionary to map ServiceNote values to max_weight\n",
    "service_note_weight = {\n",
    "    '31+lb': '999',\n",
    "    '9lb': '9',\n",
    "    '200+lb': '999',\n",
    "    '10+lb': '999',\n",
    "    '11+lb': '999'\n",
    "}\n",
    "\n",
    "# Loop through the dictionary and apply the mappings\n",
    "for service_note, max_weight in service_note_weight.items():\n",
    "    mask = ClientCharge['ServiceNote'] == service_note\n",
    "    ClientCharge.loc[mask, 'max_weight'] = max_weight\n",
    "\n",
    "# Convert min_weight and max_weight to integers, keeping NaN values intact\n",
    "ClientCharge['min_weight'] = ClientCharge['min_weight'].astype(float).astype('Int64')\n",
    "ClientCharge['max_weight'] = ClientCharge['max_weight'].astype(float).astype('Int64')\n",
    "\n",
    "\n",
    "condition = ClientCharge['ServiceNote'].isin(['<=5lb&Zone8', 'Pkg', 'ToCanada', 'SatDelivery']) \n",
    "# Apply the condition to set Column2 and Column3 to NaN where the condition is True\n",
    "ClientCharge.loc[condition, ['min_weight', 'max_weight']] = np.nan\n",
    "\n",
    "\n",
    "# Filter out rows where PriceType is 'Special' as we will handle Special cases at the end. \n",
    "filtered_ClientCharge = ClientCharge[ClientCharge['PriceType'] != 'Special']\n",
    "\n",
    "# Change data type of 'Price' column to float\n",
    "filtered_ClientCharge['Price'] = filtered_ClientCharge['Price'].astype(float)\n",
    "filtered_ClientCharge['PriceMin'] = pd.to_numeric(filtered_ClientCharge['PriceMin'], errors='coerce')\n",
    "filtered_ClientCharge['PriceMin'] = filtered_ClientCharge['PriceMin'].astype(float)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "cbcd5ae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge `PLD` with `ClientCharge`. Due to duplications for clientRate_calculation_2 groups we save to a new df.  \n",
    "merged_df = tbl_BP.merge(filtered_ClientCharge[['CompanyID', 'ServiceCode', 'ServiceNote', 'PriceMin', 'Price', 'min_weight', 'max_weight']], \n",
    "                         left_on=['CustomerID', 'ServiceCode'], \n",
    "                         right_on=['CompanyID', 'ServiceCode'], \n",
    "                         how='left')\n",
    "\n",
    "\n",
    "# Calculate client rate for group_disc\n",
    "mask1 = merged_df['ServiceCode'].isin(group_disc_cr)\n",
    "\n",
    "merged_df.loc[mask1, 'CustomerFreight'] = np.nanmax(\n",
    "    [\n",
    "        merged_df.loc[mask1, 'PublishedFreightTemp'] * (1 - merged_df.loc[mask1, 'Price']),\n",
    "        merged_df.loc[mask1, 'PriceMin']\n",
    "    ],\n",
    "    axis=0\n",
    ").round(2)\n",
    "\n",
    "\n",
    "# Calculate client rate for clientRate_calculation_2\n",
    "mask2 = (\n",
    "    merged_df['ServiceCode'].isin(group_wtbreaklb_disc_cr) &\n",
    "    (merged_df['min_weight'] <= merged_df['CustomerBilledWeightLb']) &\n",
    "    (merged_df['max_weight'] >= merged_df['CustomerBilledWeightLb'])\n",
    ")\n",
    "\n",
    "merged_df.loc[mask2, 'CustomerFreight'] = np.nanmax(\n",
    "    [\n",
    "        merged_df.loc[mask2, 'PublishedFreightTemp'] * (1 - merged_df.loc[mask2, 'Price']),\n",
    "        merged_df.loc[mask2, 'PriceMin']\n",
    "    ],\n",
    "    axis=0\n",
    ").round(2)\n",
    "\n",
    "\n",
    "# Modify the condition for dropping rows\n",
    "# We want to drop rows where 'CustomerFreight' is null AND the weight condition is not met\n",
    "mask_drop = (\n",
    "    merged_df['ServiceCode'].isin(group_wtbreaklb_disc_cr) & \n",
    "    merged_df['CustomerFreight'].isna() &\n",
    "    ~((merged_df['min_weight'] <= merged_df['CustomerBilledWeightLb']) &\n",
    "      (merged_df['max_weight'] >= merged_df['CustomerBilledWeightLb']))\n",
    ")\n",
    "\n",
    "merged_df = merged_df[~mask_drop]\n",
    "\n",
    "\n",
    "# Calculate client rate for clientRate_calculation_3\n",
    "mask3 = (\n",
    "    merged_df['ServiceCode'].isin(group_wtbreakoz_disc_cr) &\n",
    "    (merged_df['min_weight'] <= merged_df['CustomerBilledWeightOz']) &\n",
    "    (merged_df['max_weight'] >= merged_df['CustomerBilledWeightOz'])\n",
    ")\n",
    "\n",
    "merged_df.loc[mask3, 'CustomerFreight'] = np.nanmax(\n",
    "    [\n",
    "        merged_df.loc[mask3, 'PublishedFreightTemp'] * (1 - merged_df.loc[mask3, 'Price']),\n",
    "        merged_df.loc[mask3, 'PriceMin']\n",
    "    ],\n",
    "    axis=0\n",
    ").round(2)\n",
    "\n",
    "\n",
    "# Calculate client rate for clientRate_calculation_4\n",
    "mask4 = merged_df['ServiceCode'].isin(group_marg_cr)\n",
    "merged_df.loc[mask4, 'CustomerFreight'] = round(merged_df.loc[mask4, 'BarrettFreight'] / (1 - merged_df.loc[mask4, 'Price']),2)\n",
    "\n",
    "\n",
    "# International has min for every Zone. \n",
    "merged_df['min_intl_cr'] = merged_df['Zone'].map(mapping_min_br_intl) + 4\n",
    "\n",
    "\n",
    "# Calculate client rate for group_disc_5\n",
    "\n",
    "mask5 = merged_df['ServiceCode'].isin(group_intl_disc_cr)\n",
    "\n",
    "merged_df.loc[mask5, 'CustomerFreight'] = np.nanmax(\n",
    "    [\n",
    "        merged_df.loc[mask5, 'PublishedFreight'] * (1 - merged_df.loc[mask5, 'Price']),\n",
    "        merged_df.loc[mask5, 'min_intl_cr']\n",
    "    ],\n",
    "    axis=0\n",
    ").round(2)\n",
    "\n",
    "\n",
    "# Define the list of specific services\n",
    "all_specific_services = group_disc_cr + group_wtbreaklb_disc_cr + group_wtbreakoz_disc_cr + group_intl_disc_cr\n",
    "\n",
    "# Create masks for your conditions\n",
    "mask_pub_freight_null = merged_df['PublishedFreightTemp'].isnull()\n",
    "mask_price_null = merged_df['Price'].isnull()\n",
    "mask_service_in_list = merged_df['Service'].isin(all_specific_services)\n",
    "\n",
    "# Apply the condition: Set 'CustomerFreight' to NaN only when the service is in the specific list\n",
    "merged_df.loc[(mask_pub_freight_null | mask_price_null) & mask_service_in_list, 'CustomerFreight'] = np.nan\n",
    "\n",
    "\n",
    "# attached the CustomerFreight back to the original PLD\n",
    "tbl_BP = tbl_BP.merge(\n",
    "    merged_df[['TrackingNumber', 'CustomerFreight']],\n",
    "    left_on=['TrackingNumber'],\n",
    "    right_on=['TrackingNumber'],\n",
    "    how='left'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "6a1767c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add UPS Barrett Freight\n",
    "\n",
    "# Specify the ServiceCodes for the 4 different ways of calculating client rates. See the table above.  \n",
    "group_disc_br = ['REDE', 'RED', 'REDS', '2DAM', 'BLUE', 'ORNG']  \n",
    "group_wtbreaklb_disc_br = ['GRND', 'GRES', 'SRPT']\n",
    "group_wtbreakoz_disc_br = ['SRPT<1']\n",
    "group_min_br = ['CNST', 'WWX', 'WWXS', 'WWE']\n",
    "\n",
    "\n",
    "# Merge `PLD` with `ClientCharge`. Note that there will be duplications for clientRate_calculation_2 groups.  \n",
    "merged_df = tbl_BP.merge(filtered_ClientCharge[filtered_ClientCharge['Company'] == 'Barrett Distribution'][['ServiceCode', 'ServiceNote', 'PriceMin', 'Price', 'min_weight', 'max_weight']], \n",
    "                         left_on=['ServiceCode'], \n",
    "                         right_on=['ServiceCode'], \n",
    "                         how='left')\n",
    "\n",
    "\n",
    "# Calculate Barrett rate for group_disc\n",
    "mask5 = merged_df['ServiceCode'].isin(group_disc_br)\n",
    "\n",
    "merged_df.loc[mask5, 'BarrettFreight'] = np.nanmax(\n",
    "    [\n",
    "        merged_df.loc[mask5, 'PublishedFreightTemp'] * (1 - merged_df.loc[mask5, 'Price']),\n",
    "        merged_df.loc[mask5, 'PriceMin']\n",
    "    ],\n",
    "    axis=0\n",
    ").round(2)\n",
    "\n",
    "\n",
    "# Calculate Barrett rate for clientRate_calculation_2\n",
    "mask6 = (\n",
    "    merged_df['ServiceCode'].isin(group_wtbreaklb_disc_br) &\n",
    "    (merged_df['min_weight'] <= merged_df['CustomerBilledWeightLb']) &\n",
    "    (merged_df['max_weight'] >= merged_df['CustomerBilledWeightLb'])\n",
    ")\n",
    "\n",
    "\n",
    "merged_df.loc[mask6, 'BarrettFreight'] = np.nanmax(\n",
    "    [\n",
    "        merged_df.loc[mask6, 'PublishedFreightTemp'] * (1 - merged_df.loc[mask6, 'Price']),\n",
    "        merged_df.loc[mask6, 'PriceMin']\n",
    "    ],\n",
    "    axis=0\n",
    ").round(2)\n",
    "\n",
    "\n",
    "# Delete the duplications occurred in merge above only for clientRate_calculation_2\n",
    "mask_drop = (\n",
    "    merged_df['ServiceCode'].isin(group_wtbreaklb_disc_br) & \n",
    "    merged_df['BarrettFreight'].isna() &\n",
    "    ~((merged_df['min_weight'] <= merged_df['CustomerBilledWeightLb']) &\n",
    "      (merged_df['max_weight'] >= merged_df['CustomerBilledWeightLb']))\n",
    ")\n",
    "\n",
    "merged_df = merged_df[~mask_drop]\n",
    "\n",
    "\n",
    "# Calculate Barrett rate for clientRate_calculation_3\n",
    "mask7 = (\n",
    "    merged_df['ServiceCode'].isin(group_wtbreakoz_disc_br) &\n",
    "    (merged_df['min_weight'] <= merged_df['CustomerBilledWeightOz']) &\n",
    "    (merged_df['max_weight'] >= merged_df['CustomerBilledWeightOz'])\n",
    ")\n",
    "\n",
    "merged_df.loc[mask7, 'BarrettFreight'] = np.nanmax(\n",
    "    [\n",
    "        merged_df.loc[mask7, 'PublishedFreight'] * (1 - merged_df.loc[mask7, 'Price']),\n",
    "        merged_df.loc[mask7, 'PriceMin']\n",
    "    ],\n",
    "    axis=0\n",
    ").round(2)\n",
    "\n",
    "\n",
    "\n",
    "merged_df['min_intl'] = merged_df['Zone'].map(mapping_min_br_intl)\n",
    "\n",
    "mask8 = merged_df['ServiceCode'].isin(group_min_br)\n",
    "\n",
    "merged_df.loc[mask8, 'BarrettFreight'] = np.nanmax(\n",
    "    [\n",
    "        merged_df.loc[mask8, 'PublishedFreightTemp'] * (1 - merged_df.loc[mask8, 'Price']),\n",
    "        merged_df.loc[mask8, 'min_intl']\n",
    "    ],\n",
    "    axis=0\n",
    ").round(2)\n",
    "\n",
    "\n",
    "# attached the BarrettFreight back to the original PLD\n",
    "tbl_BP = tbl_BP.merge(\n",
    "    merged_df[['TrackingNumber', 'BarrettFreight']],\n",
    "    left_on=['TrackingNumber'],\n",
    "    right_on=['TrackingNumber'],\n",
    "    how='left', suffixes=('', '_merged_df')\n",
    ")\n",
    "\n",
    "\n",
    "# Replace the NaNs in 'rate' with values from 'rate_merged_df'\n",
    "tbl_BP['BarrettFreight'] = tbl_BP['BarrettFreight'].combine_first(tbl_BP['BarrettFreight_merged_df'])\n",
    "\n",
    "# Drop the 'rate_merged_df' column\n",
    "tbl_BP = tbl_BP.drop('BarrettFreight_merged_df', axis=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16809872",
   "metadata": {},
   "source": [
    "***g. Add UPS Published & Barrett & Customer Resi, Das, and Ahs fee***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "d66918be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add Resi Code\n",
    "tbl_BP['ResCode'] = tbl_BP.apply(lambda row:\n",
    "                                        'RESA' if (row['ServiceCode'] in ('REDE', 'RED', 'REDS', '2DAM', 'BLUE', 'ORNG') and (row['ResidentialFlag'] == True)) else\n",
    "                                        ('RESG' if (row['ServiceCode'] == 'GRES' and row['ResidentialFlag'] == True) else ('')), axis=1)\n",
    "\n",
    "\n",
    "\n",
    "# Add Das Code\n",
    "tbl_BP['DasCode'] = tbl_BP.apply(\n",
    "    lambda row: \n",
    "    ('DASCG' if (row['ServiceCode'] == 'GRND' and row['ResidentialFlag'] == False and row['DASCategory'] == 'DAS') else\n",
    "    ('DASECG' if (row['ServiceCode'] == 'GRND' and row['ResidentialFlag'] == False and row['DASCategory'] == 'DASE') else\n",
    "    ('DASRG' if (row['ServiceCode'] == 'GRES' and row['ResidentialFlag'] == True and row['DASCategory'] == 'DAS') else\n",
    "    ('DASERG' if (row['ServiceCode'] == 'GRES' and row['ResidentialFlag'] == True and row['DASCategory'] == 'DASE') else\n",
    "    ('DASCA' if (row['ServiceCode'] in ('REDE', 'RED', 'REDS', '2DAM', 'BLUE', 'ORNG') and row['ResidentialFlag'] == False and row['DASCategory'] == 'DAS') else\n",
    "    ('DASECA' if (row['ServiceCode'] in ('REDE', 'RED', 'REDS', '2DAM', 'BLUE', 'ORNG') and row['ResidentialFlag'] == False and row['DASCategory'] == 'DASE') else\n",
    "    ('DASRA' if (row['ServiceCode'] in ('REDE', 'RED', 'REDS', '2DAM', 'BLUE', 'ORNG') and row['ResidentialFlag'] == True and row['DASCategory'] == 'DAS') else\n",
    "    ('DASERA' if (row['ServiceCode'] in ('REDE', 'RED', 'REDS', '2DAM', 'BLUE', 'ORNG') and row['ResidentialFlag'] == True and row['DASCategory'] == 'DASE') else\n",
    "    ('DASSP' if (row['ServiceCode'] in ('SRPT', 'SRPT<1') and row['DASCategory'] == 'DAS') else\n",
    "    ('DASESP' if (row['ServiceCode'] in ('SRPT', 'SRPT<1') and row['DASCategory'] == 'DASE') else\n",
    "    ('REM' if (row['DASCategory'] == 'RA') else\n",
    "    ('REMAK' if (row['DASCategory'] == 'AK') else\n",
    "    ('REMHI' if (row['DASCategory'] == 'HI') else\n",
    "    ('')))))))))))))),  # This is your catch-all condition\n",
    "    axis=1)\n",
    "\n",
    "\n",
    "\n",
    "# Add 4 AhsCodes separately, handle international separately\n",
    "target_servicecode = ['REDE', 'RED', 'REDS', '2DAM', 'BLUE', 'ORNG', 'GRND', 'GRES', 'SRPT', 'SRPT<1']\n",
    "\n",
    "# Define a function to generate AhsCode values based on conditions\n",
    "def generate_ahs_code(row, column_name):\n",
    "    if row['ServiceCode'] not in target_servicecode:\n",
    "        return None\n",
    "\n",
    "    if column_name == 'Ahs1Code':\n",
    "        if row['GirthAndL'] > 105:\n",
    "            if row['Zone'] == '2':\n",
    "                return 'AHG2'\n",
    "            elif row['Zone'] in ('3','4'):\n",
    "                return 'AHG34'\n",
    "            elif row['Zone'] in ('5','6','7','8','9','10','11'):\n",
    "                return 'AHG5'\n",
    "    \n",
    "    elif column_name == 'Ahs2Code':\n",
    "        if row['L'] > 48:\n",
    "            if row['Zone'] == '2':\n",
    "                return 'AHL2'\n",
    "            elif row['Zone'] in ('3','4'):\n",
    "                return 'AHL34'\n",
    "            elif row['Zone'] in ('5','6','7','8','9','10','11'):\n",
    "                return 'AHL5'\n",
    "\n",
    "    elif column_name == 'Ahs3Code':\n",
    "        if row['W'] > 30:\n",
    "            if row['Zone'] == '2':\n",
    "                return 'AHW2'\n",
    "            elif row['Zone'] in ('3','4'):\n",
    "                return 'AHW34'\n",
    "            elif row['Zone'] in ('5','6','7','8','9','10','11'):\n",
    "                return 'AHW5'\n",
    "\n",
    "    elif column_name == 'Ahs4Code':\n",
    "        if row['ActualWeight'] > 50:\n",
    "            if row['Zone'] == '2':\n",
    "                return 'AHLB2'\n",
    "            elif row['Zone'] in ('3','4'):\n",
    "                return 'AHLB34'\n",
    "            elif row['Zone'] in ('5','6','7','8','9','10','11'):\n",
    "                return 'AHLB5'\n",
    "\n",
    "    return None\n",
    "\n",
    "# Generate AhsCode columns\n",
    "for i in range(1, 5):\n",
    "    column_name = f'Ahs{i}Code'\n",
    "    tbl_BP[column_name] = tbl_BP.apply(lambda row: generate_ahs_code(row, column_name), axis=1)\n",
    " \n",
    "\n",
    "\n",
    " # Get Published Rates. \n",
    "# Need to temporarily change the column name to prevent duplicated columns from confusing the merge function\n",
    "tbl_BP = tbl_BP.rename(columns={'ServiceCode': 'TempServiceCode'})\n",
    "\n",
    "# Define service code lists\n",
    "service_codes = {\n",
    "    'Res': ['RESA', 'RESG'],\n",
    "    'Das': ['DASCG', 'DASECG', 'DASRG', 'DASERG', 'DASCA', 'DASECA', 'DASRA', 'DASERA', 'DASSP', 'DASESP', 'REM', 'REMAK', 'REMHI'],\n",
    "    'Ahs1': ['AHG2', 'AHG34', 'AHG5'],\n",
    "    'Ahs2': ['AHL2', 'AHL34', 'AHL5'],\n",
    "    'Ahs3': ['AHW2', 'AHW34', 'AHW5'],\n",
    "    'Ahs4': ['AHLB2', 'AHLB34', 'AHLB5']\n",
    "}\n",
    "\n",
    "\n",
    "# Merge service codes into tbl_BP. Merge multiple columns together. \n",
    "for code_type, service_list in service_codes.items():\n",
    "    column_name = f'Published{code_type}'\n",
    "    tbl_BP = tbl_BP.merge(\n",
    "        ClientCharge[(ClientCharge['Company'] == 'UPS') & (ClientCharge['ServiceCode'].isin(service_list))][['ServiceCode', 'Price']],\n",
    "        left_on=[f'{code_type.capitalize()}Code'],\n",
    "        right_on=['ServiceCode'],\n",
    "        how='left'\n",
    "    ).rename(columns={'Price': column_name}).drop(columns=['ServiceCode'])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "724f220b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Client discounts\n",
    "for code_type, service_list in service_codes.items():\n",
    "    code_column_name = f'{code_type}Code'\n",
    "    tbl_BP = tbl_BP.merge(\n",
    "        ClientCharge[(ClientCharge['ServiceCode'].isin(service_list))][['CompanyID', 'ServiceCode', 'Price']],\n",
    "        left_on=['CustomerID', code_column_name],\n",
    "        right_on=['CompanyID', 'ServiceCode'],\n",
    "        how='left'\n",
    "    ).rename(columns={'Price': f'Discount{code_type}'}).drop(columns=['CompanyID', 'ServiceCode'])\n",
    "\n",
    "    \n",
    "# Get Client Rates. Fill empty spaces as 0 for the convenience of calculation. \n",
    "cols_to_fill = ['PublishedRes', 'PublishedDas', 'PublishedAhs1', 'PublishedAhs2', 'PublishedAhs3', 'PublishedAhs4',\n",
    "                'DiscountRes', 'DiscountDas', 'DiscountAhs1', 'DiscountAhs2', 'DiscountAhs3', 'DiscountAhs4']\n",
    "\n",
    "# Fill NaN in the specified columns with 0 for easier calculation later\n",
    "tbl_BP[cols_to_fill] = tbl_BP[cols_to_fill].fillna(0)\n",
    "\n",
    "\n",
    "# Calculate for Customer Resi, Das, and Ahs rates\n",
    "suffixes = ['Res', 'Das', 'Ahs1', 'Ahs2', 'Ahs3', 'Ahs4']\n",
    "\n",
    "\n",
    "# Looping through each suffix to perform the calculations\n",
    "for suffix in suffixes:\n",
    "    published_col = f'Published{suffix}'\n",
    "    discount_col = f'Discount{suffix}'\n",
    "    customer_col = f'Customer{suffix}'\n",
    "\n",
    "    tbl_BP[customer_col] = round(tbl_BP[published_col] * (1 - tbl_BP[discount_col]), 2)\n",
    "\n",
    "\n",
    "    #Get Client discounts\n",
    "for code_type, service_list in service_codes.items():\n",
    "    code_column_name = f'{code_type}Code'\n",
    "    tbl_BP = tbl_BP.merge(\n",
    "        ClientCharge[(ClientCharge['Company'] == 'Barrett Distribution') & (ClientCharge['ServiceCode'].isin(service_list))][['ServiceCode', 'Price']],\n",
    "        left_on=[code_column_name],\n",
    "        right_on=['ServiceCode'],\n",
    "        how='left'\n",
    "    ).rename(columns={'Price': f'BarrettDiscount{code_type}'}).drop(columns=['ServiceCode'])\n",
    "\n",
    "    \n",
    "\n",
    "# Get Client Rates. Fill empty spaces as 0 for the convenience of calculation. \n",
    "cols_to_fill = ['BarrettDiscountRes', 'BarrettDiscountDas', 'BarrettDiscountAhs1', 'BarrettDiscountAhs2', \n",
    "                'BarrettDiscountAhs3', 'BarrettDiscountAhs4']\n",
    "\n",
    "# Fill NaN in the specified columns with 0 for easier calculation later\n",
    "tbl_BP[cols_to_fill] = tbl_BP[cols_to_fill].fillna(0)\n",
    "\n",
    "\n",
    "# Calculate for Customer Resi, Das, and Ahs rates\n",
    "suffixes = ['Res', 'Das', 'Ahs1', 'Ahs2', 'Ahs3', 'Ahs4']\n",
    "\n",
    "\n",
    "# Looping through each suffix to perform the calculations\n",
    "for suffix in suffixes:\n",
    "    published_col = f'Published{suffix}'\n",
    "    barrett_discount_col = f'BarrettDiscount{suffix}'\n",
    "    barrett_col = f'Barrett{suffix}'\n",
    "\n",
    "    tbl_BP[barrett_col] = round(tbl_BP[published_col] * (1 - tbl_BP[barrett_discount_col]), 2)\n",
    "\n",
    "\n",
    "tbl_BP['PublishedAhs'] = tbl_BP['PublishedAhs1'] + tbl_BP['PublishedAhs2'] + tbl_BP['PublishedAhs3'] + tbl_BP['PublishedAhs4']\n",
    "tbl_BP['CustomerAhs'] = tbl_BP['CustomerAhs1'] + tbl_BP['CustomerAhs2'] + tbl_BP['CustomerAhs3'] + tbl_BP['CustomerAhs4']\n",
    "tbl_BP['BarrettAhs'] = tbl_BP['BarrettAhs1'] + tbl_BP['BarrettAhs2'] + tbl_BP['BarrettAhs3'] + tbl_BP['BarrettAhs4']\n",
    "\n",
    "\n",
    "# Drop the intermediate Rate columns\n",
    "columns_to_drop = ['ResCode', 'DasCode', 'Ahs1Code', 'Ahs2Code', 'Ahs3Code', 'Ahs4Code', \n",
    "                   'DiscountRes', 'DiscountDas', 'DiscountAhs1', 'DiscountAhs2', 'DiscountAhs3', 'DiscountAhs4', \n",
    "                   'BarrettDiscountRes', 'BarrettDiscountDas', 'BarrettDiscountAhs1', 'BarrettDiscountAhs2', \n",
    "                   'BarrettDiscountAhs3', 'BarrettDiscountAhs4', \n",
    "                   'PublishedAhs1', 'PublishedAhs2', 'PublishedAhs3', 'PublishedAhs4', \n",
    "                   'BarrettAhs1', 'BarrettAhs2', 'BarrettAhs3', 'BarrettAhs4',\n",
    "                   'CustomerAhs1', 'CustomerAhs2', 'CustomerAhs3', 'CustomerAhs4']\n",
    "\n",
    "tbl_BP.drop(columns=columns_to_drop, inplace=True)\n",
    "\n",
    "# Name the column back to its original name\n",
    "tbl_BP = tbl_BP.rename(columns={'TempServiceCode': 'ServiceCode'})\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "850880bb",
   "metadata": {},
   "source": [
    "***h. Add dhl & usps accessorials***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "f6eeba73",
   "metadata": {},
   "outputs": [],
   "source": [
    "#dhl_cubic_inch_threshold_1 = 1728/ dhl_cubic_inch_threshold_2 = 3456/ dhl_weight_threshold = 1 \n",
    "#dhl_g_and_l_threshold_1 = 50/ dhl_g_and_l_threshold_2 = 84\n",
    "#dhl_max_l_threshold_1 = 22/ dhl_max_l_threshold_2 = 27/ dhl_max_l_threshold_3 = 30\n",
    "#dates = ['2023-01-01', '2023-09-30', '2023-10-01', '2023-12-31']\n",
    "#nqd_dhl_date_1, nqd_dhl_date_2, nqd_dhl_date_3, nqd_dhl_date_4 = [pd.to_datetime(date) for date in dates]\n",
    "\n",
    "# Mask for specific ServiceCode values\n",
    "service_codes = [\"DHLG\", \"DHLG<1\", \"DHLE\", \"DHLE<1\", \"DHLEM\"]\n",
    "mask_dhl = tbl_BP['ServiceCode'].isin(service_codes)\n",
    "\n",
    "# Apply transformations to rows that satisfy mask_service\n",
    "tbl_BP.loc[mask_dhl, 'ShipDate'] = pd.to_datetime(tbl_BP.loc[mask_dhl, 'ShipDate'])\n",
    "tbl_BP.loc[mask_dhl, 'longest_side'] = tbl_BP.loc[mask_dhl, ['L', 'W', 'H']].max(axis=1)\n",
    "\n",
    "# Compute constant conditions\n",
    "date_conditions_1 = (tbl_BP.loc[mask_dhl, 'ShipDate'] >= nqd_dhl_date_1) & (tbl_BP.loc[mask_dhl, 'ShipDate'] <= nqd_dhl_date_2)\n",
    "date_conditions_2 = (tbl_BP.loc[mask_dhl, 'ShipDate'] >= nqd_dhl_date_3) & (tbl_BP.loc[mask_dhl, 'ShipDate'] <= nqd_dhl_date_4)\n",
    "actual_weight_condition = tbl_BP.loc[mask_dhl, 'ActualWeight'] >= dhl_weight_threshold\n",
    "cubic_inch_condition = tbl_BP.loc[mask_dhl, 'CubicInch'] >= dhl_cubic_inch_threshold_1\n",
    "girth_condition = (tbl_BP.loc[mask_dhl, 'GirthAndL'] > dhl_g_and_l_threshold_1) & (tbl_BP.loc[mask_dhl, 'GirthAndL'] <= dhl_g_and_l_threshold_2)\n",
    "\n",
    "# Compute the 'NQD_DHLG_1' column only for rows in df that satisfy the mask_service condition\n",
    "tbl_BP.loc[mask_dhl, 'NQD_DHLG_1'] = np.where(actual_weight_condition & cubic_inch_condition & date_conditions_2, \n",
    "                                                  2.5 * tbl_BP.loc[mask_dhl, 'CustomerBilledWeightLb'], \n",
    "                                                  np.where(actual_weight_condition & cubic_inch_condition & date_conditions_1, \n",
    "                                                           2 * tbl_BP.loc[mask_dhl, 'CustomerBilledWeightLb'], 0))\n",
    "\n",
    "tbl_BP.loc[mask_dhl, 'NQD_DHLG_2'] = np.where(girth_condition & cubic_inch_condition & actual_weight_condition & date_conditions_2, \n",
    "                                                  2.5 * tbl_BP.loc[mask_dhl, 'CustomerBilledWeightLb'], \n",
    "                                                  np.where(girth_condition & cubic_inch_condition & actual_weight_condition & date_conditions_1, \n",
    "                                                           2 * tbl_BP.loc[mask_dhl, 'CustomerBilledWeightLb'], \n",
    "                                                           np.where(girth_condition & (~cubic_inch_condition | ~actual_weight_condition) & date_conditions_2, \n",
    "                                                                    2.5 * np.ceil(tbl_BP.loc[mask_dhl, 'ActualWeight']),\n",
    "                                                                    np.where(girth_condition & (~cubic_inch_condition | ~actual_weight_condition) & date_conditions_1, \n",
    "                                                                             2 * np.ceil(tbl_BP.loc[mask_dhl, 'ActualWeight']),\n",
    "                                                                             np.where(tbl_BP.loc[mask_dhl,'GirthAndL'] > dhl_g_and_l_threshold_2, \n",
    "                                                                                      np.nan, 0)))))\n",
    "\n",
    "tbl_BP.loc[mask_dhl, 'NQD_DHLG_3'] = np.where((tbl_BP.loc[mask_dhl, 'longest_side'] > dhl_max_l_threshold_2) & cubic_inch_condition & actual_weight_condition & date_conditions_2, \n",
    "                                                  2.5 * tbl_BP.loc[mask_dhl, 'CustomerBilledWeightLb'], \n",
    "                                                  np.where((tbl_BP.loc[mask_dhl, 'longest_side'] > dhl_max_l_threshold_2) & cubic_inch_condition & actual_weight_condition & date_conditions_1, \n",
    "                                                           2 * tbl_BP.loc[mask_dhl, 'CustomerBilledWeightLb'],\n",
    "                                                           np.where((tbl_BP.loc[mask_dhl, 'longest_side'] > dhl_max_l_threshold_2) & (~cubic_inch_condition | ~actual_weight_condition) & date_conditions_2, \n",
    "                                                                    2.5 * np.ceil(tbl_BP.loc[mask_dhl, 'ActualWeight']),\n",
    "                                                                    np.where((tbl_BP.loc[mask_dhl, 'longest_side'] > dhl_max_l_threshold_2) & (~cubic_inch_condition | ~actual_weight_condition) & date_conditions_1, \n",
    "                                                                             2 * np.ceil(tbl_BP.loc[mask_dhl, 'ActualWeight']), 0))))\n",
    "\n",
    "\n",
    "tbl_BP.loc[mask_dhl, 'NQD_DHLG_4'] = np.where((tbl_BP.loc[mask_dhl, 'longest_side'] > dhl_max_l_threshold_1) & (tbl_BP.loc[mask_dhl, 'longest_side'] <= dhl_max_l_threshold_3), dhl_nqd_lengthfee_1, \n",
    "                                                  np.where(tbl_BP.loc[mask_dhl, 'longest_side'] > dhl_max_l_threshold_3, \n",
    "                                                           dhl_nqd_lengthfee_2, 0))\n",
    "\n",
    "tbl_BP.loc[mask_dhl, 'NQD_DHLG_5'] = np.where(tbl_BP.loc[mask_dhl, 'CubicInch'] > dhl_cubic_inch_threshold_2, dhl_nqd_volumefee, 0)\n",
    "\n",
    "cols_to_consolidate = ['NQD_DHLG_1', 'NQD_DHLG_2', 'NQD_DHLG_3', 'NQD_DHLG_4', 'NQD_DHLG_5']\n",
    "\n",
    "\n",
    "\n",
    "# Check for NaN values in 'NQD_DHLG_2'\n",
    "nan_condition = (tbl_BP['NQD_DHLG_2'].isna()) & mask_dhl\n",
    "# Compute the sum across the specified columns\n",
    "tbl_BP.loc[mask_dhl, 'BarrettExtra'] = tbl_BP.loc[mask_dhl, cols_to_consolidate].sum(axis=1)\n",
    "# If 'NQD_DHLG_2' is NaN, then set 'CustomerExtra' to NaN\n",
    "tbl_BP.loc[nan_condition, 'BarrettExtra'] = np.nan\n",
    "\n",
    "tbl_BP = tbl_BP.drop(columns=['NQD_DHLG_1', 'NQD_DHLG_2', 'NQD_DHLG_3', 'NQD_DHLG_4', 'NQD_DHLG_5', 'longest_side'])\n",
    "\n",
    "\n",
    "\n",
    "service_codes = [\"USPSPS\", \"USPSFC\", \"USPSAP\"]\n",
    "mask_usps = tbl_BP['ServiceCode'].isin(service_codes)\n",
    "\n",
    "conditions = [(tbl_BP['L'] > usps_l_threshold_1) & (tbl_BP['L'] <= usps_l_threshold_2), tbl_BP['L'] > usps_l_threshold_2]\n",
    "choices = [usps_nonestandard_lengthfee_1, usps_nonestandard_lengthfee_2]\n",
    "tbl_BP['EXTRA_1'] = np.select(conditions, choices, default=0)\n",
    "\n",
    "condition = tbl_BP['CubicInch'] > usps_cubic_inch_threshold\n",
    "tbl_BP['EXTRA_2'] = np.where(condition, usps_nonestandard_volumefee, 0)\n",
    "\n",
    "# Summing up the two new columns' values and storing in the \"Extra\" column for the desired subset of rows\n",
    "tbl_BP.loc[mask_usps, 'BarrettExtra'] = tbl_BP.loc[mask_usps, 'EXTRA_1'] + tbl_BP.loc[mask_usps, 'EXTRA_2']\n",
    "\n",
    "# If you don't need the two additional columns afterwards, you can drop them\n",
    "tbl_BP = tbl_BP.drop(columns=['EXTRA_1', 'EXTRA_2'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "ed2332a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get CustomerExtra from BarrettExtra \n",
    "filtered_ClientCharge = ClientCharge[ClientCharge['ServiceCode'].isin(['DHLG', 'DHLG<1', 'DHLE', 'DHLE<1', 'DHLEM', 'USPSFC', 'USPSPS', 'USPSAP'])][['CompanyID', 'ServiceCode', 'Price']].drop_duplicates(subset=['CompanyID', 'ServiceCode', 'Price'])\n",
    "\n",
    "# Define the specific ServiceCode values you're interested in\n",
    "specific_servicecodes = [\"DHLG\", \"DHLG<1\", \"DHLE\", \"DHLE<1\", \"DHLEM\", \"USPSFC\", \"USPSPS\", \"USPSAP\"]\n",
    "\n",
    "\n",
    "# Step 1: Create the intermediate dataframe\n",
    "intermediate_df = tbl_BP.query('ServiceCode in @specific_servicecodes').merge(\n",
    "    filtered_ClientCharge, \n",
    "    left_on=['CustomerID', 'ServiceCode'], \n",
    "    right_on=['CompanyID', 'ServiceCode'], \n",
    "    how='left'\n",
    ")\n",
    "\n",
    "intermediate_df['Price'] = intermediate_df['Price'].astype(float)\n",
    "\n",
    "# Calculate 'CustomerExtra', setting it to NaN where 'Price' is NaN\n",
    "intermediate_df['CustomerExtra'] = np.where(\n",
    "    intermediate_df['Price'].isna(),\n",
    "    np.nan,\n",
    "    np.round(intermediate_df['BarrettExtra'] / (1 - intermediate_df['Price']), 2)\n",
    ")\n",
    "\n",
    "\n",
    "# attached the CustomerFreight back to the original PLD\n",
    "tbl_BP = tbl_BP.merge(\n",
    "    intermediate_df[['TrackingNumber', 'CustomerExtra', 'Price']],\n",
    "    left_on=['TrackingNumber'],\n",
    "    right_on=['TrackingNumber'],\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "# Fill NaN in the specified columns with 0 for easier calculation later\n",
    "tbl_BP['CustomerExtra'] = tbl_BP['CustomerExtra'].fillna(0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ef251d7",
   "metadata": {},
   "source": [
    "***i. Get FSC (only for UPS and DHL)***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "e3de47ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the ServiceCodes for the three scenarios\n",
    "specific_servicecodes_upsFsc = [\"REDE\", \"RED\", \"REDS\", \"2DAM\", \"BLUE\", \"ORNG\", \"GRND\", \"GRES\", \"SRPT\", \"SRPT<1\", \"CNST\", \"WWX\", \"WWXS\", \"WWE\"]  \n",
    "specific_servicecodes_dhlFsc = [\"DHLG\", \"DHLG<1\", \"DHLE\", \"DHLE<1\", \"DHLEM\"]  \n",
    "\n",
    "\n",
    "# Compute PublishedFsc for UPS\n",
    "mask_upsFsc = tbl_BP['ServiceCode'].isin(specific_servicecodes_upsFsc)\n",
    "tbl_BP.loc[mask_upsFsc, 'PublishedFsc'] = round((tbl_BP['PublishedFreight'] + tbl_BP['PublishedRes'] + tbl_BP['PublishedDas'] + tbl_BP['PublishedAhs']) * tbl_BP['FscRate'],2)\n",
    "\n",
    "# Compute CustomerFsc for UPS\n",
    "tbl_BP.loc[mask_upsFsc, 'CustomerFsc'] = round((tbl_BP['CustomerFreight'] + tbl_BP['CustomerRes'] + tbl_BP['CustomerDas'] + tbl_BP['CustomerAhs']) * tbl_BP['FscRate'],2)\n",
    "\n",
    "# Compute CustomerFsc for DHL\n",
    "mask_dhlFsc = tbl_BP['ServiceCode'].isin(specific_servicecodes_dhlFsc)\n",
    "tbl_BP.loc[mask_dhlFsc, 'CustomerFsc'] = np.where((tbl_BP.loc[mask_dhlFsc, 'ActualWeight'] <= 1), (tbl_BP.loc[mask_dhlFsc, 'FscRate'] * tbl_BP.loc[mask_dhlFsc, 'ActualWeight']) / (1 - tbl_BP.loc[mask_dhlFsc, 'Price']),\n",
    "         np.where((tbl_BP.loc[mask_dhlFsc, 'ActualWeight'] > 1), (tbl_BP.loc[mask_dhlFsc, 'FscRate'] * tbl_BP.loc[mask_dhlFsc, 'CustomerBilledWeightLb']) / (1 - tbl_BP.loc[mask_dhlFsc, 'Price']), 0))\n",
    "\n",
    "\n",
    "# Compute BarrettFsc for UPS\n",
    "tbl_BP.loc[mask_upsFsc, 'BarrettFsc'] = round((tbl_BP['BarrettFreight'] + tbl_BP['BarrettRes'] + tbl_BP['BarrettDas'] + tbl_BP['BarrettAhs']) * (tbl_BP['FscRate']*ups_fsc_disc),2)\n",
    "\n",
    "# Compute BarrettFsc for DHL\n",
    "mask_dhlFsc = tbl_BP['ServiceCode'].isin(specific_servicecodes_dhlFsc)\n",
    "tbl_BP.loc[mask_dhlFsc, 'BarrettFsc'] = np.where((tbl_BP.loc[mask_dhlFsc, 'ActualWeight'] <= 1), (tbl_BP.loc[mask_dhlFsc, 'FscRate'] * tbl_BP.loc[mask_dhlFsc, 'ActualWeight']),\n",
    "         np.where((tbl_BP.loc[mask_dhlFsc, 'ActualWeight'] > 1), (tbl_BP.loc[mask_dhlFsc, 'FscRate'] * tbl_BP.loc[mask_dhlFsc, 'CustomerBilledWeightLb']), 0))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d65c6306",
   "metadata": {},
   "source": [
    "***j. Get Total***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "fdd7cf02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the ServiceCodes for the three scenarios\n",
    "specific_servicecodes_UPS = [\"REDE\", \"RED\", \"REDS\", \"2DAM\", \"BLUE\", \"ORNG\", \"GRND\", \"GRES\", \"SRPT\", \"SRPT<1\", \"CNST\", \"WWE\", \"WWXS\", \"WWX\"]  \n",
    "\n",
    "specific_servicecodes_DHL = [\"DHLG\", \"DHLG<1\", \"DHLE\", \"DHLE<1\", \"DHLEM\"]  \n",
    "\n",
    "specific_servicecodes_USPS = [\"USPSFC\", \"USPSPS\", \"USPSAP\"]  \n",
    "\n",
    "\n",
    "# Compute PublishedTotal for UPS\n",
    "mask_UPS = tbl_BP['ServiceCode'].isin(specific_servicecodes_UPS)\n",
    "tbl_BP.loc[mask_UPS, 'PublishedTotal'] = round(tbl_BP['PublishedFreight'] + tbl_BP['PublishedRes'] + tbl_BP['PublishedDas'] + tbl_BP['PublishedAhs'] + tbl_BP['PublishedFsc'],2)\n",
    "\n",
    "# Compute CustomerTotal for UPS\n",
    "tbl_BP.loc[mask_UPS, 'CustomerTotal'] = round(tbl_BP['CustomerFreight'] + tbl_BP['CustomerRes'] + tbl_BP['CustomerDas'] + tbl_BP['CustomerAhs'] + tbl_BP['CustomerFsc'],2)\n",
    "\n",
    "# Compute CustomerTotal for DHL\n",
    "mask_DHL = tbl_BP['ServiceCode'].isin(specific_servicecodes_DHL)\n",
    "tbl_BP.loc[mask_DHL, 'CustomerTotal'] = round(tbl_BP['CustomerFreight'] + tbl_BP['CustomerExtra'] + tbl_BP['CustomerFsc'],2)\n",
    "\n",
    "# Compute CustomerTotal for USPS\n",
    "mask_USPS = tbl_BP['ServiceCode'].isin(specific_servicecodes_USPS)\n",
    "tbl_BP.loc[mask_USPS, 'CustomerTotal'] = round(tbl_BP['CustomerFreight'] + tbl_BP['CustomerExtra'],2)\n",
    "\n",
    "# Compute BarrettTotal for UPS\n",
    "tbl_BP.loc[mask_UPS, 'BarrettTotal'] = round(tbl_BP['BarrettFreight'] + tbl_BP['BarrettRes'] + tbl_BP['BarrettDas'] + tbl_BP['BarrettAhs'] + tbl_BP['BarrettFsc'],2)\n",
    "\n",
    "# Compute BarrettTotal for DHL\n",
    "mask_DHL = tbl_BP['ServiceCode'].isin(specific_servicecodes_DHL)\n",
    "tbl_BP.loc[mask_DHL, 'BarrettTotal'] = round(tbl_BP['BarrettFreight'] + tbl_BP['BarrettExtra'] + tbl_BP['BarrettFsc'],2)\n",
    "\n",
    "# Compute BarrettTotal for USPS\n",
    "mask_USPS = tbl_BP['ServiceCode'].isin(specific_servicecodes_USPS)\n",
    "tbl_BP.loc[mask_USPS, 'BarrettTotal'] = round(tbl_BP['BarrettFreight'] + tbl_BP['BarrettExtra'],2)\n",
    "\n",
    "\n",
    "# Filter rows where 'exclude_rates' column value is in the provided list\n",
    "condition = ~tbl_BP['ServiceCode'].isin([\"REDE\", \"RED\", \"REDS\", \"2DAM\", \"BLUE\", \"ORNG\", \"GRND\", \"GRES\", \"SRPT\", \"SRPT<1\", \"CNST\", \"WWE\", \"WWXS\", \"WWX\"])\n",
    "\n",
    "# If the condition is met, then set 'PublishedFreight' to NaN\n",
    "tbl_BP.loc[condition, 'PublishedFreight'] = np.nan\n",
    "tbl_BP.loc[condition, 'PublishedRes'] = np.nan\n",
    "tbl_BP.loc[condition, 'PublishedDas'] = np.nan\n",
    "tbl_BP.loc[condition, 'PublishedAhs'] = np.nan\n",
    "tbl_BP.loc[condition, 'PublishedFsc'] = np.nan\n",
    "\n",
    "# If you don't need the two additional columns afterwards, you can drop them\n",
    "tbl_BP = tbl_BP.drop(columns=['Price', 'PublishedFreightTemp'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cbf3073",
   "metadata": {},
   "source": [
    "***k. Handle Special Cases and Flag Rate***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "a0056d0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ClientChargeSpecial = ClientCharge[(ClientCharge['PriceType'] == 'Special')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "1419df2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Johnnie-o parameters\n",
    "\n",
    "flatJo_1 = ClientChargeSpecial.loc[(ClientChargeSpecial['CompanyID'] == 9019.0) &\n",
    "                 (ClientChargeSpecial['ServiceCode'].isin(['RED', 'REDS'])), 'PriceMin'].iloc[0]\n",
    "\n",
    "flatJo_2 = ClientChargeSpecial.loc[(ClientChargeSpecial['CompanyID'] == 9019.0) &\n",
    "                 (ClientChargeSpecial['ServiceCode'].isin(['2DAM', 'BlUE'])), 'PriceMin'].iloc[0]\n",
    "\n",
    "CAJo = ClientChargeSpecial.loc[(ClientChargeSpecial['CompanyID'] == 9019.0) &\n",
    "                 (ClientChargeSpecial['ServiceCode'].isin(['WWX', 'WWXS', 'WWE'])), 'Price'].iloc[0]\n",
    "\n",
    "\n",
    "# HandleJohnnie-o flat rate special case\n",
    "tbl_BP.loc[\n",
    "    (tbl_BP['CustomerID'].isin([9019, 90190])) &\n",
    "    (tbl_BP['ServiceCode'].isin(['RED', 'REDS'])) &\n",
    "    (tbl_BP['CustomerBilledWeightLb'] <= 5) &\n",
    "    (tbl_BP['Zone'].isin(['1','2','3','4','5','6','7','8'])),\n",
    "    'CustomerFreight'\n",
    "] = flatJo_1\n",
    "\n",
    "tbl_BP.loc[\n",
    "    (tbl_BP['CustomerID'].isin([9019, 90190])) &\n",
    "    (tbl_BP['ServiceCode'].isin(['2DAM','BLUE'])) &\n",
    "    (tbl_BP['CustomerBilledWeightLb'] <= 5) &\n",
    "    (tbl_BP['Zone'].isin(['1','2','3','4','5','6','7','8'])),\n",
    "    'CustomerFreight'\n",
    "] = flatJo_2\n",
    "\n",
    "tbl_BP.loc[\n",
    "    (tbl_BP['CustomerID'] == 9019) &\n",
    "    (tbl_BP['ServiceCode'].isin(['RED','REDS'])) &\n",
    "    (tbl_BP['CustomerFreight'] == flatJo_1),\n",
    "    'CustomerTotal'\n",
    "] = flatJo_1\n",
    "\n",
    "\n",
    "tbl_BP.loc[\n",
    "    (tbl_BP['CustomerID'] == 9019) &\n",
    "    (tbl_BP['ServiceCode'].isin(['2DAM','BLUE'])) &\n",
    "    (tbl_BP['CustomerFreight'] == flatJo_2),\n",
    "    'CustomerTotal'\n",
    "] = flatJo_2\n",
    "\n",
    "\n",
    "tbl_BP.loc[\n",
    "    (tbl_BP['CustomerID'] == 9019) &\n",
    "    (tbl_BP['ServiceCode'].isin(['RED','REDS'])) &\n",
    "    (tbl_BP['CustomerFreight'] == flatJo_1),\n",
    "    ['CustomerRes', 'CustomerDas', 'CustomerAhs', 'CustomerFsc']\n",
    "] = [0, 0, 0, 0]\n",
    "\n",
    "\n",
    "tbl_BP.loc[\n",
    "    (tbl_BP['CustomerID'] == 9019) &\n",
    "    (tbl_BP['ServiceCode'].isin(['2DAM','BLUE'])) &\n",
    "    (tbl_BP['CustomerFreight'] == flatJo_2),\n",
    "    ['CustomerRes', 'CustomerDas', 'CustomerAhs', 'CustomerFsc']\n",
    "] = [0, 0, 0, 0]\n",
    "\n",
    "\n",
    "\n",
    "tbl_BP.loc[\n",
    "    (tbl_BP['CustomerID'].isin([9019, 90190])) &\n",
    "    (tbl_BP['ServiceCode'].isin(['WWX','WWXS','WWE'])) &\n",
    "    (tbl_BP['ShipToCountry'] == 'CA'),\n",
    "    'Price'\n",
    "] = CAJo\n",
    "\n",
    "\n",
    "condition_jo = (\n",
    "            (tbl_BP['CustomerID'].isin([9019, 90190])) & \n",
    "            (tbl_BP['ServiceCode'].isin(['WWX','WWXS','WWE'])) & \n",
    "            (tbl_BP['ShipToCountry'] == 'CA'))\n",
    "        \n",
    "tbl_BP.loc[condition_jo, 'CustomerFreight'] = tbl_BP.loc[condition_jo, 'PublishedFreight'] * (1 - CAJo)\n",
    "\n",
    "tbl_BP.loc[condition_jo, 'CustomerFsc'] = (tbl_BP.loc[condition_jo, 'CustomerFreight']  +\n",
    "                                        tbl_BP.loc[condition_jo, 'CustomerRes'] +\n",
    "                                        tbl_BP.loc[condition_jo, 'CustomerDas'] + \n",
    "                                        tbl_BP.loc[condition_jo, 'CustomerAhs'] +\n",
    "                                        tbl_BP.loc[condition_jo, 'CustomerExtra']) * (tbl_BP.loc[condition_jo, 'FscRate']) \n",
    "\n",
    "tbl_BP.loc[condition_jo, 'CustomerTotal'] = (tbl_BP.loc[condition_jo, 'CustomerFreight']  +\n",
    "                                          tbl_BP.loc[condition_jo, 'CustomerRes'] +\n",
    "                                          tbl_BP.loc[condition_jo, 'CustomerDas'] + \n",
    "                                          tbl_BP.loc[condition_jo, 'CustomerAhs'] +\n",
    "                                          tbl_BP.loc[condition_jo, 'CustomerExtra'] +\n",
    "                                          tbl_BP.loc[condition_jo, 'CustomerFsc'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "36243329",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ardent parameters\n",
    "flatArdent_1 = ClientChargeSpecial.loc[(ClientChargeSpecial['CompanyID'] == 1127.0) &\n",
    "                 (ClientChargeSpecial['ServiceCode'].isin(['ORNG'])), 'PriceMin'].iloc[0]\n",
    "\n",
    "flatArdent_2 = ClientChargeSpecial.loc[(ClientChargeSpecial['CompanyID'] == 1127.0) &\n",
    "                 (ClientChargeSpecial['ServiceCode'].isin(['ORNG'])), 'PriceMin'].iloc[1]\n",
    "\n",
    "flatArdent_3 = ClientChargeSpecial.loc[(ClientChargeSpecial['CompanyID'] == 1127.0) &\n",
    "                 (ClientChargeSpecial['ServiceCode'].isin(['GRND', 'GRES'])), 'PriceMin'].iloc[0]\n",
    "\n",
    "flatArdent_4 = ClientChargeSpecial.loc[(ClientChargeSpecial['CompanyID'] == 1127.0) &\n",
    "                 (ClientChargeSpecial['ServiceCode'].isin(['GRND', 'GRES'])), 'PriceMin'].iloc[1]\n",
    "\n",
    "minArdent = ClientChargeSpecial.loc[(ClientChargeSpecial['CompanyID'] == 1127.0) &\n",
    "                 (ClientChargeSpecial['ServiceCode'].isin(['ORNG'])), 'PriceMin'].iloc[2]\n",
    "\n",
    "discArdent = ClientChargeSpecial.loc[(ClientChargeSpecial['CompanyID'] == 1127.0) &\n",
    "                 (ClientChargeSpecial['ServiceCode'].isin(['ORNG'])), 'Price'].iloc[2]\n",
    "\n",
    "\n",
    "# Ardent calculation\n",
    "conditions_ard_1 = [\n",
    "    (tbl_BP['CustomerID'] == 1127) & (tbl_BP['ServiceCode'].isin(['ORNG'])) & (tbl_BP['CustomerBilledWeightLb'] >= 1) & (tbl_BP['CustomerBilledWeightLb'] <= 7),\n",
    "    (tbl_BP['CustomerID'] == 1127) & (tbl_BP['ServiceCode'].isin(['ORNG'])) & (tbl_BP['CustomerBilledWeightLb'] >= 8) & (tbl_BP['CustomerBilledWeightLb'] <= 10)\n",
    "]\n",
    "\n",
    "conditions_ard_2 = [\n",
    "    (tbl_BP['CustomerID'] == 1127) & (tbl_BP['ServiceCode'].isin(['GRND', 'GRES'])) & (tbl_BP['CustomerBilledWeightLb'] >= 1) & (tbl_BP['CustomerBilledWeightLb'] <= 7),\n",
    "    (tbl_BP['CustomerID'] == 1127) & (tbl_BP['ServiceCode'].isin(['GRND', 'GRES'])) & (tbl_BP['CustomerBilledWeightLb'] >= 8) & (tbl_BP['CustomerBilledWeightLb'] <= 10)\n",
    "]\n",
    "\n",
    "# Define the corresponding actions for each condition\n",
    "choices_1 = [\n",
    "    flatArdent_1,  \n",
    "    flatArdent_2  \n",
    "    \n",
    "]\n",
    "\n",
    "choices_2 = [\n",
    "   flatArdent_3,  \n",
    "   flatArdent_4 \n",
    "    \n",
    "]\n",
    "\n",
    "choices_3 = [\n",
    "    0,  \n",
    "    0  \n",
    "]\n",
    "\n",
    "\n",
    "# Apply the conditions and choices to the dataframe\n",
    "tbl_BP['CustomerFreight'] = np.select(conditions_ard_1, choices_1, default=tbl_BP['CustomerFreight'])\n",
    "tbl_BP['CustomerFreight'] = np.select(conditions_ard_2, choices_2, default=tbl_BP['CustomerFreight'])\n",
    "tbl_BP['CustomerRes'] = np.select(conditions_ard_1, choices_3, default=tbl_BP['CustomerRes'])\n",
    "tbl_BP['CustomerDas'] = np.select(conditions_ard_1, choices_3, default=tbl_BP['CustomerDas'])\n",
    "tbl_BP['CustomerFsc'] = np.select(conditions_ard_1, choices_3, default=tbl_BP['CustomerFsc'])\n",
    "\n",
    "\n",
    "def calculate_customer_total(row):\n",
    "    if row['CustomerID'] == 1127 and row['ServiceCode'] in ['ORNG', 'GRND', 'GRES']:\n",
    "        if 1 <= row['CustomerBilledWeightLb'] <= 10:\n",
    "            return row['CustomerFreight'] + row['CustomerAhs']\n",
    "    return row['CustomerTotal']\n",
    "\n",
    "tbl_BP['CustomerTotal'] = tbl_BP.apply(calculate_customer_total, axis=1)\n",
    "\n",
    "\n",
    "def custom_function(row):\n",
    "    if row['CustomerID'] == 1127 and row['ServiceCode'] == 'ORNG' and row['CustomerBilledWeightLb'] >= 11:\n",
    "        return round(max(row['PublishedFreight'] * (1 - discArdent), minArdent), 2)\n",
    "    else:\n",
    "        return row['CustomerFreight']\n",
    "\n",
    "tbl_BP['CustomerFreight'] = tbl_BP.apply(custom_function, axis=1)\n",
    "\n",
    "# Recalculate CustomerFsc and CustomerTotal within the same loop\n",
    "for index, row in tbl_BP.iterrows():\n",
    "    if row['CustomerID'] == 1127 and row['ServiceCode'] == 'ORNG' and row['CustomerBilledWeightLb'] >= 11:\n",
    "        # Calculate CustomerFsc based on the current row's values\n",
    "        customer_fsc = (row['CustomerFreight'] + row['CustomerRes'] + row['CustomerDas'] + \n",
    "                        row['CustomerAhs'] + row['CustomerExtra']) * row['FscRate']\n",
    "        tbl_BP.at[index, 'CustomerFsc'] = round(customer_fsc,2)\n",
    "\n",
    "        # Calculate CustomerTotal based on the updated CustomerFsc\n",
    "        customer_total = (row['CustomerFreight'] + row['CustomerRes'] + row['CustomerDas'] + \n",
    "                          row['CustomerAhs'] + row['CustomerExtra'] + customer_fsc)\n",
    "        tbl_BP.at[index, 'CustomerTotal'] = round(customer_total,2)\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "0eef6aea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Erms sport parameters\n",
    "flatErms_1 = ClientChargeSpecial.loc[(ClientChargeSpecial['CompanyID'] == 6483.0) &\n",
    "                 (ClientChargeSpecial['ServiceCode'].isin(['SRPT<1'])), 'PriceMin'].iloc[0]\n",
    "\n",
    "flatErms_2 = ClientChargeSpecial.loc[(ClientChargeSpecial['CompanyID'] == 6483.0) &\n",
    "                 (ClientChargeSpecial['ServiceCode'].isin(['SRPT'])), 'PriceMin'].iloc[0]\n",
    "\n",
    "\n",
    "# Erms sport calculation\n",
    "condition_es_1 = (\n",
    "    (tbl_BP['CustomerID'] == 6483) &\n",
    "    (tbl_BP['ServiceCode'] == 'SRPT<1') &\n",
    "    (tbl_BP['CustomerBilledWeightOz'] <= 16)\n",
    ")\n",
    "        \n",
    "condition_es_2 = (\n",
    "    (tbl_BP['CustomerID'] == 6483) &\n",
    "    (tbl_BP['ServiceCode'] == 'SRPT') &\n",
    "    ((tbl_BP['CustomerBilledWeightLb'] >= 1) & (tbl_BP['CustomerBilledWeightLb'] <= 2))\n",
    ")\n",
    "        \n",
    "        \n",
    "tbl_BP.loc[condition_es_1, 'CustomerFreight'] = round(flatErms_1,2)\n",
    "tbl_BP.loc[condition_es_2, 'CustomerFreight'] = round(flatErms_2,2)\n",
    "tbl_BP.loc[condition_es_1, 'CustomerTotal'] = round(flatErms_1,2)\n",
    "tbl_BP.loc[condition_es_2, 'CustomerTotal'] = round(flatErms_2,2)\n",
    "\n",
    "\n",
    "tbl_BP.loc[condition_es_1,\n",
    "    ['CustomerRes', 'CustomerDas', 'CustomerAhs', 'CustomerFsc', 'CustomerExtra']\n",
    "] = [0, 0, 0, 0, 0]\n",
    "\n",
    "tbl_BP.loc[condition_es_2,\n",
    "    ['CustomerRes', 'CustomerDas', 'CustomerAhs', 'CustomerFsc', 'CustomerExtra']\n",
    "] = [0, 0, 0, 0, 0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "7e4ac097",
   "metadata": {},
   "outputs": [],
   "source": [
    "# International\n",
    "#Res\n",
    "condition_intl_ground_res = (\n",
    "    (tbl_BP['ServiceCode'] == \"CNST\") &\n",
    "    (tbl_BP['ResidentialFlag'] == True)\n",
    ")\n",
    "\n",
    "condition_intl_air_res = (\n",
    "    (tbl_BP['ServiceCode'].isin([\"WWX\", \"WWXS\", \"WWE\"])) &\n",
    "    (tbl_BP['ResidentialFlag'] == True)\n",
    ")\n",
    "\n",
    "tbl_BP.loc[condition_intl_ground_res,\n",
    "    ['PublishedRes', 'CustomerRes', 'BarrettRes']\n",
    "] = [intl_ground_res, intl_ground_res, intl_ground_res] \n",
    "\n",
    "tbl_BP.loc[condition_intl_air_res,\n",
    "    ['PublishedRes', 'CustomerRes', 'BarrettRes']\n",
    "] = [intl_air_res, intl_air_res, intl_air_res]\n",
    "\n",
    "\n",
    "#Ahs\n",
    "# Define your conditions\n",
    "condition_intl_ahs_1 = (tbl_BP['ServiceCode'].isin([\"CNST\",\"WWX\", \"WWXS\", \"WWE\"])) & (tbl_BP['GirthAndL'] > 105)\n",
    "condition_intl_ahs_2 = (tbl_BP['ServiceCode'].isin([\"CNST\",\"WWX\", \"WWXS\", \"WWE\"])) & (tbl_BP['L'] > 48)\n",
    "condition_intl_ahs_3 = (tbl_BP['ServiceCode'].isin([\"CNST\",\"WWX\", \"WWXS\", \"WWE\"])) & (tbl_BP['W'] > 30)\n",
    "condition_intl_ahs_4 = (tbl_BP['ServiceCode'].isin([\"CNST\",\"WWX\", \"WWXS\", \"WWE\"])) & (tbl_BP['ActualWeight'] > intl_ahs_weight_limit)\n",
    "\n",
    "# Count the number of conditions met for each row\n",
    "conditions_met = condition_intl_ahs_1.astype(int) + condition_intl_ahs_2.astype(int) + condition_intl_ahs_3.astype(int) + condition_intl_ahs_4.astype(int)\n",
    "\n",
    "# Calculate the additional amount to be added (charge for each condition met)\n",
    "additional_amount = conditions_met * intl_ahs\n",
    "\n",
    "# Add the additional amount to the price columns\n",
    "tbl_BP['PublishedAhs'] += additional_amount\n",
    "tbl_BP['CustomerAhs'] += additional_amount\n",
    "tbl_BP['BarrettAhs'] += additional_amount\n",
    "  \n",
    "#Fsc\n",
    "tbl_BP.loc[tbl_BP['ServiceCode'].isin([\"CNST\", \"WWX\", \"WWXS\", \"WWE\"]), 'PublishedFsc'] = round((tbl_BP['PublishedFreight'] + tbl_BP['PublishedRes'] + tbl_BP['PublishedDas'] + tbl_BP['PublishedAhs']) * tbl_BP['FscRate'],2)\n",
    "tbl_BP.loc[tbl_BP['ServiceCode'].isin([\"CNST\", \"WWX\", \"WWXS\", \"WWE\"]), 'BarrettFsc'] = round((tbl_BP['BarrettFreight'] + tbl_BP['BarrettRes'] + tbl_BP['BarrettDas'] + tbl_BP['BarrettAhs']) * (tbl_BP['FscRate']*ups_fsc_disc),2)\n",
    "tbl_BP.loc[tbl_BP['ServiceCode'].isin([\"CNST\", \"WWX\", \"WWXS\", \"WWE\"]), 'CustomerFsc'] = round((tbl_BP['CustomerFreight'] + tbl_BP['CustomerRes'] + tbl_BP['CustomerDas'] + tbl_BP['CustomerAhs']) * tbl_BP['FscRate'],2)\n",
    "\n",
    "\n",
    "#Total PublishedTotal for UPS\n",
    "tbl_BP.loc[tbl_BP['ServiceCode'].isin([\"CNST\", \"WWX\", \"WWXS\", \"WWE\"]), 'PublishedTotal'] = round(tbl_BP['PublishedFreight'] + tbl_BP['PublishedRes'] + tbl_BP['PublishedDas'] + tbl_BP['PublishedAhs'] + tbl_BP['PublishedFsc'],2)\n",
    "\n",
    "# Compute CustomerTotal for UPS\n",
    "tbl_BP.loc[tbl_BP['ServiceCode'].isin([\"CNST\", \"WWX\", \"WWXS\", \"WWE\"]), 'CustomerTotal'] = round(tbl_BP['CustomerFreight'] + tbl_BP['CustomerRes'] + tbl_BP['CustomerDas'] + tbl_BP['CustomerAhs'] + tbl_BP['CustomerFsc'],2)\n",
    "\n",
    "# Compute BarrettTotal for UPS\n",
    "tbl_BP.loc[tbl_BP['ServiceCode'].isin([\"CNST\", \"WWX\", \"WWXS\", \"WWE\"]), 'BarrettTotal'] = round(tbl_BP['BarrettFreight'] + tbl_BP['BarrettRes'] + tbl_BP['BarrettDas'] + tbl_BP['BarrettAhs'] + tbl_BP['BarrettFsc'],2)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c11a0eb3",
   "metadata": {},
   "source": [
    "***l. Calculate the savings and Savings % and Format***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "f3c690a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Columns you want to fill\n",
    "cols_to_update_1 = ['CustomerFreight', 'CustomerRes', 'CustomerDas', 'CustomerAhs', 'CustomerExtra', 'CustomerFsc', 'BarrettMargin', 'CustomerSaving', 'CustomerSavingPct']\n",
    "tbl_BP.loc[tbl_BP['CustomerTotal'].isnull(), cols_to_update_1] = np.nan\n",
    "\n",
    "# Columns you want to fill\n",
    "cols_to_update_2 = ['PublishedFreight', 'PublishedRes', 'PublishedDas', 'PublishedAhs', 'PublishedFsc', 'CustomerSaving', 'CustomerSavingPct']\n",
    "tbl_BP.loc[tbl_BP['PublishedTotal'].isnull(), cols_to_update_2] = np.nan\n",
    "\n",
    "# Columns you want to fill\n",
    "cols_to_update_3 = ['BarrettFreight', 'BarrettRes', 'BarrettDas', 'BarrettAhs', 'BarrettExtra', 'BarrettFsc', 'BarrettMargin']\n",
    "tbl_BP.loc[tbl_BP['BarrettTotal'].isnull(), cols_to_update_3] = np.nan\n",
    "\n",
    "# Columns you want to fill\n",
    "cols_to_update_4 = ['CustomerFreight', 'CustomerRes', 'CustomerDas', 'CustomerAhs', 'CustomerExtra', 'CustomerFsc', 'BarrettMargin', 'CustomerSaving', 'CustomerSavingPct']\n",
    "tbl_BP.loc[tbl_BP['CustomerTotal'].notnull(), cols_to_update_4] = tbl_BP.loc[tbl_BP['CustomerTotal'].notnull(), cols_to_update_4].fillna(0)\n",
    "\n",
    "# Columns you want to fill\n",
    "cols_to_update_5 = ['PublishedFreight', 'PublishedRes', 'PublishedDas', 'PublishedAhs', 'PublishedFsc', 'CustomerSaving', 'CustomerSavingPct']\n",
    "tbl_BP.loc[tbl_BP['PublishedTotal'].notnull(), cols_to_update_5] = tbl_BP.loc[tbl_BP['PublishedTotal'].notnull(), cols_to_update_5].fillna(0)\n",
    "\n",
    "# Columns you want to fill\n",
    "cols_to_update_6 = ['BarrettFreight', 'BarrettRes', 'BarrettDas', 'BarrettAhs', 'BarrettExtra', 'BarrettFsc', 'BarrettMargin']\n",
    "tbl_BP.loc[tbl_BP['BarrettTotal'].notnull(), cols_to_update_6] = tbl_BP.loc[tbl_BP['BarrettTotal'].notnull(), cols_to_update_6].fillna(0)\n",
    "\n",
    "\n",
    "# Calculate for Customer Resi, Das, and Ahs rates\n",
    "tbl_BP['CustomerSaving'] = round(tbl_BP['PublishedTotal'] - tbl_BP['CustomerTotal'],2)\n",
    "tbl_BP['CustomerSavingPct'] = round((tbl_BP['PublishedTotal'] - tbl_BP['CustomerTotal'])/tbl_BP['PublishedTotal'],2)\n",
    "tbl_BP['BarrettMargin'] = round(1 - tbl_BP['BarrettTotal']/tbl_BP['CustomerTotal'],2)\n",
    "\n",
    "tbl_BP = tbl_BP.drop(columns=['Price'])\n",
    "\n",
    "# Columns to format\n",
    "cols_to_format_1 = ['CustomerFreight', 'CustomerRes', 'CustomerDas', 'CustomerAhs', 'CustomerExtra', 'CustomerFsc', 'CustomerTotal', \n",
    "                    'PublishedFreight', 'PublishedRes', 'PublishedDas', 'PublishedAhs', 'PublishedFsc', 'PublishedTotal', \n",
    "                    'BarrettFreight', 'BarrettRes', 'BarrettDas', 'BarrettAhs', 'BarrettExtra', 'BarrettFsc', 'BarrettTotal',\n",
    "                    'CustomerSaving']\n",
    "\n",
    "# Columns to format\n",
    "cols_to_format_2 = ['FscRate', 'CustomerSavingPct', 'BarrettMargin']\n",
    "\n",
    "# Apply currency format\n",
    "tbl_BP[cols_to_format_1] = tbl_BP[cols_to_format_1].applymap(lambda x: '${:,.2f}'.format(x) if pd.notna(x) else x)\n",
    "\n",
    "# Apply percentage format\n",
    "tbl_BP[cols_to_format_2] = tbl_BP[cols_to_format_2].applymap(lambda x: '{:.2%}'.format(x) if pd.notna(x) else x)\n",
    "\n",
    "\n",
    "# Now update the RateFlag based on the conditions. There is no possibility that SRPT<1 has lb rather than oz value. \n",
    "tbl_BP.loc[\n",
    "    (((tbl_BP['ServiceCode'].isin(['REDE', 'RED', 'REDS', '2DAM', 'BLUE', 'ORNG', 'GRND', 'GRES', 'CNST', 'WWXS', 'WWX', 'WWE'])) & tbl_BP['CustomerBilledWeightLb'] > 150) |\n",
    "     ((tbl_BP['ServiceCode'] == 'SRPT') & (tbl_BP['CustomerBilledWeightLb'] > 15)) |\n",
    "     ((tbl_BP['ServiceCode'].isin(['DHLG', 'DHLE'])) & (tbl_BP['CustomerBilledWeightLb'] > 25)) |\n",
    "     ((tbl_BP['ServiceCode'].isin(['MIPH', 'USPSAP'])) & (tbl_BP['CustomerBilledWeightLb'] > 70)) |\n",
    "     ((tbl_BP['ServiceCode'].isin(['SRPT<1', 'DHLG<1', 'DHLE<1', 'MIPLE', 'USPSFC'])) & (tbl_BP['CustomerBilledWeightOz'] > 16)) |\n",
    "     ((tbl_BP['ServiceCode'] == 'DHLEM') & (tbl_BP['CustomerBilledWeightOz'] > 400))),\n",
    "    'RateFlag'\n",
    "] = 0\n",
    "tbl_BP.loc[(tbl_BP.Zone == '35'), 'RateFlag'] = 0\n",
    "\n",
    "\n",
    "\n",
    "lessThan1_services = ['SRPT<1', 'DHLG<1', 'DHLE<1', 'MIPLE', 'USPSFC']  # Add your valid services here\n",
    "\n",
    "# Update 'RateFlag' to 0 for rows where 'Service' is in the LessThan1_services\n",
    "tbl_BP.loc[(tbl_BP['ServiceCode'].isin(lessThan1_services)) & (tbl_BP['ActualWeight'] >= 1), 'RateFlag'] = 0\n",
    "\n",
    "\n",
    "columns_are_null = ['CustomerTotal']\n",
    "\n",
    "# Create a condition that checks if any of the specified columns are null\n",
    "condition = tbl_BP[columns_are_null].isnull().any(axis=1)\n",
    "\n",
    "# Set 'RateFlag' to 0 where the condition is True\n",
    "tbl_BP.loc[condition, 'RateFlag'] = 0\n",
    "\n",
    "\n",
    "# As long as there is \"Total Rate\", all other corresponding null rate columns will be filled as 0 for easy calculation. \n",
    "# As long as there is no total rate, all other corresponding rate columns will be null. \n",
    "columns_to_null = ['CustomerFreight', 'CustomerRes', 'CustomerDas', 'CustomerAhs', 'CustomerExtra', 'CustomerFsc', 'CustomerTotal', \n",
    "                   'PublishedFreight', 'PublishedRes', 'PublishedDas', 'PublishedAhs', 'PublishedFsc', 'PublishedTotal',\n",
    "                   'BarrettFreight', 'BarrettRes', 'BarrettDas', 'BarrettAhs', 'BarrettExtra', 'BarrettFsc', 'BarrettTotal', \n",
    "                   'CustomerSaving', 'CustomerSavingPct', 'BarrettMargin']\n",
    "\n",
    "# Set the specified columns to null where the trigger column has the specific value\n",
    "tbl_BP.loc[tbl_BP['RateFlag'] == 0, columns_to_null] = np.nan\n",
    "\n",
    "\n",
    "# The output columns must be below order. \n",
    "tbl_BP = tbl_BP.reindex(columns=['CustomerID', 'CustomerName', 'Facility', 'BarrettOrderNumber', 'Reference', 'PoNumber', \n",
    "       'TrackingNumber', 'ShipDate', 'Service', 'Shipper', 'JoChannel', 'ShipToName', 'ShipToContact', 'ShipToCity', 'ShipToState',\n",
    "       'ZipCode', 'ShipToCountry', 'Zone', 'Quantity', 'Dimensions', 'ActualWeight', 'ResidentialFlag', 'FromZip', 'RateFlag', \n",
    "       'L', 'W', 'H', 'GirthAndL', 'CubicInch', 'ServiceCode', 'DASCategory', 'FscRate', 'CustomerBilledWeightLb', \n",
    "       'CustomerBilledWeightOz', 'CustomerFreight', 'CustomerRes', 'CustomerDas', 'CustomerAhs', 'CustomerExtra', \n",
    "       'CustomerFsc', 'CustomerTotal', 'PublishedBilledWeightLb', 'PublishedBilledWeightOz', 'PublishedFreight', \n",
    "       'PublishedRes', 'PublishedDas', 'PublishedAhs', 'PublishedFsc', 'PublishedTotal', 'BarrettFreight', 'BarrettRes', \n",
    "       'BarrettDas', 'BarrettAhs', 'BarrettExtra', 'BarrettFsc', 'BarrettTotal', 'CustomerSaving', 'CustomerSavingPct', \n",
    "       'BarrettMargin'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "90968d55",
   "metadata": {},
   "outputs": [],
   "source": [
    "tbl_BP.to_csv('RateCurrent_Automation_2023.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "2fbacdb8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(44621, 59)"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tbl_BP.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7dc4f32",
   "metadata": {},
   "source": [
    "# The End"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ee84a26",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
